{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 라이브러리 설치 및 불러오기"
      ],
      "metadata": {
        "id": "mElLrh8EZgj-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZId2SR2ZaVI"
      },
      "outputs": [],
      "source": [
        "# Selenium, BeautifulSoup4, Requests 설치\n",
        "!pip install selenium # 동적 페이지 크롤링 (무한스크롤)\n",
        "!pip install beautifulsoup4 # HTML 파싱\n",
        "!pip install requests\n",
        "\n",
        "# apt 업데이트\n",
        "!apt-get update\n",
        "\n",
        "# chromedriver 설치\n",
        "!apt install chromium-chromedriver\n",
        "\n",
        "# 해당 경로에 설치한 chromedriver 복사\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 크롤링"
      ],
      "metadata": {
        "id": "Fle4v4sTa6A6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import time\n",
        "import csv\n",
        "import requests\n",
        "from selenium.webdriver.common.by import By\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "\n",
        "def crawl_infinite_scroll_website_and_save_to_csv(urls, names, csv_filename):\n",
        "    # 웹 드라이버 초기화\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument('--headless')\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "    names_len = len(names)\n",
        "\n",
        "    # 이거 나중에 다하고 주석 풀어야 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
        "    # CSV 파일을 쓰기 모드로 열기\n",
        "    with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        csv_writer = csv.writer(csvfile)\n",
        "        csv_writer.writerow([\"category\", \"question\", \"answer\"]) # 제목행\n",
        "\n",
        "        # 이거 나중에 다하고 주석 풀어야 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
        "        for i in range(names_len):\n",
        "        # for i in range(1):\n",
        "          site_name = names[i]\n",
        "          url = urls[site_name]\n",
        "          print('==========', site_name, ' : ', url, '사이트 크롤링 진행중', '==========')\n",
        "\n",
        "          # FAQ 웹사이트에 접속\n",
        "          driver = webdriver.Chrome(options=chrome_options)\n",
        "          driver.get(url)\n",
        "\n",
        "          # 해당 사이트의 Question들 추출\n",
        "          items = driver.find_elements(By.CSS_SELECTOR, 'div.eb-groupable-sections .eb-groupable-faq-section-wrapper div.section-expander div.expandable-section')  # 원하는 요소의 클래스명\n",
        "          print(len(items))\n",
        "\n",
        "          # 데이터를 CSV 파일에 작성\n",
        "          # 모든 Question에 대해서 Answer 추출 (Questions 순회)\n",
        "          for item in items:\n",
        "              category=''; question=''; answer='';\n",
        "\n",
        "              # BeautifulSoup을 사용하여 HTML 파싱\n",
        "              soup = BeautifulSoup(item.get_attribute('innerHTML'), 'html.parser')\n",
        "\n",
        "              # Question 추출\n",
        "              soup.find('div', _class=\"expandable-section-header\")\n",
        "              question = soup.select_one('.eb-header-title').get_text().strip()\n",
        "              print(question)\n",
        "\n",
        "              # Answer 추출\n",
        "              soup.find('div', _class=\"expandable-section-content\")\n",
        "              answer = soup.select_one('.eb-rich-text-content').get_text().strip()\n",
        "              print(answer)\n",
        "\n",
        "              print(f'\"category : {site_name}, question : {question}, answer : {answer}\"')\n",
        "              csv_writer.writerow([site_name, question, answer])\n",
        "\n",
        "          # 브라우저 닫기\n",
        "          driver.quit()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 크롤링할 웹사이트 이름에 따른 URL 딕셔너리\n",
        "    website_urls = {\n",
        "        'Batch': 'https://aws.amazon.com/ko/batch/faqs/',\n",
        "        'EBS' : 'https://aws.amazon.com/ko/ebs/faqs/',\n",
        "        'EFS' : 'https://aws.amazon.com/ko/efs/faq/',\n",
        "        'FSx for Lustre' : 'https://aws.amazon.com/ko/fsx/lustre/faqs/',\n",
        "        'FSx for Windows File Server' : 'https://aws.amazon.com/ko/fsx/windows/faqs/',\n",
        "        'Backup' : 'https://aws.amazon.com/ko/backup/faqs/',\n",
        "        'Transfer for SFTP' : 'https://aws.amazon.com/ko/aws-transfer-family/faqs/',\n",
        "        'VPC' : 'https://aws.amazon.com/ko/vpc/faqs/',\n",
        "        'RDS' : 'https://aws.amazon.com/ko/rds/faqs/',\n",
        "        'Redshift' : 'https://aws.amazon.com/ko/redshift/faqs/',\n",
        "        'DocumentDB' : 'https://aws.amazon.com/ko/documentdb/faqs/',\n",
        "        'Config' : 'https://aws.amazon.com/ko/config/faqs/',\n",
        "        'Guardduty' : 'https://aws.amazon.com/ko/guardduty/faqs/',\n",
        "        'Athena' : 'https://aws.amazon.com/ko/athena/faqs/',\n",
        "        'QuickSight' : 'https://aws.amazon.com/ko/quicksight/resources/faqs/',\n",
        "        'SageMaker' : 'https://aws.amazon.com/ko/sagemaker/faqs/',\n",
        "        'CloudFormation' : 'https://aws.amazon.com/ko/cloudformation/faqs/',\n",
        "        'CloudWatch' : 'https://aws.amazon.com/ko/cloudwatch/faqs/'\n",
        "    }\n",
        "\n",
        "    # 크롤링할 웹사이트 이름 배열\n",
        "    website_name = ['Batch', 'EBS', 'EFS', 'FSx for Lustre', 'FSx for Windows File Server', 'Backup',\n",
        "                    'Transfer for SFTP', 'VPC', 'RDS', 'Redshift', 'DocumentDB', 'Config', 'Guardduty',\n",
        "                    'Athena', 'QuickSight', 'SageMaker', 'CloudFormation', 'CloudWatch']\n",
        "\n",
        "    # 저장할 CSV 파일명 설정\n",
        "    csv_filename = \"crawling.csv\"\n",
        "\n",
        "    # 함수 호출하여 크롤링 및 CSV 저장 수행\n",
        "    crawl_infinite_scroll_website_and_save_to_csv(website_urls, website_name, csv_filename)\n"
      ],
      "metadata": {
        "id": "_0OH6Iuea05f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "082e7fd9-b1df-485a-af91-8fe706604dc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "VPC 종단점이 프로비저닝되어 있는 동안 시간당 요금이 청구됩니다. 또한 데이터 처리 요금은 트래픽 소스나 대상과 관계없이 VPC 엔드포인트를 통해 처리된 각 기가바이트에 적용됩니다. 자세히 알아보려면 AWS PrivateLink 요금을 참조하세요.\n",
            "\"category : Backup, question : VPC 엔드포인트를 AWS Backup 게이트웨이와 함께 사용할 때의 비용은 얼마인가요?, answer : VPC 종단점이 프로비저닝되어 있는 동안 시간당 요금이 청구됩니다. 또한 데이터 처리 요금은 트래픽 소스나 대상과 관계없이 VPC 엔드포인트를 통해 처리된 각 기가바이트에 적용됩니다. 자세히 알아보려면 AWS PrivateLink 요금을 참조하세요.\"\n",
            "========== Transfer for SFTP  :  https://aws.amazon.com/ko/aws-transfer-family/faqs/ 사이트 크롤링 진행중 ==========\n",
            "180\n",
            "AWS Transfer Family란 무엇인가요?\n",
            "AWS Transfer Family는 SFTP, AS2, FTPS 및 FTP를 통해 Amazon S3 또는 Amazon EFS의 파일을 송수신할 수 있는 완전관리형 지원을 제공합니다. 인증, 액세스 및 방화벽에 대해 기존 클라이언트 측 구성을 유지하여 파일 전송 워크플로를 원활하게 마이그레이션, 자동화 및 모니터링할 수 있으므로 고객, 파트너 및 내부 팀 또는 애플리케이션에 대한 변경 사항은 없습니다.\n",
            "\"category : Transfer for SFTP, question : AWS Transfer Family란 무엇인가요?, answer : AWS Transfer Family는 SFTP, AS2, FTPS 및 FTP를 통해 Amazon S3 또는 Amazon EFS의 파일을 송수신할 수 있는 완전관리형 지원을 제공합니다. 인증, 액세스 및 방화벽에 대해 기존 클라이언트 측 구성을 유지하여 파일 전송 워크플로를 원활하게 마이그레이션, 자동화 및 모니터링할 수 있으므로 고객, 파트너 및 내부 팀 또는 애플리케이션에 대한 변경 사항은 없습니다.\"\n",
            "SFTP란 무엇인가요?\n",
            "SFTP는 인터넷을 통해 데이터를 안전하게 전송하는 데 사용되는 네트워크 프로토콜인 Secure Shell(SSH) File Transfer Protocol의 약자입니다. 이 프로토콜은 SSH의 모든 보안 및 인증 기능을 지원하며, 금융 서비스, 의료 서비스, 미디어 및 엔터테인먼트, 소매 및 광고 등 다양한 산업에서 비즈니스 파트너 간 데이터 교환에 널리 사용됩니다.\n",
            "\"category : Transfer for SFTP, question : SFTP란 무엇인가요?, answer : SFTP는 인터넷을 통해 데이터를 안전하게 전송하는 데 사용되는 네트워크 프로토콜인 Secure Shell(SSH) File Transfer Protocol의 약자입니다. 이 프로토콜은 SSH의 모든 보안 및 인증 기능을 지원하며, 금융 서비스, 의료 서비스, 미디어 및 엔터테인먼트, 소매 및 광고 등 다양한 산업에서 비즈니스 파트너 간 데이터 교환에 널리 사용됩니다.\"\n",
            "FTP란 무엇인가요?\n",
            "FTP는 File Transfer Protocol의 약어이며, 데이터 전송에 사용되는 네트워크 프로토콜입니다. FTP는 제어 및 데이터 전송을 위해 별도의 채널을 사용합니다. 제어 채널은 종료되거나 비활성 시간이 초과될 때까지 개방된 상태이며, 데이터 채널은 전송 기간 동안 활성화되어 있습니다. FTP는 일반 텍스트를 사용하며 트래픽 암호화를 지원하지 않습니다.\n",
            "\"category : Transfer for SFTP, question : FTP란 무엇인가요?, answer : FTP는 File Transfer Protocol의 약어이며, 데이터 전송에 사용되는 네트워크 프로토콜입니다. FTP는 제어 및 데이터 전송을 위해 별도의 채널을 사용합니다. 제어 채널은 종료되거나 비활성 시간이 초과될 때까지 개방된 상태이며, 데이터 채널은 전송 기간 동안 활성화되어 있습니다. FTP는 일반 텍스트를 사용하며 트래픽 암호화를 지원하지 않습니다.\"\n",
            "FTPS란 무엇인가요?\n",
            "FTPS는 File Transfer Protocol over SSL의 약어이며, FTP의 확장입니다. FTP와 마찬가지로 FTPS는 제어 및 데이터 전송에 별도의 채널을 사용합니다. 제어 채널은 종료되거나 비활성 시간이 초과될 때까지 열려 있지만, 데이터 채널은 전송 기간 동안 활성화되어 있습니다. FTPS는 전송 계층 보안(TLS)을 사용하여 트래픽을 암호화하고, 제어 및 데이터 채널 연결을 동시에 또는 독립적으로 암호화할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : FTPS란 무엇인가요?, answer : FTPS는 File Transfer Protocol over SSL의 약어이며, FTP의 확장입니다. FTP와 마찬가지로 FTPS는 제어 및 데이터 전송에 별도의 채널을 사용합니다. 제어 채널은 종료되거나 비활성 시간이 초과될 때까지 열려 있지만, 데이터 채널은 전송 기간 동안 활성화되어 있습니다. FTPS는 전송 계층 보안(TLS)을 사용하여 트래픽을 암호화하고, 제어 및 데이터 채널 연결을 동시에 또는 독립적으로 암호화할 수 있습니다.\"\n",
            "AS2란 무엇인가요?\n",
            "Applicability Statement 2를 나타내는 AS2는 퍼블릭 인터넷에서 HTTP/HTTPS(또는 모든 TCP/IP 네트워크)를 통해 B2B 데이터를 안전하고 안정적으로 전송하기 위해 사용되는 네트워크 프로토콜입니다.\n",
            "\"category : Transfer for SFTP, question : AS2란 무엇인가요?, answer : Applicability Statement 2를 나타내는 AS2는 퍼블릭 인터넷에서 HTTP/HTTPS(또는 모든 TCP/IP 네트워크)를 통해 B2B 데이터를 안전하고 안정적으로 전송하기 위해 사용되는 네트워크 프로토콜입니다.\"\n",
            "SFTP 커넥터란 무엇인가요?\n",
            "AWS Transfer Family의 SFTP 커넥터는 외부 호스팅 SFTP 서버와 AWS 스토리지 서비스 간에 대규모로 쉽고 안정적으로 파일을 복사하는 데 사용됩니다.\n",
            "\"category : Transfer for SFTP, question : SFTP 커넥터란 무엇인가요?, answer : AWS Transfer Family의 SFTP 커넥터는 외부 호스팅 SFTP 서버와 AWS 스토리지 서비스 간에 대규모로 쉽고 안정적으로 파일을 복사하는 데 사용됩니다.\"\n",
            "AWS Transfer Family를 사용해야 하는 이유는 무엇인가요?\n",
            "AWS Transfer Family는 B2B 파일 전송을 위한 여러 프로토콜을 지원하므로, 여러 이해 관계자, 서드 파티 공급업체, 비즈니스 파트너 또는 고객과 데이터를 쉽고 안전하게 교환할 수 있습니다. Transfer Family가 없으면 자체 파일 전송 서비스를 호스팅 및 관리해야 하고, 이 경우 인프라 운영 및 관리, 서버 패치 적용, 가동 시간 및 가용성 모니터링, 사용자 프로비저닝과 관련 활동 감사를 위한 일회성 메커니즘 구축에 투자해야 합니다. AWS Transfer Family는 B2B 파일 전송을 위해 SFTP, AS2, FTPS 및 FTP를 통한 완전관리형 보안 연결 옵션을 제공합니다. 파일 전송 관련 인프라를 관리할 필요가 없기 때문에 이러한 문제가 해결됩니다. 최종 사용자의 워크플로는 변경 없이 그대로 유지되면서, 선택한 프로토콜을 통해 업로드 및 다운로드된 데이터는 Amazon S3 버킷 또는 Amazon EFS 파일 시스템에 저장됩니다. 데이터가 AWS에 저장되므로 이제 고객의 규정 준수 요구 사항에 최적화된 환경에서 데이터 처리, 콘텐츠 관리, 분석, 기계 학습, 아카이브 등을 위해 다양한 AWS 서비스에서 데이터를 손쉽게 사용할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : AWS Transfer Family를 사용해야 하는 이유는 무엇인가요?, answer : AWS Transfer Family는 B2B 파일 전송을 위한 여러 프로토콜을 지원하므로, 여러 이해 관계자, 서드 파티 공급업체, 비즈니스 파트너 또는 고객과 데이터를 쉽고 안전하게 교환할 수 있습니다. Transfer Family가 없으면 자체 파일 전송 서비스를 호스팅 및 관리해야 하고, 이 경우 인프라 운영 및 관리, 서버 패치 적용, 가동 시간 및 가용성 모니터링, 사용자 프로비저닝과 관련 활동 감사를 위한 일회성 메커니즘 구축에 투자해야 합니다. AWS Transfer Family는 B2B 파일 전송을 위해 SFTP, AS2, FTPS 및 FTP를 통한 완전관리형 보안 연결 옵션을 제공합니다. 파일 전송 관련 인프라를 관리할 필요가 없기 때문에 이러한 문제가 해결됩니다. 최종 사용자의 워크플로는 변경 없이 그대로 유지되면서, 선택한 프로토콜을 통해 업로드 및 다운로드된 데이터는 Amazon S3 버킷 또는 Amazon EFS 파일 시스템에 저장됩니다. 데이터가 AWS에 저장되므로 이제 고객의 규정 준수 요구 사항에 최적화된 환경에서 데이터 처리, 콘텐츠 관리, 분석, 기계 학습, 아카이브 등을 위해 다양한 AWS 서비스에서 데이터를 손쉽게 사용할 수 있습니다.\"\n",
            "AWS Transfer Family를 사용하여 AWS에서 이벤트 기반 관리형 파일 전송(MFT) 워크플로를 구축할 수 있나요?\n",
            "예. AWS Transfer Family는 각 파일 전송 작업에 대한 이벤트 알림을 Amazon EventBridge에 게시합니다. Amazon EventBridge에서 AWS Transfer Family 이벤트를 구독하고 Amazon EventBridge 또는 이러한 이벤트와 통합되는 기타 오케스트레이션 엔진에서 이를 사용하여 이벤트 기반 MFT 워크플로를 오케스트레이션할 수 있습니다. 자세한 내용은 파일 처리 자동화 섹션을 참조하세요.\n",
            "\"category : Transfer for SFTP, question : AWS Transfer Family를 사용하여 AWS에서 이벤트 기반 관리형 파일 전송(MFT) 워크플로를 구축할 수 있나요?, answer : 예. AWS Transfer Family는 각 파일 전송 작업에 대한 이벤트 알림을 Amazon EventBridge에 게시합니다. Amazon EventBridge에서 AWS Transfer Family 이벤트를 구독하고 Amazon EventBridge 또는 이러한 이벤트와 통합되는 기타 오케스트레이션 엔진에서 이를 사용하여 이벤트 기반 MFT 워크플로를 오케스트레이션할 수 있습니다. 자세한 내용은 파일 처리 자동화 섹션을 참조하세요.\"\n",
            "AWS Transfer Family를 사용하면 어떤 이점이 있나요?\n",
            "AWS Transfer Family는 가용성이 뛰어나고 Auto Scaling 기능을 통해 완전관리형 파일 전송 서비스를 제공하므로 사용자가 파일 전송 관련 인프라를 관리할 필요가 없습니다. 최종 사용자의 워크플로는 변경 없이 그대로 유지되면서, 선택한 프로토콜을 통해 업로드 및 다운로드된 데이터는 Amazon S3 버킷 또는 Amazon EFS 파일 시스템에 저장됩니다. 데이터가 AWS에 저장되므로 이제 고객의 규정 준수 요구 사항에 최적화된 환경에서 데이터 처리, 콘텐츠 관리, 분석, 기계 학습, 아카이브 등을 위해 다양한 AWS 서비스에서 데이터를 손쉽게 사용할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : AWS Transfer Family를 사용하면 어떤 이점이 있나요?, answer : AWS Transfer Family는 가용성이 뛰어나고 Auto Scaling 기능을 통해 완전관리형 파일 전송 서비스를 제공하므로 사용자가 파일 전송 관련 인프라를 관리할 필요가 없습니다. 최종 사용자의 워크플로는 변경 없이 그대로 유지되면서, 선택한 프로토콜을 통해 업로드 및 다운로드된 데이터는 Amazon S3 버킷 또는 Amazon EFS 파일 시스템에 저장됩니다. 데이터가 AWS에 저장되므로 이제 고객의 규정 준수 요구 사항에 최적화된 환경에서 데이터 처리, 콘텐츠 관리, 분석, 기계 학습, 아카이브 등을 위해 다양한 AWS 서비스에서 데이터를 손쉽게 사용할 수 있습니다.\"\n",
            "SFTP, FTPS 및 FTP 서버 엔드포인트에 대해 AWS Transfer를 시작하려면 어떻게 해야 하나요?\n",
            "간단한 3단계를 수행하여 SFTP, FTPS, FTP에 대해 상시 가동 서버 엔드포인트를 활성화합니다. 먼저 최종 사용자가 엔드포인트에 연결할 수 있도록 하려는 프로토콜을 선택합니다. 그런 다음, AWS Transfer Family의 내장된 인증 관리자(서비스 관리형) 또는 Microsoft Active Directory(AD)를 사용하거나 Okta 또는 Microsoft AzureAD(‘BYO’ 인증)와 같은 자체 또는 서드 파티 자격 증명 공급자와 통합하여 사용자 액세스를 구성합니다. 마지막으로 S3 버킷 또는 EFS 파일 시스템 액세스를 위한 서버를 선택합니다. 프로토콜, ID 제공업체 및 파일 시스템 액세스가 활성화되면, 액세스한 데이터가 선택한 파일 시스템에 저장되어 있는 동안 사용자는 기존 SFTP, FTPS 또는 FTP 클라이언트 및 구성을 계속 사용할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : SFTP, FTPS 및 FTP 서버 엔드포인트에 대해 AWS Transfer를 시작하려면 어떻게 해야 하나요?, answer : 간단한 3단계를 수행하여 SFTP, FTPS, FTP에 대해 상시 가동 서버 엔드포인트를 활성화합니다. 먼저 최종 사용자가 엔드포인트에 연결할 수 있도록 하려는 프로토콜을 선택합니다. 그런 다음, AWS Transfer Family의 내장된 인증 관리자(서비스 관리형) 또는 Microsoft Active Directory(AD)를 사용하거나 Okta 또는 Microsoft AzureAD(‘BYO’ 인증)와 같은 자체 또는 서드 파티 자격 증명 공급자와 통합하여 사용자 액세스를 구성합니다. 마지막으로 S3 버킷 또는 EFS 파일 시스템 액세스를 위한 서버를 선택합니다. 프로토콜, ID 제공업체 및 파일 시스템 액세스가 활성화되면, 액세스한 데이터가 선택한 파일 시스템에 저장되어 있는 동안 사용자는 기존 SFTP, FTPS 또는 FTP 클라이언트 및 구성을 계속 사용할 수 있습니다.\"\n",
            "AS2에서 AWS Transfer를 시작하려면 어떻게 해야 하나요?\n",
            "간단한 3단계로 AS2를 사용하여 거래 파트너와 메시지를 교환할 수 있습니다. 먼저, 자신의 인증서 및 프라이빗 키와 거래 파트너의 인증서 및 인증서 체인을 가져옵니다. 그런 다음 자신과 파트너의 AS2 ID를 사용하여 프로필을 생성합니다. 마지막으로 데이터 수신 계약과 데이터 전송 커넥터를 사용하여 자신과 파트너의 프로필 정보를 페어링합니다. 그러면 거래 파트너의 AS2 서버로 메시지 교환을 시작할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : AS2에서 AWS Transfer를 시작하려면 어떻게 해야 하나요?, answer : 간단한 3단계로 AS2를 사용하여 거래 파트너와 메시지를 교환할 수 있습니다. 먼저, 자신의 인증서 및 프라이빗 키와 거래 파트너의 인증서 및 인증서 체인을 가져옵니다. 그런 다음 자신과 파트너의 AS2 ID를 사용하여 프로필을 생성합니다. 마지막으로 데이터 수신 계약과 데이터 전송 커넥터를 사용하여 자신과 파트너의 프로필 정보를 페어링합니다. 그러면 거래 파트너의 AS2 서버로 메시지 교환을 시작할 수 있습니다.\"\n",
            "AWS Transfer SFTP 커넥터를 시작하려면 어떻게 해야 하나요?\n",
            "간단한 세 가지 단계로 SFTP 커넥터를 사용하여 원격 SFTP 서버와 Amazon S3 간에 파일을 복사할 수 있습니다. 먼저, SFTP 커넥터에서 원격 서버 인증에 사용할 보안 인증 정보를 저장할 암호를 생성합니다. 둘째, 원격 서버의 URL과 비밀을 제공하여 SFTP 커넥터를 생성합니다. 셋째, 커넥터가 생성되면 StartFileTransfer API를 간접적으로 호출하여 커넥터를 통해 원격 서버와 Amazon S3 버킷 간에 파일을 복사할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : AWS Transfer SFTP 커넥터를 시작하려면 어떻게 해야 하나요?, answer : 간단한 세 가지 단계로 SFTP 커넥터를 사용하여 원격 SFTP 서버와 Amazon S3 간에 파일을 복사할 수 있습니다. 먼저, SFTP 커넥터에서 원격 서버 인증에 사용할 보안 인증 정보를 저장할 암호를 생성합니다. 둘째, 원격 서버의 URL과 비밀을 제공하여 SFTP 커넥터를 생성합니다. 셋째, 커넥터가 생성되면 StartFileTransfer API를 간접적으로 호출하여 커넥터를 통해 원격 서버와 Amazon S3 버킷 간에 파일을 복사할 수 있습니다.\"\n",
            "SFTP와 FTPS의 차이점은 무엇인가요? 언제 어떤 서비스를 사용해야 합니까?\n",
            "FTPS와 SFTP는 모두 안전한 전송에 사용될 수 있습니다. 서로 다른 프로토콜이기 때문에 명령 및 데이터 전송을 위한 보안 터널을 제공하기 위해 다른 클라이언트와 기술을 사용합니다. SFTP가 최신 프로토콜이며 명령 및 데이터에 단일 채널을 사용하므로, 열어야 하는 포트 수가 FTPS보다 적습니다.\n",
            "\"category : Transfer for SFTP, question : SFTP와 FTPS의 차이점은 무엇인가요? 언제 어떤 서비스를 사용해야 합니까?, answer : FTPS와 SFTP는 모두 안전한 전송에 사용될 수 있습니다. 서로 다른 프로토콜이기 때문에 명령 및 데이터 전송을 위한 보안 터널을 제공하기 위해 다른 클라이언트와 기술을 사용합니다. SFTP가 최신 프로토콜이며 명령 및 데이터에 단일 채널을 사용하므로, 열어야 하는 포트 수가 FTPS보다 적습니다.\"\n",
            "SFTP, FTPS, AS2 프로토콜의 차이점은 무엇인가요? AS2 프로토콜을 사용해야 하는 경우는 언제인가요?\n",
            "SFTP, FTPS, AS2 모두 보안 전송에 사용될 수 있습니다. 이들은 서로 다른 프로토콜이기 때문에 서로 다른 클라이언트와 기술을 사용하여 데이터의 보안 전송 기능을 제공합니다. 암호화된 메시지와 서명된 메시지를 지원하는 것 외에도 AS2에는 수신자가 메시지를 성공적으로 수신하고 복호화했음을 발신자에게 알려주는 메시지 처리 통지(MDN) 메커니즘이 기본적으로 포함되어 있습니다. 이 통지는 메시지가 전송 중에 임의 변경되지 않고 배달되었다는 증거를 발신자에게 제공합니다. AS2는 소매, 전자 상거래, 결제, 공급망 운영 워크플로에서 메시지의 안전한 전송 및 배달을 위해 AS2를 메시지 트랜잭션에 사용할 수 있는 비즈니스 파트너와의 상호 작용에 널리 사용됩니다. AS2는 발신자와 수신자의 ID, 메시지의 무결성과 메시지가 수신자에게 성공적으로 배달되어 복호화되었는지 여부를 확인할 수 있는 옵션을 제공합니다.\n",
            "\"category : Transfer for SFTP, question : SFTP, FTPS, AS2 프로토콜의 차이점은 무엇인가요? AS2 프로토콜을 사용해야 하는 경우는 언제인가요?, answer : SFTP, FTPS, AS2 모두 보안 전송에 사용될 수 있습니다. 이들은 서로 다른 프로토콜이기 때문에 서로 다른 클라이언트와 기술을 사용하여 데이터의 보안 전송 기능을 제공합니다. 암호화된 메시지와 서명된 메시지를 지원하는 것 외에도 AS2에는 수신자가 메시지를 성공적으로 수신하고 복호화했음을 발신자에게 알려주는 메시지 처리 통지(MDN) 메커니즘이 기본적으로 포함되어 있습니다. 이 통지는 메시지가 전송 중에 임의 변경되지 않고 배달되었다는 증거를 발신자에게 제공합니다. AS2는 소매, 전자 상거래, 결제, 공급망 운영 워크플로에서 메시지의 안전한 전송 및 배달을 위해 AS2를 메시지 트랜잭션에 사용할 수 있는 비즈니스 파트너와의 상호 작용에 널리 사용됩니다. AS2는 발신자와 수신자의 ID, 메시지의 무결성과 메시지가 수신자에게 성공적으로 배달되어 복호화되었는지 여부를 확인할 수 있는 옵션을 제공합니다.\"\n",
            "사용자는 기존 파일 전송 클라이언트 및 애플리케이션을 계속 사용할 수 있나요?\n",
            "예. 선택한 프로토콜에 대해 엔드포인트가 활성화되어 있는 한 기존 파일 전송 클라이언트 애플리케이션은 계속 작동합니다. 흔히 사용되는 SFTP/FTPS/FTP 클라이언트의 예로는 WinSCP, FileZilla, CyberDuck, lftp 및 OpenSSH 클라이언트가 있습니다.\n",
            "\"category : Transfer for SFTP, question : 사용자는 기존 파일 전송 클라이언트 및 애플리케이션을 계속 사용할 수 있나요?, answer : 예. 선택한 프로토콜에 대해 엔드포인트가 활성화되어 있는 한 기존 파일 전송 클라이언트 애플리케이션은 계속 작동합니다. 흔히 사용되는 SFTP/FTPS/FTP 클라이언트의 예로는 WinSCP, FileZilla, CyberDuck, lftp 및 OpenSSH 클라이언트가 있습니다.\"\n",
            "사용자가 웹 포털을 사용하여 파일을 업로드하고 다운로드할 수 있나요?\n",
            "예. Web Client for AWS Transfer Family를 사용하면 웹 포털을 통해 파일을 업로드하고 다운로드하는 기능을 사용자에게 제공할 수 있습니다. 비기술 사용자를 위해 설계된 직관적인 웹 브라우저 인터페이스를 추가하여 AWS Transfer for SFTP와 동일한 인증 및 액세스 제어의 이점을 고객에게 제공할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 사용자가 웹 포털을 사용하여 파일을 업로드하고 다운로드할 수 있나요?, answer : 예. Web Client for AWS Transfer Family를 사용하면 웹 포털을 통해 파일을 업로드하고 다운로드하는 기능을 사용자에게 제공할 수 있습니다. 비기술 사용자를 위해 설계된 직관적인 웹 브라우저 인터페이스를 추가하여 AWS Transfer for SFTP와 동일한 인증 및 액세스 제어의 이점을 고객에게 제공할 수 있습니다.\"\n",
            "외부 SFTP 사이트에 저장된 파일에 액세스하려면 어떻게 해야 하나요?\n",
            "AWS Transfer SFTP 커넥터를 사용하여 외부 SFTP 사이트에 저장된 파일에 액세스할 수 있습니다. SFTP 커넥터를 시작하려면 SFTP 커넥터 설명서를 참조합니다.\n",
            "\"category : Transfer for SFTP, question : 외부 SFTP 사이트에 저장된 파일에 액세스하려면 어떻게 해야 하나요?, answer : AWS Transfer SFTP 커넥터를 사용하여 외부 SFTP 사이트에 저장된 파일에 액세스할 수 있습니다. SFTP 커넥터를 시작하려면 SFTP 커넥터 설명서를 참조합니다.\"\n",
            "거래 파트너의 비즈니스 시스템에서 내 S3 버킷으로 파일을 옮기려면 어떻게 해야 하나요?\n",
            "AWS Transfer Family의 완전관리형 SFTP, FTPS, AS2 기능을 사용하여 거래 파트너의 비즈니스 시스템에서 생성된 EDI 문서를 받을 수 있습니다. AWS Transfer Family의 연결 기능을 사용하여 수신한 EDI 문서는 Amazon S3에 자동으로 업로드되며, AWS B2B Data Interchange를 사용하여 JSON 및 XML 형식의 출력으로 변환할 수 있습니다. 또는 다른 모든 EDI 연결 도구를 사용하여 EDI 문서를 S3에 업로드할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 거래 파트너의 비즈니스 시스템에서 내 S3 버킷으로 파일을 옮기려면 어떻게 해야 하나요?, answer : AWS Transfer Family의 완전관리형 SFTP, FTPS, AS2 기능을 사용하여 거래 파트너의 비즈니스 시스템에서 생성된 EDI 문서를 받을 수 있습니다. AWS Transfer Family의 연결 기능을 사용하여 수신한 EDI 문서는 Amazon S3에 자동으로 업로드되며, AWS B2B Data Interchange를 사용하여 JSON 및 XML 형식의 출력으로 변환할 수 있습니다. 또는 다른 모든 EDI 연결 도구를 사용하여 EDI 문서를 S3에 업로드할 수 있습니다.\"\n",
            "이 서비스를 사용할 때 사용자가 SCP 또는 HTTPS를 통해 파일을 전송할 수 있나요?\n",
            "아니요. 사용자는 SFTP, AS2, FTPS 또는 FTP를 사용하여 파일을 전송해야 합니다. 대부분의 파일 전송 클라이언트는 이러한 프로토콜 중 하나를 인증 중에 선택해야 하는 옵션으로 제공합니다. 특정 프로토콜의 지원을 원하는 경우 AWS Support 팀이나 AWS Account 팀을 통해 알려주시기 바랍니다.\n",
            "\"category : Transfer for SFTP, question : 이 서비스를 사용할 때 사용자가 SCP 또는 HTTPS를 통해 파일을 전송할 수 있나요?, answer : 아니요. 사용자는 SFTP, AS2, FTPS 또는 FTP를 사용하여 파일을 전송해야 합니다. 대부분의 파일 전송 클라이언트는 이러한 프로토콜 중 하나를 인증 중에 선택해야 하는 옵션으로 제공합니다. 특정 프로토콜의 지원을 원하는 경우 AWS Support 팀이나 AWS Account 팀을 통해 알려주시기 바랍니다.\"\n",
            "내 Transfer Family 서버에 연결하는 사용자를 위한 로그인 배너를 사용자 지정할 수 있나요?\n",
            "예. 조직 정책 또는 이용 약관과 같은 사용자 지정된 배너를 사용자에게 표시하도록 Transfer Family 서버를 구성할 수 있습니다. 또한 사용자 지정된 오늘의 메시지(MOTD)를 인증된 사용자에게 표시할 수도 있습니다. 자세한 내용은 설명서를 참조하세요.\n",
            "\"category : Transfer for SFTP, question : 내 Transfer Family 서버에 연결하는 사용자를 위한 로그인 배너를 사용자 지정할 수 있나요?, answer : 예. 조직 정책 또는 이용 약관과 같은 사용자 지정된 배너를 사용자에게 표시하도록 Transfer Family 서버를 구성할 수 있습니다. 또한 사용자 지정된 오늘의 메시지(MOTD)를 인증된 사용자에게 표시할 수도 있습니다. 자세한 내용은 설명서를 참조하세요.\"\n",
            "회사 도메인 이름(sftp.mycompanyname.com)을 사용하여 엔드포인트에 액세스할 수 있나요?\n",
            "예. 이 서비스에서는 엔드포인트 액세스를 위한 도메인 이름을 기본적으로 제공합니다. 기존에 도메인 이름이 있는 경우 Amazon Route 53 또는 다른 DNS 서비스를 사용하여, 등록된 도메인에서 AWS의 서버 엔드포인트로 사용자의 트래픽을 라우팅할 수 있습니다. AWS Transfer Family가 사용자 지정 도메인 이름에 Amazon Route 53을 사용하는 방법(인터넷에 연결된 엔드포인트만 해당)에 대한 설명서를 참조하세요.\n",
            "\"category : Transfer for SFTP, question : 회사 도메인 이름(sftp.mycompanyname.com)을 사용하여 엔드포인트에 액세스할 수 있나요?, answer : 예. 이 서비스에서는 엔드포인트 액세스를 위한 도메인 이름을 기본적으로 제공합니다. 기존에 도메인 이름이 있는 경우 Amazon Route 53 또는 다른 DNS 서비스를 사용하여, 등록된 도메인에서 AWS의 서버 엔드포인트로 사용자의 트래픽을 라우팅할 수 있습니다. AWS Transfer Family가 사용자 지정 도메인 이름에 Amazon Route 53을 사용하는 방법(인터넷에 연결된 엔드포인트만 해당)에 대한 설명서를 참조하세요.\"\n",
            "내 VPC에서만 리소스에 액세스할 수 있도록 서버를 설정할 수 있나요?\n",
            "예. 서버를 만들거나 기존 서버를 업데이트할 때 퍼블릭 인터넷에서 액세스하거나 VPC 내부에서 호스트할 수 있도록 엔드포인트를 허용할지를 지정할 수 있습니다. 서버에 대한 VPC 호스팅 엔드포인트를 사용함으로써, 동일한 VPC 내의 클라이언트, 사용자가 지정하는 다른 VPC 또는 AWS Direct Connect, AWS VPN이나 VPC 피어링과 같은 VPC를 확장하는 네트워킹 기술을 사용하는 온프레미스 환경에서만 액세스하도록 제한할 수 있습니다. 서브넷 네트워크 액세스 제어 목록(NACL) 또는 보안 그룹을 사용하면 VPC 내의 특정 서브넷에 있는 리소스에 대한 액세스를 추가적으로 제한할 수 있습니다. 자세한 내용은 AWS PrivateLink를 사용하여 VPC 내부에 서버 엔드포인트를 만드는 방법에 대한 설명서를 참조하세요.\n",
            "\"category : Transfer for SFTP, question : 내 VPC에서만 리소스에 액세스할 수 있도록 서버를 설정할 수 있나요?, answer : 예. 서버를 만들거나 기존 서버를 업데이트할 때 퍼블릭 인터넷에서 액세스하거나 VPC 내부에서 호스트할 수 있도록 엔드포인트를 허용할지를 지정할 수 있습니다. 서버에 대한 VPC 호스팅 엔드포인트를 사용함으로써, 동일한 VPC 내의 클라이언트, 사용자가 지정하는 다른 VPC 또는 AWS Direct Connect, AWS VPN이나 VPC 피어링과 같은 VPC를 확장하는 네트워킹 기술을 사용하는 온프레미스 환경에서만 액세스하도록 제한할 수 있습니다. 서브넷 네트워크 액세스 제어 목록(NACL) 또는 보안 그룹을 사용하면 VPC 내의 특정 서브넷에 있는 리소스에 대한 액세스를 추가적으로 제한할 수 있습니다. 자세한 내용은 AWS PrivateLink를 사용하여 VPC 내부에 서버 엔드포인트를 만드는 방법에 대한 설명서를 참조하세요.\"\n",
            "인터넷 연결 엔드포인트에서 FTP를 사용할 수 있나요?\n",
            "아니요, FTP를 활성화하면 FTP가 데이터를 일반 텍스트로 전송하기 때문에 VPC 호스팅 엔드포인트의 내부 액세스 옵션만 사용할 수 있습니다. 트래픽이 퍼블릭 네트워크를 통과해야 하는 경우에는 SFTP 또는 FTPS와 같은 보안 프로토콜을 사용해야 합니다.\n",
            "\"category : Transfer for SFTP, question : 인터넷 연결 엔드포인트에서 FTP를 사용할 수 있나요?, answer : 아니요, FTP를 활성화하면 FTP가 데이터를 일반 텍스트로 전송하기 때문에 VPC 호스팅 엔드포인트의 내부 액세스 옵션만 사용할 수 있습니다. 트래픽이 퍼블릭 네트워크를 통과해야 하는 경우에는 SFTP 또는 FTPS와 같은 보안 프로토콜을 사용해야 합니다.\"\n",
            "VPC 없이 FTP를 사용할 수 있나요?\n",
            "아니요. FTP 서버 엔드포인트를 호스팅하려면 VPC가 필요합니다. 서버 생성 중에 VPC 리소스 생성을 자동화하여 엔드포인트를 호스팅하려면 CloudFormation 템플릿에 대한 설명서를 참조하십시오.\n",
            "\"category : Transfer for SFTP, question : VPC 없이 FTP를 사용할 수 있나요?, answer : 아니요. FTP 서버 엔드포인트를 호스팅하려면 VPC가 필요합니다. 서버 생성 중에 VPC 리소스 생성을 자동화하여 엔드포인트를 호스팅하려면 CloudFormation 템플릿에 대한 설명서를 참조하십시오.\"\n",
            "최종 사용자가 고정 IP 주소를 사용하여 방화벽 내에 있는 내 서버의 엔드포인트에 대한 액세스를 허용 목록에 포함할 수 있나요?\n",
            "예. 서버에 대한 VPC 호스팅 엔드포인트를 선택하고 인터넷 연결 옵션을 선택하여 서버 엔드포인트에 고정 IP를 활성화할 수 있습니다. 그러면 엔드포인트의 IP 주소로 할당된 엔드포인트에 탄력적 IP(BYO IP 포함)를 직접 연결할 수 있습니다. VPC 내에 서버 엔드포인트 생성 설명서에서 인터넷에 연결된 엔드포인트 생성 섹션을 참조하세요.\n",
            "\"category : Transfer for SFTP, question : 최종 사용자가 고정 IP 주소를 사용하여 방화벽 내에 있는 내 서버의 엔드포인트에 대한 액세스를 허용 목록에 포함할 수 있나요?, answer : 예. 서버에 대한 VPC 호스팅 엔드포인트를 선택하고 인터넷 연결 옵션을 선택하여 서버 엔드포인트에 고정 IP를 활성화할 수 있습니다. 그러면 엔드포인트의 IP 주소로 할당된 엔드포인트에 탄력적 IP(BYO IP 포함)를 직접 연결할 수 있습니다. VPC 내에 서버 엔드포인트 생성 설명서에서 인터넷에 연결된 엔드포인트 생성 섹션을 참조하세요.\"\n",
            "최종 사용자의 소스 IP 주소를 기준으로 수신 트래픽을 제한할 수 있나요?\n",
            "예. 사용자의 소스 IP 주소를 기준으로 수신 트래픽을 제한하는 세 가지 옵션이 있습니다. VPC 내 서버 엔드포인트를 호스팅하는 경우 소스 IP 주소 나열 또는 AWS Network Firewall 서비스 사용을 허용하도록 보안 그룹을 사용하는 방법에 관한 이 블로그 게시물을 참조하세요. 퍼블릭 EndpointType 전송 서버 및 API Gateway를 사용하여 자격 증명 관리 시스템을 통합하는 경우 AWS WAF를 사용하여 최종 사용자의 소스 IP 주소로 액세스를 허용 또는 차단하거나 비율을 제한할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 최종 사용자의 소스 IP 주소를 기준으로 수신 트래픽을 제한할 수 있나요?, answer : 예. 사용자의 소스 IP 주소를 기준으로 수신 트래픽을 제한하는 세 가지 옵션이 있습니다. VPC 내 서버 엔드포인트를 호스팅하는 경우 소스 IP 주소 나열 또는 AWS Network Firewall 서비스 사용을 허용하도록 보안 그룹을 사용하는 방법에 관한 이 블로그 게시물을 참조하세요. 퍼블릭 EndpointType 전송 서버 및 API Gateway를 사용하여 자격 증명 관리 시스템을 통합하는 경우 AWS WAF를 사용하여 최종 사용자의 소스 IP 주소로 액세스를 허용 또는 차단하거나 비율을 제한할 수 있습니다.\"\n",
            "공유 VPC 환경에서 서버의 엔드포인트를 호스팅할 수 있나요?\n",
            "예. 보안, 비용 모니터링 및 확장성을 위해 AWS Landing Zone과 같은 도구를 사용하여 AWS 환경을 분할하는 경우에 일반적으로 사용되는 공유 VPC 환경에서 서버 엔드포인트를 배포할 수 있습니다. AWS Transfer Family를 통해 공유 VPC 환경에서 VPC 호스팅 엔드포인트를 사용하는 방법은 이 블로그 게시물을 참조하세요.\n",
            "\"category : Transfer for SFTP, question : 공유 VPC 환경에서 서버의 엔드포인트를 호스팅할 수 있나요?, answer : 예. 보안, 비용 모니터링 및 확장성을 위해 AWS Landing Zone과 같은 도구를 사용하여 AWS 환경을 분할하는 경우에 일반적으로 사용되는 공유 VPC 환경에서 서버 엔드포인트를 배포할 수 있습니다. AWS Transfer Family를 통해 공유 VPC 환경에서 VPC 호스팅 엔드포인트를 사용하는 방법은 이 블로그 게시물을 참조하세요.\"\n",
            "원격 위치에 있는 최종 사용자를 위한 파일 전송 성능을 개선하려면 어떻게 해야 하나요?\n",
            "전송 서버 엔드포인트에서 AWS Global Accelerator를 사용하여 파일 전송 처리량 및 왕복 시간을 개선할 수 있습니다. 자세한 내용은 이 블로그 게시물을 참조하세요.\n",
            "\"category : Transfer for SFTP, question : 원격 위치에 있는 최종 사용자를 위한 파일 전송 성능을 개선하려면 어떻게 해야 하나요?, answer : 전송 서버 엔드포인트에서 AWS Global Accelerator를 사용하여 파일 전송 처리량 및 왕복 시간을 개선할 수 있습니다. 자세한 내용은 이 블로그 게시물을 참조하세요.\"\n",
            "최종 사용자의 클라이언트가 서버 엔드포인트에 연결된 경우 사용할 수 있는 암호화 알고리즘을 선택할 수 있나요?\n",
            "예. 사용 가능한 서비스 관리형 보안 정책 중에서 보안 및 규정 준수 요구 사항에 적합한 정책을 하나 선택하여 서버 엔드포인트를 통해 알릴 암호화 알고리즘을 제어할 수 있습니다. 최종 사용자의 파일 전송 클라이언트에서 서버에 연결을 시도하면 정책에 지정된 알고리즘만 연결 협상에 사용할 수 있습니다. 사전 정의된 보안 정책에 대한 설명서를 참조하세요.\n",
            "\"category : Transfer for SFTP, question : 최종 사용자의 클라이언트가 서버 엔드포인트에 연결된 경우 사용할 수 있는 암호화 알고리즘을 선택할 수 있나요?, answer : 예. 사용 가능한 서비스 관리형 보안 정책 중에서 보안 및 규정 준수 요구 사항에 적합한 정책을 하나 선택하여 서버 엔드포인트를 통해 알릴 암호화 알고리즘을 제어할 수 있습니다. 최종 사용자의 파일 전송 클라이언트에서 서버에 연결을 시도하면 정책에 지정된 알고리즘만 연결 협상에 사용할 수 있습니다. 사전 정의된 보안 정책에 대한 설명서를 참조하세요.\"\n",
            "AWS Transfer Family는 퍼블릭 키의 양자 보안 교환을 지원하나요?\n",
            "예. AWS Transfer Family는 SFTP 파일 전송을 위한 양자 보안 퍼블릭 키 교환을 지원합니다. 사전 정의된 하이브리드 PQ 보안 정책 중 하나를 SFTP 서버에 연결하여 PQ 암호화 알고리즘을 지원하는 클라이언트와의 양자 보안 키 교환을 지원할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : AWS Transfer Family는 퍼블릭 키의 양자 보안 교환을 지원하나요?, answer : 예. AWS Transfer Family는 SFTP 파일 전송을 위한 양자 보안 퍼블릭 키 교환을 지원합니다. 사전 정의된 하이브리드 PQ 보안 정책 중 하나를 SFTP 서버에 연결하여 PQ 암호화 알고리즘을 지원하는 클라이언트와의 양자 보안 키 교환을 지원할 수 있습니다.\"\n",
            "최종 사용자가 고정 IP 주소를 사용하여 엔드포인트 유형이 퍼블릭인 서버에 액세스할 수 있나요?\n",
            "아니요. 방화벽 화이트리스트에 지정할 목적으로 주로 사용되는 고정 IP 주소는 현재 퍼블릭 엔드포인트 유형에서 지원되지 않습니다. 엔드포인트에 대한 고정 IP 주소를 지정하려면 VPC 호스팅 엔드포인트를 사용합니다.\n",
            "\"category : Transfer for SFTP, question : 최종 사용자가 고정 IP 주소를 사용하여 엔드포인트 유형이 퍼블릭인 서버에 액세스할 수 있나요?, answer : 아니요. 방화벽 화이트리스트에 지정할 목적으로 주로 사용되는 고정 IP 주소는 현재 퍼블릭 엔드포인트 유형에서 지원되지 않습니다. 엔드포인트에 대한 고정 IP 주소를 지정하려면 VPC 호스팅 엔드포인트를 사용합니다.\"\n",
            "최종 사용자가 SFTP 서버의 퍼블릭 엔드포인트 유형에 액세스하기 위해 목록에 허용해야 하는 IP 범위는 어떻게 되나요?\n",
            "퍼블릭 엔드포인트 유형을 사용하는 경우 사용자는 여기에 게시된 AWS IP 주소 범위 목록을 허용해야 합니다. AWS IP 주소 범위를 최신으로 유지하는 방법에 대한 자세한 내용은 설명서를 참조하세요.\n",
            "\"category : Transfer for SFTP, question : 최종 사용자가 SFTP 서버의 퍼블릭 엔드포인트 유형에 액세스하기 위해 목록에 허용해야 하는 IP 범위는 어떻게 되나요?, answer : 퍼블릭 엔드포인트 유형을 사용하는 경우 사용자는 여기에 게시된 AWS IP 주소 범위 목록을 허용해야 합니다. AWS IP 주소 범위를 최신으로 유지하는 방법에 대한 자세한 내용은 설명서를 참조하세요.\"\n",
            "서버를 생성한 후 AWS Transfer for SFTP 서버의 호스트 키가 변경되나요?\n",
            "아니요. 서버를 생성할 때 할당되는 서버의 호스트 키는 새 호스트 키를 추가하고 원본을 수동으로 삭제하기 전까지 동일하게 유지됩니다.\n",
            "\"category : Transfer for SFTP, question : 서버를 생성한 후 AWS Transfer for SFTP 서버의 호스트 키가 변경되나요?, answer : 아니요. 서버를 생성할 때 할당되는 서버의 호스트 키는 새 호스트 키를 추가하고 원본을 수동으로 삭제하기 전까지 동일하게 유지됩니다.\"\n",
            "지원되는 SFTP 서버 호스트 키의 유형은 무엇인가요?\n",
            "SFTP 서버 호스트 키의 경우 RSA, ED25519 및 ECDSA 키 유형이 지원됩니다.\n",
            "\"category : Transfer for SFTP, question : 지원되는 SFTP 서버 호스트 키의 유형은 무엇인가요?, answer : SFTP 서버 호스트 키의 경우 RSA, ED25519 및 ECDSA 키 유형이 지원됩니다.\"\n",
            "사용자가 서버의 신뢰성을 다시 확인할 필요가 없도록 현재 SFTP 서버에서 키를 가져올 수 있나요?\n",
            "예. 서버를 생성할 때 단일 호스트 키를 가져오거나 서버를 업로드할 때 여러 호스트 키를 가져올 수 있습니다. SFTP 사용 서버의 호스트 키 관리에 대한 설명서를 참조하세요.\n",
            "\"category : Transfer for SFTP, question : 사용자가 서버의 신뢰성을 다시 확인할 필요가 없도록 현재 SFTP 서버에서 키를 가져올 수 있나요?, answer : 예. 서버를 생성할 때 단일 호스트 키를 가져오거나 서버를 업로드할 때 여러 호스트 키를 가져올 수 있습니다. SFTP 사용 서버의 호스트 키 관리에 대한 설명서를 참조하세요.\"\n",
            "SFTP 서버의 신뢰성을 확인하는 데 여러 호스트 키를 사용할 수 있나요?\n",
            "예. 각 키 유형에서 가장 오래된 호스트 키를 사용하여 SFTP 서버의 신뢰성을 확인할 수 있습니다. RSA, ED25519 및 ECDSA 호스트 키를 추가하면 3개의 개별 호스트 키를 사용하여 SFTP 서버를 식별할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : SFTP 서버의 신뢰성을 확인하는 데 여러 호스트 키를 사용할 수 있나요?, answer : 예. 각 키 유형에서 가장 오래된 호스트 키를 사용하여 SFTP 서버의 신뢰성을 확인할 수 있습니다. RSA, ED25519 및 ECDSA 호스트 키를 추가하면 3개의 개별 호스트 키를 사용하여 SFTP 서버를 식별할 수 있습니다.\"\n",
            "SFTP 서버의 신뢰성을 확인하는 데 사용되는 호스트 키는 어느 것인가요?\n",
            "각 키 유형에서 가장 오래된 호스트 키가 SFTP 서버의 신뢰성을 확인하는 데 사용됩니다.\n",
            "\"category : Transfer for SFTP, question : SFTP 서버의 신뢰성을 확인하는 데 사용되는 호스트 키는 어느 것인가요?, answer : 각 키 유형에서 가장 오래된 호스트 키가 SFTP 서버의 신뢰성을 확인하는 데 사용됩니다.\"\n",
            "보안 연결을 위해 SFTP 서버 호스트 키를 교체할 수 있나요?\n",
            "예. 호스트 키를 추가하고 제거하여 언제든지 SFTP 서버 호스트 키를 교체할 수 있습니다. SFTP 사용 서버의 호스트 키 관리에 대한 설명서를 참조하세요.\n",
            "\"category : Transfer for SFTP, question : 보안 연결을 위해 SFTP 서버 호스트 키를 교체할 수 있나요?, answer : 예. 호스트 키를 추가하고 제거하여 언제든지 SFTP 서버 호스트 키를 교체할 수 있습니다. SFTP 사용 서버의 호스트 키 관리에 대한 설명서를 참조하세요.\"\n",
            "최종 사용자의 FTPS 클라이언트는 FTPS 서버의 ID를 어떻게 확인하나요?\n",
            "FTPS 액세스를 활성화할 때 Amazon Certificate Manager(ACM)의 인증서를 제공해야 합니다. 이 인증서는 최종 사용자 클라이언트에서 FTPS 서버의 ID를 확인하는 데 사용됩니다. ACM 문서에서 새 인증서 요청 또는 ACM으로 기존 인증서 가져오기를 참조하세요.\n",
            "\"category : Transfer for SFTP, question : 최종 사용자의 FTPS 클라이언트는 FTPS 서버의 ID를 어떻게 확인하나요?, answer : FTPS 액세스를 활성화할 때 Amazon Certificate Manager(ACM)의 인증서를 제공해야 합니다. 이 인증서는 최종 사용자 클라이언트에서 FTPS 서버의 ID를 확인하는 데 사용됩니다. ACM 문서에서 새 인증서 요청 또는 ACM으로 기존 인증서 가져오기를 참조하세요.\"\n",
            "FTPS 및 FTP의 능동 모드 및 수동 모드를 지원하나요?\n",
            "최종 사용자의 클라이언트가 서버와 연결을 시작할 수 있도록 하는 수동 모드만 지원합니다. 수동 모드는 클라이언트 측에 열어야 하는 포트 수가 적으므로 보호된 방화벽 뒤의 최종 사용자와 서버 엔드포인트의 호환성이 높아집니다.\n",
            "\"category : Transfer for SFTP, question : FTPS 및 FTP의 능동 모드 및 수동 모드를 지원하나요?, answer : 최종 사용자의 클라이언트가 서버와 연결을 시작할 수 있도록 하는 수동 모드만 지원합니다. 수동 모드는 클라이언트 측에 열어야 하는 포트 수가 적으므로 보호된 방화벽 뒤의 최종 사용자와 서버 엔드포인트의 호환성이 높아집니다.\"\n",
            "명시적 FTPS 모드 및 암시적 FTPS 모드를 지원하나요?\n",
            "명시적 FTPS 모드만 지원합니다.\n",
            "\"category : Transfer for SFTP, question : 명시적 FTPS 모드 및 암시적 FTPS 모드를 지원하나요?, answer : 명시적 FTPS 모드만 지원합니다.\"\n",
            "클라이언트와 서버 사이에 방화벽 또는 라우터를 구성한 경우 FTPS/FTP 프로토콜에서 파일을 전송할 수 있나요?\n",
            "예. 방화벽 또는 라우터를 우회하는 파일 전송은 기본적으로 확장 수동 연결 모드(EPSV)를 사용하여 지원됩니다. EPSV 모드를 지원하지 않는 FTPS/FTP 클라이언트를 사용하는 경우 이 블로그 게시물에서 PASV 모드로 서버를 구성하여 다양한 클라이언트로 서버 호환성을 확장하는 방법을 확인할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 클라이언트와 서버 사이에 방화벽 또는 라우터를 구성한 경우 FTPS/FTP 프로토콜에서 파일을 전송할 수 있나요?, answer : 예. 방화벽 또는 라우터를 우회하는 파일 전송은 기본적으로 확장 수동 연결 모드(EPSV)를 사용하여 지원됩니다. EPSV 모드를 지원하지 않는 FTPS/FTP 클라이언트를 사용하는 경우 이 블로그 게시물에서 PASV 모드로 서버를 구성하여 다양한 클라이언트로 서버 호환성을 확장하는 방법을 확인할 수 있습니다.\"\n",
            "AWS Transfer Family는 SFTP 서버의 기본 포트가 아닌 포트를 지원하나요?\n",
            "예. AWS Transfer Family는 표준 포트 22 외에 대체 포트 2222 또한 지원합니다. 포트 22는 SFTP 서버에 대해 기본적으로 구성됩니다. 서버 보안을 강화하기 위해 포트 22, 2222 또는 둘 다를 사용하도록 SSH 트래픽을 구성할 수 있습니다. 여기 설명서를 참조하세요.\n",
            "\"category : Transfer for SFTP, question : AWS Transfer Family는 SFTP 서버의 기본 포트가 아닌 포트를 지원하나요?, answer : 예. AWS Transfer Family는 표준 포트 22 외에 대체 포트 2222 또한 지원합니다. 포트 22는 SFTP 서버에 대해 기본적으로 구성됩니다. 서버 보안을 강화하기 위해 포트 22, 2222 또는 둘 다를 사용하도록 SSH 트래픽을 구성할 수 있습니다. 여기 설명서를 참조하세요.\"\n",
            "원격 SFTP 서버와의 연결을 설정할 때 지원되는 인증 방법으로는 어떤 것이 있나요?\n",
            "원격 서버 요구 사항에 따라 SSH 키 페어나 암호 중에서 하나 또는 둘 다를 사용하여 원격 서버에 대한 연결을 인증할 수 있습니다. 사용자 이름, SSH 프라이빗 키 또는 암호를 저장하여 AWS Secrets Manager 계정에서 원격 서버에 로그인하세요. 커넥터의 인증 보안 인증 정보를 저장하고 관리하는 방법에 대해 자세히 알아보려면 설명서를 참조하세요.\n",
            "\"category : Transfer for SFTP, question : 원격 SFTP 서버와의 연결을 설정할 때 지원되는 인증 방법으로는 어떤 것이 있나요?, answer : 원격 서버 요구 사항에 따라 SSH 키 페어나 암호 중에서 하나 또는 둘 다를 사용하여 원격 서버에 대한 연결을 인증할 수 있습니다. 사용자 이름, SSH 프라이빗 키 또는 암호를 저장하여 AWS Secrets Manager 계정에서 원격 서버에 로그인하세요. 커넥터의 인증 보안 인증 정보를 저장하고 관리하는 방법에 대해 자세히 알아보려면 설명서를 참조하세요.\"\n",
            "SFTP 커넥터가 지원하는 SSH 호스트 키 알고리즘은 무엇인가요?\n",
            "RSA 및 ECDSA 호스트 키 알고리즘을 지원합니다. 지원되는 키 유형에 대한 자세한 내용은 여기의 설명서를 참조하세요.\n",
            "\"category : Transfer for SFTP, question : SFTP 커넥터가 지원하는 SSH 호스트 키 알고리즘은 무엇인가요?, answer : RSA 및 ECDSA 호스트 키 알고리즘을 지원합니다. 지원되는 키 유형에 대한 자세한 내용은 여기의 설명서를 참조하세요.\"\n",
            "SFTP 커넥터를 사용하여 파일을 전송할 수 있는 AWS 스토리지 서비스는 무엇인가요?\n",
            "SFTP 커넥터를 사용하여 Amazon S3에서 원격 SFTP 서버로 또는 원격 SFTP 서버에서 Amazon S3로 파일을 전송할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : SFTP 커넥터를 사용하여 파일을 전송할 수 있는 AWS 스토리지 서비스는 무엇인가요?, answer : SFTP 커넥터를 사용하여 Amazon S3에서 원격 SFTP 서버로 또는 원격 SFTP 서버에서 Amazon S3로 파일을 전송할 수 있습니다.\"\n",
            "Amazon S3 버킷과 SFTP 커넥터를 별도의 AWS 계정에 구성해도 되나요?\n",
            "예. Amazon S3 버킷과 SFTP 커넥터 리소스를 서로 다른 AWS 계정에 프로비저닝할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : Amazon S3 버킷과 SFTP 커넥터를 별도의 AWS 계정에 구성해도 되나요?, answer : 예. Amazon S3 버킷과 SFTP 커넥터 리소스를 서로 다른 AWS 계정에 프로비저닝할 수 있습니다.\"\n",
            "하나의 AWS 계정에 SFTP 커넥터를 생성하고 별도의 AWS 계정에서 이를 사용하여 파일을 전송할 수 있나요?\n",
            "예. 한 AWS 계정에 SFTP 커넥터를 생성하고 커넥터에 연결된 IAM 역할에 적절한 액세스 권한을 제공하여 다른 계정에서 파일을 전송하는 데 사용할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 하나의 AWS 계정에 SFTP 커넥터를 생성하고 별도의 AWS 계정에서 이를 사용하여 파일을 전송할 수 있나요?, answer : 예. 한 AWS 계정에 SFTP 커넥터를 생성하고 커넥터에 연결된 IAM 역할에 적절한 액세스 권한을 제공하여 다른 계정에서 파일을 전송하는 데 사용할 수 있습니다.\"\n",
            "연결을 생성할 때 원격 SFTP 서버의 ID를 검증하려면 어떻게 해야 하나요?\n",
            "커넥터는 호스트 지문을 사용하여 원격 서버의 ID를 검증합니다. 원격 서버에서 제공한 지문이 커넥터 구성에 업로드된 지문과 일치하지 않으면 연결이 실패하고 CloudWatch에 오류 세부 정보가 기록됩니다. ID 확인을 위해 원격 서버 SSH 키의 공개 부분을 업로드하는 방법에 대해 자세히 알아보려면 여기의 SFTP 커넥터 설명서로 이동하세요.\n",
            "\"category : Transfer for SFTP, question : 연결을 생성할 때 원격 SFTP 서버의 ID를 검증하려면 어떻게 해야 하나요?, answer : 커넥터는 호스트 지문을 사용하여 원격 서버의 ID를 검증합니다. 원격 서버에서 제공한 지문이 커넥터 구성에 업로드된 지문과 일치하지 않으면 연결이 실패하고 CloudWatch에 오류 세부 정보가 기록됩니다. ID 확인을 위해 원격 서버 SSH 키의 공개 부분을 업로드하는 방법에 대해 자세히 알아보려면 여기의 SFTP 커넥터 설명서로 이동하세요.\"\n",
            "파일 전송 없이 원격 서버에 대한 연결을 확인하려면 어떻게 해야 하나요?\n",
            "AWS Management Console 또는 TestConnection API/CLI/CDK 명령을 사용하여 원격 서버에 대한 연결을 테스트할 수 있습니다. 커넥터를 생성하는 즉시 원격 서버에 대한 연결을 테스트하여 올바르게 구성되었는지 확인하는 것이 좋습니다. 필요한 경우 커넥터와 연결된 고정 IP 주소가 원격 서버의 허용 목록으로 추가되어 있는지 확인하세요. 자세히 알아보려면 SFTP 커넥터 설명서를 참조하세요.\n",
            "\"category : Transfer for SFTP, question : 파일 전송 없이 원격 서버에 대한 연결을 확인하려면 어떻게 해야 하나요?, answer : AWS Management Console 또는 TestConnection API/CLI/CDK 명령을 사용하여 원격 서버에 대한 연결을 테스트할 수 있습니다. 커넥터를 생성하는 즉시 원격 서버에 대한 연결을 테스트하여 올바르게 구성되었는지 확인하는 것이 좋습니다. 필요한 경우 커넥터와 연결된 고정 IP 주소가 원격 서버의 허용 목록으로 추가되어 있는지 확인하세요. 자세히 알아보려면 SFTP 커넥터 설명서를 참조하세요.\"\n",
            "SFTP 커넥터는 어떤 파일 전송 작업을 지원하나요?\n",
            "SFTP 커넥터를 사용하여 Amazon S3에서 원격 SFTP 서버의 디렉터리로 파일을 전송하거나 원격 SFTP 서버의 디렉터리에서 Amazon S3로 파일을 검색할 수 있습니다. StartFileTransfer API를 사용하여 파일 전송 작업을 시작하는 방법에 대해 자세히 알아보려면 SFTP 커넥터 설명서로 이동하세요.\n",
            "\"category : Transfer for SFTP, question : SFTP 커넥터는 어떤 파일 전송 작업을 지원하나요?, answer : SFTP 커넥터를 사용하여 Amazon S3에서 원격 SFTP 서버의 디렉터리로 파일을 전송하거나 원격 SFTP 서버의 디렉터리에서 Amazon S3로 파일을 검색할 수 있습니다. StartFileTransfer API를 사용하여 파일 전송 작업을 시작하는 방법에 대해 자세히 알아보려면 SFTP 커넥터 설명서로 이동하세요.\"\n",
            "파일 전송 상태를 추적하려면 어떻게 해야 하나요?\n",
            "Amazon CloudWatch Logs에서 파일 전송 상태를 모니터링할 수 있습니다. 작업(전송 또는 검색), 타임스탬프, 파일 경로, 오류 설명(있는 경우)과 같은 추가 세부 정보와 함께 파일 전송의 완료 또는 실패 여부를 추적하여 데이터 계보를 유지 관리할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 파일 전송 상태를 추적하려면 어떻게 해야 하나요?, answer : Amazon CloudWatch Logs에서 파일 전송 상태를 모니터링할 수 있습니다. 작업(전송 또는 검색), 타임스탬프, 파일 경로, 오류 설명(있는 경우)과 같은 추가 세부 정보와 함께 파일 전송의 완료 또는 실패 여부를 추적하여 데이터 계보를 유지 관리할 수 있습니다.\"\n",
            "SFTP 커넥터를 사용하여 파일 전송을 예약할 수 있나요?\n",
            "예. Amazon EventBridge 스케줄러를 사용하여 파일 전송을 예약할 수 있습니다. EventBridge의 스케줄러를 사용하여 비즈니스 요구 사항에 적합한 일정을 생성하고 AWS Transfer Family의 StartFileTransfer API를 일정의 범용 대상으로 지정하세요.\n",
            "\"category : Transfer for SFTP, question : SFTP 커넥터를 사용하여 파일 전송을 예약할 수 있나요?, answer : 예. Amazon EventBridge 스케줄러를 사용하여 파일 전송을 예약할 수 있습니다. EventBridge의 스케줄러를 사용하여 비즈니스 요구 사항에 적합한 일정을 생성하고 AWS Transfer Family의 StartFileTransfer API를 일정의 범용 대상으로 지정하세요.\"\n",
            "AWS Step Function의 상태 머신에서 SFTP 커넥터를 사용하여 파일 전송을 간접적으로 호출할 수 있나요?\n",
            "예. AWS Step Functions는 AWS Transfer Family를 비롯한 다양한 AWS 서비스와 통합되므로 상태 머신에서 직접 SFTP 커넥터의 StartFileTransfer 작업을 간접적으로 호출할 수 있습니다. AWS Transfer Family로 SFTP 커넥터를 생성한 후에 Step Functions의 AWS SDK 통합을 활용하여 StartFileTransfer API를 직접적으로 호출하세요. 자세히 알아보려면 Step Functions 설명서로 이동하세요.\n",
            "\"category : Transfer for SFTP, question : AWS Step Function의 상태 머신에서 SFTP 커넥터를 사용하여 파일 전송을 간접적으로 호출할 수 있나요?, answer : 예. AWS Step Functions는 AWS Transfer Family를 비롯한 다양한 AWS 서비스와 통합되므로 상태 머신에서 직접 SFTP 커넥터의 StartFileTransfer 작업을 간접적으로 호출할 수 있습니다. AWS Transfer Family로 SFTP 커넥터를 생성한 후에 Step Functions의 AWS SDK 통합을 활용하여 StartFileTransfer API를 직접적으로 호출하세요. 자세히 알아보려면 Step Functions 설명서로 이동하세요.\"\n",
            "SFTP 커넥터를 사용하여 전송된 파일의 이벤트 기반 처리를 오케스트레이션할 수 있나요?\n",
            "예. SFTP 커넥터를 사용하는 모든 파일 전송 작업은 Amazon EventBridge의 기본 이벤트 버스에 이벤트 알림을 게시합니다. SFTP 커넥터 이벤트를 구독하고 Amazon EventBridge 또는 이러한 이벤트와 통합되는 기타 워크플로 오케스트레이션 서비스에서 이 이벤트를 사용하여 전송된 파일의 이벤트 기반 처리를 오케스트레이션할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : SFTP 커넥터를 사용하여 전송된 파일의 이벤트 기반 처리를 오케스트레이션할 수 있나요?, answer : 예. SFTP 커넥터를 사용하는 모든 파일 전송 작업은 Amazon EventBridge의 기본 이벤트 버스에 이벤트 알림을 게시합니다. SFTP 커넥터 이벤트를 구독하고 Amazon EventBridge 또는 이러한 이벤트와 통합되는 기타 워크플로 오케스트레이션 서비스에서 이 이벤트를 사용하여 전송된 파일의 이벤트 기반 처리를 오케스트레이션할 수 있습니다.\"\n",
            "비즈니스 파트너가 방화벽에 커넥터의 IP 주소를 나열할 수 있도록 SFTP 커넥터에 고정 IP 주소를 사용할 수 있나요?\n",
            "예. 고정 IP 주소는 기본적으로 커넥터와 연결되어 있어 비즈니스 파트너의 방화벽에서 연결을 허용 목록에 추가하는 데 사용할 수 있습니다. AWS Transfer Family 콘솔의 커넥터 세부 정보 페이지로 이동하거나 DescribeConnector API/CLI/CDK 명령을 사용하여 커넥터와 연결된 고정 IP 주소를 식별할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 비즈니스 파트너가 방화벽에 커넥터의 IP 주소를 나열할 수 있도록 SFTP 커넥터에 고정 IP 주소를 사용할 수 있나요?, answer : 예. 고정 IP 주소는 기본적으로 커넥터와 연결되어 있어 비즈니스 파트너의 방화벽에서 연결을 허용 목록에 추가하는 데 사용할 수 있습니다. AWS Transfer Family 콘솔의 커넥터 세부 정보 페이지로 이동하거나 DescribeConnector API/CLI/CDK 명령을 사용하여 커넥터와 연결된 고정 IP 주소를 식별할 수 있습니다.\"\n",
            "고정 IP 주소는 내 계정의 모든 SFTP 커넥터에서 동일한가요?\n",
            "예. AWS 계정 리전의 모든 SFTP 커넥터는 고정 IP 주소 세트를 공유합니다. 특정 유형의 커넥터 간에 IP 주소를 공유하면 허용 목록 설명서의 양은 물론 외부 파트너와 필요한 온보딩 통신이 줄어듭니다.\n",
            "\"category : Transfer for SFTP, question : 고정 IP 주소는 내 계정의 모든 SFTP 커넥터에서 동일한가요?, answer : 예. AWS 계정 리전의 모든 SFTP 커넥터는 고정 IP 주소 세트를 공유합니다. 특정 유형의 커넥터 간에 IP 주소를 공유하면 허용 목록 설명서의 양은 물론 외부 파트너와 필요한 온보딩 통신이 줄어듭니다.\"\n",
            "SFTP 커넥터를 사용하여 복사할 파일의 파일 이름 패턴을 와일드카드를 사용하여 지정할 수 있나요?\n",
            "아니요. SFTP 커넥터를 사용하여 파일을 복사할 때 StartFileTransfer API 작업을 사용하는 경우 전체 파일 경로를 지정해야 합니다. 와일드카드를 사용하여 전송할 파일을 지정해야 하는 사용 사례의 경우 AWS Support 또는 AWS Account 팀을 통해 문의하세요.\n",
            "\"category : Transfer for SFTP, question : SFTP 커넥터를 사용하여 복사할 파일의 파일 이름 패턴을 와일드카드를 사용하여 지정할 수 있나요?, answer : 아니요. SFTP 커넥터를 사용하여 파일을 복사할 때 StartFileTransfer API 작업을 사용하는 경우 전체 파일 경로를 지정해야 합니다. 와일드카드를 사용하여 전송할 파일을 지정해야 하는 사용 사례의 경우 AWS Support 또는 AWS Account 팀을 통해 문의하세요.\"\n",
            "SFTP 커넥터를 사용하여 프라이빗 네트워크 내의 서버에 연결할 수 있나요?\n",
            "아니요. 현재 SFTP 커넥터는 인터넷 액세스 엔드포인트를 제공하는 서버에 연결할 때만 사용할 수 있습니다. 프라이빗 네트워크를 통해서만 액세스 가능한 서버에 연결해야 하는 경우 AWS Support 또는 AWS Account 팀을 통해 문의하세요.\n",
            "\"category : Transfer for SFTP, question : SFTP 커넥터를 사용하여 프라이빗 네트워크 내의 서버에 연결할 수 있나요?, answer : 아니요. 현재 SFTP 커넥터는 인터넷 액세스 엔드포인트를 제공하는 서버에 연결할 때만 사용할 수 있습니다. 프라이빗 네트워크를 통해서만 액세스 가능한 서버에 연결해야 하는 경우 AWS Support 또는 AWS Account 팀을 통해 문의하세요.\"\n",
            "동일한 서버 엔드포인트에서 여러 프로토콜을 사용할 수 있나요?\n",
            "예. 설정하는 동안 클라이언트가 엔드포인트에 연결할 수 있도록 사용하려는 프로토콜을 선택할 수 있습니다. 선택한 프로토콜 전체에서 서버 호스트 이름, IP 주소 및 ID 제공업체가 공유됩니다. 마찬가지로 기존 AWS Transfer Family 엔드포인트에 대한 추가 프로토콜 지원을 사용하도록 설정할 수도 있습니다. 이 경우 엔드포인트 구성은 사용하려는 모든 프로토콜의 요구 사항을 충족해야 합니다.\n",
            "\"category : Transfer for SFTP, question : 동일한 서버 엔드포인트에서 여러 프로토콜을 사용할 수 있나요?, answer : 예. 설정하는 동안 클라이언트가 엔드포인트에 연결할 수 있도록 사용하려는 프로토콜을 선택할 수 있습니다. 선택한 프로토콜 전체에서 서버 호스트 이름, IP 주소 및 ID 제공업체가 공유됩니다. 마찬가지로 기존 AWS Transfer Family 엔드포인트에 대한 추가 프로토콜 지원을 사용하도록 설정할 수도 있습니다. 이 경우 엔드포인트 구성은 사용하려는 모든 프로토콜의 요구 사항을 충족해야 합니다.\"\n",
            "각 프로토콜에 대해 별도의 서버 엔드포인트를 만들어야 하는 경우와 여러 프로토콜에 대해 동일한 엔드포인트를 사용해야 하는 경우는 언제인가요?\n",
            "VPC 내에서만 액세스가 지원되는 FTP를 사용해야 하고 인터넷을 통해 SFTP, AS2 또는 FTPS를 지원해야 하는 경우 FTP를 위한 별도의 서버 엔드포인트가 필요합니다. 여러 프로토콜을 통해 연결하는 클라이언트에 동일한 엔드포인트 호스트 이름 및 IP 주소를 사용하려는 경우 여러 프로토콜에 동일한 엔드포인트를 사용할 수 있습니다. 또한 SFTP 및 FTPS에 대해 동일한 자격 증명을 공유하려는 경우, 둘 중 하나의 프로토콜을 통해 연결하는 클라이언트를 인증하기 위해 단일 자격 증명 공급자를 설정하고 사용할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 각 프로토콜에 대해 별도의 서버 엔드포인트를 만들어야 하는 경우와 여러 프로토콜에 대해 동일한 엔드포인트를 사용해야 하는 경우는 언제인가요?, answer : VPC 내에서만 액세스가 지원되는 FTP를 사용해야 하고 인터넷을 통해 SFTP, AS2 또는 FTPS를 지원해야 하는 경우 FTP를 위한 별도의 서버 엔드포인트가 필요합니다. 여러 프로토콜을 통해 연결하는 클라이언트에 동일한 엔드포인트 호스트 이름 및 IP 주소를 사용하려는 경우 여러 프로토콜에 동일한 엔드포인트를 사용할 수 있습니다. 또한 SFTP 및 FTPS에 대해 동일한 자격 증명을 공유하려는 경우, 둘 중 하나의 프로토콜을 통해 연결하는 클라이언트를 인증하기 위해 단일 자격 증명 공급자를 설정하고 사용할 수 있습니다.\"\n",
            "동일한 최종 사용자가 여러 프로토콜을 통해 엔드포인트에 액세스하도록 설정할 수 있나요?\n",
            "예. 예. 프로토콜과 관련된 보안 인증 정보가 ID 제공업체에 설정되어 있는 한 여러 프로토콜을 통해 동일한 사용자 액세스를 제공할 수 있습니다. FTP를 활성화한 경우 FTP에 대해 별도의 보안 인증 정보를 유지하는 것이 좋습니다. FTP에 대한 별도의 자격 증명을 설정하려면 설명서를 참조하십시오.\n",
            "\"category : Transfer for SFTP, question : 동일한 최종 사용자가 여러 프로토콜을 통해 엔드포인트에 액세스하도록 설정할 수 있나요?, answer : 예. 예. 프로토콜과 관련된 보안 인증 정보가 ID 제공업체에 설정되어 있는 한 여러 프로토콜을 통해 동일한 사용자 액세스를 제공할 수 있습니다. FTP를 활성화한 경우 FTP에 대해 별도의 보안 인증 정보를 유지하는 것이 좋습니다. FTP에 대한 별도의 자격 증명을 설정하려면 설명서를 참조하십시오.\"\n",
            "FTP 사용자에 대해 별도의 보안 인증 정보를 유지해야 하는 이유는 무엇인가요?\n",
            "SFTP 및 FTPS와 달리 FTP는 자격 증명을 일반 텍스트로 전송합니다. 실수로 FTP 자격 증명이 공유되거나 노출되는 경우 SFTP 또는 FTPS를 사용하는 워크로드는 안전하게 유지되므로 SFTP 또는 FTPS에서 FTP 자격 증명을 격리하는 것이 좋습니다.\n",
            "\"category : Transfer for SFTP, question : FTP 사용자에 대해 별도의 보안 인증 정보를 유지해야 하는 이유는 무엇인가요?, answer : SFTP 및 FTPS와 달리 FTP는 자격 증명을 일반 텍스트로 전송합니다. 실수로 FTP 자격 증명이 공유되거나 노출되는 경우 SFTP 또는 FTPS를 사용하는 워크로드는 안전하게 유지되므로 SFTP 또는 FTPS에서 FTP 자격 증명을 격리하는 것이 좋습니다.\"\n",
            "내 사용자가 브라우저를 사용하여 AWS Transfer Family의 SFTP 엔드포인트에 액세스할 수 있나요?\n",
            "예. AWS Transfer Family의 SFTP 엔드포인트를 사용하여 브라우저 기반 인터페이스를 제공할 수 있는 이 오픈 소스 솔루션을 배포할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 내 사용자가 브라우저를 사용하여 AWS Transfer Family의 SFTP 엔드포인트에 액세스할 수 있나요?, answer : 예. AWS Transfer Family의 SFTP 엔드포인트를 사용하여 브라우저 기반 인터페이스를 제공할 수 있는 이 오픈 소스 솔루션을 배포할 수 있습니다.\"\n",
            "이 서비스에서 지원하는 ID 제공업체 옵션으로는 어떤 것이 있나요?\n",
            "이 서비스는 세 가지 ID 제공업체 옵션을 지원합니다. 서비스 내에 사용자 ID를 저장하는 서비스 관리형, Microsoft Active Directory 및 선택한 ID 제공업체를 통합할 수 있는 사용자 지정 ID 제공업체입니다. SFTP에만 사용되는 서버 엔드포인트에 대해 서비스 관리형 인증이 지원됩니다.\n",
            "\"category : Transfer for SFTP, question : 이 서비스에서 지원하는 ID 제공업체 옵션으로는 어떤 것이 있나요?, answer : 이 서비스는 세 가지 ID 제공업체 옵션을 지원합니다. 서비스 내에 사용자 ID를 저장하는 서비스 관리형, Microsoft Active Directory 및 선택한 ID 제공업체를 통합할 수 있는 사용자 지정 ID 제공업체입니다. SFTP에만 사용되는 서버 엔드포인트에 대해 서비스 관리형 인증이 지원됩니다.\"\n",
            "서비스 관리형 인증을 사용하여 내 사용자를 인증하는 방법은 무엇인가요?\n",
            "서비스 관리형 인증을 사용하면 SSH 키를 사용하여 SFTP 사용자를 인증할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 서비스 관리형 인증을 사용하여 내 사용자를 인증하는 방법은 무엇인가요?, answer : 서비스 관리형 인증을 사용하면 SSH 키를 사용하여 SFTP 사용자를 인증할 수 있습니다.\"\n",
            "SFTP 사용자당 몇 개의 SSH 키를 업로드할 수 있나요? 지원되는 키 유형은 무엇인가요?\n",
            "사용자당 최대 10개의 SSH 키를 업로드할 수 있습니다. RSA, ED25519 및 ECDSA 키가 지원됩니다.\n",
            "\"category : Transfer for SFTP, question : SFTP 사용자당 몇 개의 SSH 키를 업로드할 수 있나요? 지원되는 키 유형은 무엇인가요?, answer : 사용자당 최대 10개의 SSH 키를 업로드할 수 있습니다. RSA, ED25519 및 ECDSA 키가 지원됩니다.\"\n",
            "서비스 관리형 인증은 SSH 키 교체를 지원하나요?\n",
            "예. SFTP 사용자에 대한 키 교체를 설정하는 자세한 방법은 설명서를 참조하세요.\n",
            "\"category : Transfer for SFTP, question : 서비스 관리형 인증은 SSH 키 교체를 지원하나요?, answer : 예. SFTP 사용자에 대한 키 교체를 설정하는 자세한 방법은 설명서를 참조하세요.\"\n",
            "Microsoft AD 사용을 시작하려면 어떻게 해야 하나요?\n",
            "서버를 생성할 때 AWS Managed Microsoft AD, 온프레미스 환경 또는 Amazon EC2의 자체 관리형 AD를 ID 제공업체로 선택합니다. 보안 식별자(SID)를 사용한 액세스에 대해 활성화하려는 AD 그룹을 지정해야 합니다. AD 그룹을 IAM 역할, 범위 축소 정책(S3만 해당), POSIX 프로파일(EFS만 해당), 홈 디렉터리 위치 및 논리적 디렉터리 매핑과 같은 액세스 제어 정보에 연결하면 그룹의 멤버가 AD 보안 인증 정보를 사용하여 인증하고 활성화된 프로토콜(SFTP, FTPS, FTP)을 통해 파일을 전송할 수 있게 됩니다.\n",
            "\"category : Transfer for SFTP, question : Microsoft AD 사용을 시작하려면 어떻게 해야 하나요?, answer : 서버를 생성할 때 AWS Managed Microsoft AD, 온프레미스 환경 또는 Amazon EC2의 자체 관리형 AD를 ID 제공업체로 선택합니다. 보안 식별자(SID)를 사용한 액세스에 대해 활성화하려는 AD 그룹을 지정해야 합니다. AD 그룹을 IAM 역할, 범위 축소 정책(S3만 해당), POSIX 프로파일(EFS만 해당), 홈 디렉터리 위치 및 논리적 디렉터리 매핑과 같은 액세스 제어 정보에 연결하면 그룹의 멤버가 AD 보안 인증 정보를 사용하여 인증하고 활성화된 프로토콜(SFTP, FTPS, FTP)을 통해 파일을 전송할 수 있게 됩니다.\"\n",
            "S3 버킷의 서로 다른 부분에 대한 액세스가 격리되도록 AD 사용자를 설정하려면 어떻게 해야 하나요?\n",
            "사용자를 설정할 때 런타임 시 사용자 정보(예: 사용자 이름)를 기반으로 평가되는 범위 축소 정책을 제공합니다. 동일한 범위 축소 정책을 모든 사용자에게 사용하여 사용자 이름을 기반으로 버킷의 고유한 접두사에 대한 액세스를 제공할 수 있습니다. 또한, S3 버킷 또는 EFS 파일 시스템 콘텐츠를 사용자에게 표시하는 방법에 대한 표준화된 템플릿을 제공하여 사용자 이름을 사용해 논리적 디렉터리 매핑을 평가할 수도 있습니다. 자세한 내용은 AD 그룹에 대한 액세스 권한 부여에 관한 설명서를 참조하세요.\n",
            "\"category : Transfer for SFTP, question : S3 버킷의 서로 다른 부분에 대한 액세스가 격리되도록 AD 사용자를 설정하려면 어떻게 해야 하나요?, answer : 사용자를 설정할 때 런타임 시 사용자 정보(예: 사용자 이름)를 기반으로 평가되는 범위 축소 정책을 제공합니다. 동일한 범위 축소 정책을 모든 사용자에게 사용하여 사용자 이름을 기반으로 버킷의 고유한 접두사에 대한 액세스를 제공할 수 있습니다. 또한, S3 버킷 또는 EFS 파일 시스템 콘텐츠를 사용자에게 표시하는 방법에 대한 표준화된 템플릿을 제공하여 사용자 이름을 사용해 논리적 디렉터리 매핑을 평가할 수도 있습니다. 자세한 내용은 AD 그룹에 대한 액세스 권한 부여에 관한 설명서를 참조하세요.\"\n",
            "Microsoft AD를 지원되는 모든 프로토콜의 ID 제공업체로 사용할 수 있나요?\n",
            "예. Microsoft AD를 사용하여 SFTP, FTPS 및 FTP를 통해 액세스하는 사용자를 인증할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : Microsoft AD를 지원되는 모든 프로토콜의 ID 제공업체로 사용할 수 있나요?, answer : 예. Microsoft AD를 사용하여 SFTP, FTPS 및 FTP를 통해 액세스하는 사용자를 인증할 수 있습니다.\"\n",
            "활성화된 AD 그룹의 액세스 권한을 취소할 수 있나요?\n",
            "예. 개별 AD 그룹의 파일 전송 액세스 권한을 취소할 수 있습니다. 취소하면 해당 AD 그룹의 멤버가 AD 보안 인증 정보를 사용하여 파일을 전송할 수 없게 됩니다.\n",
            "\"category : Transfer for SFTP, question : 활성화된 AD 그룹의 액세스 권한을 취소할 수 있나요?, answer : 예. 개별 AD 그룹의 파일 전송 액세스 권한을 취소할 수 있습니다. 취소하면 해당 AD 그룹의 멤버가 AD 보안 인증 정보를 사용하여 파일을 전송할 수 없게 됩니다.\"\n",
            "왜 사용자 지정 인증 모드를 사용해야 하나요?\n",
            "사용자 지정 모드(‘BYO’ 인증)를 사용하면 기존 ID 제공업체를 활용하여 모든 프로토콜 유형(SFTP, FTPS 및 FTP)에 대한 최종 사용자를 관리할 수 있으며, 이를 통해 사용자를 쉽고 원활하게 마이그레이션할 수 있습니다. 자격 증명은 회사 디렉터리 또는 사내 자격 증명 데이터 스토어에 저장될 수 있으며 최종 사용자 인증을 위해 통합할 수 있습니다. ID 제공업체의 예로는 Okta, Microsoft AzureAD 또는 전체 프로비저닝 포털의 일부로 사용할 수 있는 사용자 지정 ID 제공업체가 있습니다.\n",
            "\"category : Transfer for SFTP, question : 왜 사용자 지정 인증 모드를 사용해야 하나요?, answer : 사용자 지정 모드(‘BYO’ 인증)를 사용하면 기존 ID 제공업체를 활용하여 모든 프로토콜 유형(SFTP, FTPS 및 FTP)에 대한 최종 사용자를 관리할 수 있으며, 이를 통해 사용자를 쉽고 원활하게 마이그레이션할 수 있습니다. 자격 증명은 회사 디렉터리 또는 사내 자격 증명 데이터 스토어에 저장될 수 있으며 최종 사용자 인증을 위해 통합할 수 있습니다. ID 제공업체의 예로는 Okta, Microsoft AzureAD 또는 전체 프로비저닝 포털의 일부로 사용할 수 있는 사용자 지정 ID 제공업체가 있습니다.\"\n",
            "ID 제공업체를 AWS Transfer Family 서버와 통합하려면 어떤 옵션이 있나요?\n",
            "ID 제공업체를 AWS Transfer Family 서버와 통합하려면 AWS Lambda 함수 또는 Amazon API Gateway 엔드포인트를 사용하면 됩니다. 자격 증명 공급자에 연결하기 위해 RESTful API가 필요하거나 지리적 차단 및 속도 제한 기능을 위해 AWS WAF를 활용하려는 경우 Amazon API Gateway를 사용하세요. AWS Cognito, Okta 및 AWS Secrets Manager와 같은 일반 ID 제공업체 통합에 대해 자세히 알아보려면 설명서를 참조하세요.\n",
            "\"category : Transfer for SFTP, question : ID 제공업체를 AWS Transfer Family 서버와 통합하려면 어떤 옵션이 있나요?, answer : ID 제공업체를 AWS Transfer Family 서버와 통합하려면 AWS Lambda 함수 또는 Amazon API Gateway 엔드포인트를 사용하면 됩니다. 자격 증명 공급자에 연결하기 위해 RESTful API가 필요하거나 지리적 차단 및 속도 제한 기능을 위해 AWS WAF를 활용하려는 경우 Amazon API Gateway를 사용하세요. AWS Cognito, Okta 및 AWS Secrets Manager와 같은 일반 ID 제공업체 통합에 대해 자세히 알아보려면 설명서를 참조하세요.\"\n",
            "클라이언트 소스 IP를 기반으로 액세스 제어를 적용할 수 있나요?\n",
            "예. AWS Lambda 또는 API Gateway를 사용하여 사용자 정의 자격 증명 공급자를 연결할 때 클라이언트 소스 IP가 자격 증명 공급자에게 전달됩니다. 이를 통해 클라이언트의 IP 주소를 기반으로 액세스를 허용, 거부 또는 제한하여 신뢰할 수 있는 것으로 지정한 IP 주소에서만 데이터에 액세스할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 클라이언트 소스 IP를 기반으로 액세스 제어를 적용할 수 있나요?, answer : 예. AWS Lambda 또는 API Gateway를 사용하여 사용자 정의 자격 증명 공급자를 연결할 때 클라이언트 소스 IP가 자격 증명 공급자에게 전달됩니다. 이를 통해 클라이언트의 IP 주소를 기반으로 액세스를 허용, 거부 또는 제한하여 신뢰할 수 있는 것으로 지정한 IP 주소에서만 데이터에 액세스할 수 있습니다.\"\n",
            "SFTP 서버에 연결하려고 하는 사용자에게 여러 가지 인증 방법을 요구할 수 있나요?\n",
            "예. 다수의 인증 방법을 적용하여 SFTP를 통해 데이터에 액세스할 때 추가 보안 계층을 제공할 수 있습니다. 암호와 SSH 키, 암호 또는 SSH 키를 모두 요구하거나 암호만 요구하거나 SSH 키만 요구하도록 SFTP 서버를 구성할 수 있습니다. 고객 ID 제공업체를 사용하여 다양한 인증 방법을 지원하는 방법에 대한 자세한 내용은 설명서를 참조하세요.\n",
            "\"category : Transfer for SFTP, question : SFTP 서버에 연결하려고 하는 사용자에게 여러 가지 인증 방법을 요구할 수 있나요?, answer : 예. 다수의 인증 방법을 적용하여 SFTP를 통해 데이터에 액세스할 때 추가 보안 계층을 제공할 수 있습니다. 암호와 SSH 키, 암호 또는 SSH 키를 모두 요구하거나 암호만 요구하거나 SSH 키만 요구하도록 SFTP 서버를 구성할 수 있습니다. 고객 ID 제공업체를 사용하여 다양한 인증 방법을 지원하는 방법에 대한 자세한 내용은 설명서를 참조하세요.\"\n",
            "암호 인증에 서비스 관리형 옵션을 사용할 수 있나요?\n",
            "아니요. 인증을 위해 이 서비스 내에 암호를 저장하는 기능은 현재 지원되지 않습니다. 암호 인증이 필요한 경우 AWS Directory Service에서 디렉터리를 선택하여 Active Directory를 사용하거나 Secrets Manager를 사용하여 암호 인증 사용에 대한 이 블로그에 설명된 아키텍처를 따르세요.\n",
            "\"category : Transfer for SFTP, question : 암호 인증에 서비스 관리형 옵션을 사용할 수 있나요?, answer : 아니요. 인증을 위해 이 서비스 내에 암호를 저장하는 기능은 현재 지원되지 않습니다. 암호 인증이 필요한 경우 AWS Directory Service에서 디렉터리를 선택하여 Active Directory를 사용하거나 Secrets Manager를 사용하여 암호 인증 사용에 대한 이 블로그에 설명된 아키텍처를 따르세요.\"\n",
            "익명 사용자가 지원되나요?\n",
            "아니요. 익명 사용자는 현재 어떤 프로토콜에서도 지원되지 않습니다.\n",
            "\"category : Transfer for SFTP, question : 익명 사용자가 지원되나요?, answer : 아니요. 익명 사용자는 현재 어떤 프로토콜에서도 지원되지 않습니다.\"\n",
            "디렉터리의 개별 AD 사용자 또는 모든 사용자에게 액세스 권한을 제공할 수 있나요?\n",
            "아니요. AD 그룹별 액세스 설정만 지원됩니다.\n",
            "\"category : Transfer for SFTP, question : 디렉터리의 개별 AD 사용자 또는 모든 사용자에게 액세스 권한을 제공할 수 있나요?, answer : 아니요. AD 그룹별 액세스 설정만 지원됩니다.\"\n",
            "AD를 사용하여 SSH 키를 사용하는 사용자를 인증할 수 있나요?\n",
            "아니요. Microsoft AD에 대한 AWS Transfer Family 지원은 암호 기반 인증에만 사용할 수 있습니다. 인증 모드를 혼합하여 사용하려면 사용자 지정 권한 부여자 옵션을 사용합니다.\n",
            "\"category : Transfer for SFTP, question : AD를 사용하여 SSH 키를 사용하는 사용자를 인증할 수 있나요?, answer : 아니요. Microsoft AD에 대한 AWS Transfer Family 지원은 암호 기반 인증에만 사용할 수 있습니다. 인증 모드를 혼합하여 사용하려면 사용자 지정 권한 부여자 옵션을 사용합니다.\"\n",
            "AWS Transfer Family의 AS2 지원은 Drummond 인증을 받았나요?\n",
            "예. AWS Transfer Family의 AS2 지원은 공식 Drummond Group AS2 Cloud Certification Seal을 획득했습니다. AWS Transfer Family AS2 기능은 14개의 다른 서드 파티 AS2 솔루션과의 보안 및 메시지 교환 호환성에 대해 철저한 검증을 받았습니다. 자세한 내용은 공지 사항을 참조하세요.\n",
            "\"category : Transfer for SFTP, question : AWS Transfer Family의 AS2 지원은 Drummond 인증을 받았나요?, answer : 예. AWS Transfer Family의 AS2 지원은 공식 Drummond Group AS2 Cloud Certification Seal을 획득했습니다. AWS Transfer Family AS2 기능은 14개의 다른 서드 파티 AS2 솔루션과의 보안 및 메시지 교환 호환성에 대해 철저한 검증을 받았습니다. 자세한 내용은 공지 사항을 참조하세요.\"\n",
            "내 AS2 거래 파트너를 고유하게 식별하려면 어떻게 해야 하나요?\n",
            "거래 파트너는 AS2 식별자(AS2 ID)를 사용하여 고유하게 식별됩니다. 마찬가지로, 거래 파트너는 AS2 ID를 사용하여 메시지를 식별합니다.\n",
            "\"category : Transfer for SFTP, question : 내 AS2 거래 파트너를 고유하게 식별하려면 어떻게 해야 하나요?, answer : 거래 파트너는 AS2 식별자(AS2 ID)를 사용하여 고유하게 식별됩니다. 마찬가지로, 거래 파트너는 AS2 ID를 사용하여 메시지를 식별합니다.\"\n",
            "AS2에서 사용할 수 있는 AWS Transfer Family의 기존 기능은 무엇인가요? 사용할 수 없는 기능은 무엇인가요?\n",
            "SFTP, FTPS 및 FTP와 마찬가지로 Amazon S3, 네트워킹 기능(VPC 엔드포인트, 보안 그룹 및 탄력적 IP) 및 액세스 제어(AWS IAM)에 대한 AWS Transfer Family의 기존 지원을 AS2에 사용할 수 있습니다. 사용자 인증, 논리적 디렉터리, 사용자 지정 배너 및 Amazon EFS 스토리지 백엔드는 AS2에 대해 지원되지 않습니다.\n",
            "\"category : Transfer for SFTP, question : AS2에서 사용할 수 있는 AWS Transfer Family의 기존 기능은 무엇인가요? 사용할 수 없는 기능은 무엇인가요?, answer : SFTP, FTPS 및 FTP와 마찬가지로 Amazon S3, 네트워킹 기능(VPC 엔드포인트, 보안 그룹 및 탄력적 IP) 및 액세스 제어(AWS IAM)에 대한 AWS Transfer Family의 기존 지원을 AS2에 사용할 수 있습니다. 사용자 인증, 논리적 디렉터리, 사용자 지정 배너 및 Amazon EFS 스토리지 백엔드는 AS2에 대해 지원되지 않습니다.\"\n",
            "부인 방지란 무엇이고 왜 중요한가요?\n",
            "부인 방지는 AS2에 고유한 기능으로, 두 당사자 간에 메시지가 성공적으로 교환되었는지 검증합니다. AS2에서 부인 방지는 메시지 처리 통지(MDN)를 사용하여 달성됩니다. 트랜잭션에서 MDN이 요청되면 MDN은 발신자가 메시지를 보냈고 수신자가 성공적으로 받았으며 발신자가 보낸 메시지가 수신자가 받은 메시지와 동일한 메시지인지 확인합니다.\n",
            "\"category : Transfer for SFTP, question : 부인 방지란 무엇이고 왜 중요한가요?, answer : 부인 방지는 AS2에 고유한 기능으로, 두 당사자 간에 메시지가 성공적으로 교환되었는지 검증합니다. AS2에서 부인 방지는 메시지 처리 통지(MDN)를 사용하여 달성됩니다. 트랜잭션에서 MDN이 요청되면 MDN은 발신자가 메시지를 보냈고 수신자가 성공적으로 받았으며 발신자가 보낸 메시지가 수신자가 받은 메시지와 동일한 메시지인지 확인합니다.\"\n",
            "AS2 프로토콜을 사용한 메시지 전송에 관련된 단계는 무엇인가요?\n",
            "메시지 전송에는 발신자와 수신자라는 2가지 측면이 있습니다. 발신자가 전송할 메시지를 결정하면 발신자의 개인 키를 사용하여 메시지가 서명되고 수신자의 인증서를 사용하여 암호화되며 해시를 사용하여 메시지 무결성이 계산됩니다. 서명되고 암호화된 이 메시지는 유선을 통해 수신자에게 전송됩니다. 메시지가 수신되면 수신자의 개인 키를 사용하여 복호화되고 발신자의 공개 키를 사용하여 검증되고 처리됩니다. 서명된 메시지 처리 통지(MDN)를 요청한 경우 메시지의 성공적인 배달을 확인하는 MDN이 발신자에게 다시 전송됩니다. AS2의 메시지 전송 처리 방법에 대한 설명서를 참조하세요.\n",
            "\"category : Transfer for SFTP, question : AS2 프로토콜을 사용한 메시지 전송에 관련된 단계는 무엇인가요?, answer : 메시지 전송에는 발신자와 수신자라는 2가지 측면이 있습니다. 발신자가 전송할 메시지를 결정하면 발신자의 개인 키를 사용하여 메시지가 서명되고 수신자의 인증서를 사용하여 암호화되며 해시를 사용하여 메시지 무결성이 계산됩니다. 서명되고 암호화된 이 메시지는 유선을 통해 수신자에게 전송됩니다. 메시지가 수신되면 수신자의 개인 키를 사용하여 복호화되고 발신자의 공개 키를 사용하여 검증되고 처리됩니다. 서명된 메시지 처리 통지(MDN)를 요청한 경우 메시지의 성공적인 배달을 확인하는 MDN이 발신자에게 다시 전송됩니다. AS2의 메시지 전송 처리 방법에 대한 설명서를 참조하세요.\"\n",
            "메시지 전송에 사용할 수 있는 옵션은 무엇인가요?\n",
            "가능한 옵션 조합은 발신자의 관점에서 결정됩니다. 발신자는 데이터 암호화 또는 서명만 선택하거나 둘 다를 선택하고 메시지 처리 통지(MDN) 요청을 선택할 수 있습니다. 발신자가 MDN 요청을 선택하는 경우에는 서명되거나 서명되지 않은 MDN을 요청할 수 있습니다. 수신자는 이러한 옵션을 따르게 됩니다.\n",
            "\"category : Transfer for SFTP, question : 메시지 전송에 사용할 수 있는 옵션은 무엇인가요?, answer : 가능한 옵션 조합은 발신자의 관점에서 결정됩니다. 발신자는 데이터 암호화 또는 서명만 선택하거나 둘 다를 선택하고 메시지 처리 통지(MDN) 요청을 선택할 수 있습니다. 발신자가 MDN 요청을 선택하는 경우에는 서명되거나 서명되지 않은 MDN을 요청할 수 있습니다. 수신자는 이러한 옵션을 따르게 됩니다.\"\n",
            "메시지 처리 통지(MDN) 요청은 선택 사항인가요?\n",
            "예. 발신자는 MDN 요청을 선택하고 서명되거나 서명되지 않은 MDN을 요청할 수 있으며 MDN 서명에 사용할 서명 알고리즘을 선택할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 메시지 처리 통지(MDN) 요청은 선택 사항인가요?, answer : 예. 발신자는 MDN 요청을 선택하고 서명되거나 서명되지 않은 MDN을 요청할 수 있으며 MDN 서명에 사용할 서명 알고리즘을 선택할 수 있습니다.\"\n",
            "동기식 및 비동기식 MDN을 지원하나요? 각 옵션을 사용해야 하는 경우는 언제인가요?\n",
            "현재, 동기식 및 비동기식 MDN 응답을 모두 지원합니다. 따라서 AS2 메시지를 받은 후 동기식 또는 비동기식 MDN을 통해 거래 파트너에게 응답할 수 있습니다. 동기식 MDN은 메시지와 동일한 연결 채널을 통해 전송되므로 훨씬 간단하고 그래서 권장되는 옵션입니다. MDN을 전송하기 전에 메시지를 처리할 시간이 더 필요하다면 비동기식 MDN을 사용하는 것이 편리합니다. 거래 파트너에게 메시지를 보낼 때 비동기식 MDN을 요청하고 수신해야 하는 경우 AWS Support 또는 담당 Account Manager를 통해 문의하세요.\n",
            "\"category : Transfer for SFTP, question : 동기식 및 비동기식 MDN을 지원하나요? 각 옵션을 사용해야 하는 경우는 언제인가요?, answer : 현재, 동기식 및 비동기식 MDN 응답을 모두 지원합니다. 따라서 AS2 메시지를 받은 후 동기식 또는 비동기식 MDN을 통해 거래 파트너에게 응답할 수 있습니다. 동기식 MDN은 메시지와 동일한 연결 채널을 통해 전송되므로 훨씬 간단하고 그래서 권장되는 옵션입니다. MDN을 전송하기 전에 메시지를 처리할 시간이 더 필요하다면 비동기식 MDN을 사용하는 것이 편리합니다. 거래 파트너에게 메시지를 보낼 때 비동기식 MDN을 요청하고 수신해야 하는 경우 AWS Support 또는 담당 Account Manager를 통해 문의하세요.\"\n",
            "전송 및 수신된 페이로드와 MDN을 추적하고 검색하려면 어떻게 해야 하나요?\n",
            "AWS Transfer Family는 교환된 페이로드와 MDN에서 핵심 AS2 정보를 추출하고 사용자의 Amazon S3 버킷에 JSON 파일로 저장합니다. 사용자는 S3 Select 또는 Amazon Athena를 사용하여 이 JSON 파일을 쿼리하거나 분석을 위해 Amazon OpenSearch 또는 Amazon DocumentDB를 사용하여 파일을 인덱싱할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 전송 및 수신된 페이로드와 MDN을 추적하고 검색하려면 어떻게 해야 하나요?, answer : AWS Transfer Family는 교환된 페이로드와 MDN에서 핵심 AS2 정보를 추출하고 사용자의 Amazon S3 버킷에 JSON 파일로 저장합니다. 사용자는 S3 Select 또는 Amazon Athena를 사용하여 이 JSON 파일을 쿼리하거나 분석을 위해 Amazon OpenSearch 또는 Amazon DocumentDB를 사용하여 파일을 인덱싱할 수 있습니다.\"\n",
            "MDN을 요청한 발신자는 수신된 MDN을 아카이빙할 수 있나요?\n",
            "예. 거래 파트너로부터 MDN이 수신된 후에는 발신자의 인증서를 사용하여 MDN이 검증되고 발신자의 Amazon S3 버킷에 메시지가 저장됩니다. 발신자는 S3 수명 주기 정책을 활용하여 메시지를 아카이빙하도록 선택할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : MDN을 요청한 발신자는 수신된 MDN을 아카이빙할 수 있나요?, answer : 예. 거래 파트너로부터 MDN이 수신된 후에는 발신자의 인증서를 사용하여 MDN이 검증되고 발신자의 Amazon S3 버킷에 메시지가 저장됩니다. 발신자는 S3 수명 주기 정책을 활용하여 메시지를 아카이빙하도록 선택할 수 있습니다.\"\n",
            "거래 파트너의 엔드포인트로 메시지를 배달할 준비가 되었음을 AWS Transfer Family에 알리려면 어떻게 해야 하나요?\n",
            "데이터를 전송할 준비가 되면 수신자의 AS2 서버 정보가 포함된 AS2 커넥터를 사용하여 StartFileTransfer API를 간접적으로 호출해야 합니다. 이렇게 하면 거래 파트너의 서버로 메시지를 보내라는 알림이 서비스에 전송됩니다. AS2를 통해 거래 파트너에게 메시지를 전송하는 커넥터에 대한 설명서를 참조하세요.\n",
            "\"category : Transfer for SFTP, question : 거래 파트너의 엔드포인트로 메시지를 배달할 준비가 되었음을 AWS Transfer Family에 알리려면 어떻게 해야 하나요?, answer : 데이터를 전송할 준비가 되면 수신자의 AS2 서버 정보가 포함된 AS2 커넥터를 사용하여 StartFileTransfer API를 간접적으로 호출해야 합니다. 이렇게 하면 거래 파트너의 서버로 메시지를 보내라는 알림이 서비스에 전송됩니다. AS2를 통해 거래 파트너에게 메시지를 전송하는 커넥터에 대한 설명서를 참조하세요.\"\n",
            "서로 다른 인바운드 및 아웃바운드 위치를 메시지에 사용하도록 각 거래 파트너를 격리할 수 있나요?\n",
            "예. 거래 파트너의 프로필을 설정할 때 각각 다른 폴더를 사용할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 서로 다른 인바운드 및 아웃바운드 위치를 메시지에 사용하도록 각 거래 파트너를 격리할 수 있나요?, answer : 예. 거래 파트너의 프로필을 설정할 때 각각 다른 폴더를 사용할 수 있습니다.\"\n",
            "거래 파트너의 기존 키와 인증서를 내 AWS Transfer Family AS2 엔드포인트에서 사용할 수 있나요?\n",
            "예. 파트너의 기존 키와 인증서를 가져와서 갱신 및 교체를 관리할 수 있습니다. 인증서 가져오기에 대한 설명서를 참조하세요.\n",
            "\"category : Transfer for SFTP, question : 거래 파트너의 기존 키와 인증서를 내 AWS Transfer Family AS2 엔드포인트에서 사용할 수 있나요?, answer : 예. 파트너의 기존 키와 인증서를 가져와서 갱신 및 교체를 관리할 수 있습니다. 인증서 가져오기에 대한 설명서를 참조하세요.\"\n",
            "거래 파트너의 인증서가 만료되는 시기를 확인하려면 어떻게 해야 하나요?\n",
            "AWS Transfer Family 콘솔을 사용하여 만료 날짜별로 정렬된 인증서 대시보드를 볼 수 있습니다. 또한 인증서 만료 전에 알림을 수신하도록 선택하면 충분한 시간 안에 인증서를 교체하여 운영 중단을 방지할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 거래 파트너의 인증서가 만료되는 시기를 확인하려면 어떻게 해야 하나요?, answer : AWS Transfer Family 콘솔을 사용하여 만료 날짜별로 정렬된 인증서 대시보드를 볼 수 있습니다. 또한 인증서 만료 전에 알림을 수신하도록 선택하면 충분한 시간 안에 인증서를 교체하여 운영 중단을 방지할 수 있습니다.\"\n",
            "고정 IP 주소를 사용하여 거래 파트너의 AS2 서버에 연결할 수 있나요?\n",
            "예. 고정 IP 주소는 기본적으로 커넥터와 연결되어 있어 거래 파트너의 AS2 서버에 연결을 허용 목록에 추가하는 데 사용할 수 있습니다. AWS Transfer Family 콘솔의 커넥터 세부 정보 페이지로 이동하거나 DescribeConnector API/CLI/CDK 명령을 사용하여 커넥터와 연결된 고정 IP 주소를 식별할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 고정 IP 주소를 사용하여 거래 파트너의 AS2 서버에 연결할 수 있나요?, answer : 예. 고정 IP 주소는 기본적으로 커넥터와 연결되어 있어 거래 파트너의 AS2 서버에 연결을 허용 목록에 추가하는 데 사용할 수 있습니다. AWS Transfer Family 콘솔의 커넥터 세부 정보 페이지로 이동하거나 DescribeConnector API/CLI/CDK 명령을 사용하여 커넥터와 연결된 고정 IP 주소를 식별할 수 있습니다.\"\n",
            "사용자 이름 및 암호 보안 인증 정보를 사용하여 인증해야 하는 거래 파트너의 AS2 호스트에 연결할 수 있나요?\n",
            "예. 기본 인증을 사용하여 거래 파트너의 AS2 서버에 연결하는 기능을 지원합니다. AS2 커넥터의 기본 인증 구성에 대한 설명서를 참조하세요.\n",
            "\"category : Transfer for SFTP, question : 사용자 이름 및 암호 보안 인증 정보를 사용하여 인증해야 하는 거래 파트너의 AS2 호스트에 연결할 수 있나요?, answer : 예. 기본 인증을 사용하여 거래 파트너의 AS2 서버에 연결하는 기능을 지원합니다. AS2 커넥터의 기본 인증 구성에 대한 설명서를 참조하세요.\"\n",
            "고정 IP 주소를 사용하여 거래 파트너 AS2 서버에 메시지를 보낼 수 있나요?\n",
            "예. AS2 커넥터는 원격 AS2 서버로 메시지를 보낼 때와 비동기식 메시지 처리 알림(MDN) 응답을 반환할 때 고정 IP 주소를 사용합니다. AWS Transfer Family Management Console의 커넥터 또는 서버 세부 정보 페이지로 이동하거나 DescribeConnector 또는 DescribeServer API/CLI/CDK 명령을 사용하면 커넥터와 연결된 고정 IP 주소를 파악할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 고정 IP 주소를 사용하여 거래 파트너 AS2 서버에 메시지를 보낼 수 있나요?, answer : 예. AS2 커넥터는 원격 AS2 서버로 메시지를 보낼 때와 비동기식 메시지 처리 알림(MDN) 응답을 반환할 때 고정 IP 주소를 사용합니다. AWS Transfer Family Management Console의 커넥터 또는 서버 세부 정보 페이지로 이동하거나 DescribeConnector 또는 DescribeServer API/CLI/CDK 명령을 사용하면 커넥터와 연결된 고정 IP 주소를 파악할 수 있습니다.\"\n",
            "고정 IP 지원 엔드포인트를 통해 거래 파트너로부터 AS2 메시지를 받을 수 있나요?\n",
            "예. AS2 서버 엔드포인트는 인터넷에 연결된 VPC 호스팅 엔드포인트가 있는 보안 그룹을 사용하여 IP 허용 목록 제어를 구성하는 것을 지원합니다.\n",
            "\"category : Transfer for SFTP, question : 고정 IP 지원 엔드포인트를 통해 거래 파트너로부터 AS2 메시지를 받을 수 있나요?, answer : 예. AS2 서버 엔드포인트는 인터넷에 연결된 VPC 호스팅 엔드포인트가 있는 보안 그룹을 사용하여 IP 허용 목록 제어를 구성하는 것을 지원합니다.\"\n",
            "AS2 비동기 MDN 응답은 고정 IP 주소를 사용하나요?\n",
            "예. AS2 비동기 MDN 응답은 고정 IP 주소를 사용합니다. AWS Transfer Family 관리 콘솔의 서버 세부 정보 페이지로 이동하거나 DescribeServer API/CLI/CDK 명령을 사용하여 비동기 MDN 응답을 전송하는 데 사용되는 고정 IP 주소를 식별할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : AS2 비동기 MDN 응답은 고정 IP 주소를 사용하나요?, answer : 예. AS2 비동기 MDN 응답은 고정 IP 주소를 사용합니다. AWS Transfer Family 관리 콘솔의 서버 세부 정보 페이지로 이동하거나 DescribeServer API/CLI/CDK 명령을 사용하여 비동기 MDN 응답을 전송하는 데 사용되는 고정 IP 주소를 식별할 수 있습니다.\"\n",
            "거래 파트너로부터 받은 AS2 메시지의 처리를 오케스트레이션하려면 어떻게 해야 하나요?\n",
            "수신한 모든 AS2 메시지에 대한 이벤트는 Amazon EventBridge의 기본 이벤트 버스에 게시됩니다. 이러한 이벤트를 구독하고 Amazon EventBridge 또는 기타 워크플로 오케스트레이션 서비스를 사용하여 수신한 메시지의 이벤트 기반 처리를 오케스트레이션할 수 있습니다. 예를 들어 이러한 이벤트를 사용하여 수신 메시지를 S3의 다른 위치로 복사하거나, 사용자 지정 Lambda를 사용하여 메시지 콘텐츠에서 맬웨어를 검사하거나, Amazon CloudSearch와 같은 서비스에서 인덱싱하고 검색할 수 있도록 콘텐츠를 기반으로 메시지를 태깅할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 거래 파트너로부터 받은 AS2 메시지의 처리를 오케스트레이션하려면 어떻게 해야 하나요?, answer : 수신한 모든 AS2 메시지에 대한 이벤트는 Amazon EventBridge의 기본 이벤트 버스에 게시됩니다. 이러한 이벤트를 구독하고 Amazon EventBridge 또는 기타 워크플로 오케스트레이션 서비스를 사용하여 수신한 메시지의 이벤트 기반 처리를 오케스트레이션할 수 있습니다. 예를 들어 이러한 이벤트를 사용하여 수신 메시지를 S3의 다른 위치로 복사하거나, 사용자 지정 Lambda를 사용하여 메시지 콘텐츠에서 맬웨어를 검사하거나, Amazon CloudSearch와 같은 서비스에서 인덱싱하고 검색할 수 있도록 콘텐츠를 기반으로 메시지를 태깅할 수 있습니다.\"\n",
            "AWS B2B Data Interchange를 사용하여 인바운드 AS2 메시지의 EDI 콘텐츠를 자동으로 변환할 수 있나요?\n",
            "예. AWS B2B Data Interchange를 사용하여 인바운드 AS2 메시지의 X12 EDI 콘텐츠를 JSON 및 XML과 같은 일반적인 데이터 표현으로 자동 변환할 수 있습니다. 그렇게 하려면 AWS Transfer Family의 AS2 Payload Receive Completed(AS2 페이로드 수신 완료) 이벤트의 이벤트 패턴과 일치하는 Amazon EventBridge 규칙을 생성하고 AWS B2B Data Interchange의 StartTransformerJob API를 규칙의 범용 대상으로 지정해야 합니다. AWS B2B Data Interchange를 사용하여 인바운드 AS2 메시지의 X12 EDI 콘텐츠를 변환하면 다운스트림 비즈니스 애플리케이션 및 시스템으로의 EDI 데이터 통합을 자동화하고 가속화할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : AWS B2B Data Interchange를 사용하여 인바운드 AS2 메시지의 EDI 콘텐츠를 자동으로 변환할 수 있나요?, answer : 예. AWS B2B Data Interchange를 사용하여 인바운드 AS2 메시지의 X12 EDI 콘텐츠를 JSON 및 XML과 같은 일반적인 데이터 표현으로 자동 변환할 수 있습니다. 그렇게 하려면 AWS Transfer Family의 AS2 Payload Receive Completed(AS2 페이로드 수신 완료) 이벤트의 이벤트 패턴과 일치하는 Amazon EventBridge 규칙을 생성하고 AWS B2B Data Interchange의 StartTransformerJob API를 규칙의 범용 대상으로 지정해야 합니다. AWS B2B Data Interchange를 사용하여 인바운드 AS2 메시지의 X12 EDI 콘텐츠를 변환하면 다운스트림 비즈니스 애플리케이션 및 시스템으로의 EDI 데이터 통합을 자동화하고 가속화할 수 있습니다.\"\n",
            "거래 파트너에게 전송되는 AS2 메시지 전송을 자동화하려면 어떻게 해야 하나요?\n",
            "Amazon EventBridge 스케줄러로 일정을 잡거나 Amazon EventBridge 규칙을 사용하여 메시지를 트리거하여 AS2 메시지 전송을 자동화할 수 있습니다. AS2 메시지 전송을 위한 자동화된 시간 기반 워크플로를 생성하려면 EventBridge의 스케줄러를 사용하여 비즈니스 요구 사항에 적합한 일정을 생성하고 AWS Transfer Family의 StartFileTransfer API를 일정의 범용 대상으로 지정합니다. AS2 메시지 전송을 위한 자동화된 이벤트 기반 워크플로를 생성하려면 EventBridge에 게시된 이벤트와 일치하는 Amazon EventBridge 규칙을 생성하고 AWS Transfer Family의 StartFileTransfer API를 규칙의 범용 대상으로 지정합니다.\n",
            "\"category : Transfer for SFTP, question : 거래 파트너에게 전송되는 AS2 메시지 전송을 자동화하려면 어떻게 해야 하나요?, answer : Amazon EventBridge 스케줄러로 일정을 잡거나 Amazon EventBridge 규칙을 사용하여 메시지를 트리거하여 AS2 메시지 전송을 자동화할 수 있습니다. AS2 메시지 전송을 위한 자동화된 시간 기반 워크플로를 생성하려면 EventBridge의 스케줄러를 사용하여 비즈니스 요구 사항에 적합한 일정을 생성하고 AWS Transfer Family의 StartFileTransfer API를 일정의 범용 대상으로 지정합니다. AS2 메시지 전송을 위한 자동화된 이벤트 기반 워크플로를 생성하려면 EventBridge에 게시된 이벤트와 일치하는 Amazon EventBridge 규칙을 생성하고 AWS Transfer Family의 StartFileTransfer API를 규칙의 범용 대상으로 지정합니다.\"\n",
            "내 거래에 전송된 AS2 메시지 또는 MDN 응답을 아카이빙할 수 있나요?\n",
            "예. 전송된 모든 AS2 메시지 및 MDN에 대한 이벤트는 Amazon EventBridge의 기본 이벤트 버스에 게시됩니다. 이러한 이벤트를 구독하고 이를 사용하여 거래 파트너에게 성공적으로 전송된 AS2 메시지 및 MDN을 삭제하거나 아카이브할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 내 거래에 전송된 AS2 메시지 또는 MDN 응답을 아카이빙할 수 있나요?, answer : 예. 전송된 모든 AS2 메시지 및 MDN에 대한 이벤트는 Amazon EventBridge의 기본 이벤트 버스에 게시됩니다. 이러한 이벤트를 구독하고 이를 사용하여 거래 파트너에게 성공적으로 전송된 AS2 메시지 및 MDN을 삭제하거나 아카이브할 수 있습니다.\"\n",
            "아웃바운드 AS2 메시지 전송에 실패하거나 인바운드 AS2 메시지 처리에 실패할 경우 알림을 받을 수 있나요?\n",
            "예. AWS Transfer Family는 성공 또는 실패한 모든 AS2 메시지 또는 송수신한 MDN에 대한 이벤트를 Amazon EventBridge에 게시합니다. 이러한 이벤트는 Amazon EventBridge의 기본 이벤트 버스에 게시되며, 여기서 Amazon SNS와 같은 서비스를 사용하여 자신 또는 파트너에게 이메일 알림을 트리거하는 데 사용할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 아웃바운드 AS2 메시지 전송에 실패하거나 인바운드 AS2 메시지 처리에 실패할 경우 알림을 받을 수 있나요?, answer : 예. AWS Transfer Family는 성공 또는 실패한 모든 AS2 메시지 또는 송수신한 MDN에 대한 이벤트를 Amazon EventBridge에 게시합니다. 이러한 이벤트는 Amazon EventBridge의 기본 이벤트 버스에 게시되며, 여기서 Amazon SNS와 같은 서비스를 사용하여 자신 또는 파트너에게 이메일 알림을 트리거하는 데 사용할 수 있습니다.\"\n",
            "Transfer Family의 관리형 워크플로를 사용하면 거래 파트너로부터 AS2를 통해 받은 메시지를 처리할 수 있나요?\n",
            "아니요. 현재 AS2 엔드포인트에는 관리형 워크플로가 지원되지 않습니다. Amazon EventBridge에 게시된 Transfer Family의 이벤트 알림을 사용하여 AS2 메시지 처리를 오케스트레이션하는 것이 좋습니다. 자세한 내용은 파일 처리 자동화 섹션을 참조하세요.\n",
            "\"category : Transfer for SFTP, question : Transfer Family의 관리형 워크플로를 사용하면 거래 파트너로부터 AS2를 통해 받은 메시지를 처리할 수 있나요?, answer : 아니요. 현재 AS2 엔드포인트에는 관리형 워크플로가 지원되지 않습니다. Amazon EventBridge에 게시된 Transfer Family의 이벤트 알림을 사용하여 AS2 메시지 처리를 오케스트레이션하는 것이 좋습니다. 자세한 내용은 파일 처리 자동화 섹션을 참조하세요.\"\n",
            "AS3 및 AS4를 지원하나요?\n",
            "아니요. AWS Transfer Family는 현재 AS3 또는 AS4를 지원하지 않습니다.\n",
            "\"category : Transfer for SFTP, question : AS3 및 AS4를 지원하나요?, answer : 아니요. AWS Transfer Family는 현재 AS3 또는 AS4를 지원하지 않습니다.\"\n",
            "AWS Transfer Family를 사용하여 전송한 파일의 처리를 자동화하려면 어떤 옵션이 있나요?\n",
            "두 가지 옵션이 있습니다. 1) AWS Transfer Family는 SFTP, AS2, FTPS 및 FTP를 통해 전송된 파일에 대한 파일 전송 이벤트 알림을 Amazon EventBridge에 게시합니다. 이 이벤트를 사용하면 EventBridge 이벤트와 통합되는 모든 서비스를 사용하여 파일 처리를 트리거할 수 있습니다. 2) AWS Transfer Family에서 제공하는 관리형 워크플로에서 사전 구축된 파일 처리 단계를 사용하여 SFTP, FTPS 및 FTP 서버 엔드포인트를 통해 업로드된 파일의 업로드 후처리를 자동으로 실행할 수 있습니다. 관리형 워크플로를 서버 엔드포인트에 연결하면 해당 엔드포인트에 업로드된 모든 파일이 동일한 워크플로 단계를 사용하여 처리됩니다.\n",
            "\"category : Transfer for SFTP, question : AWS Transfer Family를 사용하여 전송한 파일의 처리를 자동화하려면 어떤 옵션이 있나요?, answer : 두 가지 옵션이 있습니다. 1) AWS Transfer Family는 SFTP, AS2, FTPS 및 FTP를 통해 전송된 파일에 대한 파일 전송 이벤트 알림을 Amazon EventBridge에 게시합니다. 이 이벤트를 사용하면 EventBridge 이벤트와 통합되는 모든 서비스를 사용하여 파일 처리를 트리거할 수 있습니다. 2) AWS Transfer Family에서 제공하는 관리형 워크플로에서 사전 구축된 파일 처리 단계를 사용하여 SFTP, FTPS 및 FTP 서버 엔드포인트를 통해 업로드된 파일의 업로드 후처리를 자동으로 실행할 수 있습니다. 관리형 워크플로를 서버 엔드포인트에 연결하면 해당 엔드포인트에 업로드된 모든 파일이 동일한 워크플로 단계를 사용하여 처리됩니다.\"\n",
            "Amazon EventBridge에 이벤트 알림을 게시하는 Transfer Family 작업은 무엇인가요?\n",
            "AWS Transfer Family는 서버 및 커넥터 리소스 모두에 대한 각 파일 전송 작업이 성공하거나 실패할 시 Amazon EventBridge에 이벤트 알림을 게시합니다. Amazon EventBridge에 게시되는 Transfer Family 이벤트에 대한 자세한 내용은 설명서를 참조하세요.\n",
            "\"category : Transfer for SFTP, question : Amazon EventBridge에 이벤트 알림을 게시하는 Transfer Family 작업은 무엇인가요?, answer : AWS Transfer Family는 서버 및 커넥터 리소스 모두에 대한 각 파일 전송 작업이 성공하거나 실패할 시 Amazon EventBridge에 이벤트 알림을 게시합니다. Amazon EventBridge에 게시되는 Transfer Family 이벤트에 대한 자세한 내용은 설명서를 참조하세요.\"\n",
            "업로드 후처리에 대한 관리형 워크플로는 무엇인가요?\n",
            "AWS Transfer Family 관리형 워크플로는 SFTP, FTPS 및 FTP 서버 엔드포인트를 통해 업로드된 파일의 처리를 위한 선형의 순차적 단계를 생성, 실행 및 모니터링할 수 있도록 사전 구축된 프레임워크를 제공합니다. 이 기능을 사용하면 사전 구축된 단계를 통해 파일 복사, 태깅, 복호화와 같은 일반적인 파일 처리 태스크를 실행할 수 있으므로 시간이 절약됩니다. 또한 파일을 검사하여 PII, 바이러스 및 맬웨어 또는 잘못된 파일 형식 또는 유형과 같은 기타 오류를 찾는 태스크에 Lambda 함수를 사용하여 파일 처리를 사용자 지정함으로써 이상 징후를 신속하게 탐지하고 규정 준수 요구 사항을 충족할 수 있습니다. 관리형 워크플로를 서버 엔드포인트에 연결하면 해당 엔드포인트에 업로드된 모든 파일이 동일한 워크플로 단계를 사용하여 처리됩니다.\n",
            "\"category : Transfer for SFTP, question : 업로드 후처리에 대한 관리형 워크플로는 무엇인가요?, answer : AWS Transfer Family 관리형 워크플로는 SFTP, FTPS 및 FTP 서버 엔드포인트를 통해 업로드된 파일의 처리를 위한 선형의 순차적 단계를 생성, 실행 및 모니터링할 수 있도록 사전 구축된 프레임워크를 제공합니다. 이 기능을 사용하면 사전 구축된 단계를 통해 파일 복사, 태깅, 복호화와 같은 일반적인 파일 처리 태스크를 실행할 수 있으므로 시간이 절약됩니다. 또한 파일을 검사하여 PII, 바이러스 및 맬웨어 또는 잘못된 파일 형식 또는 유형과 같은 기타 오류를 찾는 태스크에 Lambda 함수를 사용하여 파일 처리를 사용자 지정함으로써 이상 징후를 신속하게 탐지하고 규정 준수 요구 사항을 충족할 수 있습니다. 관리형 워크플로를 서버 엔드포인트에 연결하면 해당 엔드포인트에 업로드된 모든 파일이 동일한 워크플로 단계를 사용하여 처리됩니다.\"\n",
            "관리형 워크플로가 필요한 이유는 무엇인가요?\n",
            "비즈니스 파트너와 교환하는 파일을 처리해야 하는 경우 사용자 지정 코드를 실행할 인프라를 설정하고, 런타임 오류 및 이상을 지속적으로 모니터링하며, 데이터에 대한 모든 변경 사항과 변환을 감사 및 로깅해야 합니다. 또한 오류 시나리오를 기술적, 비즈니스 측면에서 모두 고려하는 동시에, 유사 시 안전 모드가 적절히 트리거되도록 보장해야 합니다. 추적성이 필요한 경우에는 시스템의 여러 구성 요소를 따라 이동하는 데이터의 계보를 추적해야 합니다. 파일 처리 워크플로의 개별 구성 요소를 유지하고 관리하다 보면 비즈니스를 위한 작업을 차별화하는 데 집중할 시간이 부족해집니다. 관리형 워크플로는 여러 태스크를 관리하는 데서 오는 복잡성을 제거하고 표준화된 파일 처리 솔루션을 제공합니다. 이 표준화된 파일 처리 솔루션을 조직 전체에 복제하고 각 단계에 대한 예외 처리 및 파일 추적성을 위한 기본 제공 기능을 활용하여 비즈니스 및 법적 요구 사항을 충족할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 관리형 워크플로가 필요한 이유는 무엇인가요?, answer : 비즈니스 파트너와 교환하는 파일을 처리해야 하는 경우 사용자 지정 코드를 실행할 인프라를 설정하고, 런타임 오류 및 이상을 지속적으로 모니터링하며, 데이터에 대한 모든 변경 사항과 변환을 감사 및 로깅해야 합니다. 또한 오류 시나리오를 기술적, 비즈니스 측면에서 모두 고려하는 동시에, 유사 시 안전 모드가 적절히 트리거되도록 보장해야 합니다. 추적성이 필요한 경우에는 시스템의 여러 구성 요소를 따라 이동하는 데이터의 계보를 추적해야 합니다. 파일 처리 워크플로의 개별 구성 요소를 유지하고 관리하다 보면 비즈니스를 위한 작업을 차별화하는 데 집중할 시간이 부족해집니다. 관리형 워크플로는 여러 태스크를 관리하는 데서 오는 복잡성을 제거하고 표준화된 파일 처리 솔루션을 제공합니다. 이 표준화된 파일 처리 솔루션을 조직 전체에 복제하고 각 단계에 대한 예외 처리 및 파일 추적성을 위한 기본 제공 기능을 활용하여 비즈니스 및 법적 요구 사항을 충족할 수 있습니다.\"\n",
            "관리형 워크플로를 사용하면 어떤 이점이 있나요?\n",
            "관리형 워크플로를 사용하면 업로드된 파일을 사용자별 폴더로 이동하고, PGP 키를 사용하여 파일을 복호화하고, 맬웨어를 검사하고 태깅하는 것과 같은 선형의 순차적 파일 처리 태스크를 서버 엔드포인트에 업로드된 모든 파일에 실행하여 다운스트림 애플리케이션에서 데이터를 사용하기 전에 데이터를 손쉽게 사전 처리할 수 있습니다. 코드형 인프라(IaC)를 사용하여 워크플로를 배포할 수 있으므로 일반적인 업로드 후 파일 처리 태스크를 조직의 여러 사업부에 걸쳐 빠르게 복제하고 표준화할 수 있습니다. 완전히 업로드된 파일에서만 트리거되는 서버 엔드포인트에 관리형 워크플로를 연결하고, 부분적으로 업로드된 파일에 대해서만 트리거되어 불완전한 업로드를 처리하는 별도의 관리형 워크플로를 연결하여 세부적으로 제어할 수 있습니다. 워크플로의 기본 제공 예외 처리를 사용할 수도 있습니다. 이 예외 처리를 사용하면 워크플로 실행에서 오류 또는 예외가 발생할 경우 파일 처리 결과에 손쉽게 대응하여 비즈니스 및 기술 SLA를 유지할 수 있습니다. 워크플로의 각 파일 처리 단계에서는 세부 로그도 생성되며, 이를 감사하여 데이터 계보를 추적할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 관리형 워크플로를 사용하면 어떤 이점이 있나요?, answer : 관리형 워크플로를 사용하면 업로드된 파일을 사용자별 폴더로 이동하고, PGP 키를 사용하여 파일을 복호화하고, 맬웨어를 검사하고 태깅하는 것과 같은 선형의 순차적 파일 처리 태스크를 서버 엔드포인트에 업로드된 모든 파일에 실행하여 다운스트림 애플리케이션에서 데이터를 사용하기 전에 데이터를 손쉽게 사전 처리할 수 있습니다. 코드형 인프라(IaC)를 사용하여 워크플로를 배포할 수 있으므로 일반적인 업로드 후 파일 처리 태스크를 조직의 여러 사업부에 걸쳐 빠르게 복제하고 표준화할 수 있습니다. 완전히 업로드된 파일에서만 트리거되는 서버 엔드포인트에 관리형 워크플로를 연결하고, 부분적으로 업로드된 파일에 대해서만 트리거되어 불완전한 업로드를 처리하는 별도의 관리형 워크플로를 연결하여 세부적으로 제어할 수 있습니다. 워크플로의 기본 제공 예외 처리를 사용할 수도 있습니다. 이 예외 처리를 사용하면 워크플로 실행에서 오류 또는 예외가 발생할 경우 파일 처리 결과에 손쉽게 대응하여 비즈니스 및 기술 SLA를 유지할 수 있습니다. 워크플로의 각 파일 처리 단계에서는 세부 로그도 생성되며, 이를 감사하여 데이터 계보를 추적할 수 있습니다.\"\n",
            "전송된 파일의 처리를 오케스트레이션하기 위해 Amazon EventBridge를 사용해야 하는 경우는 언제이고, AWS Transfer Family 관리형 워크플로를 사용해야 하는 경우는 언제인가요?\n",
            "AWS Transfer Family 서버 엔드포인트 및 커넥터는 파일 전송 작업이 완료되면 파일 위치, 발신자의 사용자 이름, 서버 ID 또는 커넥터 ID, 전송 상태 등과 같은 운영 정보와 함께 이벤트 알림을 Amazon EventBridge에 자동으로 게시합니다. 파일 소스를 기반으로 조건부 로직을 사용하는 것처럼 파일 처리를 정의할 때 세밀한 제어가 필요하거나 다른 AWS 서비스, 서드 파티 애플리케이션 및 자체 애플리케이션과 통합할 이벤트 기반 아키텍처를 구축해야 하는 경우 이러한 이벤트를 사용할 수 있습니다. 반면, AWS Transfer Family 관리형 워크플로는 SFTP, FTPS 및 FTP 서버 엔드포인트를 통해 업로드된 모든 파일에 적용되는 일반적인 파일 처리 단계를 순서대로 정의할 수 있는 사전 구축된 프레임워크를 제공합니다. 업로드된 모든 파일을 동일한 일반 파일 처리 단계를 사용하여 처리해야 하는 경우 세분화된 로직 또는 조건부 로직을 적용할 필요 없이 관리형 워크플로를 엔드포인트에 연결할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 전송된 파일의 처리를 오케스트레이션하기 위해 Amazon EventBridge를 사용해야 하는 경우는 언제이고, AWS Transfer Family 관리형 워크플로를 사용해야 하는 경우는 언제인가요?, answer : AWS Transfer Family 서버 엔드포인트 및 커넥터는 파일 전송 작업이 완료되면 파일 위치, 발신자의 사용자 이름, 서버 ID 또는 커넥터 ID, 전송 상태 등과 같은 운영 정보와 함께 이벤트 알림을 Amazon EventBridge에 자동으로 게시합니다. 파일 소스를 기반으로 조건부 로직을 사용하는 것처럼 파일 처리를 정의할 때 세밀한 제어가 필요하거나 다른 AWS 서비스, 서드 파티 애플리케이션 및 자체 애플리케이션과 통합할 이벤트 기반 아키텍처를 구축해야 하는 경우 이러한 이벤트를 사용할 수 있습니다. 반면, AWS Transfer Family 관리형 워크플로는 SFTP, FTPS 및 FTP 서버 엔드포인트를 통해 업로드된 모든 파일에 적용되는 일반적인 파일 처리 단계를 순서대로 정의할 수 있는 사전 구축된 프레임워크를 제공합니다. 업로드된 모든 파일을 동일한 일반 파일 처리 단계를 사용하여 처리해야 하는 경우 세분화된 로직 또는 조건부 로직을 적용할 필요 없이 관리형 워크플로를 엔드포인트에 연결할 수 있습니다.\"\n",
            "관리형 워크플로를 시작하려면 어떻게 해야 하나요?\n",
            "먼저, 복사 및 태그 지정과 같은 작업 및 요구 사항에 기반한 순차적 단계에서 사용자 지정 단계를 포함할 수 있는 일련의 작업을 포함하도록 워크플로를 설정합니다. 그런 다음, 서버에 워크플로를 매핑합니다. 그러면 파일이 도착할 때 이 워크플로에 지정된 작업이 실시간으로 평가되고 트리거됩니다. 자세히 알아보려면 설명서를 참조하거나 관리형 워크플로 시작에 대한 이 데모를 시청하거나 이 블로그 게시물을 사용하여 클라우드 네이티브 파일 전송 플랫폼을 배포해보세요.\n",
            "\"category : Transfer for SFTP, question : 관리형 워크플로를 시작하려면 어떻게 해야 하나요?, answer : 먼저, 복사 및 태그 지정과 같은 작업 및 요구 사항에 기반한 순차적 단계에서 사용자 지정 단계를 포함할 수 있는 일련의 작업을 포함하도록 워크플로를 설정합니다. 그런 다음, 서버에 워크플로를 매핑합니다. 그러면 파일이 도착할 때 이 워크플로에 지정된 작업이 실시간으로 평가되고 트리거됩니다. 자세히 알아보려면 설명서를 참조하거나 관리형 워크플로 시작에 대한 이 데모를 시청하거나 이 블로그 게시물을 사용하여 클라우드 네이티브 파일 전송 플랫폼을 배포해보세요.\"\n",
            "동일한 관리형 워크플로를 여러 서버에서 사용할 수 있나요?\n",
            "예. 동일한 워크플로를 여러 서버에 연결하여 구성을 더 쉽게 유지 관리하고 표준화할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 동일한 관리형 워크플로를 여러 서버에서 사용할 수 있나요?, answer : 예. 동일한 워크플로를 여러 서버에 연결하여 구성을 더 쉽게 유지 관리하고 표준화할 수 있습니다.\"\n",
            "워크플로를 사용하여 내 파일에서 수행할 수 있는 작업은 무엇인가요?\n",
            "전송 서버가 클라이언트로부터 파일을 수신하면 다음과 같은 일반적인 작업을 사용할 수 있습니다.\n",
            "PGP 키를 사용하여 파일을 복호화합니다. PGP를 사용한 파일 암호화 및 복호화에 대한 이 블로그 게시물을 참조하세요.\n",
            "데이터가 도착한 위치에서 데이터를 이용해야 하는 위치로 데이터를 이동하거나 복사합니다.\n",
            "새 위치로 아카이빙하거나 복사한 후 원본 파일을 삭제합니다.\n",
            "다운스트림 서비스(S3만 해당)에서 인덱싱 및 검색할 수 있도록 해당 콘텐츠에 기반하여 파일에 태그를 지정합니다.\n",
            "워크플로에 사용자 지정 단계로 Lambda 함수를 제공하여 사용자 지정 파일 처리 로직을 수행합니다. 예를 들어 파일 유형에 대한 호환성 검사, 파일에서 멀웨어 검사, 개인 식별 정보(PII) 감지, 데이터 분석을 위한 파일 수집 전 메타데이터 추출과 같은 작업이 이에 포함됩니다.\n",
            "\"category : Transfer for SFTP, question : 워크플로를 사용하여 내 파일에서 수행할 수 있는 작업은 무엇인가요?, answer : 전송 서버가 클라이언트로부터 파일을 수신하면 다음과 같은 일반적인 작업을 사용할 수 있습니다.\n",
            "PGP 키를 사용하여 파일을 복호화합니다. PGP를 사용한 파일 암호화 및 복호화에 대한 이 블로그 게시물을 참조하세요.\n",
            "데이터가 도착한 위치에서 데이터를 이용해야 하는 위치로 데이터를 이동하거나 복사합니다.\n",
            "새 위치로 아카이빙하거나 복사한 후 원본 파일을 삭제합니다.\n",
            "다운스트림 서비스(S3만 해당)에서 인덱싱 및 검색할 수 있도록 해당 콘텐츠에 기반하여 파일에 태그를 지정합니다.\n",
            "워크플로에 사용자 지정 단계로 Lambda 함수를 제공하여 사용자 지정 파일 처리 로직을 수행합니다. 예를 들어 파일 유형에 대한 호환성 검사, 파일에서 멀웨어 검사, 개인 식별 정보(PII) 감지, 데이터 분석을 위한 파일 수집 전 메타데이터 추출과 같은 작업이 이에 포함됩니다.\"\n",
            "각 워크플로 단계에서 처리할 파일을 선택할 수 있나요?\n",
            "예. 원래 업로드한 파일을 처리하거나 이전 워크플로 단계의 출력 파일을 처리하도록 워크플로 단계를 구성할 수 있습니다. 이렇게 하면 Amazon S3에 업로드된 파일의 이동 및 이름 바꾸기를 손쉽게 자동화할 수 있습니다. 예를 들어, 파일 아카이브 또는 보존을 위해 파일을 다른 위치로 이동하려면 워크플로에서 2개 단계를 구성합니다. 첫 번째 단계에서는 다른 Amazon S3 위치에 파일을 복사하고, 두 번째 단계에서는 원래 업로드한 파일을 삭제합니다. 워크플로 단계에서 파일 위치 선택에 대한 자세한 내용은 설명서를 참조하세요.\n",
            "\"category : Transfer for SFTP, question : 각 워크플로 단계에서 처리할 파일을 선택할 수 있나요?, answer : 예. 원래 업로드한 파일을 처리하거나 이전 워크플로 단계의 출력 파일을 처리하도록 워크플로 단계를 구성할 수 있습니다. 이렇게 하면 Amazon S3에 업로드된 파일의 이동 및 이름 바꾸기를 손쉽게 자동화할 수 있습니다. 예를 들어, 파일 아카이브 또는 보존을 위해 파일을 다른 위치로 이동하려면 워크플로에서 2개 단계를 구성합니다. 첫 번째 단계에서는 다른 Amazon S3 위치에 파일을 복사하고, 두 번째 단계에서는 원래 업로드한 파일을 삭제합니다. 워크플로 단계에서 파일 위치 선택에 대한 자세한 내용은 설명서를 참조하세요.\"\n",
            "워크플로에서 PGP를 사용하여 자동으로 파일을 복호화할 수 있나요?\n",
            "예. SFTP, FTPS 및 FTP 서버 엔드포인트를 통해 업로드된 파일의 PGP 복호화에는 사전 구축된 완전관리형 워크플로 단계를 사용할 수 있습니다. 자세한 내용은 관리형 워크플로 설명서와 PGP를 사용한 파일 암호화 및 복호화에 대한 이 블로그 게시물을 참조하세요.\n",
            "\"category : Transfer for SFTP, question : 워크플로에서 PGP를 사용하여 자동으로 파일을 복호화할 수 있나요?, answer : 예. SFTP, FTPS 및 FTP 서버 엔드포인트를 통해 업로드된 파일의 PGP 복호화에는 사전 구축된 완전관리형 워크플로 단계를 사용할 수 있습니다. 자세한 내용은 관리형 워크플로 설명서와 PGP를 사용한 파일 암호화 및 복호화에 대한 이 블로그 게시물을 참조하세요.\"\n",
            "각 관리형 워크플로 단계에서 처리할 파일을 선택하려면 어떤 옵션을 사용해야 하나요?\n",
            "서버 엔드포인트에 원래 업로드한 파일을 처리하거나 워크플로에서 이전 단계의 출력 파일을 처리하도록 워크플로 단계를 구성할 수 있습니다. 이렇게 하면 Amazon S3에 업로드된 파일의 이동 및 이름 바꾸기를 손쉽게 자동화할 수 있습니다. 예를 들어, 파일 아카이브 또는 보존을 위해 파일을 다른 위치로 이동하려면 워크플로에서 2개 단계를 구성합니다. 첫 번째 단계에서는 다른 Amazon S3 위치에 파일을 복사하고, 두 번째 단계에서는 원래 업로드한 파일을 삭제합니다. 워크플로 단계에서 파일 위치 선택에 대한 자세한 내용은 설명서를 참조하세요.\n",
            "\"category : Transfer for SFTP, question : 각 관리형 워크플로 단계에서 처리할 파일을 선택하려면 어떤 옵션을 사용해야 하나요?, answer : 서버 엔드포인트에 원래 업로드한 파일을 처리하거나 워크플로에서 이전 단계의 출력 파일을 처리하도록 워크플로 단계를 구성할 수 있습니다. 이렇게 하면 Amazon S3에 업로드된 파일의 이동 및 이름 바꾸기를 손쉽게 자동화할 수 있습니다. 예를 들어, 파일 아카이브 또는 보존을 위해 파일을 다른 위치로 이동하려면 워크플로에서 2개 단계를 구성합니다. 첫 번째 단계에서는 다른 Amazon S3 위치에 파일을 복사하고, 두 번째 단계에서는 원래 업로드한 파일을 삭제합니다. 워크플로 단계에서 파일 위치 선택에 대한 자세한 내용은 설명서를 참조하세요.\"\n",
            "원래 업로드한 파일을 레코드 보존을 위해 유지할 수 있나요?\n",
            "예. 관리형 워크플로를 사용할 때는 레코드 보존을 위해 원본 파일을 유지하면서 원본 파일의 여러 복사본을 생성할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 원래 업로드한 파일을 레코드 보존을 위해 유지할 수 있나요?, answer : 예. 관리형 워크플로를 사용할 때는 레코드 보존을 위해 원본 파일을 유지하면서 원본 파일의 여러 복사본을 생성할 수 있습니다.\"\n",
            "관리형 워크플로를 사용하여 파일 경로를 사용자별 Amazon S3 폴더로 동적으로 지정할 수 있나요?\n",
            "예. 워크플로 복사 단계에서 사용자 이름을 변수로 활용하여 Amazon S3의 사용자별 폴더로 파일 경로를 동적으로 지정할 수 있습니다. 이렇게 하면 파일을 복사할 때 대상 폴더 위치를 하드코딩할 필요가 없고 Amazon S3에서 사용자별 폴더 생성을 자동화하여 파일 자동화 워크플로를 확장할 수 있습니다. 자세히 알아보려면 설명서를 읽어보세요.\n",
            "\"category : Transfer for SFTP, question : 관리형 워크플로를 사용하여 파일 경로를 사용자별 Amazon S3 폴더로 동적으로 지정할 수 있나요?, answer : 예. 워크플로 복사 단계에서 사용자 이름을 변수로 활용하여 Amazon S3의 사용자별 폴더로 파일 경로를 동적으로 지정할 수 있습니다. 이렇게 하면 파일을 복사할 때 대상 폴더 위치를 하드코딩할 필요가 없고 Amazon S3에서 사용자별 폴더 생성을 자동화하여 파일 자동화 워크플로를 확장할 수 있습니다. 자세히 알아보려면 설명서를 읽어보세요.\"\n",
            "관리형 워크플로 활동을 모니터링하려면 어떻게 해야 하나요?\n",
            "관리형 워크플로 활동 로깅에 지원되는 기능에 대한 자세한 내용은 모니터링 섹션을 참조하세요.\n",
            "\"category : Transfer for SFTP, question : 관리형 워크플로 활동을 모니터링하려면 어떻게 해야 하나요?, answer : 관리형 워크플로 활동 로깅에 지원되는 기능에 대한 자세한 내용은 모니터링 섹션을 참조하세요.\"\n",
            "내 파일 처리 단계를 오케스트레이션하기 위해 AWS Step Functions를 사용하고 있습니다. AWS Transfer Family의 관리형 워크플로는 현재 AWS Step Functions 설정과 어떻게 다릅니까?\n",
            "AWS Step Functions는 간단한 단계에서 비즈니스 애플리케이션의 실행을 정의하기 위해 AWS Lambda를 다른 서비스와 결합할 수 있는 서버리스 오케스트레이션 서비스입니다. AWS Step Functions에서 파일 처리 단계를 수행하려면 Amazon S3의 이벤트 트리거에서 AWS Lambda 함수를 사용하여 자체 워크플로를 구성합니다. 관리형 워크플로에서는 선형 순차적 처리를 손쉽게 오케스트레이션하기 위한 프레임워크를 제공하며, 다음 방식으로 기존 솔루션과 차별됩니다. 1) 전체 파일 업로드 시에만 실행할 워크플로와 부분 파일 업로드시에만 실행할 워크플로를 세분화하여 정의할 수 있습니다. 2) S3 및 EFS(업로드 후처리 이벤트를 제공하지 않음)에 대해 자동으로 워크플로를 트리거할 수 있습니다. 3) 워크플로는 PGP 복호화 같은 공통 파일 처리를 위한 코드 및 사전 구축된 옵션을 제공하지 않습니다. 4) 고객이 CloudWatch Logs에서 파일 전송 및 처리에 대한 완벽한 가시성을 확보할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 내 파일 처리 단계를 오케스트레이션하기 위해 AWS Step Functions를 사용하고 있습니다. AWS Transfer Family의 관리형 워크플로는 현재 AWS Step Functions 설정과 어떻게 다릅니까?, answer : AWS Step Functions는 간단한 단계에서 비즈니스 애플리케이션의 실행을 정의하기 위해 AWS Lambda를 다른 서비스와 결합할 수 있는 서버리스 오케스트레이션 서비스입니다. AWS Step Functions에서 파일 처리 단계를 수행하려면 Amazon S3의 이벤트 트리거에서 AWS Lambda 함수를 사용하여 자체 워크플로를 구성합니다. 관리형 워크플로에서는 선형 순차적 처리를 손쉽게 오케스트레이션하기 위한 프레임워크를 제공하며, 다음 방식으로 기존 솔루션과 차별됩니다. 1) 전체 파일 업로드 시에만 실행할 워크플로와 부분 파일 업로드시에만 실행할 워크플로를 세분화하여 정의할 수 있습니다. 2) S3 및 EFS(업로드 후처리 이벤트를 제공하지 않음)에 대해 자동으로 워크플로를 트리거할 수 있습니다. 3) 워크플로는 PGP 복호화 같은 공통 파일 처리를 위한 코드 및 사전 구축된 옵션을 제공하지 않습니다. 4) 고객이 CloudWatch Logs에서 파일 전송 및 처리에 대한 완벽한 가시성을 확보할 수 있습니다.\"\n",
            "관리형 워크플로를 사용하여 파일 전송 알림을 사용자 지정할 수 있나요?\n",
            "예. 파일 전송 알림에 관리형 워크플로를 사용하는 것에 대한 이 블로그 게시물을 참조하세요.\n",
            "\"category : Transfer for SFTP, question : 관리형 워크플로를 사용하여 파일 전송 알림을 사용자 지정할 수 있나요?, answer : 예. 파일 전송 알림에 관리형 워크플로를 사용하는 것에 대한 이 블로그 게시물을 참조하세요.\"\n",
            "부분 업로드에서 관리형 워크플로를 트리거할 수 있나요?\n",
            "예. 전체 파일 업로드 및 부분 파일 업로드 시 트리거할 별도의 워크플로를 정의할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 부분 업로드에서 관리형 워크플로를 트리거할 수 있나요?, answer : 예. 전체 파일 업로드 및 부분 파일 업로드 시 트리거할 별도의 워크플로를 정의할 수 있습니다.\"\n",
            "관리형 워크플로에서 지원되지 않는 Transfer Family 작업은 무엇인가요?\n",
            "현재는 SFTP, FTPS 및 FTP 서버 엔드포인트를 통해 업로드된 파일에 대해서만 관리형 워크플로를 트리거할 수 있으며 실행당 하나의 파일이 처리됩니다. AS2를 통해 교환되는 메시지, 서버 엔드포인트를 통한 파일 다운로드, SFTP 커넥터를 사용하여 전송된 파일에는 관리형 워크플로가 지원되지 않습니다.\n",
            "\"category : Transfer for SFTP, question : 관리형 워크플로에서 지원되지 않는 Transfer Family 작업은 무엇인가요?, answer : 현재는 SFTP, FTPS 및 FTP 서버 엔드포인트를 통해 업로드된 파일에 대해서만 관리형 워크플로를 트리거할 수 있으며 실행당 하나의 파일이 처리됩니다. AS2를 통해 교환되는 메시지, 서버 엔드포인트를 통한 파일 다운로드, SFTP 커넥터를 사용하여 전송된 파일에는 관리형 워크플로가 지원되지 않습니다.\"\n",
            "사용자 다운로드에서 워크플로 작업을 트리거할 수 있나요?\n",
            "아니요. 인바운드 엔드포인트를 사용하여 파일이 도착할 때만 처리를 간접적으로 호출할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 사용자 다운로드에서 워크플로 작업을 트리거할 수 있나요?, answer : 아니요. 인바운드 엔드포인트를 사용하여 파일이 도착할 때만 처리를 간접적으로 호출할 수 있습니다.\"\n",
            "세션의 파일 배치에서 동일한 워크플로를 트리거할 수 있습니까?\n",
            "아니요. 워크플로는 현재 실행당 1개 파일을 처리합니다.\n",
            "\"category : Transfer for SFTP, question : 세션의 파일 배치에서 동일한 워크플로를 트리거할 수 있습니까?, answer : 아니요. 워크플로는 현재 실행당 1개 파일을 처리합니다.\"\n",
            "파일을 업로드한 사용자를 기반으로 관리형 워크플로를 세분화하여 트리거할 수 있나요?\n",
            "아니요. 사용자별로 세분화하여 관리형 워크플로를 간접적으로 호출할 수 없습니다. Amazon EventBridge에 게시되는 파일 전송 이벤트 알림을 사용하면 파일을 업로드한 사용자를 기반으로 조건부 파일 처리 로직을 정의할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 파일을 업로드한 사용자를 기반으로 관리형 워크플로를 세분화하여 트리거할 수 있나요?, answer : 아니요. 사용자별로 세분화하여 관리형 워크플로를 간접적으로 호출할 수 없습니다. Amazon EventBridge에 게시되는 파일 전송 이벤트 알림을 사용하면 파일을 업로드한 사용자를 기반으로 조건부 파일 처리 로직을 정의할 수 있습니다.\"\n",
            "AWS Transfer Family는 Amazon S3와 어떻게 통신하나요?\n",
            "AWS Transfer Family 서버 및 Amazon S3 사이의 데이터 전송은 내부 AWS 네트워크를 통해 수행되며, 퍼블릭 인터넷을 통과하지 않습니다. 이러한 특성 때문에 AWS Transfer Family 서버에서 Simple Storage Service(Amazon S3)로 전송되는 데이터에 대해 AWS PrivateLink를 사용하지 않아도 됩니다. Transfer Family 서비스를 사용하면 Simple Storage Service(Amazon S3)에 대한 AWS PrivateLink 엔드포인트에서 인터넷으로의 트래픽을 차단하지 않아도 됩니다. 따라서 이들을 사용하여 스토리지 서비스와 통신할 수 없습니다. 이때 모두 AWS 스토리지 서비스와 Transfer Family 서버는 동일한 리전에 있다고 가정합니다.\n",
            "\"category : Transfer for SFTP, question : AWS Transfer Family는 Amazon S3와 어떻게 통신하나요?, answer : AWS Transfer Family 서버 및 Amazon S3 사이의 데이터 전송은 내부 AWS 네트워크를 통해 수행되며, 퍼블릭 인터넷을 통과하지 않습니다. 이러한 특성 때문에 AWS Transfer Family 서버에서 Simple Storage Service(Amazon S3)로 전송되는 데이터에 대해 AWS PrivateLink를 사용하지 않아도 됩니다. Transfer Family 서비스를 사용하면 Simple Storage Service(Amazon S3)에 대한 AWS PrivateLink 엔드포인트에서 인터넷으로의 트래픽을 차단하지 않아도 됩니다. 따라서 이들을 사용하여 스토리지 서비스와 통신할 수 없습니다. 이때 모두 AWS 스토리지 서비스와 Transfer Family 서버는 동일한 리전에 있다고 가정합니다.\"\n",
            "AWS IAM 역할을 제공해야 하는 이유는 무엇이며 이 역할은 어떻게 사용되나요?\n",
            "AWS IAM은 사용자에게 제공하려는 액세스 수준을 결정하는 데 사용됩니다. 여기에는 사용자의 클라이언트에서 활성화하려는 작업과 액세스할 수 있는 Amazon S3 버킷(전체 버킷 또는 일부분)이 포함됩니다.\n",
            "\"category : Transfer for SFTP, question : AWS IAM 역할을 제공해야 하는 이유는 무엇이며 이 역할은 어떻게 사용되나요?, answer : AWS IAM은 사용자에게 제공하려는 액세스 수준을 결정하는 데 사용됩니다. 여기에는 사용자의 클라이언트에서 활성화하려는 작업과 액세스할 수 있는 Amazon S3 버킷(전체 버킷 또는 일부분)이 포함됩니다.\"\n",
            "홈 디렉터리 정보를 제공해야 하는 이유는 무엇이며 이 정보는 어떻게 사용되나요?\n",
            "사용자에 대해 설정한 홈 디렉터리가 사용자의 로그인 디렉터리를 결정합니다. 이것은 사용자 클라이언트가 서버에 성공적으로 인증되자마자 사용자를 배치할 디렉터리 경로입니다. 제공되는 IAM 역할이 홈 디렉터리에 대한 사용자 액세스를 제공해야 합니다.\n",
            "\"category : Transfer for SFTP, question : 홈 디렉터리 정보를 제공해야 하는 이유는 무엇이며 이 정보는 어떻게 사용되나요?, answer : 사용자에 대해 설정한 홈 디렉터리가 사용자의 로그인 디렉터리를 결정합니다. 이것은 사용자 클라이언트가 서버에 성공적으로 인증되자마자 사용자를 배치할 디렉터리 경로입니다. 제공되는 IAM 역할이 홈 디렉터리에 대한 사용자 액세스를 제공해야 합니다.\"\n",
            "액세스 설정은 유사하지만 버킷에서 할당된 부분은 다른 수백 명의 사용자가 있습니다. 동일한 IAM 역할과 정책을 사용하여 액세스하도록 설정할 수 있습니까?\n",
            "예. 모든 사용자에게 단일 IAM 역할을 할당하고 최종 사용자에게 표시하려는 Amazon S3 버킷의 절대 경로와 클라이언트가 최종 사용자에게 해당 경로를 표시하는 방법을 지정하는 논리적 디렉터리 매핑을 사용할 수 있습니다. Chroot와 논리적 디렉터리로 내 AWS SFTP/FTPS/FTP 구조를 단순화하는 방법에 관한 블로그를 참조하세요.\n",
            "\"category : Transfer for SFTP, question : 액세스 설정은 유사하지만 버킷에서 할당된 부분은 다른 수백 명의 사용자가 있습니다. 동일한 IAM 역할과 정책을 사용하여 액세스하도록 설정할 수 있습니까?, answer : 예. 모든 사용자에게 단일 IAM 역할을 할당하고 최종 사용자에게 표시하려는 Amazon S3 버킷의 절대 경로와 클라이언트가 최종 사용자에게 해당 경로를 표시하는 방법을 지정하는 논리적 디렉터리 매핑을 사용할 수 있습니다. Chroot와 논리적 디렉터리로 내 AWS SFTP/FTPS/FTP 구조를 단순화하는 방법에 관한 블로그를 참조하세요.\"\n",
            "AWS Transfer를 사용하여 전송된 파일은 Amazon S3 버킷에 어떻게 저장되나요?\n",
            "지원되는 프로토콜을 통해 전송된 파일은 Amazon S3 버킷에 객체로 저장되며 파일과 객체 간에 일대일 매핑이 이루어집니다. 따라서 AWS 서비스를 사용하여 이러한 객체에 네이티브 액세스하여 처리 또는 분석을 수행할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : AWS Transfer를 사용하여 전송된 파일은 Amazon S3 버킷에 어떻게 저장되나요?, answer : 지원되는 프로토콜을 통해 전송된 파일은 Amazon S3 버킷에 객체로 저장되며 파일과 객체 간에 일대일 매핑이 이루어집니다. 따라서 AWS 서비스를 사용하여 이러한 객체에 네이티브 액세스하여 처리 또는 분석을 수행할 수 있습니다.\"\n",
            "버킷에 저장된 Amazon S3 객체는 사용자에게 어떻게 표시되나요?\n",
            "인증에 성공하면, 사용자의 자격 증명에 따라 서비스가 Amazon S3 객체와 폴더를 파일과 디렉터리로 사용자의 전송 애플리케이션에 표시합니다.\n",
            "\"category : Transfer for SFTP, question : 버킷에 저장된 Amazon S3 객체는 사용자에게 어떻게 표시되나요?, answer : 인증에 성공하면, 사용자의 자격 증명에 따라 서비스가 Amazon S3 객체와 폴더를 파일과 디렉터리로 사용자의 전송 애플리케이션에 표시합니다.\"\n",
            "지원하는 파일 작업은 무엇인가요? 지원하지 않는 파일 작업은 무엇인가요?\n",
            "파일 및 디렉터리를 생성하고, 읽고, 업데이트하고, 삭제하는 일반적인 명령이 지원됩니다. 파일은 Amazon S3 버킷에 개별 객체로 저장됩니다. 디렉터리는 S3의 폴더 객체로 관리되며 S3 콘솔과 같은 구문을 사용합니다.\n",
            "디렉터리 이름 변경 작업, 작업 추가, 소유권 변경, 권한 및 타임 스탬프와 기호 링크 및 하드 링크 사용은 현재 지원되지 않습니다.\n",
            "\"category : Transfer for SFTP, question : 지원하는 파일 작업은 무엇인가요? 지원하지 않는 파일 작업은 무엇인가요?, answer : 파일 및 디렉터리를 생성하고, 읽고, 업데이트하고, 삭제하는 일반적인 명령이 지원됩니다. 파일은 Amazon S3 버킷에 개별 객체로 저장됩니다. 디렉터리는 S3의 폴더 객체로 관리되며 S3 콘솔과 같은 구문을 사용합니다.\n",
            "디렉터리 이름 변경 작업, 작업 추가, 소유권 변경, 권한 및 타임 스탬프와 기호 링크 및 하드 링크 사용은 현재 지원되지 않습니다.\"\n",
            "사용자에게 허용되는 작업을 제어할 수 있나요?\n",
            "예. 사용자 이름에 매핑한 AWS IAM 역할을 사용하여 파일 작업을 활성화 및 비활성화할 수 있습니다. 최종 사용자 액세스를 제어하는 IAM 정책 및 역할 생성에 대한 설명서를 참조하세요.\n",
            "\"category : Transfer for SFTP, question : 사용자에게 허용되는 작업을 제어할 수 있나요?, answer : 예. 사용자 이름에 매핑한 AWS IAM 역할을 사용하여 파일 작업을 활성화 및 비활성화할 수 있습니다. 최종 사용자 액세스를 제어하는 IAM 정책 및 역할 생성에 대한 설명서를 참조하세요.\"\n",
            "최종 사용자에게 둘 이상의 Amazon S3 버킷에 대한 액세스를 제공할 수 있나요?\n",
            "예. 사용자가 액세스할 수 있는 버킷은 AWS IAM 역할 및 해당 사용자에 대해 지정하는 선택형 범위 축소 정책에 의해 결정됩니다. 사용자는 사용자의 홈 디렉터리로 단일 버킷만 사용할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 최종 사용자에게 둘 이상의 Amazon S3 버킷에 대한 액세스를 제공할 수 있나요?, answer : 예. 사용자가 액세스할 수 있는 버킷은 AWS IAM 역할 및 해당 사용자에 대해 지정하는 선택형 범위 축소 정책에 의해 결정됩니다. 사용자는 사용자의 홈 디렉터리로 단일 버킷만 사용할 수 있습니다.\"\n",
            "AWS Transfer Family와 함께 S3 액세스 포인트를 사용하여 공유 데이터 세트에 대한 사용자 액세스를 간소화할 수 있나요?\n",
            "예. AWS Transfer Family와 함께 S3 액세스 포인트 별칭을 사용하면 단일 버킷 정책을 관리할 필요 없이 대규모 데이터 집합에 대한 세분화된 액세스를 제공할 수 있습니다. AWS Transfer Family의 논리적 디렉터리와 결합된 S3 액세스 포인트 별칭을 사용하면 다양한 애플리케이션, 팀 및 부서에 대한 세분화된 액세스 제어를 생성하는 동시에 버킷 정책 관리의 오버헤드를 줄일 수 있습니다. 자세히 알아보고 시작하려면 AWS Transfer Family 및 Amazon S3 액세스 포인트를 통해 데이터 액세스 제어 향상에 대한 블로그 게시물을 참조하세요.\n",
            "\"category : Transfer for SFTP, question : AWS Transfer Family와 함께 S3 액세스 포인트를 사용하여 공유 데이터 세트에 대한 사용자 액세스를 간소화할 수 있나요?, answer : 예. AWS Transfer Family와 함께 S3 액세스 포인트 별칭을 사용하면 단일 버킷 정책을 관리할 필요 없이 대규모 데이터 집합에 대한 세분화된 액세스를 제공할 수 있습니다. AWS Transfer Family의 논리적 디렉터리와 결합된 S3 액세스 포인트 별칭을 사용하면 다양한 애플리케이션, 팀 및 부서에 대한 세분화된 액세스 제어를 생성하는 동시에 버킷 정책 관리의 오버헤드를 줄일 수 있습니다. 자세히 알아보고 시작하려면 AWS Transfer Family 및 Amazon S3 액세스 포인트를 통해 데이터 액세스 제어 향상에 대한 블로그 게시물을 참조하세요.\"\n",
            "AWS 계정 A를 사용하여 서버를 만들고 사용자를 AWS 계정 B 소유의 Amazon S3 버킷에 매핑할 수 있나요?\n",
            "예. CLI 및 API를 사용하여 지원되는 프로토콜을 통해 전송된 파일을 저장하는 데 사용하려는 버킷과 서버 간에 교차 계정 액세스를 설정할 수 있습니다. 콘솔 드롭다운은 계정 A의 버킷 목록만을 표시합니다. 또한 사용자에게 할당된 역할이 계정 A에 속하는지 확인해야 합니다.\n",
            "\"category : Transfer for SFTP, question : AWS 계정 A를 사용하여 서버를 만들고 사용자를 AWS 계정 B 소유의 Amazon S3 버킷에 매핑할 수 있나요?, answer : 예. CLI 및 API를 사용하여 지원되는 프로토콜을 통해 전송된 파일을 저장하는 데 사용하려는 버킷과 서버 간에 교차 계정 액세스를 설정할 수 있습니다. 콘솔 드롭다운은 계정 A의 버킷 목록만을 표시합니다. 또한 사용자에게 할당된 역할이 계정 A에 속하는지 확인해야 합니다.\"\n",
            "Amazon S3에 업로드된 파일의 처리를 자동화할 수 있나요?\n",
            "예. AWS Transfer Family는 파일 전송 작업이 완료되면 Amazon EventBridge에 이벤트 알림을 게시합니다. 이 이벤트를 사용하여 파일의 업로드 후처리를 자동화할 수 있습니다. 또는 업로드된 모든 파일을 조건부 로직 없이 동일한 파일 처리 단계를 사용하여 처리해야 하는 경우 AWS Transfer Family 관리형 워크플로를 사용하여 사용자가 SFTP, FTPS 또는 FTP 서버 엔드포인트를 통해 업로드하는 각 파일에 대해 자동으로 간접 호출되는 일반적인 파일 처리 단계를 순서대로 정의할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : Amazon S3에 업로드된 파일의 처리를 자동화할 수 있나요?, answer : 예. AWS Transfer Family는 파일 전송 작업이 완료되면 Amazon EventBridge에 이벤트 알림을 게시합니다. 이 이벤트를 사용하여 파일의 업로드 후처리를 자동화할 수 있습니다. 또는 업로드된 모든 파일을 조건부 로직 없이 동일한 파일 처리 단계를 사용하여 처리해야 하는 경우 AWS Transfer Family 관리형 워크플로를 사용하여 사용자가 SFTP, FTPS 또는 FTP 서버 엔드포인트를 통해 업로드하는 각 파일에 대해 자동으로 간접 호출되는 일반적인 파일 처리 단계를 순서대로 정의할 수 있습니다.\"\n",
            "파일을 업로드하는 사용자를 기반으로 처리하는 규칙을 사용자 지정할 수 있나요?\n",
            "예. 사용자가 파일을 업로드하면 업로드에 사용된 서버의 서버 ID 및 사용자 이름이 연결된 S3 객체의 메타데이터의 일부로 저장됩니다. 업로드 후처리에 대해 사용하는 정보에 대한 설명서를 참조하세요. AWS Transfer Family에서 Amazon EventBridge에 게시한 자동 파일 업로드 이벤트 알림에서도 최종 사용자 정보를 확인할 수 있습니다. 이 정보를 사용하여 사용자에 따라 파일의 세부적인 업로드 후처리를 오케스트레이션할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 파일을 업로드하는 사용자를 기반으로 처리하는 규칙을 사용자 지정할 수 있나요?, answer : 예. 사용자가 파일을 업로드하면 업로드에 사용된 서버의 서버 ID 및 사용자 이름이 연결된 S3 객체의 메타데이터의 일부로 저장됩니다. 업로드 후처리에 대해 사용하는 정보에 대한 설명서를 참조하세요. AWS Transfer Family에서 Amazon EventBridge에 게시한 자동 파일 업로드 이벤트 알림에서도 최종 사용자 정보를 확인할 수 있습니다. 이 정보를 사용하여 사용자에 따라 파일의 세부적인 업로드 후처리를 오케스트레이션할 수 있습니다.\"\n",
            "Amazon S3 이벤트 알림은 Amazon EventBridge의 AWS Transfer Family 서비스 이벤트와 어떻게 다른가요? 그리고 파일의 업로드 후처리를 트리거하려면 무엇을 사용해야 하나요?\n",
            "Amazon S3는 버킷에 생성된 모든 새 객체에 대한 이벤트 알림을 게시할 수 있습니다. 반면, AWS Transfer Family는 각 파일 전송 작업의 성공 또는 실패 시 이벤트 알림을 게시합니다. Amazon S3 이벤트 알림과 다른 점은 다음과 같습니다. 1) Transfer Family의 이벤트 알림을 사용할 때 전체 파일 업로드와 부분 파일 업로드에 대한 업로드 후처리를 정의할 수 있습니다. 2) Transfer Family 이벤트는 S3와 EFS 모두의 파일 업로드에 대해 게시됩니다. 3) Transfer Family를 통해 생성되는 이벤트에는 발신자의 사용자 이름, 서버 ID, 전송 상태 등과 같은 운영 정보가 포함되며 이러한 속성에 대한 조건부 논리를 기반으로 파일 처리를 세부적으로 정의할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : Amazon S3 이벤트 알림은 Amazon EventBridge의 AWS Transfer Family 서비스 이벤트와 어떻게 다른가요? 그리고 파일의 업로드 후처리를 트리거하려면 무엇을 사용해야 하나요?, answer : Amazon S3는 버킷에 생성된 모든 새 객체에 대한 이벤트 알림을 게시할 수 있습니다. 반면, AWS Transfer Family는 각 파일 전송 작업의 성공 또는 실패 시 이벤트 알림을 게시합니다. Amazon S3 이벤트 알림과 다른 점은 다음과 같습니다. 1) Transfer Family의 이벤트 알림을 사용할 때 전체 파일 업로드와 부분 파일 업로드에 대한 업로드 후처리를 정의할 수 있습니다. 2) Transfer Family 이벤트는 S3와 EFS 모두의 파일 업로드에 대해 게시됩니다. 3) Transfer Family를 통해 생성되는 이벤트에는 발신자의 사용자 이름, 서버 ID, 전송 상태 등과 같은 운영 정보가 포함되며 이러한 속성에 대한 조건부 논리를 기반으로 파일 처리를 세부적으로 정의할 수 있습니다.\"\n",
            "현재 최종 사용자가 S3 디렉터리를 볼 수 있으려면 몇 분 정도 걸립니다. 이를 가속화할 수 있는 방법이 있나요?\n",
            "예. S3 디렉터리 리스팅을 최적화하면 디렉터리 리스팅을 몇 분에서 몇 초로 가속화할 수 있습니다. 2023년 11월 17일 이후에 콘솔을 통해 새 서버를 생성하는 경우 Amazon S3를 스토리지로 사용하는 경우 서버에는 최적화된 S3 디렉터리 리스팅이 기본적으로 활성화됩니다. 언제든지 켜거나 끌 수 있습니다. 이 기능을 끄면 S3 디렉터리 리스팅이 기본 성능으로 복원됩니다. CloudFormation, CLI 또는 API를 사용하여 서버를 생성하는 경우 최적화된 S3 디렉터리 리스팅이 기본적으로 비활성화되지만 언제든지 활성화할 수 있습니다. 최적화된 S3 디렉터리 리스팅을 활성화하는 방법은 설명서를 참조하세요.\n",
            "\"category : Transfer for SFTP, question : 현재 최종 사용자가 S3 디렉터리를 볼 수 있으려면 몇 분 정도 걸립니다. 이를 가속화할 수 있는 방법이 있나요?, answer : 예. S3 디렉터리 리스팅을 최적화하면 디렉터리 리스팅을 몇 분에서 몇 초로 가속화할 수 있습니다. 2023년 11월 17일 이후에 콘솔을 통해 새 서버를 생성하는 경우 Amazon S3를 스토리지로 사용하는 경우 서버에는 최적화된 S3 디렉터리 리스팅이 기본적으로 활성화됩니다. 언제든지 켜거나 끌 수 있습니다. 이 기능을 끄면 S3 디렉터리 리스팅이 기본 성능으로 복원됩니다. CloudFormation, CLI 또는 API를 사용하여 서버를 생성하는 경우 최적화된 S3 디렉터리 리스팅이 기본적으로 비활성화되지만 언제든지 활성화할 수 있습니다. 최적화된 S3 디렉터리 리스팅을 활성화하는 방법은 설명서를 참조하세요.\"\n",
            "최종 사용자가 파일에 안전하게 액세스할 수 있도록 하려면 논리적 디렉터리 외에 개별 세션 정책도 필요한가요?\n",
            "세션 정책 사용과 기타 내부 요구 사항에 따라 다르지만 일반적으로 사용자가 의도한 파일에만 액세스하도록 하기 위해 세션 정책과 논리적 디렉터리가 모두 필요한 것은 아닙니다. 논리적 디렉터리 매핑은 사용자가 지정된 논리적 경로와 하위 디렉터리에만 액세스할 수 있도록 허용하며 논리적 루트를 통과하는 상대 경로는 금지합니다. 상대 요소를 포함할 수 있는 상대 표기법을 사용하여 모든 경로를 검증하고 이러한 경로를 S3로 전달하기 전에 해당 경로가 확인되지 않도록 적극적으로 차단하여 사용자가 논리적 매핑을 벗어나는 것을 방지합니다.\n",
            "\"category : Transfer for SFTP, question : 최종 사용자가 파일에 안전하게 액세스할 수 있도록 하려면 논리적 디렉터리 외에 개별 세션 정책도 필요한가요?, answer : 세션 정책 사용과 기타 내부 요구 사항에 따라 다르지만 일반적으로 사용자가 의도한 파일에만 액세스하도록 하기 위해 세션 정책과 논리적 디렉터리가 모두 필요한 것은 아닙니다. 논리적 디렉터리 매핑은 사용자가 지정된 논리적 경로와 하위 디렉터리에만 액세스할 수 있도록 허용하며 논리적 루트를 통과하는 상대 경로는 금지합니다. 상대 요소를 포함할 수 있는 상대 표기법을 사용하여 모든 경로를 검증하고 이러한 경로를 S3로 전달하기 전에 해당 경로가 확인되지 않도록 적극적으로 차단하여 사용자가 논리적 매핑을 벗어나는 것을 방지합니다.\"\n",
            "AWS Transfer Family에서 사용하기 위해 EFS 파일 시스템을 설정하려면 어떻게 해야 하나요?\n",
            "Amazon EFS 파일 시스템에서 사용하도록 AWS Transfer Family를 설정하기 전에 AWS Transfer Family 사용자에게 할당하려는 동일한 POSIX ID(사용자 ID/그룹 ID)를 사용하여 파일 및 폴더의 소유권을 설정해야 합니다. 또한 다른 계정의 파일 시스템에 액세스하는 경우 교차 계정 액세스를 사용하도록 파일 시스템에서 리소스 정책도 구성해야 합니다. EFS와 함께 AWS Transfer Family를 사용하는 방법에 대한 단계별 지침은 이 블로그 게시물을 참조하세요.\n",
            "\"category : Transfer for SFTP, question : AWS Transfer Family에서 사용하기 위해 EFS 파일 시스템을 설정하려면 어떻게 해야 하나요?, answer : Amazon EFS 파일 시스템에서 사용하도록 AWS Transfer Family를 설정하기 전에 AWS Transfer Family 사용자에게 할당하려는 동일한 POSIX ID(사용자 ID/그룹 ID)를 사용하여 파일 및 폴더의 소유권을 설정해야 합니다. 또한 다른 계정의 파일 시스템에 액세스하는 경우 교차 계정 액세스를 사용하도록 파일 시스템에서 리소스 정책도 구성해야 합니다. EFS와 함께 AWS Transfer Family를 사용하는 방법에 대한 단계별 지침은 이 블로그 게시물을 참조하세요.\"\n",
            "AWS Transfer Family는 Amazon EFS와 어떻게 통신하나요?\n",
            "AWS Transfer Family 서버 및 Amazon EFS 사이의 데이터 전송은 내부 AWS 네트워크를 통해 수행되며, 퍼블릭 인터넷을 통과하지 않습니다. 이러한 특성 때문에 AWS Transfer Family 서버에서 Amazon EFS로 전송되는 데이터에 대해 AWS PrivateLink를 사용하지 않아도 됩니다. Transfer Family 서비스를 사용하면 Amazon EFS에 대한 AWS PrivateLink 엔드포인트에서 인터넷으로의 트래픽을 차단하지 않아도 됩니다. 따라서 이들을 사용하여 스토리지 서비스와 통신할 수 없습니다. 이때 모두 AWS 스토리지 서비스와 Transfer Family 서버는 동일한 리전에 있다고 가정합니다.\n",
            "\"category : Transfer for SFTP, question : AWS Transfer Family는 Amazon EFS와 어떻게 통신하나요?, answer : AWS Transfer Family 서버 및 Amazon EFS 사이의 데이터 전송은 내부 AWS 네트워크를 통해 수행되며, 퍼블릭 인터넷을 통과하지 않습니다. 이러한 특성 때문에 AWS Transfer Family 서버에서 Amazon EFS로 전송되는 데이터에 대해 AWS PrivateLink를 사용하지 않아도 됩니다. Transfer Family 서비스를 사용하면 Amazon EFS에 대한 AWS PrivateLink 엔드포인트에서 인터넷으로의 트래픽을 차단하지 않아도 됩니다. 따라서 이들을 사용하여 스토리지 서비스와 통신할 수 없습니다. 이때 모두 AWS 스토리지 서비스와 Transfer Family 서버는 동일한 리전에 있다고 가정합니다.\"\n",
            "파일 시스템에서 파일을 업로드하거나 다운로드할 액세스 권한을 사용자에게 제공하려면 어떻게 해야 하나요?\n",
            "Amazon EFS는 운영 제체 사용자 ID, 그룹 ID, 보조 그룹 ID로 구성된 POSIX ID를 사용하여 파일 시스템에 대한 액세스를 제어합니다. AWS Transfer Family 콘솔, CLI 및 API에서 사용자를 설정하는 경우 EFS 파일 시스템에 액세스하려면 사용자 이름, 사용자 POSIX 구성 및 IAM 역할을 지정해야 합니다. 또한, EFS 파일 시스템 ID를 지정해야 합니다. 선택적으로 사용자 랜딩 디렉터리로 해당 파일 시스템 내에서 디렉터리도 지정할 수 있습니다. 파일 전송 클라이언트를 사용하여 AWS Transfer Family 사용자를 성공적으로 인증하면 지정된 EFS 파일 시스템의 루트 또는 지정된 홈 디렉터리에 직접 배치됩니다. 파일 전송 클라이언트를 통해 수행된 모든 요청에는 운영 체제 POSIX ID가 적용됩니다. EFS 관리자인 경우 EFS 파일 시스템의 해당되는 POSIX ID가 AWS Transfer Family 사용자가 액세스하려는 파일 및 디렉터리를 소유하고 있는지 확인해야 합니다. EFS에서 하위 디렉터리 소유권을 구성하는 방법에 대한 자세한 내용은 설명서를 참조하세요. Amazon EFS를 스토리지로 사용하는 경우 Transfer Family는 액세스 포인트를 지원하지 않는다는 점에 유의하세요.\n",
            "\"category : Transfer for SFTP, question : 파일 시스템에서 파일을 업로드하거나 다운로드할 액세스 권한을 사용자에게 제공하려면 어떻게 해야 하나요?, answer : Amazon EFS는 운영 제체 사용자 ID, 그룹 ID, 보조 그룹 ID로 구성된 POSIX ID를 사용하여 파일 시스템에 대한 액세스를 제어합니다. AWS Transfer Family 콘솔, CLI 및 API에서 사용자를 설정하는 경우 EFS 파일 시스템에 액세스하려면 사용자 이름, 사용자 POSIX 구성 및 IAM 역할을 지정해야 합니다. 또한, EFS 파일 시스템 ID를 지정해야 합니다. 선택적으로 사용자 랜딩 디렉터리로 해당 파일 시스템 내에서 디렉터리도 지정할 수 있습니다. 파일 전송 클라이언트를 사용하여 AWS Transfer Family 사용자를 성공적으로 인증하면 지정된 EFS 파일 시스템의 루트 또는 지정된 홈 디렉터리에 직접 배치됩니다. 파일 전송 클라이언트를 통해 수행된 모든 요청에는 운영 체제 POSIX ID가 적용됩니다. EFS 관리자인 경우 EFS 파일 시스템의 해당되는 POSIX ID가 AWS Transfer Family 사용자가 액세스하려는 파일 및 디렉터리를 소유하고 있는지 확인해야 합니다. EFS에서 하위 디렉터리 소유권을 구성하는 방법에 대한 자세한 내용은 설명서를 참조하세요. Amazon EFS를 스토리지로 사용하는 경우 Transfer Family는 액세스 포인트를 지원하지 않는다는 점에 유의하세요.\"\n",
            "프로토콜에서 전송된 파일은 어떻게 Amazon EFS 파일 시스템에 저장되나요?\n",
            "활성화된 프로토콜을 통해 전송된 파일은 Amazon EFS 파일 시스템에 직접 저장되며, Amazon EFS 파일 시스템에 액세스할 수 있는 AWS 서비스 또는 표준 파일 시스템 인터페이스를 통해 액세스할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 프로토콜에서 전송된 파일은 어떻게 Amazon EFS 파일 시스템에 저장되나요?, answer : 활성화된 프로토콜을 통해 전송된 파일은 Amazon EFS 파일 시스템에 직접 저장되며, Amazon EFS 파일 시스템에 액세스할 수 있는 AWS 서비스 또는 표준 파일 시스템 인터페이스를 통해 액세스할 수 있습니다.\"\n",
            "Amazon S3 및 Amazon EFS를 사용할 때 프로토콜에서 어떤 파일 작업이 지원되나요?\n",
            "A: 파일, 디렉터리 및 기호 링크를 생성하고, 읽으며, 업데이트하고, 삭제하는 SFTP, FTPS 및 FTP 명령이 지원됩니다. EFS 및 S3에 대해 지원되는 명령은 아래 표를 참조하세요.\n",
            "\n",
            "\n",
            "\n",
            " 명령 \n",
            " Amazon S3 \n",
            " Amazon EFS \n",
            "\n",
            "\n",
            "      cd \n",
            " 지원 \n",
            " 지원 \n",
            "\n",
            "\n",
            "      ls/dir \n",
            " 지원 \n",
            " 지원 \n",
            "\n",
            "\n",
            "      pwd \n",
            " 지원 \n",
            " 지원 \n",
            "\n",
            "\n",
            "      put \n",
            " 지원 \n",
            " 지원 \n",
            "\n",
            "\n",
            "      get \n",
            " 지원 \n",
            " 지원됨(심볼릭 링크 및 하드 링크 확인 포함) \n",
            "\n",
            "\n",
            "      rename \n",
            " 지원됨1 \n",
            " 지원됨1 \n",
            "\n",
            "\n",
            "      chown \n",
            " 지원되지 않음 \n",
            " 지원됨2 \n",
            "\n",
            "\n",
            "      chmod \n",
            " 지원되지 않음 \n",
            " 지원됨2 \n",
            "\n",
            "\n",
            "      chgrp \n",
            " 지원되지 않음 \n",
            " 지원됨3 \n",
            "\n",
            "\n",
            "      ln -s/symlink \n",
            " 지원되지 않음 \n",
            " 지원 \n",
            "\n",
            "\n",
            "      mkdir \n",
            " 지원 \n",
            " 지원 \n",
            "\n",
            "\n",
            "      rm/delete \n",
            " 지원 \n",
            " 지원 \n",
            "\n",
            "\n",
            "      rmdir \n",
            " 지원됨4 \n",
            " 지원 \n",
            "\n",
            "\n",
            "      chmtime \n",
            " 지원되지 않음 \n",
            " 지원 \n",
            "\n",
            "\n",
            "\n",
            "1. 파일 이름 바꾸기만 지원됩니다. 기존 파일을 덮어쓰는 파일 이름 바꾸기와 디렉터리 이름 바꾸기는 지원되지 않습니다.\n",
            "2. 루트(즉, uid=0인 사용자)만 파일 및 디렉터리의 소유권과 권한을 변경할 수 있습니다.\n",
            "3. 루트(예: uid=0) 또는 파일 그룹을 보조 그룹 중 하나로만 변경할 수 있는 파일 소유자에 대해서만 지원됩니다.\n",
            "4. 비어 있지 않은 폴더에 대해서만 지원됩니다.\n",
            "\"category : Transfer for SFTP, question : Amazon S3 및 Amazon EFS를 사용할 때 프로토콜에서 어떤 파일 작업이 지원되나요?, answer : A: 파일, 디렉터리 및 기호 링크를 생성하고, 읽으며, 업데이트하고, 삭제하는 SFTP, FTPS 및 FTP 명령이 지원됩니다. EFS 및 S3에 대해 지원되는 명령은 아래 표를 참조하세요.\n",
            "\n",
            "\n",
            "\n",
            " 명령 \n",
            " Amazon S3 \n",
            " Amazon EFS \n",
            "\n",
            "\n",
            "      cd \n",
            " 지원 \n",
            " 지원 \n",
            "\n",
            "\n",
            "      ls/dir \n",
            " 지원 \n",
            " 지원 \n",
            "\n",
            "\n",
            "      pwd \n",
            " 지원 \n",
            " 지원 \n",
            "\n",
            "\n",
            "      put \n",
            " 지원 \n",
            " 지원 \n",
            "\n",
            "\n",
            "      get \n",
            " 지원 \n",
            " 지원됨(심볼릭 링크 및 하드 링크 확인 포함) \n",
            "\n",
            "\n",
            "      rename \n",
            " 지원됨1 \n",
            " 지원됨1 \n",
            "\n",
            "\n",
            "      chown \n",
            " 지원되지 않음 \n",
            " 지원됨2 \n",
            "\n",
            "\n",
            "      chmod \n",
            " 지원되지 않음 \n",
            " 지원됨2 \n",
            "\n",
            "\n",
            "      chgrp \n",
            " 지원되지 않음 \n",
            " 지원됨3 \n",
            "\n",
            "\n",
            "      ln -s/symlink \n",
            " 지원되지 않음 \n",
            " 지원 \n",
            "\n",
            "\n",
            "      mkdir \n",
            " 지원 \n",
            " 지원 \n",
            "\n",
            "\n",
            "      rm/delete \n",
            " 지원 \n",
            " 지원 \n",
            "\n",
            "\n",
            "      rmdir \n",
            " 지원됨4 \n",
            " 지원 \n",
            "\n",
            "\n",
            "      chmtime \n",
            " 지원되지 않음 \n",
            " 지원 \n",
            "\n",
            "\n",
            "\n",
            "1. 파일 이름 바꾸기만 지원됩니다. 기존 파일을 덮어쓰는 파일 이름 바꾸기와 디렉터리 이름 바꾸기는 지원되지 않습니다.\n",
            "2. 루트(즉, uid=0인 사용자)만 파일 및 디렉터리의 소유권과 권한을 변경할 수 있습니다.\n",
            "3. 루트(예: uid=0) 또는 파일 그룹을 보조 그룹 중 하나로만 변경할 수 있는 파일 소유자에 대해서만 지원됩니다.\n",
            "4. 비어 있지 않은 폴더에 대해서만 지원됩니다.\"\n",
            "사용자가 액세스 권한을 갖고 있는 파일 및 폴더와 허용되는 작업과 허용되지 않는 작업을 제어하려면 어떻게 해야 하나요?\n",
            "AWS Transfer Family 사용자에 대해 제공하는 IAM 정책은 파일 시스템에 대해 읽기 전용, 읽기-쓰기 및 루트 액세스 권한을 보유하는지 여부를 결정합니다. 또한, 파일 시스템 관리자인 경우 소유권을 설정하고 사용자 ID 및 그룹 ID를 사용하여 파일 시스템 내 파일 및 디렉터리에 대한 액세스 권한을 부여할 수 있습니다. 이는 서비스에 저장되었든(서비스에서 관리됨), 아니면 자격 증명 관리 시스템에 있든(\"BYO 인증\") 상관없이 사용자에게 적용됩니다.\n",
            "\"category : Transfer for SFTP, question : 사용자가 액세스 권한을 갖고 있는 파일 및 폴더와 허용되는 작업과 허용되지 않는 작업을 제어하려면 어떻게 해야 하나요?, answer : AWS Transfer Family 사용자에 대해 제공하는 IAM 정책은 파일 시스템에 대해 읽기 전용, 읽기-쓰기 및 루트 액세스 권한을 보유하는지 여부를 결정합니다. 또한, 파일 시스템 관리자인 경우 소유권을 설정하고 사용자 ID 및 그룹 ID를 사용하여 파일 시스템 내 파일 및 디렉터리에 대한 액세스 권한을 부여할 수 있습니다. 이는 서비스에 저장되었든(서비스에서 관리됨), 아니면 자격 증명 관리 시스템에 있든(\"BYO 인증\") 상관없이 사용자에게 적용됩니다.\"\n",
            "각 사용자가 파일 시스템의 서로 다른 디렉터리와 해당 디렉터리 내 파일에만 액세스하도록 제한할 수 있나요?\n",
            "예. 사용자를 설정할 때 각 사용자에 대해 서로 다른 파일 시스템과 디렉터리를 지정할 수 있습니다. 인증에 성공하면 EFS는 활성화된 프로토콜을 사용하여 수행한 모든 파일 시스템 요청에 대해 디렉터리를 적용합니다.\n",
            "\"category : Transfer for SFTP, question : 각 사용자가 파일 시스템의 서로 다른 디렉터리와 해당 디렉터리 내 파일에만 액세스하도록 제한할 수 있나요?, answer : 예. 사용자를 설정할 때 각 사용자에 대해 서로 다른 파일 시스템과 디렉터리를 지정할 수 있습니다. 인증에 성공하면 EFS는 활성화된 프로토콜을 사용하여 수행한 모든 파일 시스템 요청에 대해 디렉터리를 적용합니다.\"\n",
            "사용자에게 파일 시스템 이름이 노출되지 않도록 숨길 수 있나요?\n",
            "예. AWS Transfer Family의 논리 디렉터리 매핑을 사용하면 절대 경로를 최종 사용자에게 표시되는 경로 이름에 매핑하여 파일 시스템에서 최종 사용자의 디렉터리 보기를 제한할 수 있습니다. 또한 지정된 홈 디렉터리로 이동(‘chroot’)하는 기능도 포함되어 있습니다.\n",
            "\"category : Transfer for SFTP, question : 사용자에게 파일 시스템 이름이 노출되지 않도록 숨길 수 있나요?, answer : 예. AWS Transfer Family의 논리 디렉터리 매핑을 사용하면 절대 경로를 최종 사용자에게 표시되는 경로 이름에 매핑하여 파일 시스템에서 최종 사용자의 디렉터리 보기를 제한할 수 있습니다. 또한 지정된 홈 디렉터리로 이동(‘chroot’)하는 기능도 포함되어 있습니다.\"\n",
            "심볼 링크가 지원되나요?\n",
            "예. 사용자가 액세스할 수 있는 기호 링크가 디렉터리에 있고 사용자가 이에 액세스하려고 하면 링크는 해당 대상으로 해석됩니다. 사용자가 논리 디렉터리 매핑을 사용하여 사용자 액세스를 설정하는 경우 기호 링크는 지원되지 않습니다.\n",
            "\"category : Transfer for SFTP, question : 심볼 링크가 지원되나요?, answer : 예. 사용자가 액세스할 수 있는 기호 링크가 디렉터리에 있고 사용자가 이에 액세스하려고 하면 링크는 해당 대상으로 해석됩니다. 사용자가 논리 디렉터리 매핑을 사용하여 사용자 액세스를 설정하는 경우 기호 링크는 지원되지 않습니다.\"\n",
            "둘 이상의 파일 시스템에 대한 액세스를 개별 SFTP, FTPS 및 FTP 사용자에게 제공할 수 있나요?\n",
            "예. AWS Transfer Family 사용자를 설정하는 경우 여러 파일 시스템에 대한 액세스를 부여하기 위해 설정한 사용자의 일부로 제공하는 IAM 정책에서 하나 이상의 파일 시스템을 지정할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 둘 이상의 파일 시스템에 대한 액세스를 개별 SFTP, FTPS 및 FTP 사용자에게 제공할 수 있나요?, answer : 예. AWS Transfer Family 사용자를 설정하는 경우 여러 파일 시스템에 대한 액세스를 부여하기 위해 설정한 사용자의 일부로 제공하는 IAM 정책에서 하나 이상의 파일 시스템을 지정할 수 있습니다.\"\n",
            "AWS Transfer Family를 통해 EFS 파일 시스템에 액세스하기 위해 사용할 수 있는 운영 체제로 무엇이 있나요?\n",
            "Microsoft Windows, Linux, macOS 또는 EFS 파일 시스템에 저장된 파일을 업로드하고 액세스하기 위해 SFTP, FTPS 및 FTP를 지원하는 운영 체제를 위해 구축된 클라이언트 및 애플리케이션을 사용할 수 있습니다. 모든 운영 체제의 파일 시스템에 액세스하도록 EFS 파일 시스템에 대한 적절한 권한이 있는 사용자와 서버를 구성하면 됩니다.\n",
            "\"category : Transfer for SFTP, question : AWS Transfer Family를 통해 EFS 파일 시스템에 액세스하기 위해 사용할 수 있는 운영 체제로 무엇이 있나요?, answer : Microsoft Windows, Linux, macOS 또는 EFS 파일 시스템에 저장된 파일을 업로드하고 액세스하기 위해 SFTP, FTPS 및 FTP를 지원하는 운영 체제를 위해 구축된 클라이언트 및 애플리케이션을 사용할 수 있습니다. 모든 운영 체제의 파일 시스템에 액세스하도록 EFS 파일 시스템에 대한 적절한 권한이 있는 사용자와 서버를 구성하면 됩니다.\"\n",
            "EFS에 업로드된 파일의 처리를 자동화하려면 어떻게 해야 하나요?\n",
            "두 가지 옵션이 있습니다. 1) AWS Transfer Family는 파일 전송 작업이 완료될 때 Amazon EventBridge에 이벤트 알림을 게시합니다. 이 이벤트를 사용하여 파일의 업로드 후처리를 자동화할 수 있습니다. 2) 조건부 로직 없이 동일한 파일 처리 단계를 사용하여 업로드된 모든 파일을 처리해야 한다면 AWS Transfer Family 관리형 워크플로를 사용하여 일반적인 파일 처리 단계를 순서대로 정의하면 됩니다. 이 파일 처리 단계는 SFTP, FTPS 또는 FTP 서버 엔드포인트를 통해 업로드된 각 파일에 적용됩니다.\n",
            "\"category : Transfer for SFTP, question : EFS에 업로드된 파일의 처리를 자동화하려면 어떻게 해야 하나요?, answer : 두 가지 옵션이 있습니다. 1) AWS Transfer Family는 파일 전송 작업이 완료될 때 Amazon EventBridge에 이벤트 알림을 게시합니다. 이 이벤트를 사용하여 파일의 업로드 후처리를 자동화할 수 있습니다. 2) 조건부 로직 없이 동일한 파일 처리 단계를 사용하여 업로드된 모든 파일을 처리해야 한다면 AWS Transfer Family 관리형 워크플로를 사용하여 일반적인 파일 처리 단계를 순서대로 정의하면 됩니다. 이 파일 처리 단계는 SFTP, FTPS 또는 FTP 서버 엔드포인트를 통해 업로드된 각 파일에 적용됩니다.\"\n",
            "파일을 EFS에 업로드한 사용자를 확인하려면 어떻게 해야 하나요?\n",
            "새 파일의 경우 파일을 업로드한 사용자와 연결된 POSIX 사용자 ID는 EFS 파일 시스템에서 파일의 소유자로 설정됩니다. 또한 Amazon CloudWatch를 사용하여 파일 생성, 업데이트, 삭제 및 읽기 작업을 위한 사용자 활동을 추적할 수 있습니다. Amazon CloudWatch 로깅을 활성화하는 방법에 대한 자세한 내용은 설명서를 참조하세요.\n",
            "\"category : Transfer for SFTP, question : 파일을 EFS에 업로드한 사용자를 확인하려면 어떻게 해야 하나요?, answer : 새 파일의 경우 파일을 업로드한 사용자와 연결된 POSIX 사용자 ID는 EFS 파일 시스템에서 파일의 소유자로 설정됩니다. 또한 Amazon CloudWatch를 사용하여 파일 생성, 업데이트, 삭제 및 읽기 작업을 위한 사용자 활동을 추적할 수 있습니다. Amazon CloudWatch 로깅을 활성화하는 방법에 대한 자세한 내용은 설명서를 참조하세요.\"\n",
            "AWS Transfer Family를 사용하여 다른 계정의 파일 시스템에 액세스할 수 있나요?\n",
            "예. CLI 및 API를 사용하여 AWS Transfer Family 리소스 및 EFS 파일 시스템 사이에서 교차 계정 액세스를 설정할 수 있습니다. AWS Transfer Family 콘솔은 동일한 계정의 파일 시스템만 나열합니다. 또한, 파일 시스템에 액세스하기 위해 사용자에게 지정된 IAM 역할이 계정 A에 속하는지 확인해야 합니다.\n",
            "\"category : Transfer for SFTP, question : AWS Transfer Family를 사용하여 다른 계정의 파일 시스템에 액세스할 수 있나요?, answer : 예. CLI 및 API를 사용하여 AWS Transfer Family 리소스 및 EFS 파일 시스템 사이에서 교차 계정 액세스를 설정할 수 있습니다. AWS Transfer Family 콘솔은 동일한 계정의 파일 시스템만 나열합니다. 또한, 파일 시스템에 액세스하기 위해 사용자에게 지정된 IAM 역할이 계정 A에 속하는지 확인해야 합니다.\"\n",
            "EFS 파일 시스템에 교차 계정 액세스를 위해 활성화된 올바른 정책이 없으면 어떻게 되나요?\n",
            "교차 계정 액세스를 위해 활성화되지 않은 교차 계정 EFS 파일 시스템에 액세스하도록 AWS Transfer Family 서버를 설정하는 경우 SFTP, FTP 및 FTPS 사용자는 파일 시스템에 대한 액세스가 거부됩니다. 서버에서 CloudWatch 로깅을 활성화한 경우 교차 계정 액세스 오류가 CloudWatch Logs에 기록됩니다.\n",
            "\"category : Transfer for SFTP, question : EFS 파일 시스템에 교차 계정 액세스를 위해 활성화된 올바른 정책이 없으면 어떻게 되나요?, answer : 교차 계정 액세스를 위해 활성화되지 않은 교차 계정 EFS 파일 시스템에 액세스하도록 AWS Transfer Family 서버를 설정하는 경우 SFTP, FTP 및 FTPS 사용자는 파일 시스템에 대한 액세스가 거부됩니다. 서버에서 CloudWatch 로깅을 활성화한 경우 교차 계정 액세스 오류가 CloudWatch Logs에 기록됩니다.\"\n",
            "AWS Transfer Family를 사용하여 다른 AWS 리전의 EFS 파일 시스템에 액세스할 수 있나요?\n",
            "아니요. AWS Transfer Family를 사용하여 동일한 AWS 리전의 EFS 파일 시스템에만 액세스할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : AWS Transfer Family를 사용하여 다른 AWS 리전의 EFS 파일 시스템에 액세스할 수 있나요?, answer : 아니요. AWS Transfer Family를 사용하여 동일한 AWS 리전의 EFS 파일 시스템에만 액세스할 수 있습니다.\"\n",
            "모든 EFS 스토리지 클래스에서 AWS Transfer Family를 사용할 수 있나요?\n",
            "예. AWS Transfer를 사용하여 파일을 EFS에 쓰고 일정 시간 동안 액세스하지 않은 파일을 Infrequent Access(IA) 스토리지 클래스로 마이그레이션하도록 EFS 수명 주기 관리를 구성할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 모든 EFS 스토리지 클래스에서 AWS Transfer Family를 사용할 수 있나요?, answer : 예. AWS Transfer를 사용하여 파일을 EFS에 쓰고 일정 시간 동안 액세스하지 않은 파일을 Infrequent Access(IA) 스토리지 클래스로 마이그레이션하도록 EFS 수명 주기 관리를 구성할 수 있습니다.\"\n",
            "동일한 파일에서 데이터를 동시에 읽고 쓰기 위해 애플리케이션에서 SFTP, FTPS 및 FTP를 사용할 수 있나요?\n",
            "예. Amazon EFS는 최대 수천 개의 NFS, SFTP, FTPS 및 FTP 클라이언트를 위한 파일 시스템 인터페이스, 파일 시스템 액세스 시맨틱(예: 강력한 일관성 및 파일 잠금) 및 동시에 액세스 가능한 스토리지를 제공합니다.\n",
            "\"category : Transfer for SFTP, question : 동일한 파일에서 데이터를 동시에 읽고 쓰기 위해 애플리케이션에서 SFTP, FTPS 및 FTP를 사용할 수 있나요?, answer : 예. Amazon EFS는 최대 수천 개의 NFS, SFTP, FTPS 및 FTP 클라이언트를 위한 파일 시스템 인터페이스, 파일 시스템 액세스 시맨틱(예: 강력한 일관성 및 파일 잠금) 및 동시에 액세스 가능한 스토리지를 제공합니다.\"\n",
            "AWS Transfer Family를 사용하여 파일 시스템에 액세스할 때 EFS 버스트 크레딧을 이용하나요?\n",
            "예. AWS Transfer Family 서버를 사용하여 EFS 파일 시스템에 액세스하면 처리량 모드에 상관없이 EFS 버스트 크레딧을 이용합니다. 사용 가능한 성능 및 처리량 모드와 몇 가지 유용한 성능 팁은 설명서를 참조하세요.\n",
            "\"category : Transfer for SFTP, question : AWS Transfer Family를 사용하여 파일 시스템에 액세스할 때 EFS 버스트 크레딧을 이용하나요?, answer : 예. AWS Transfer Family 서버를 사용하여 EFS 파일 시스템에 액세스하면 처리량 모드에 상관없이 EFS 버스트 크레딧을 이용합니다. 사용 가능한 성능 및 처리량 모드와 몇 가지 유용한 성능 팁은 설명서를 참조하세요.\"\n",
            "데이터가 퍼블릭 네트워크에서 전송되는 동안 데이터 보안을 위해 사용해야 하는 프로토콜은 무엇인가요?\n",
            "퍼블릭 네트워크의 보안 전송에는 SFTP 또는 FTPS를 사용해야 합니다. SSH 및 TLS 암호화 알고리즘을 기반으로 하는 프로토콜의 기본 보안을 통해 데이터와 명령이 암호화된 채널을 통해 안전하게 전송됩니다.\n",
            "\"category : Transfer for SFTP, question : 데이터가 퍼블릭 네트워크에서 전송되는 동안 데이터 보안을 위해 사용해야 하는 프로토콜은 무엇인가요?, answer : 퍼블릭 네트워크의 보안 전송에는 SFTP 또는 FTPS를 사용해야 합니다. SSH 및 TLS 암호화 알고리즘을 기반으로 하는 프로토콜의 기본 보안을 통해 데이터와 명령이 암호화된 채널을 통해 안전하게 전송됩니다.\"\n",
            "저장 데이터를 암호화하는 옵션으로는 어떤 것이 있나요?\n",
            "버킷에 저장되는 파일을 Amazon S3 서버 측 암호화(SSE-S3) 또는 Amazon KMS(SSE-KMS)를 사용하여 암호화하도록 선택할 수 있습니다. EFS에 저장된 파일의 경우 저장 중 파일을 암호화하기 위해 AWS 또는 고객 관리형 CMK를 선택할 수 있습니다. Amazon EFS를 사용하여 파일 데이터 및 메타데이터를 저장 중에 암호화하기 위한 옵션에 대한 자세한 내용은 설명서를 참조하세요.\n",
            "\"category : Transfer for SFTP, question : 저장 데이터를 암호화하는 옵션으로는 어떤 것이 있나요?, answer : 버킷에 저장되는 파일을 Amazon S3 서버 측 암호화(SSE-S3) 또는 Amazon KMS(SSE-KMS)를 사용하여 암호화하도록 선택할 수 있습니다. EFS에 저장된 파일의 경우 저장 중 파일을 암호화하기 위해 AWS 또는 고객 관리형 CMK를 선택할 수 있습니다. Amazon EFS를 사용하여 파일 데이터 및 메타데이터를 저장 중에 암호화하기 위한 옵션에 대한 자세한 내용은 설명서를 참조하세요.\"\n",
            "AWS Transfer Family는 어떤 규정 준수 프로그램을 지원하나요?\n",
            "AWS Transfer Family는 PCI-DSS, GDPR, FedRAMP, SOC 1, 2, 3을 준수합니다. 또한 이 서비스는 HIPAA 적격 서비스입니다. 규정 준수 프로그램 제공 범위 내 서비스에 대해 자세히 알아보세요.\n",
            "\"category : Transfer for SFTP, question : AWS Transfer Family는 어떤 규정 준수 프로그램을 지원하나요?, answer : AWS Transfer Family는 PCI-DSS, GDPR, FedRAMP, SOC 1, 2, 3을 준수합니다. 또한 이 서비스는 HIPAA 적격 서비스입니다. 규정 준수 프로그램 제공 범위 내 서비스에 대해 자세히 알아보세요.\"\n",
            "AWS Transfer Family는 FISMA 규정을 준수하나요?\n",
            "AWS 동부 및 서부와 GovCloud(미국) 리전은 FISMA 규정을 준수합니다. AWS Transfer Family가 FedRAMP 승인을 받으면 해당하는 리전에서 FISMA 규정을 준수하게 될 것입니다. 이러한 규정 준수는 두 리전의 FedRAMP 인증을 FedRAMP Moderate 및 FedRAMP High로 입증됩니다. AWS는 시스템 보안 계획 내에서 연간 평가 및 범위 내 NIST SP 800-53 제어 준수를 문서화하여 규정 준수를 시연합니다. FedRAMP에서 요구하는 NIST 제어를 충족하는 의무를 자세하게 보여주는 고객 책임 매트릭스(CRM)와 함께 템플릿을 아티팩트에서 사용할 수 있습니다. 아티팩트는 AWS 계정으로 동부 및 서부와 GovCloud 모두에 액세스할 수 있는 관리 콘솔을 통해 제공됩니다. 해당 주제에 관한 추가 질문이 있는 경우 콘솔에서 확인하세요.\n",
            "\"category : Transfer for SFTP, question : AWS Transfer Family는 FISMA 규정을 준수하나요?, answer : AWS 동부 및 서부와 GovCloud(미국) 리전은 FISMA 규정을 준수합니다. AWS Transfer Family가 FedRAMP 승인을 받으면 해당하는 리전에서 FISMA 규정을 준수하게 될 것입니다. 이러한 규정 준수는 두 리전의 FedRAMP 인증을 FedRAMP Moderate 및 FedRAMP High로 입증됩니다. AWS는 시스템 보안 계획 내에서 연간 평가 및 범위 내 NIST SP 800-53 제어 준수를 문서화하여 규정 준수를 시연합니다. FedRAMP에서 요구하는 NIST 제어를 충족하는 의무를 자세하게 보여주는 고객 책임 매트릭스(CRM)와 함께 템플릿을 아티팩트에서 사용할 수 있습니다. 아티팩트는 AWS 계정으로 동부 및 서부와 GovCloud 모두에 액세스할 수 있는 관리 콘솔을 통해 제공됩니다. 해당 주제에 관한 추가 질문이 있는 경우 콘솔에서 확인하세요.\"\n",
            "이 서비스에서 업로드된 파일의 무결성은 어떻게 보장되나요?\n",
            "서비스를 통해 업로드된 파일은 파일의 업로드 전과 후의 MD5 체크섬을 비교하는 방식으로 확인합니다.\n",
            "\"category : Transfer for SFTP, question : 이 서비스에서 업로드된 파일의 무결성은 어떻게 보장되나요?, answer : 서비스를 통해 업로드된 파일은 파일의 업로드 전과 후의 MD5 체크섬을 비교하는 방식으로 확인합니다.\"\n",
            "전송 중 파일 암호화 및 복호화 옵션으로는 무엇이 있나요?\n",
            "AWS Transfer Family 관리형 워크플로를 사용하면 파일이 AWS Transfer Family SFTP, FTPS 또는 FTP 서버 엔드포인트에 업로드될 때 PGP 키를 사용하여 파일을 자동으로 복호화할 수 있습니다. 자세한 정보는 관리형 워크플로 문서를 참조하세요. 또는 Amazon Eventbridge에 게시된 AWS Transfer Family 이벤트 알림을 구독하면 자체 암호화/복호화 로직을 사용하여 전송된 파일의 세분화된 이벤트 기반 처리를 오케스트레이션할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 전송 중 파일 암호화 및 복호화 옵션으로는 무엇이 있나요?, answer : AWS Transfer Family 관리형 워크플로를 사용하면 파일이 AWS Transfer Family SFTP, FTPS 또는 FTP 서버 엔드포인트에 업로드될 때 PGP 키를 사용하여 파일을 자동으로 복호화할 수 있습니다. 자세한 정보는 관리형 워크플로 문서를 참조하세요. 또는 Amazon Eventbridge에 게시된 AWS Transfer Family 이벤트 알림을 구독하면 자체 암호화/복호화 로직을 사용하여 전송된 파일의 세분화된 이벤트 기반 처리를 오케스트레이션할 수 있습니다.\"\n",
            "최종 사용자와 이들의 파일 전송 활동을 모니터링하려면 어떻게 해야 하나요?\n",
            "Amazon CloudWatch에 전송되는 JSON 형식의 로그를 사용하여 최종 사용자와 해당 파일 전송 활동을 모니터링할 수 있습니다. CloudWatch 안에서 JSON 형식의 필드를 자동으로 검색하는 CloudWatch Log Insights를 사용하여 로그를 쉽게 구문 분석하고 쿼리할 수 있습니다. 또한 CloudWatch Contributor Insights를 통해 상위 사용자, 총 고유 사용자 수 및 지속적인 사용량을 추적할 수 있습니다. 또한 AWS Transfer Family 관리 콘솔에서 사전 작성된 CloudWatch 지표 및 그래프에도 액세스할 수 있습니다. 자세한 내용은 설명서를 참조하세요.\n",
            "\"category : Transfer for SFTP, question : 최종 사용자와 이들의 파일 전송 활동을 모니터링하려면 어떻게 해야 하나요?, answer : Amazon CloudWatch에 전송되는 JSON 형식의 로그를 사용하여 최종 사용자와 해당 파일 전송 활동을 모니터링할 수 있습니다. CloudWatch 안에서 JSON 형식의 필드를 자동으로 검색하는 CloudWatch Log Insights를 사용하여 로그를 쉽게 구문 분석하고 쿼리할 수 있습니다. 또한 CloudWatch Contributor Insights를 통해 상위 사용자, 총 고유 사용자 수 및 지속적인 사용량을 추적할 수 있습니다. 또한 AWS Transfer Family 관리 콘솔에서 사전 작성된 CloudWatch 지표 및 그래프에도 액세스할 수 있습니다. 자세한 내용은 설명서를 참조하세요.\"\n",
            "통합 지표를 생성하여 여러 서버의 사용자 및 파일 전송 활동을 추적할 수 있나요?\n",
            "예. 여러 AWS Transfer Family 서버의 로그 스트림을 단일 CloudWatch 로그 그룹으로 결합할 수 있습니다. 따라서 통합된 로그 지표 및 시각화를 생성할 수 있으며, 이를 CloudWatch 대시보드에 추가하여 서버 사용량 및 성능을 추적할 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 통합 지표를 생성하여 여러 서버의 사용자 및 파일 전송 활동을 추적할 수 있나요?, answer : 예. 여러 AWS Transfer Family 서버의 로그 스트림을 단일 CloudWatch 로그 그룹으로 결합할 수 있습니다. 따라서 통합된 로그 지표 및 시각화를 생성할 수 있으며, 이를 CloudWatch 대시보드에 추가하여 서버 사용량 및 성능을 추적할 수 있습니다.\"\n",
            "내 워크플로를 모니터링하려면 어떻게 해야 하나요?\n",
            "AWS CloudWatch 지표(예: 워크플로 실행, 성공한 실행 및 실패한 실행의 총 수)를 사용하여 워크플로 실행을 모니터링할 수 있습니다. AWS Management Console을 사용하여 진행 중인 워크플로 실행에 대한 실시간 상태를 검색하고 볼 수도 있습니다. 워크플로 실행의 상세 로그를 확인하려면 CloudWatch 로그를 사용합니다.\n",
            "\"category : Transfer for SFTP, question : 내 워크플로를 모니터링하려면 어떻게 해야 하나요?, answer : AWS CloudWatch 지표(예: 워크플로 실행, 성공한 실행 및 실패한 실행의 총 수)를 사용하여 워크플로 실행을 모니터링할 수 있습니다. AWS Management Console을 사용하여 진행 중인 워크플로 실행에 대한 실시간 상태를 검색하고 볼 수도 있습니다. 워크플로 실행의 상세 로그를 확인하려면 CloudWatch 로그를 사용합니다.\"\n",
            "AWS Transfer Family 로그의 형식은 어떻게 지정되나요?\n",
            "AWS Transfer Family는 서버, 커넥터, 워크플로를 비롯한 모든 리소스와 SFTP, FTPS, FTP 및 AS2를 비롯한 모든 프로토콜에서 JSON 형식으로 로그를 제공합니다.\n",
            "\"category : Transfer for SFTP, question : AWS Transfer Family 로그의 형식은 어떻게 지정되나요?, answer : AWS Transfer Family는 서버, 커넥터, 워크플로를 비롯한 모든 리소스와 SFTP, FTPS, FTP 및 AS2를 비롯한 모든 프로토콜에서 JSON 형식으로 로그를 제공합니다.\"\n",
            "AWS Transfer Family를 사용하여 파일 전송 알림을 받으려면 어떻게 해야 하나요?\n",
            "Transfer Family의 관리형 워크플로를 사용하여 SFTP, FTPS 및 FTP 서버 엔드포인트를 통해 업로드된 파일에 대한 알림을 받을 수 있습니다. 이 블로그 게시물을 참조하세요. 또는 Amazon EventBridge에서 AWS Transfer Family 이벤트를 구독하여 Amazon Simple Notification Service(SNS)를 통해 알림을 받을 수도 있습니다.\n",
            "\"category : Transfer for SFTP, question : AWS Transfer Family를 사용하여 파일 전송 알림을 받으려면 어떻게 해야 하나요?, answer : Transfer Family의 관리형 워크플로를 사용하여 SFTP, FTPS 및 FTP 서버 엔드포인트를 통해 업로드된 파일에 대한 알림을 받을 수 있습니다. 이 블로그 게시물을 참조하세요. 또는 Amazon EventBridge에서 AWS Transfer Family 이벤트를 구독하여 Amazon Simple Notification Service(SNS)를 통해 알림을 받을 수도 있습니다.\"\n",
            "워크플로 파일 검증 검사에 실패한 경우 알림을 보낼 수 있나요?\n",
            "예. 미리 구성된 워크플로 검증 단계에서 파일 검증 검사에 실패하면 예외 처리기를 사용하여 Amazon SNS 주제를 통해 모니터링 시스템을 간접적으로 호출하거나 팀원에게 알릴 수 있습니다.\n",
            "\"category : Transfer for SFTP, question : 워크플로 파일 검증 검사에 실패한 경우 알림을 보낼 수 있나요?, answer : 예. 미리 구성된 워크플로 검증 단계에서 파일 검증 검사에 실패하면 예외 처리기를 사용하여 Amazon SNS 주제를 통해 모니터링 시스템을 간접적으로 호출하거나 팀원에게 알릴 수 있습니다.\"\n",
            "서비스 이용 요금은 어떻게 청구됩니까?\n",
            "서버 엔드포인트를 만들고 구성한 시점부터 삭제한 시점까지 활성화된 각 프로토콜에 대해 시간 단위로 요금이 청구됩니다. 또한 SFTP, FTPS 또는 FTP를 통해 업로드되고 다운로드되는 데이터의 양, AS2를 통해 교환되는 메시지 수, 복호화 워크플로 단계를 사용하여 처리한 데이터의 양에 따라 요금이 청구됩니다. SFTP 커넥터를 사용하는 경우 전송된 데이터의 양과 파일 전송 API 직접 호출 수에 대해 요금이 청구됩니다. 자세한 내용은 요금 페이지를 참조하세요.\n",
            "\"category : Transfer for SFTP, question : 서비스 이용 요금은 어떻게 청구됩니까?, answer : 서버 엔드포인트를 만들고 구성한 시점부터 삭제한 시점까지 활성화된 각 프로토콜에 대해 시간 단위로 요금이 청구됩니다. 또한 SFTP, FTPS 또는 FTP를 통해 업로드되고 다운로드되는 데이터의 양, AS2를 통해 교환되는 메시지 수, 복호화 워크플로 단계를 사용하여 처리한 데이터의 양에 따라 요금이 청구됩니다. SFTP 커넥터를 사용하는 경우 전송된 데이터의 양과 파일 전송 API 직접 호출 수에 대해 요금이 청구됩니다. 자세한 내용은 요금 페이지를 참조하세요.\"\n",
            "여러 프로토콜에 동일한 서버 엔드포인트를 사용하거나 각 프로토콜에 서로 다른 엔드포인트를 사용하면 요금이 달라지나요?\n",
            "아니요. 사용하도록 설정한 프로토콜 및 각 프로토콜을 통해 전송된 데이터 양에 대해 시간 단위로 요금이 청구됩니다. 여러 프로토콜에 동일한 엔드포인트가 사용되는지 또는 각 프로토콜에 서로 다른 엔드포인트를 사용하는지는 관계가 없습니다.\n",
            "\"category : Transfer for SFTP, question : 여러 프로토콜에 동일한 서버 엔드포인트를 사용하거나 각 프로토콜에 서로 다른 엔드포인트를 사용하면 요금이 달라지나요?, answer : 아니요. 사용하도록 설정한 프로토콜 및 각 프로토콜을 통해 전송된 데이터 양에 대해 시간 단위로 요금이 청구됩니다. 여러 프로토콜에 동일한 엔드포인트가 사용되는지 또는 각 프로토콜에 서로 다른 엔드포인트를 사용하는지는 관계가 없습니다.\"\n",
            "서버를 중지했습니다. 중지 기간 동안 요금이 청구되나요?\n",
            "예. 콘솔을 사용하거나 ‘stop-server’ CLI 명령 또는 ‘StopServer’ API 명령을 실행하여 서버를 중지하는 것은 요금 청구에 영향을 미치지 않습니다. 서버 엔드포인트를 생성하여 하나 이상의 프로토콜을 통한 액세스를 구성한 시점부터 삭제한 시점까지 시간 단위로 요금이 청구됩니다.\n",
            "\"category : Transfer for SFTP, question : 서버를 중지했습니다. 중지 기간 동안 요금이 청구되나요?, answer : 예. 콘솔을 사용하거나 ‘stop-server’ CLI 명령 또는 ‘StopServer’ API 명령을 실행하여 서버를 중지하는 것은 요금 청구에 영향을 미치지 않습니다. 서버 엔드포인트를 생성하여 하나 이상의 프로토콜을 통한 액세스를 구성한 시점부터 삭제한 시점까지 시간 단위로 요금이 청구됩니다.\"\n",
            "관리형 워크플로를 사용하는 경우 어떻게 요금이 부과되나요?\n",
            "PGP 키를 사용하여 복호화하는 데이터의 양에 따라 복호화 워크플로 단계에 대한 요금이 부과됩니다. 관리형 워크플로 사용에 따른 추가 요금은 없습니다. 워크플로 구성에 따라 Amazon S3, Amazon EFS, AWS Secrets Manager, AWS Lambda 사용에 대한 요금이 부과됩니다.\n",
            "\"category : Transfer for SFTP, question : 관리형 워크플로를 사용하는 경우 어떻게 요금이 부과되나요?, answer : PGP 키를 사용하여 복호화하는 데이터의 양에 따라 복호화 워크플로 단계에 대한 요금이 부과됩니다. 관리형 워크플로 사용에 따른 추가 요금은 없습니다. 워크플로 구성에 따라 Amazon S3, Amazon EFS, AWS Secrets Manager, AWS Lambda 사용에 대한 요금이 부과됩니다.\"\n",
            "SFTP 커넥터 사용에 대해 시간당 요금이 청구되나요?\n",
            "아니요. SFTP 커넥터에 대한 시간당 요금은 없습니다. 커넥터를 사용하여 전송하거나 검색한 데이터의 양을 기준으로 요금이 청구됩니다. 또한 StartFileTransfer API 호출에서 제공한 각 파일 경로에 대해 요금이 청구됩니다.\n",
            "\"category : Transfer for SFTP, question : SFTP 커넥터 사용에 대해 시간당 요금이 청구되나요?, answer : 아니요. SFTP 커넥터에 대한 시간당 요금은 없습니다. 커넥터를 사용하여 전송하거나 검색한 데이터의 양을 기준으로 요금이 청구됩니다. 또한 StartFileTransfer API 호출에서 제공한 각 파일 경로에 대해 요금이 청구됩니다.\"\n",
            "========== VPC  :  https://aws.amazon.com/ko/vpc/faqs/ 사이트 크롤링 진행중 ==========\n",
            "145\n",
            "Amazon Virtual Private Cloud란 무엇입니까?\n",
            "Amazon VPC를 사용하면 Amazon Web Services(AWS) 클라우드에서 논리적으로 격리된 공간을 프로비저닝하고, 정의한 가상 네트워크에서 AWS 리소스를 시작할 수 있습니다. 자체 IP 주소 범위 선택, 서브넷 생성, 라우팅 테이블 및 네트워크 게이트웨이 구성 등 가상 네트워킹 환경을 완벽하게 제어할 수 있습니다. 또한, 기업 데이터 센터와 VPC 사이에 하드웨어 가상 프라이빗 네트워크(VPN) 연결을 생성하여, 기업 데이터 센터의 확장으로서 AWS 클라우드를 사용할 수 있습니다.\n",
            "Amazon VPC용 네트워크 구성을 손쉽게 사용자 지정할 수 있습니다. 예를 들어, 인터넷에 액세스할 수 있는 웹 서버를 위해 퍼블릭 서브넷을 생성할 수 있으며, 인터넷 액세스가 없는 프라이빗 서브넷에 데이터베이스나 애플리케이션 서버와 같은 백엔드 시스템을 배치할 수 있습니다. 보안 그룹 및 네트워크 액세스 제어 목록을 포함한 다중 보안 계층을 활용하여 각 서브넷에서 Amazon EC2 인스턴스에 대한 액세스를 제어하도록 지원할 수 있습니다.\n",
            "\"category : VPC, question : Amazon Virtual Private Cloud란 무엇입니까?, answer : Amazon VPC를 사용하면 Amazon Web Services(AWS) 클라우드에서 논리적으로 격리된 공간을 프로비저닝하고, 정의한 가상 네트워크에서 AWS 리소스를 시작할 수 있습니다. 자체 IP 주소 범위 선택, 서브넷 생성, 라우팅 테이블 및 네트워크 게이트웨이 구성 등 가상 네트워킹 환경을 완벽하게 제어할 수 있습니다. 또한, 기업 데이터 센터와 VPC 사이에 하드웨어 가상 프라이빗 네트워크(VPN) 연결을 생성하여, 기업 데이터 센터의 확장으로서 AWS 클라우드를 사용할 수 있습니다.\n",
            "Amazon VPC용 네트워크 구성을 손쉽게 사용자 지정할 수 있습니다. 예를 들어, 인터넷에 액세스할 수 있는 웹 서버를 위해 퍼블릭 서브넷을 생성할 수 있으며, 인터넷 액세스가 없는 프라이빗 서브넷에 데이터베이스나 애플리케이션 서버와 같은 백엔드 시스템을 배치할 수 있습니다. 보안 그룹 및 네트워크 액세스 제어 목록을 포함한 다중 보안 계층을 활용하여 각 서브넷에서 Amazon EC2 인스턴스에 대한 액세스를 제어하도록 지원할 수 있습니다.\"\n",
            "Amazon VPC의 구성 요소는 무엇인가요?\n",
            "Amazon VPC는 기존 네트워크를 사용하던 고객에게 익숙한 여러 객체로 구성되어 있습니다.\n",
            "\n",
            "Virtual Private Cloud: AWS 클라우드 내 논리적으로 격리된 가상 네트워크입니다. 선택한 범위에서 VPC의 IP 주소 공간을 정의합니다.\n",
            "서브넷: 격리된 리소스 그룹을 배치할 수 있는 VPC IP 주소 범위의 한 세그먼트입니다.\n",
            "인터넷 게이트웨이: 인터넷에 연결되는 Amazon VPC 측 게이트웨이입니다.\n",
            "NAT 게이트웨이: 프라이빗 서브넷에 있는 리소스가 인터넷에 액세스할 수 있게 해주는 고가용성 관리형 Network Address Translation(NAT) 서비스입니다.\n",
            "가상 프라이빗 게이트웨이: VPN에 연결되는 Amazon VPC 측 게이트웨이입니다.\n",
            "피어링 연결: 피어링 연결을 사용하여 프라이빗 IP 주소를 통해 피어링되는 두 VPC 간 트래픽을 라우팅할 수 있습니다.\n",
            "VPC 엔드포인트: 인터넷 게이트웨이, VPN, Network Address Translation(NAT) 디바이스 또는 방화벽 프록시를 사용하지 않고 AWS에서 호스팅되는 서비스에 VPC 내에서부터 비공개로 연결할 수 있습니다.\n",
            "송신 전용 인터넷 게이트웨이: VPC에서 인터넷으로 IPv6 트래픽에 대하여 송신 전용 액세스를 제공하는 상태 저장 게이트웨이입니다.\n",
            "\"category : VPC, question : Amazon VPC의 구성 요소는 무엇인가요?, answer : Amazon VPC는 기존 네트워크를 사용하던 고객에게 익숙한 여러 객체로 구성되어 있습니다.\n",
            "\n",
            "Virtual Private Cloud: AWS 클라우드 내 논리적으로 격리된 가상 네트워크입니다. 선택한 범위에서 VPC의 IP 주소 공간을 정의합니다.\n",
            "서브넷: 격리된 리소스 그룹을 배치할 수 있는 VPC IP 주소 범위의 한 세그먼트입니다.\n",
            "인터넷 게이트웨이: 인터넷에 연결되는 Amazon VPC 측 게이트웨이입니다.\n",
            "NAT 게이트웨이: 프라이빗 서브넷에 있는 리소스가 인터넷에 액세스할 수 있게 해주는 고가용성 관리형 Network Address Translation(NAT) 서비스입니다.\n",
            "가상 프라이빗 게이트웨이: VPN에 연결되는 Amazon VPC 측 게이트웨이입니다.\n",
            "피어링 연결: 피어링 연결을 사용하여 프라이빗 IP 주소를 통해 피어링되는 두 VPC 간 트래픽을 라우팅할 수 있습니다.\n",
            "VPC 엔드포인트: 인터넷 게이트웨이, VPN, Network Address Translation(NAT) 디바이스 또는 방화벽 프록시를 사용하지 않고 AWS에서 호스팅되는 서비스에 VPC 내에서부터 비공개로 연결할 수 있습니다.\n",
            "송신 전용 인터넷 게이트웨이: VPC에서 인터넷으로 IPv6 트래픽에 대하여 송신 전용 액세스를 제공하는 상태 저장 게이트웨이입니다.\"\n",
            "Amazon VPC를 사용해야 하는 이유는 무엇인가요?\n",
            "Amazon VPC를 통해 AWS 클라우드에서 가상 네트워크를 구축할 수 있으며, VPN, 하드웨어, 물리적 데이터 센터는 필요하지 않습니다. 자체 네트워크 공간을 정의하고 네트워크 및 네트워크 내에 있는 Amazon EC2 리소스가 인터넷에 노출되는 방식을 제어할 수 있습니다. 또한, Amazon VPC의 월등하게 향상된 보안 옵션을 이용하여 가상 네트워크 내 Amazon EC2 인스턴스에 더 세분화된 방식으로 액세스할 수 있습니다.\n",
            "\"category : VPC, question : Amazon VPC를 사용해야 하는 이유는 무엇인가요?, answer : Amazon VPC를 통해 AWS 클라우드에서 가상 네트워크를 구축할 수 있으며, VPN, 하드웨어, 물리적 데이터 센터는 필요하지 않습니다. 자체 네트워크 공간을 정의하고 네트워크 및 네트워크 내에 있는 Amazon EC2 리소스가 인터넷에 노출되는 방식을 제어할 수 있습니다. 또한, Amazon VPC의 월등하게 향상된 보안 옵션을 이용하여 가상 네트워크 내 Amazon EC2 인스턴스에 더 세분화된 방식으로 액세스할 수 있습니다.\"\n",
            "Amazon VPC를 시작하려면 어떻게 해야 하나요?\n",
            "바로 사용 가능한 기본 VPC에 AWS 리소스가 자동으로 프로비저닝됩니다. AWS Management Console의 Amazon VPC 페이지로 이동하여 \"Start VPC Wizard\"를 선택하면 추가 VPC를 만들 수 있습니다.\n",
            "네트워크 아키텍처에 대한 네 가지 기본 옵션을 확인할 수 있습니다. 옵션을 선택한 다음, VPC와 서브넷의 크기 및 IP 주소 범위를 변경할 수 있습니다. Hardware VPN Access 옵션을 선택하면 네트워크 상에서 VPN 하드웨어의 IP 주소를 지정해야 합니다. 보조 IP 범위와 게이트웨이를 추가 또는 제거하거나 더 많은 서브넷을 IP 범위에 추가하도록 VPC를 변경할 수 있습니다.\n",
            "네 가지 옵션은 다음과 같습니다.\n",
            "\n",
            "단일 퍼블릭 서브넷만 있는 Amazon VPC\n",
            "퍼블릭 및 프라이빗 서브넷이 있는 Amazon VPC\n",
            "퍼블릭 및 프라이빗 서브넷이 있고 AWS Site-to-Site VPN 액세스를 제공하는 Amazon VPC\n",
            "퍼블릭 서브넷만 있고 AWS Site-to-Site VPN 액세스를 제공하는 Amazon VPC\n",
            "\"category : VPC, question : Amazon VPC를 시작하려면 어떻게 해야 하나요?, answer : 바로 사용 가능한 기본 VPC에 AWS 리소스가 자동으로 프로비저닝됩니다. AWS Management Console의 Amazon VPC 페이지로 이동하여 \"Start VPC Wizard\"를 선택하면 추가 VPC를 만들 수 있습니다.\n",
            "네트워크 아키텍처에 대한 네 가지 기본 옵션을 확인할 수 있습니다. 옵션을 선택한 다음, VPC와 서브넷의 크기 및 IP 주소 범위를 변경할 수 있습니다. Hardware VPN Access 옵션을 선택하면 네트워크 상에서 VPN 하드웨어의 IP 주소를 지정해야 합니다. 보조 IP 범위와 게이트웨이를 추가 또는 제거하거나 더 많은 서브넷을 IP 범위에 추가하도록 VPC를 변경할 수 있습니다.\n",
            "네 가지 옵션은 다음과 같습니다.\n",
            "\n",
            "단일 퍼블릭 서브넷만 있는 Amazon VPC\n",
            "퍼블릭 및 프라이빗 서브넷이 있는 Amazon VPC\n",
            "퍼블릭 및 프라이빗 서브넷이 있고 AWS Site-to-Site VPN 액세스를 제공하는 Amazon VPC\n",
            "퍼블릭 서브넷만 있고 AWS Site-to-Site VPN 액세스를 제공하는 Amazon VPC\"\n",
            "Amazon VPC에서 사용할 수 있는 VPC 엔드포인트 유형에는 어떤 것이 있나요?\n",
            "VPC 엔드포인트를 사용하면 인터넷 게이트웨이, NAT 또는 방화벽 프록시를 사용하지 않고도 VPN을 AWS에서 호스팅하는 서비스와 비공개로 연결할 수 있습니다. 엔드포인트는 수평적으로 확장 가능하며 가용성이 매우 뛰어난 가상 디바이스로서, VPC 내 인스턴스와 AWS 서비스 간에 통신을 허용합니다. Amazon VPC는 게이트웨이 유형 엔드포인트와 인터페이스 유형 엔드포인트라는 두 가지 유형의 엔드포인트를 제공합니다.\n",
            "게이트웨이 유형 엔드포인트는 S3 및 DynamoDB를 비롯한 AWS 서비스에서만 이용할 수 있습니다. 이러한 엔드포인트는 사용자가 선택한 라우팅 테이블에 항목을 추가하고, 트래픽을 Amazon의 프라이빗 네트워크를 통해 지원되는 서비스로 라우팅합니다.\n",
            "인터페이스 유형 엔드포인트는 AWS 서비스인 PrivateLink에서 지원하는 서비스, 자체 서비스 또는 SaaS 솔루션에 비공개로 연결할 수 있으며, Direct Connect를 통한 연결을 지원합니다. 앞으로 이러한 엔드포인트에서 더 많은 AWS 및 SaaS 솔루션을 지원할 예정입니다. 인터페이스 유형 엔드포인트의 요금은 VPC 요금 페이지를 참조하십시오.\n",
            "\"category : VPC, question : Amazon VPC에서 사용할 수 있는 VPC 엔드포인트 유형에는 어떤 것이 있나요?, answer : VPC 엔드포인트를 사용하면 인터넷 게이트웨이, NAT 또는 방화벽 프록시를 사용하지 않고도 VPN을 AWS에서 호스팅하는 서비스와 비공개로 연결할 수 있습니다. 엔드포인트는 수평적으로 확장 가능하며 가용성이 매우 뛰어난 가상 디바이스로서, VPC 내 인스턴스와 AWS 서비스 간에 통신을 허용합니다. Amazon VPC는 게이트웨이 유형 엔드포인트와 인터페이스 유형 엔드포인트라는 두 가지 유형의 엔드포인트를 제공합니다.\n",
            "게이트웨이 유형 엔드포인트는 S3 및 DynamoDB를 비롯한 AWS 서비스에서만 이용할 수 있습니다. 이러한 엔드포인트는 사용자가 선택한 라우팅 테이블에 항목을 추가하고, 트래픽을 Amazon의 프라이빗 네트워크를 통해 지원되는 서비스로 라우팅합니다.\n",
            "인터페이스 유형 엔드포인트는 AWS 서비스인 PrivateLink에서 지원하는 서비스, 자체 서비스 또는 SaaS 솔루션에 비공개로 연결할 수 있으며, Direct Connect를 통한 연결을 지원합니다. 앞으로 이러한 엔드포인트에서 더 많은 AWS 및 SaaS 솔루션을 지원할 예정입니다. 인터페이스 유형 엔드포인트의 요금은 VPC 요금 페이지를 참조하십시오.\"\n",
            "Amazon VPC의 사용료는 어떻게 과금되어 청구되나요?\n",
            "VPC 자체를 생성 및 사용하는 데에는 별도의 비용이 없습니다. Amazon EC2와 같은 다른 Amazon Web Services에 대한 사용 요금에 대해서는, 데이터 전송 요금을 비롯하여 공개된 요금이 적용됩니다. 선택 사항인 하드웨어 VPN 연결을 사용하여 VPC를 회사의 데이터 센터에 연결하는 경우, VPN 연결 시간(VPN 연결이 \"사용 가능\" 상태인 시간)당 요금이 부과됩니다. 1시간 미만으로 사용해도 1시간 사용 금액이 청구됩니다. VPN 연결을 통해 전송된 데이터는 표준 AWS 데이터 전송 요금이 청구됩니다. VPC-VPN 요금에 대한 자세한 정보는 Amazon VPC 제품 페이지의 요금 섹션을 참조하세요.\n",
            "\"category : VPC, question : Amazon VPC의 사용료는 어떻게 과금되어 청구되나요?, answer : VPC 자체를 생성 및 사용하는 데에는 별도의 비용이 없습니다. Amazon EC2와 같은 다른 Amazon Web Services에 대한 사용 요금에 대해서는, 데이터 전송 요금을 비롯하여 공개된 요금이 적용됩니다. 선택 사항인 하드웨어 VPN 연결을 사용하여 VPC를 회사의 데이터 센터에 연결하는 경우, VPN 연결 시간(VPN 연결이 \"사용 가능\" 상태인 시간)당 요금이 부과됩니다. 1시간 미만으로 사용해도 1시간 사용 금액이 청구됩니다. VPN 연결을 통해 전송된 데이터는 표준 AWS 데이터 전송 요금이 청구됩니다. VPC-VPN 요금에 대한 자세한 정보는 Amazon VPC 제품 페이지의 요금 섹션을 참조하세요.\"\n",
            "개인 VPC에서 Amazon EC2 인스턴스의 다른 AWS 서비스(예: Amazon S3)를 사용하면 어떤 사용료가 발생하나요?\n",
            "Amazon EC2와 같은 다른 Amazon Web Services에 대한 사용 요금에 대해서는 해당 리소스에 대해 명시된 요금이 적용됩니다. VPC의 인터넷 게이트웨이를 통해 Amazon S3와 같은 Amazon Web Services에 액세스하는 동안에는 데이터 전송 요금이 발생하지 않습니다.\n",
            "VPN 연결을 통해 AWS 리소스에 액세스하면 인터넷 데이터 전송 요금이 청구됩니다.\n",
            "\"category : VPC, question : 개인 VPC에서 Amazon EC2 인스턴스의 다른 AWS 서비스(예: Amazon S3)를 사용하면 어떤 사용료가 발생하나요?, answer : Amazon EC2와 같은 다른 Amazon Web Services에 대한 사용 요금에 대해서는 해당 리소스에 대해 명시된 요금이 적용됩니다. VPC의 인터넷 게이트웨이를 통해 Amazon S3와 같은 Amazon Web Services에 액세스하는 동안에는 데이터 전송 요금이 발생하지 않습니다.\n",
            "VPN 연결을 통해 AWS 리소스에 액세스하면 인터넷 데이터 전송 요금이 청구됩니다.\"\n",
            "Amazon VPC의 연결 옵션에는 무엇이 있나요?\n",
            "Amazon VPC를 다음 대상에 연결할 수 있습니다.\n",
            "\n",
            "인터넷 (인터넷 게이트웨이를 통해)\n",
            "AWS Site-to-Site VPN 연결을 사용하여 회사 데이터 센터 (가상 프라이빗 게이트웨이를 통해)\n",
            "인터넷과 사용자의 회사 데이터 센터 모두 (인터넷 게이트웨이와 가상 프라이빗 게이트웨이를 모두 사용)\n",
            "다른 AWS 서비스 (인터넷 게이트웨이, NAT, 가상 프라이빗 게이트웨이 또는 VPC 엔드포인트를 통해)\n",
            "다른 Amazon VPC (VPC 피어링 연결을 통해)\n",
            "\"category : VPC, question : Amazon VPC의 연결 옵션에는 무엇이 있나요?, answer : Amazon VPC를 다음 대상에 연결할 수 있습니다.\n",
            "\n",
            "인터넷 (인터넷 게이트웨이를 통해)\n",
            "AWS Site-to-Site VPN 연결을 사용하여 회사 데이터 센터 (가상 프라이빗 게이트웨이를 통해)\n",
            "인터넷과 사용자의 회사 데이터 센터 모두 (인터넷 게이트웨이와 가상 프라이빗 게이트웨이를 모두 사용)\n",
            "다른 AWS 서비스 (인터넷 게이트웨이, NAT, 가상 프라이빗 게이트웨이 또는 VPC 엔드포인트를 통해)\n",
            "다른 Amazon VPC (VPC 피어링 연결을 통해)\"\n",
            "VPC를 인터넷에 연결하려면 어떻게 해야 하나요?\n",
            "Amazon VPC는 인터넷 게이트웨이 생성을 지원합니다. 이 게이트웨이를 사용하면 VPC 내 Amazon EC2 인스턴스가 인터넷에 직접 액세스할 수 있습니다. VPC에서 인터넷으로 IPv6 트래픽에 대하여 송신 전용 액세스를 제공하는 상태 유지 게이트웨이인 송신 전용 인터넷 게이트웨이를 사용할 수도 있습니다.\n",
            "\"category : VPC, question : VPC를 인터넷에 연결하려면 어떻게 해야 하나요?, answer : Amazon VPC는 인터넷 게이트웨이 생성을 지원합니다. 이 게이트웨이를 사용하면 VPC 내 Amazon EC2 인스턴스가 인터넷에 직접 액세스할 수 있습니다. VPC에서 인터넷으로 IPv6 트래픽에 대하여 송신 전용 액세스를 제공하는 상태 유지 게이트웨이인 송신 전용 인터넷 게이트웨이를 사용할 수도 있습니다.\"\n",
            "인터넷 게이트웨이에 대한 대역폭 제한이 있습니까? 가용성에 대해 걱정해야 합니까? 단일 장애 지점일 수 있나요?\n",
            "아니요. 인터넷 게이트웨이는 수평적으로 확장되고, 중복적이며, 가용성이 뛰어납니다. 인터넷 게이트웨이에 대한 대역폭 제한은 없습니다.\n",
            "\"category : VPC, question : 인터넷 게이트웨이에 대한 대역폭 제한이 있습니까? 가용성에 대해 걱정해야 합니까? 단일 장애 지점일 수 있나요?, answer : 아니요. 인터넷 게이트웨이는 수평적으로 확장되고, 중복적이며, 가용성이 뛰어납니다. 인터넷 게이트웨이에 대한 대역폭 제한은 없습니다.\"\n",
            "VPC에 있는 인스턴스는 어떻게 인터넷에 액세스하나요?\n",
            "탄력적 IP 주소(EIP)와 IPv6 글로벌 고유 주소(GUA)를 비롯한 퍼블릭 IP 주소를 사용하면 VPC 내의 인스턴스가 인터넷으로 직접 아웃바운드 통신을 수행하고 인터넷으로부터(예: 웹 서버) 임의의 인바운드 트래픽을 수신할 수 있습니다. 다음 질문에 있는 솔루션을 사용할 수도 있습니다.\n",
            "\"category : VPC, question : VPC에 있는 인스턴스는 어떻게 인터넷에 액세스하나요?, answer : 탄력적 IP 주소(EIP)와 IPv6 글로벌 고유 주소(GUA)를 비롯한 퍼블릭 IP 주소를 사용하면 VPC 내의 인스턴스가 인터넷으로 직접 아웃바운드 통신을 수행하고 인터넷으로부터(예: 웹 서버) 임의의 인바운드 트래픽을 수신할 수 있습니다. 다음 질문에 있는 솔루션을 사용할 수도 있습니다.\"\n",
            "IP 주소는 언제 퍼블릭 IP 주소로 간주되나요?\n",
            "인터넷을 통해 액세스할 수 있는 VPC에서 호스팅되는 서비스 또는 인스턴스에 할당된 모든 IP 주소는 퍼블릭 IP 주소로 간주됩니다. 탄력적 IP 주소(EIP) 및 IPv6 GUA를 포함한 퍼블릭 IPv4 주소만 인터넷에서 라우팅할 수 있습니다. 이렇게 하려면 먼저 VPC를 인터넷에 연결한 다음 인터넷에서 연결할 수 있도록 라우팅 테이블을 업데이트해야 합니다.\n",
            "\"category : VPC, question : IP 주소는 언제 퍼블릭 IP 주소로 간주되나요?, answer : 인터넷을 통해 액세스할 수 있는 VPC에서 호스팅되는 서비스 또는 인스턴스에 할당된 모든 IP 주소는 퍼블릭 IP 주소로 간주됩니다. 탄력적 IP 주소(EIP) 및 IPv6 GUA를 포함한 퍼블릭 IPv4 주소만 인터넷에서 라우팅할 수 있습니다. 이렇게 하려면 먼저 VPC를 인터넷에 연결한 다음 인터넷에서 연결할 수 있도록 라우팅 테이블을 업데이트해야 합니다.\"\n",
            "퍼블릭 IP 주소가 없는 인스턴스가 인터넷에 액세스하려면 어떻게 해야 하나요?\n",
            "퍼블릭 IP 주소가 없는 인스턴스는 다음 두 가지 방법 중 하나를 사용하여 인터넷에 액세스할 수 있습니다.\n",
            "\n",
            "퍼블릭 IP 주소가 없는 인스턴스는 NAT 게이트웨이 또는 NAT 인스턴스를 통해 트래픽을 라우팅하여 인터넷에 액세스할 수 있습니다. 이러한 인스턴스는 인터넷을 통과하기 위해 NAT 게이트웨이 또는 NAT 인스턴스의 퍼블릭 IP 주소를 사용합니다. NAT 게이트웨이 또는 NAT 인스턴스는 아웃바운드 통신을 허용하지만, 인터넷상에서 시스템이 프라이빗 주소의 인스턴스에 연결을 시도하는 것을 허용하지 않습니다.\n",
            "하드웨어 VPN 연결 또는 Direct Connect 연결이 지원되는 VPC의 경우, 인스턴스는 가상 프라이빗 게이트웨이의 인터넷 트래픽을 사용자의 기존 데이터 센터로 라우팅할 수 있습니다. 여기서 라우터 및 네트워크 보안/모니터링 디바이스를 통해 인터넷에 액세스할 수 있습니다.\n",
            "\"category : VPC, question : 퍼블릭 IP 주소가 없는 인스턴스가 인터넷에 액세스하려면 어떻게 해야 하나요?, answer : 퍼블릭 IP 주소가 없는 인스턴스는 다음 두 가지 방법 중 하나를 사용하여 인터넷에 액세스할 수 있습니다.\n",
            "\n",
            "퍼블릭 IP 주소가 없는 인스턴스는 NAT 게이트웨이 또는 NAT 인스턴스를 통해 트래픽을 라우팅하여 인터넷에 액세스할 수 있습니다. 이러한 인스턴스는 인터넷을 통과하기 위해 NAT 게이트웨이 또는 NAT 인스턴스의 퍼블릭 IP 주소를 사용합니다. NAT 게이트웨이 또는 NAT 인스턴스는 아웃바운드 통신을 허용하지만, 인터넷상에서 시스템이 프라이빗 주소의 인스턴스에 연결을 시도하는 것을 허용하지 않습니다.\n",
            "하드웨어 VPN 연결 또는 Direct Connect 연결이 지원되는 VPC의 경우, 인스턴스는 가상 프라이빗 게이트웨이의 인터넷 트래픽을 사용자의 기존 데이터 센터로 라우팅할 수 있습니다. 여기서 라우터 및 네트워크 보안/모니터링 디바이스를 통해 인터넷에 액세스할 수 있습니다.\"\n",
            "소프트웨어 VPN을 사용하여 VPC에 연결할 수 있나요?\n",
            "예. 타사 소프트웨어 VPN을 사용하여 인터넷 게이트웨이를 통한 VPC와 사이트 간 또는 원격 액세스 VPN 연결을 생성할 수 있습니다.\n",
            "\"category : VPC, question : 소프트웨어 VPN을 사용하여 VPC에 연결할 수 있나요?, answer : 예. 타사 소프트웨어 VPN을 사용하여 인터넷 게이트웨이를 통한 VPC와 사이트 간 또는 원격 액세스 VPN 연결을 생성할 수 있습니다.\"\n",
            "두 인스턴스가 퍼블릭 IP 주소를 사용하여 통신할 경우 또는 인스턴스가 퍼블릭 AWS 서비스 엔드포인트와 통신할 경우 트래픽이 인터넷을 통해 전송되나요?\n",
            "아니요. 퍼블릭 IP 주소를 사용할 경우 AWS에서 호스팅되는 인스턴스와 서비스 간의 모든 통신에는 AWS의 프라이빗 네트워크가 사용됩니다. AWS 네트워크에서 생성되고 그 대상도 AWS 네트워크에 있는 패킷은 AWS 글로벌 네트워크를 벗어나지 않습니다. 단, AWS 중국 리전으로 들어오거나 나가는 트래픽은 예외입니다.\n",
            "또한 데이터 센터 및 리전을 상호 연결하는 AWS 글로벌 네트워크를 통해 이동하는 모든 데이터는 보안 시설을 떠나기 전에 물리적 계층에서 자동으로 암호화됩니다. 예를 들어 모든 VPC 교차 리전 피어링 트래픽, 고객 또는 서비스 간 TLS(전송 계층 보안) 연결 등 추가적인 암호화 계층도 존재합니다.\n",
            "\"category : VPC, question : 두 인스턴스가 퍼블릭 IP 주소를 사용하여 통신할 경우 또는 인스턴스가 퍼블릭 AWS 서비스 엔드포인트와 통신할 경우 트래픽이 인터넷을 통해 전송되나요?, answer : 아니요. 퍼블릭 IP 주소를 사용할 경우 AWS에서 호스팅되는 인스턴스와 서비스 간의 모든 통신에는 AWS의 프라이빗 네트워크가 사용됩니다. AWS 네트워크에서 생성되고 그 대상도 AWS 네트워크에 있는 패킷은 AWS 글로벌 네트워크를 벗어나지 않습니다. 단, AWS 중국 리전으로 들어오거나 나가는 트래픽은 예외입니다.\n",
            "또한 데이터 센터 및 리전을 상호 연결하는 AWS 글로벌 네트워크를 통해 이동하는 모든 데이터는 보안 시설을 떠나기 전에 물리적 계층에서 자동으로 암호화됩니다. 예를 들어 모든 VPC 교차 리전 피어링 트래픽, 고객 또는 서비스 간 TLS(전송 계층 보안) 연결 등 추가적인 암호화 계층도 존재합니다.\"\n",
            "AWS Site-to-Site VPN 연결은 Amazon VPC에서 어떻게 작동하나요?\n",
            "AWS Site-to-Site VPN 연결은 사용자의 VPC를 데이터센터에 연결합니다. Amazon은 인터넷 프로토콜 보안 (IPsec) VPN 연결을 지원합니다. VPC와 데이터센터 간에 전송되는 데이터는 암호화된 VPN 연결을 통해 라우팅되어 전송 데이터의 기밀성과 무결성이 유지됩니다. AWS Site-to-Site VPN 연결을 형성하는데 인터넷 게이트웨이는 필요하지 않습니다.\n",
            "\"category : VPC, question : AWS Site-to-Site VPN 연결은 Amazon VPC에서 어떻게 작동하나요?, answer : AWS Site-to-Site VPN 연결은 사용자의 VPC를 데이터센터에 연결합니다. Amazon은 인터넷 프로토콜 보안 (IPsec) VPN 연결을 지원합니다. VPC와 데이터센터 간에 전송되는 데이터는 암호화된 VPN 연결을 통해 라우팅되어 전송 데이터의 기밀성과 무결성이 유지됩니다. AWS Site-to-Site VPN 연결을 형성하는데 인터넷 게이트웨이는 필요하지 않습니다.\"\n",
            "Amazon VPC에서 사용할 수 있는 IP 주소 범위는 어떻게 되나요?\n",
            "RFC 1918 또는 공개적으로 라우팅이 가능한 IP 범위를 비롯하여 모든 IPv4 주소 범위를 기본 CIDR 블록에 사용할 수 있습니다. 보조 CIDR 블록에는 특정 제약 조건이 적용됩니다. 공개적으로 라우팅이 가능한 IP 블록은 가상 프라이빗 게이트웨이를 통해서만 연결할 수 있으며, 인터넷 게이트웨이를 통해서는 인터넷상에서 액세스할 수 없습니다. AWS는 고객 소유 IP 주소 블록을 인터넷에 노출하지 않습니다. 관련 API를 호출하거나 AWS 관리 콘솔을 통해 최대 5개의 AWS 제공 또는 BYOIP IPv6 GUA CIDR 블록을 VPC에 할당할 수 있습니다.\n",
            "\"category : VPC, question : Amazon VPC에서 사용할 수 있는 IP 주소 범위는 어떻게 되나요?, answer : RFC 1918 또는 공개적으로 라우팅이 가능한 IP 범위를 비롯하여 모든 IPv4 주소 범위를 기본 CIDR 블록에 사용할 수 있습니다. 보조 CIDR 블록에는 특정 제약 조건이 적용됩니다. 공개적으로 라우팅이 가능한 IP 블록은 가상 프라이빗 게이트웨이를 통해서만 연결할 수 있으며, 인터넷 게이트웨이를 통해서는 인터넷상에서 액세스할 수 없습니다. AWS는 고객 소유 IP 주소 블록을 인터넷에 노출하지 않습니다. 관련 API를 호출하거나 AWS 관리 콘솔을 통해 최대 5개의 AWS 제공 또는 BYOIP IPv6 GUA CIDR 블록을 VPC에 할당할 수 있습니다.\"\n",
            "IP 주소 범위를 Amazon VPC에 할당하려면 어떻게 해야 하나요?\n",
            "VPC를 생성할 때 단일 Classless Internet Domain Routing(CIDR) IP 주소 범위를 기본 CIDR 블록으로 할당하며, VPC를 생성한 후 최대 4개의 보조 CIDR 블록을 추가할 수 있습니다. VPC 내의 서브넷은 사용자가 이 CIDR 범위에서 주소를 지정합니다. 겹치는 IP 주소 범위로 여러 VPC를 생성할 수는 있으나, 그런 경우 해당 VPC를 하드웨어 VPN 연결을 통해 일반 홈 네트워크에 연결하지 못하게 됩니다. 그러므로 겹치지 않는 IP 주소 범위를 사용하는 것이 좋습니다. 최대 5개의 Amazon 제공 또는 BYOIP IPv6 CIDR 블록을 VPC에 할당할 수 있습니다.\n",
            "\"category : VPC, question : IP 주소 범위를 Amazon VPC에 할당하려면 어떻게 해야 하나요?, answer : VPC를 생성할 때 단일 Classless Internet Domain Routing(CIDR) IP 주소 범위를 기본 CIDR 블록으로 할당하며, VPC를 생성한 후 최대 4개의 보조 CIDR 블록을 추가할 수 있습니다. VPC 내의 서브넷은 사용자가 이 CIDR 범위에서 주소를 지정합니다. 겹치는 IP 주소 범위로 여러 VPC를 생성할 수는 있으나, 그런 경우 해당 VPC를 하드웨어 VPN 연결을 통해 일반 홈 네트워크에 연결하지 못하게 됩니다. 그러므로 겹치지 않는 IP 주소 범위를 사용하는 것이 좋습니다. 최대 5개의 Amazon 제공 또는 BYOIP IPv6 CIDR 블록을 VPC에 할당할 수 있습니다.\"\n",
            "기본 Amazon VPC에는 어떠한 IP 주소 범위가 할당되나요?\n",
            "기본 VPC에는 CIDR 범위 172.31.0.0/16이 할당됩니다. 기본 VPC 내 기본 서브넷에는 VPC CIDR 범위 내 /20 네트블록이 할당됩니다.\n",
            "\"category : VPC, question : 기본 Amazon VPC에는 어떠한 IP 주소 범위가 할당되나요?, answer : 기본 VPC에는 CIDR 범위 172.31.0.0/16이 할당됩니다. 기본 VPC 내 기본 서브넷에는 VPC CIDR 범위 내 /20 네트블록이 할당됩니다.\"\n",
            "VPC에서 IP 주소를 사용하고 인터넷을 통해 액세스할 수 있나요?\n",
            "예. 퍼블릭 IPv4 주소와 IPv6 GUA 주소를 AWS VPC로 가져와서 이를 서브넷과 EC2 인스턴스에 정적으로 할당할 수 있습니다. 인터넷을 통해 이러한 주소에 액세스하려면 온프레미스 네트워크에서 인터넷에 이를 알려야 합니다. 또한, AWS DX 또는 AWS VPN 연결을 사용하여 VPC와 온프레미스 네트워크 간에 이러한 주소를 통해 트래픽을 라우팅해야 합니다. 가상 프라이빗 게이트웨이를 사용하여 VPC에서 트래픽을 라우팅할 수 있습니다. 마찬가지로 라우터를 사용하여 온프레미스 네트워크에서 VPC로 다시 트래픽을 라우팅할 수 있습니다.\n",
            "\"category : VPC, question : VPC에서 IP 주소를 사용하고 인터넷을 통해 액세스할 수 있나요?, answer : 예. 퍼블릭 IPv4 주소와 IPv6 GUA 주소를 AWS VPC로 가져와서 이를 서브넷과 EC2 인스턴스에 정적으로 할당할 수 있습니다. 인터넷을 통해 이러한 주소에 액세스하려면 온프레미스 네트워크에서 인터넷에 이를 알려야 합니다. 또한, AWS DX 또는 AWS VPN 연결을 사용하여 VPC와 온프레미스 네트워크 간에 이러한 주소를 통해 트래픽을 라우팅해야 합니다. 가상 프라이빗 게이트웨이를 사용하여 VPC에서 트래픽을 라우팅할 수 있습니다. 마찬가지로 라우터를 사용하여 온프레미스 네트워크에서 VPC로 다시 트래픽을 라우팅할 수 있습니다.\"\n",
            "생성할 수 있는 VPC의 크기는 어느 정도인가요?\n",
            "현재 Amazon VPC에서는 IPv4에 대해 5개의 IP 주소 범위(기본 1개와 보조 4개)를 지원합니다. 각 범위의 크기는 /28(CIDR 표기법)과 /16 사이가 될 수 있습니다. VPC의 IP 주소 범위는 기존 네트워크의 IP 주소 범위와 겹치지 않아야 합니다.\n",
            "IPv6의 경우 VPC는 고정 크기 /56입니다(CIDR 표기법). VPC는 IPv4 및 IPv6 CIDR 블록 둘 다 연계할 수 있습니다.\n",
            "\"category : VPC, question : 생성할 수 있는 VPC의 크기는 어느 정도인가요?, answer : 현재 Amazon VPC에서는 IPv4에 대해 5개의 IP 주소 범위(기본 1개와 보조 4개)를 지원합니다. 각 범위의 크기는 /28(CIDR 표기법)과 /16 사이가 될 수 있습니다. VPC의 IP 주소 범위는 기존 네트워크의 IP 주소 범위와 겹치지 않아야 합니다.\n",
            "IPv6의 경우 VPC는 고정 크기 /56입니다(CIDR 표기법). VPC는 IPv4 및 IPv6 CIDR 블록 둘 다 연계할 수 있습니다.\"\n",
            "VPC 크기를 변경할 수 있나요?\n",
            "예. VPC에 4개의 보조 IPv4 IP 범위(CIDR)를 추가하면 기존 VPC를 확장할 수 있습니다. VPC에 추가한 보존 CIDR 블록을 삭제하면 VPC의 크기를 줄일 수 있습니다.   마찬가지로 최대 5개의 추가 IPv6 IP 범위(CIDR)를 VPC에 추가할 수 있습니다.  이 추가 범위를 삭제하여 VPC를 축소할 수 있습니다.\n",
            "\"category : VPC, question : VPC 크기를 변경할 수 있나요?, answer : 예. VPC에 4개의 보조 IPv4 IP 범위(CIDR)를 추가하면 기존 VPC를 확장할 수 있습니다. VPC에 추가한 보존 CIDR 블록을 삭제하면 VPC의 크기를 줄일 수 있습니다.   마찬가지로 최대 5개의 추가 IPv6 IP 범위(CIDR)를 VPC에 추가할 수 있습니다.  이 추가 범위를 삭제하여 VPC를 축소할 수 있습니다.\"\n",
            "VPC당 몇 개의 서브넷을 생성할 수 있나요?\n",
            "현재 VPC당 서브넷 200개를 생성할 수 있습니다. 더 생성하려는 경우 지원 센터에 사례를 제출하세요.\n",
            "\"category : VPC, question : VPC당 몇 개의 서브넷을 생성할 수 있나요?, answer : 현재 VPC당 서브넷 200개를 생성할 수 있습니다. 더 생성하려는 경우 지원 센터에 사례를 제출하세요.\"\n",
            "서브넷 크기에 제한이 있나요?\n",
            "IPv4의 경우 서브넷의 최소 크기는 /28(또는 IP 주소 14개)입니다. 서브넷은 생성한 VPC보다 더 클 수 없습니다.\n",
            "IPv6의 경우 서브넷 크기는 /64로 고정됩니다. 서브넷에는 IPv6 CIDR 블록 단 한 개만 할당할 수 있습니다.\n",
            "\"category : VPC, question : 서브넷 크기에 제한이 있나요?, answer : IPv4의 경우 서브넷의 최소 크기는 /28(또는 IP 주소 14개)입니다. 서브넷은 생성한 VPC보다 더 클 수 없습니다.\n",
            "IPv6의 경우 서브넷 크기는 /64로 고정됩니다. 서브넷에는 IPv6 CIDR 블록 단 한 개만 할당할 수 있습니다.\"\n",
            "서브넷에 할당한 IP 주소를 모두 사용할 수 있나요?\n",
            "아니요. Amazon은 IP 네트워킹 목적으로 모든 서브넷의 처음 4개 IP 주소와 마지막 1개 IP 주소를 예약합니다.\n",
            "\"category : VPC, question : 서브넷에 할당한 IP 주소를 모두 사용할 수 있나요?, answer : 아니요. Amazon은 IP 네트워킹 목적으로 모든 서브넷의 처음 4개 IP 주소와 마지막 1개 IP 주소를 예약합니다.\"\n",
            "프라이빗 IP 주소를 어떻게 VPC 내의 Amazon EC2 인스턴스에 할당하나요?\n",
            "IPv6 전용이 아닌 서브넷 내에서 Amazon EC2 인스턴스를 시작할 때 선택적으로 인스턴스에 대한 기본 프라이빗 IPv4 주소를 지정할 수 있습니다. 기본 프라이빗 IPv4 주소를 지정하지 않으면 AWS는 사용자가 해당 서브넷에 할당한 IPv4 주소 범위에서 자동으로 주소를 지정합니다. 보조 프라이빗 IPv4 주소는 인스턴스를 시작할 때, 탄력적 네트워크 인터페이스를 생성할 때 또는 인스턴스가 시작되거나 인터페이스가 생성된 후 언제든지 할당할 수 있습니다. IPv6 전용 서브넷 내에서 Amazon EC2 인스턴스를 시작하는 경우 AWS는 해당 서브넷의 Amazon 제공 IPv6 GUA CIDR에서 자동으로 주소를 지정합니다. 인스턴스의 IPv6 GUA는 올바른 보안 그룹, NACL 및 라우팅 테이블 구성을 사용하여 인터넷에서 연결할 수 있도록 설정하지 않는 한 프라이빗으로 유지됩니다.\n",
            "\"category : VPC, question : 프라이빗 IP 주소를 어떻게 VPC 내의 Amazon EC2 인스턴스에 할당하나요?, answer : IPv6 전용이 아닌 서브넷 내에서 Amazon EC2 인스턴스를 시작할 때 선택적으로 인스턴스에 대한 기본 프라이빗 IPv4 주소를 지정할 수 있습니다. 기본 프라이빗 IPv4 주소를 지정하지 않으면 AWS는 사용자가 해당 서브넷에 할당한 IPv4 주소 범위에서 자동으로 주소를 지정합니다. 보조 프라이빗 IPv4 주소는 인스턴스를 시작할 때, 탄력적 네트워크 인터페이스를 생성할 때 또는 인스턴스가 시작되거나 인터페이스가 생성된 후 언제든지 할당할 수 있습니다. IPv6 전용 서브넷 내에서 Amazon EC2 인스턴스를 시작하는 경우 AWS는 해당 서브넷의 Amazon 제공 IPv6 GUA CIDR에서 자동으로 주소를 지정합니다. 인스턴스의 IPv6 GUA는 올바른 보안 그룹, NACL 및 라우팅 테이블 구성을 사용하여 인터넷에서 연결할 수 있도록 설정하지 않는 한 프라이빗으로 유지됩니다.\"\n",
            "Amazon EC2 인스턴스가 VPC에서 실행 및/또는 중단되었을 때, Amazon EC2 인스턴스의 프라이빗 IP 주소를 변경할 수 있나요?\n",
            "IPv4 또는 이중 스택 서브넷에서 시작된 인스턴스의 경우 기본 프라이빗 IPv4 주소는 인스턴스 또는 인터페이스의 수명 동안 유지됩니다. 보조 프라이빗 IPv4 주소는 할당 또는 할당 취소될 수 있고 인터페이스나 인스턴스 간에 언제든지 이전될 수 있습니다. IPv6 전용 서브넷에서 시작된 인스턴스의 경우 인스턴스의 기본 네트워크 인터페이스에서 첫 번째 IP 주소이기도 한 할당된 IPv6 GUA는 언제든지 새 IPv6 GUA를 연결하고 기존 IPv6 GUA를 제거하여 수정할 수 있습니다.\n",
            "\"category : VPC, question : Amazon EC2 인스턴스가 VPC에서 실행 및/또는 중단되었을 때, Amazon EC2 인스턴스의 프라이빗 IP 주소를 변경할 수 있나요?, answer : IPv4 또는 이중 스택 서브넷에서 시작된 인스턴스의 경우 기본 프라이빗 IPv4 주소는 인스턴스 또는 인터페이스의 수명 동안 유지됩니다. 보조 프라이빗 IPv4 주소는 할당 또는 할당 취소될 수 있고 인터페이스나 인스턴스 간에 언제든지 이전될 수 있습니다. IPv6 전용 서브넷에서 시작된 인스턴스의 경우 인스턴스의 기본 네트워크 인터페이스에서 첫 번째 IP 주소이기도 한 할당된 IPv6 GUA는 언제든지 새 IPv6 GUA를 연결하고 기존 IPv6 GUA를 제거하여 수정할 수 있습니다.\"\n",
            "VPC 내에서 Amazon EC2 인스턴스가 중단되는 경우 동일 VPC에서 동일한 IP 주소를 가진 다른 인스턴스를 시작할 수 있나요?\n",
            "실행 중인 인스턴스에 할당된 IPv4 주소의 경우 원래 실행 중이던 인스턴스가 \"종료\" 상태일 경우에는 다른 인스턴스에서 다시 사용할 수 있습니다. 그러나 실행 중인 인스턴스에 할당된 IPv6 GUA는 첫 번째 인스턴스에서 제거된 후 다른 인스턴스에서 다시 사용할 수 있습니다.\n",
            "\"category : VPC, question : VPC 내에서 Amazon EC2 인스턴스가 중단되는 경우 동일 VPC에서 동일한 IP 주소를 가진 다른 인스턴스를 시작할 수 있나요?, answer : 실행 중인 인스턴스에 할당된 IPv4 주소의 경우 원래 실행 중이던 인스턴스가 \"종료\" 상태일 경우에는 다른 인스턴스에서 다시 사용할 수 있습니다. 그러나 실행 중인 인스턴스에 할당된 IPv6 GUA는 첫 번째 인스턴스에서 제거된 후 다른 인스턴스에서 다시 사용할 수 있습니다.\"\n",
            "여러 인스턴스에 동시에 IP 주소를 할당할 수 있나요?\n",
            "인스턴스를 시작하면 한 번에 하나의 인스턴스의 IP 주소를 지정할 수 있습니다.\n",
            "\"category : VPC, question : 여러 인스턴스에 동시에 IP 주소를 할당할 수 있나요?, answer : 인스턴스를 시작하면 한 번에 하나의 인스턴스의 IP 주소를 지정할 수 있습니다.\"\n",
            "인스턴스에 어떤 IP 주소든 할당할 수 있나요?\n",
            "다음에 해당하는 경우 어떤 IP 주소든 인스턴스에 할당할 수 있습니다.\n",
            "\n",
            "연결된 서브넷의 IP 주소 범위에 포함될 때\n",
            "IP 네트워킹을 위해 Amazon이 예약한 것이 아닐 때\n",
            "현재 다른 인터페이스에 할당되어 있지 않을 때\n",
            "\"category : VPC, question : 인스턴스에 어떤 IP 주소든 할당할 수 있나요?, answer : 다음에 해당하는 경우 어떤 IP 주소든 인스턴스에 할당할 수 있습니다.\n",
            "\n",
            "연결된 서브넷의 IP 주소 범위에 포함될 때\n",
            "IP 네트워킹을 위해 Amazon이 예약한 것이 아닐 때\n",
            "현재 다른 인터페이스에 할당되어 있지 않을 때\"\n",
            "인스턴스에 여러 IP 주소를 할당할 수 있나요?\n",
            "예. Amazon VPC에서는 탄력적 네트워크 인터페이스 또는 EC2 인스턴스에 하나 이상의 보조 프라이빗 IP 주소를 할당할 수 있습니다. 할당할 수 있는 보조 프라이빗 IP 주소의 수는 인스턴스 유형에 따라 다릅니다. 인스턴스 유형당 할당할 수 있는 보조 프라이빗 IP 주소 수에 대한 자세한 내용은 EC2 사용 설명서를 참조하세요.\n",
            "\"category : VPC, question : 인스턴스에 여러 IP 주소를 할당할 수 있나요?, answer : 예. Amazon VPC에서는 탄력적 네트워크 인터페이스 또는 EC2 인스턴스에 하나 이상의 보조 프라이빗 IP 주소를 할당할 수 있습니다. 할당할 수 있는 보조 프라이빗 IP 주소의 수는 인스턴스 유형에 따라 다릅니다. 인스턴스 유형당 할당할 수 있는 보조 프라이빗 IP 주소 수에 대한 자세한 내용은 EC2 사용 설명서를 참조하세요.\"\n",
            "VPC 기반 Amazon EC2 인스턴스에 하나 이상의 탄력적 IP(EIP) 주소를 할당할 수 있나요?\n",
            "예. 하지만 EIP 주소는 인터넷을 통해서만 연결할 수 있습니다(VPN 연결을 통해서는 안 됨). 각 EIP 주소는 인스턴스의 고유한 프라이빗 IP 주소와 연결되어야 합니다. EIP 주소는 트래픽을 인터넷 게이트웨이로 직접 라우팅하도록 구성된 서브넷의 인스턴스에서만 사용해야 합니다. EIP는 인터넷에 액세스하는데 NAT 게이트웨이 또는 NAT 인스턴스를 사용하도록 구성된 서브넷의 인스턴스에서는 사용할 수 없습니다. 이는 IPv4에만 해당됩니다. Amazon VPC는 현재 IPv6용 EIP를 지원하지 않습니다.\n",
            "\"category : VPC, question : VPC 기반 Amazon EC2 인스턴스에 하나 이상의 탄력적 IP(EIP) 주소를 할당할 수 있나요?, answer : 예. 하지만 EIP 주소는 인터넷을 통해서만 연결할 수 있습니다(VPN 연결을 통해서는 안 됨). 각 EIP 주소는 인스턴스의 고유한 프라이빗 IP 주소와 연결되어야 합니다. EIP 주소는 트래픽을 인터넷 게이트웨이로 직접 라우팅하도록 구성된 서브넷의 인스턴스에서만 사용해야 합니다. EIP는 인터넷에 액세스하는데 NAT 게이트웨이 또는 NAT 인스턴스를 사용하도록 구성된 서브넷의 인스턴스에서는 사용할 수 없습니다. 이는 IPv4에만 해당됩니다. Amazon VPC는 현재 IPv6용 EIP를 지원하지 않습니다.\"\n",
            "Bring Your Own IP 기능은 무엇인가요?\n",
            "Bring Your Own IP(BYOIP)를 사용하면 고객이 자체 보유한 공개적으로 라우팅 가능한 IPv4 또는 IPv6 주소 공간의 전부 또는 일부를 AWS로 옮겨 AWS 리소스로 사용할 수 있습니다. 고객은 계속해서 IP 범위를 소유합니다. 고객은 AWS로 가져온 IPv4 공간에서 탄력적 IP를 생성하여 EC2 인스턴스, NAT 게이트웨이 및 Network Load Balancer와 함께 사용할 수 있습니다. 또한 AWS로 가져온 IPv6 공간에서 VPC에 최대 5개의 CIDR을 연결할 수 있습니다. Amazon에서 제공한 IP에 계속 액세스할 수 있으며 BYOIP 탄력적 IP, Amazon에서 제공한 IP 또는 둘 모두를 사용할 수 있습니다.\n",
            "\"category : VPC, question : Bring Your Own IP 기능은 무엇인가요?, answer : Bring Your Own IP(BYOIP)를 사용하면 고객이 자체 보유한 공개적으로 라우팅 가능한 IPv4 또는 IPv6 주소 공간의 전부 또는 일부를 AWS로 옮겨 AWS 리소스로 사용할 수 있습니다. 고객은 계속해서 IP 범위를 소유합니다. 고객은 AWS로 가져온 IPv4 공간에서 탄력적 IP를 생성하여 EC2 인스턴스, NAT 게이트웨이 및 Network Load Balancer와 함께 사용할 수 있습니다. 또한 AWS로 가져온 IPv6 공간에서 VPC에 최대 5개의 CIDR을 연결할 수 있습니다. Amazon에서 제공한 IP에 계속 액세스할 수 있으며 BYOIP 탄력적 IP, Amazon에서 제공한 IP 또는 둘 모두를 사용할 수 있습니다.\"\n",
            "왜 BYOIP를 사용해야 하나요?\n",
            "다음과 같은 이유로 자체 IP 주소를 AWS에 가져올 수 있습니다.IP 신뢰도: 많은 고객들은 자체 IP 주소의 신뢰도를 전략적 자산으로 간주하고 AWS에서 해당 IP를 리소스와 함께 사용하려고 합니다. 예를 들어 아웃바운드 이메일 MTA와 같은 서비스를 유지하고 높은 IP 신뢰도를 보유한 고객은 이제 IP 공간을 가져와서 기존의 성공적인 전송 성공률을 유지할 수 있습니다.\n",
            "고객 화이트리스트: BYOIP를 사용하는 고객은 IP 주소 화이트리스트에 의존하는 워크로드를 새 IP 주소를 포함하는 화이트리스트의 재설정 없이 그대로 AWS로 옮길 수 있습니다.\n",
            "하드 코딩된 종속성: 여러 고객이 IP를 디바이스에 하드 코딩하거나 IP에 대한 아키텍처 종속성을 가집니다. BYOIP를 사용하면 이러한 고객이 AWS로 자유롭게 마이그레이션할 수 있습니다.\n",
            "규정 및 준수: 많은 고객은 규정 및 준수 이유로 인해 특정 IP를 사용해야 합니다. 이러한 IP도 BYOIP를 통해 자유롭게 사용할 수 있습니다.\n",
            "온프레미스 IPv6 네트워크 정책: 많은 고객은 자체 온프레미스 네트워크에서 자신의 IPv6만 라우팅할 수 있습니다. 이런 고객은 VPC에 자체 IPv6 범위를 할당함에 따라 BYOIP를 통해 자유롭게 사용하고 인터넷 또는 직접 연결을 사용해 온프레미스 네트워크에 라우팅하도록 선택할 수 있습니다.\n",
            "\"category : VPC, question : 왜 BYOIP를 사용해야 하나요?, answer : 다음과 같은 이유로 자체 IP 주소를 AWS에 가져올 수 있습니다.IP 신뢰도: 많은 고객들은 자체 IP 주소의 신뢰도를 전략적 자산으로 간주하고 AWS에서 해당 IP를 리소스와 함께 사용하려고 합니다. 예를 들어 아웃바운드 이메일 MTA와 같은 서비스를 유지하고 높은 IP 신뢰도를 보유한 고객은 이제 IP 공간을 가져와서 기존의 성공적인 전송 성공률을 유지할 수 있습니다.\n",
            "고객 화이트리스트: BYOIP를 사용하는 고객은 IP 주소 화이트리스트에 의존하는 워크로드를 새 IP 주소를 포함하는 화이트리스트의 재설정 없이 그대로 AWS로 옮길 수 있습니다.\n",
            "하드 코딩된 종속성: 여러 고객이 IP를 디바이스에 하드 코딩하거나 IP에 대한 아키텍처 종속성을 가집니다. BYOIP를 사용하면 이러한 고객이 AWS로 자유롭게 마이그레이션할 수 있습니다.\n",
            "규정 및 준수: 많은 고객은 규정 및 준수 이유로 인해 특정 IP를 사용해야 합니다. 이러한 IP도 BYOIP를 통해 자유롭게 사용할 수 있습니다.\n",
            "온프레미스 IPv6 네트워크 정책: 많은 고객은 자체 온프레미스 네트워크에서 자신의 IPv6만 라우팅할 수 있습니다. 이런 고객은 VPC에 자체 IPv6 범위를 할당함에 따라 BYOIP를 통해 자유롭게 사용하고 인터넷 또는 직접 연결을 사용해 온프레미스 네트워크에 라우팅하도록 선택할 수 있습니다.\"\n",
            "BYOIP 탄력적 IP를 해제하면 어떻게 되나요?\n",
            "BYOIP 탄력적 IP를 해제하면 할당된 BYOIP IP 풀로 돌아갑니다.\n",
            "\"category : VPC, question : BYOIP 탄력적 IP를 해제하면 어떻게 되나요?, answer : BYOIP 탄력적 IP를 해제하면 할당된 BYOIP IP 풀로 돌아갑니다.\"\n",
            "어느 AWS 리전에서 BYOIP를 사용할 수 있나요?\n",
            "BYOIP 가용성에 대한 자세한 내용은  설명서를 참조하세요.\n",
            "\"category : VPC, question : 어느 AWS 리전에서 BYOIP를 사용할 수 있나요?, answer : BYOIP 가용성에 대한 자세한 내용은  설명서를 참조하세요.\"\n",
            "동일한 계정에서 BYOIP 접두사를 여러 VPC와 공유할 수 있나요?\n",
            "예. 동일한 계정에서 BYOIP 접두사를 여러 VPC와 함께 사용할 수 있습니다.\n",
            "\"category : VPC, question : 동일한 계정에서 BYOIP 접두사를 여러 VPC와 공유할 수 있나요?, answer : 예. 동일한 계정에서 BYOIP 접두사를 여러 VPC와 함께 사용할 수 있습니다.\"\n",
            "BYOIP를 통해 얼마나 많은 IP 범위를 가져올 수 있나요?\n",
            "계정에 최대 5개의 IP 범위를 가져올 수 있습니다.\n",
            "\"category : VPC, question : BYOIP를 통해 얼마나 많은 IP 범위를 가져올 수 있나요?, answer : 계정에 최대 5개의 IP 범위를 가져올 수 있습니다.\"\n",
            "BYOIP를 통해 가져올 수 있는 가장 구체적인 접두사는 무엇인가요?\n",
            "BYOIP를 통해 가져올 수 있는 가장 구체적인 IPv4 접두사는 /24 IPv4 접두사와 /56 IPv6 접두사입니다. IPv6 접두사를 인터넷에 알리려는 경우 가장 구체적인 IPv6 접두사는 /48입니다.\n",
            "\"category : VPC, question : BYOIP를 통해 가져올 수 있는 가장 구체적인 접두사는 무엇인가요?, answer : BYOIP를 통해 가져올 수 있는 가장 구체적인 IPv4 접두사는 /24 IPv4 접두사와 /56 IPv6 접두사입니다. IPv6 접두사를 인터넷에 알리려는 경우 가장 구체적인 IPv6 접두사는 /48입니다.\"\n",
            "BYOIP에 사용할 수 있는 RIR 접두사는 무엇인가요?\n",
            "ARIN, RIPE 및 APNIC 등록 접두사를 사용할 수 있습니다.\n",
            "\"category : VPC, question : BYOIP에 사용할 수 있는 RIR 접두사는 무엇인가요?, answer : ARIN, RIPE 및 APNIC 등록 접두사를 사용할 수 있습니다.\"\n",
            "재지정되거나 재할당된 접두사를 가져올 수 있나요?\n",
            "현재 재지정되거나 재할당된 접두사는 허용되지 않습니다. IP 범위는 직접 할당 또는 직접 지정의 순 유형이어야 합니다.\n",
            "\"category : VPC, question : 재지정되거나 재할당된 접두사를 가져올 수 있나요?, answer : 현재 재지정되거나 재할당된 접두사는 허용되지 않습니다. IP 범위는 직접 할당 또는 직접 지정의 순 유형이어야 합니다.\"\n",
            "BYOIP 접두사를 AWS 리전 간에 이동할 수 있나요?\n",
            "예. 현재 리전에서 BYOIP 접두사의 프로비저닝을 해제한 다음 새 리전에 프로비저닝하면 됩니다.\n",
            "\"category : VPC, question : BYOIP 접두사를 AWS 리전 간에 이동할 수 있나요?, answer : 예. 현재 리전에서 BYOIP 접두사의 프로비저닝을 해제한 다음 새 리전에 프로비저닝하면 됩니다.\"\n",
            "VPC IP 주소 관리자(IPAM)란 무엇인가요?\n",
            "Amazon VPC IP 주소 관리자(IPAM)는 AWS 워크로드의 IP 주소를 더욱 쉽게 계획하고 추적하며 모니터링할 수 있도록 지원하는 관리형 서비스입니다. IPAM을 사용하면 라우팅 및 보안 요구에 따라 IP 주소를 쉽게 구성할 수 있으며 IP 주소 할당을 제어하는 간단한 비즈니스 규칙을 설정할 수 있습니다. 또한 VPC에 대한 IP 주소 할당을 자동화할 수 있으므로, 유지 관리가 어렵고 시간이 많이 소요될 수 있는 스프레드시트 기반 또는 자체 개발 IP 주소 계획 애플리케이션을 사용할 필요가 없습니다. IPAM은 신뢰할 수 있는 단일 소스로 사용할 수 있는 통합 운영 보기를 제공하므로 IP 사용률 추적, 문제 해결, 감사와 같은 일상적인 IP 주소 관리 활동을 빠르고 효율적으로 수행할 수 있습니다.\n",
            "\"category : VPC, question : VPC IP 주소 관리자(IPAM)란 무엇인가요?, answer : Amazon VPC IP 주소 관리자(IPAM)는 AWS 워크로드의 IP 주소를 더욱 쉽게 계획하고 추적하며 모니터링할 수 있도록 지원하는 관리형 서비스입니다. IPAM을 사용하면 라우팅 및 보안 요구에 따라 IP 주소를 쉽게 구성할 수 있으며 IP 주소 할당을 제어하는 간단한 비즈니스 규칙을 설정할 수 있습니다. 또한 VPC에 대한 IP 주소 할당을 자동화할 수 있으므로, 유지 관리가 어렵고 시간이 많이 소요될 수 있는 스프레드시트 기반 또는 자체 개발 IP 주소 계획 애플리케이션을 사용할 필요가 없습니다. IPAM은 신뢰할 수 있는 단일 소스로 사용할 수 있는 통합 운영 보기를 제공하므로 IP 사용률 추적, 문제 해결, 감사와 같은 일상적인 IP 주소 관리 활동을 빠르고 효율적으로 수행할 수 있습니다.\"\n",
            "IPAM을 사용해야 하는 이유는 무엇인가요?\n",
            "IP 주소를 더 효율적으로 관리하려면 IPAM을 사용해야 합니다. 스프레드시트 또는 자체 개발 도구를 활용하는 기존 메커니즘에는 수동 작업이 필요하고 오류가 발생하기 쉽습니다. 예를 들어 IPAM을 사용하면 개발자가 더 이상 중앙 IP 주소 관리 팀의 IP 주소 할당을 기다리지 않아도 될 만큼 신속하게 애플리케이션을 롤아웃할 수 있습니다. 또한 중복된 IP 주소를 감지하고 네트워크가 중단되기 전에 해결할 수 있습니다. 그리고 주소 풀이 거의 소진되거나 리소스가 풀에 설정된 할당 규칙을 준수하지 않는 경우 알림을 받을 수 있도록 IPAM에 대한 경보를 생성할 수 있습니다. 이것들은 IPAM을 사용해야 하는 많은 이유 중 일부입니다.\n",
            "\"category : VPC, question : IPAM을 사용해야 하는 이유는 무엇인가요?, answer : IP 주소를 더 효율적으로 관리하려면 IPAM을 사용해야 합니다. 스프레드시트 또는 자체 개발 도구를 활용하는 기존 메커니즘에는 수동 작업이 필요하고 오류가 발생하기 쉽습니다. 예를 들어 IPAM을 사용하면 개발자가 더 이상 중앙 IP 주소 관리 팀의 IP 주소 할당을 기다리지 않아도 될 만큼 신속하게 애플리케이션을 롤아웃할 수 있습니다. 또한 중복된 IP 주소를 감지하고 네트워크가 중단되기 전에 해결할 수 있습니다. 그리고 주소 풀이 거의 소진되거나 리소스가 풀에 설정된 할당 규칙을 준수하지 않는 경우 알림을 받을 수 있도록 IPAM에 대한 경보를 생성할 수 있습니다. 이것들은 IPAM을 사용해야 하는 많은 이유 중 일부입니다.\"\n",
            "IPAM에서 제공하는 주요 기능은 무엇인가요?\n",
            "AWS IPAM은 다음 기능을 제공합니다. \n",
            "\n",
            "대규모 네트워크에 대한 IP 주소 할당: IPAM은 구성 가능한 비즈니스 규칙을 기반으로 수백 개의 계정 및 VPC에 걸쳐 IP 주소 할당을 자동화할 수 있습니다. \n",
            "네트워크의 IP 사용량 모니터링: IPAM은 IP 주소를 모니터링할 수 있으며, IPAM에서 네트워크 확장이 지연될 수 있는 IP 주소 소진이나 잘못된 라우팅이 발생할 수 있는 IP 주소 중복과 같은 잠재적 문제를 감지할 때 알림을 받을 수 있도록 설정할 수 있습니다. \n",
            "네트워크 문제 해결: IPAM은 잘못된 IP 주소 구성 또는 기타 문제로 인해 연결 문제가 발생한 경우 신속하게 식별하는 데 도움이 될 수 있습니다. \n",
            "IP 주소 감사: IPAM은 자동으로 IP 주소 모니터링 데이터를 유지합니다(최대 3년). 이 기록 데이터를 사용하여 네트워크에 대한 소급 분석 및 감사를 수행할 수 있습니다.\n",
            "\"category : VPC, question : IPAM에서 제공하는 주요 기능은 무엇인가요?, answer : AWS IPAM은 다음 기능을 제공합니다. \n",
            "\n",
            "대규모 네트워크에 대한 IP 주소 할당: IPAM은 구성 가능한 비즈니스 규칙을 기반으로 수백 개의 계정 및 VPC에 걸쳐 IP 주소 할당을 자동화할 수 있습니다. \n",
            "네트워크의 IP 사용량 모니터링: IPAM은 IP 주소를 모니터링할 수 있으며, IPAM에서 네트워크 확장이 지연될 수 있는 IP 주소 소진이나 잘못된 라우팅이 발생할 수 있는 IP 주소 중복과 같은 잠재적 문제를 감지할 때 알림을 받을 수 있도록 설정할 수 있습니다. \n",
            "네트워크 문제 해결: IPAM은 잘못된 IP 주소 구성 또는 기타 문제로 인해 연결 문제가 발생한 경우 신속하게 식별하는 데 도움이 될 수 있습니다. \n",
            "IP 주소 감사: IPAM은 자동으로 IP 주소 모니터링 데이터를 유지합니다(최대 3년). 이 기록 데이터를 사용하여 네트워크에 대한 소급 분석 및 감사를 수행할 수 있습니다.\"\n",
            "IPAM의 핵심 구성 요소는 무엇인가요?\n",
            "다음은 IPAM의 핵심 구성 요소입니다.\n",
            "\n",
            "영역이란 IPAM 내에서 가장 높은 수준의 컨테이너입니다. IPAM에는 두 가지 기본 영역이 포함되어 있습니다. 각 영역은 단일 네트워크의 IP 공간을 나타냅니다. 프라이빗 영역은 모든 프라이빗 공간을 위한 것입니다. 퍼블릭 영역은 모든 퍼블릭 공간을 위한 것입니다. 영역을 사용하면 IP 주소가 중복되거나 충돌하지 않으면서 연결되지 않은 여러 네트워크의 IP 주소를 재사용할 수 있습니다. 영역 내에서 IPAM 풀을 생성합니다. \n",
            "풀이란 인접한 IP 주소 범위(또는 CIDR) 모음입니다. IPAM 풀을 사용하면 라우팅 및 보안 요구 사항에 따라 IP 주소를 구성할 수 있습니다. 최상위 풀 내에 여러 풀을 보유할 수 있습니다. 예를 들어 개발 및 프로덕션 애플리케이션에 대해 별도의 라우팅 및 보안 요구 사항이 있는 경우 각각의 풀을 생성할 수 있습니다. IPAM 풀 내에서 CIDR을 AWS 리소스에 할당할 수 있습니다.\n",
            "할당이란 IPAM 풀에서 다른 리소스 또는 IPAM 풀로 CIDR을 배정하는 것입니다. VPC를 생성하고 VPC의 CIDR에 대한 IPAM 풀을 선택하면 CIDR은 프로비저닝된 CIDR에서 IPAM 풀로 할당됩니다. IPAM을 통해 할당을 모니터링하고 관리할 수 있습니다.\n",
            "\"category : VPC, question : IPAM의 핵심 구성 요소는 무엇인가요?, answer : 다음은 IPAM의 핵심 구성 요소입니다.\n",
            "\n",
            "영역이란 IPAM 내에서 가장 높은 수준의 컨테이너입니다. IPAM에는 두 가지 기본 영역이 포함되어 있습니다. 각 영역은 단일 네트워크의 IP 공간을 나타냅니다. 프라이빗 영역은 모든 프라이빗 공간을 위한 것입니다. 퍼블릭 영역은 모든 퍼블릭 공간을 위한 것입니다. 영역을 사용하면 IP 주소가 중복되거나 충돌하지 않으면서 연결되지 않은 여러 네트워크의 IP 주소를 재사용할 수 있습니다. 영역 내에서 IPAM 풀을 생성합니다. \n",
            "풀이란 인접한 IP 주소 범위(또는 CIDR) 모음입니다. IPAM 풀을 사용하면 라우팅 및 보안 요구 사항에 따라 IP 주소를 구성할 수 있습니다. 최상위 풀 내에 여러 풀을 보유할 수 있습니다. 예를 들어 개발 및 프로덕션 애플리케이션에 대해 별도의 라우팅 및 보안 요구 사항이 있는 경우 각각의 풀을 생성할 수 있습니다. IPAM 풀 내에서 CIDR을 AWS 리소스에 할당할 수 있습니다.\n",
            "할당이란 IPAM 풀에서 다른 리소스 또는 IPAM 풀로 CIDR을 배정하는 것입니다. VPC를 생성하고 VPC의 CIDR에 대한 IPAM 풀을 선택하면 CIDR은 프로비저닝된 CIDR에서 IPAM 풀로 할당됩니다. IPAM을 통해 할당을 모니터링하고 관리할 수 있습니다.\"\n",
            "IPAM에서 고유 IP 주소 가져오기(BYOIP)를 지원하나요?\n",
            "예. IPAM에서는 BYOIPv4 및 BYOIPv6 주소를 지원합니다. BYOIP는 사용자가 소유한 IP 주소를 AWS로 가져올 수 있는 EC2 기능입니다. IPAM을 사용하면 직접 프로비저닝하고 계정 및 조직 전반에 걸쳐 IP 주소 블록을 공유할 수 있습니다. IPv4를 사용하는 기존 BYOIP 고객은 풀을 IPAM로 마이그레이션하여 IP 관리를 간소화할 수 있습니다.\n",
            "\"category : VPC, question : IPAM에서 고유 IP 주소 가져오기(BYOIP)를 지원하나요?, answer : 예. IPAM에서는 BYOIPv4 및 BYOIPv6 주소를 지원합니다. BYOIP는 사용자가 소유한 IP 주소를 AWS로 가져올 수 있는 EC2 기능입니다. IPAM을 사용하면 직접 프로비저닝하고 계정 및 조직 전반에 걸쳐 IP 주소 블록을 공유할 수 있습니다. IPv4를 사용하는 기존 BYOIP 고객은 풀을 IPAM로 마이그레이션하여 IP 관리를 간소화할 수 있습니다.\"\n",
            "Amazon에서 인접 CIDR 블록을 제공하나요? 그리고 이는 IPAM과 함께 어떻게 작동하나요?\n",
            "예. Amazon에서는 VPC 할당을 위한 인접 IPv6 CIDR 블록을 제공합니다. 인접 CIDR 블록을 사용하면 액세스 제어 목록, 라우팅 테이블, 보안 그룹, 방화벽과 같은 네트워킹 및 보안 구성에서 단일 항목의 CIDR을 집계할 수 있습니다. Amazon IPv6 CIDR을 퍼블릭으로 영역이 제한된 풀로 프로비저닝하고 모든 IPAM 기능을 사용하여 IP 사용량을 관리 및 모니터링할 수 있습니다. 이러한 CIDR 블록 할당은 /52 증분으로 시작되며, 요청 시 더 큰 블록을 사용할 수 있습니다. 예를 들어 Amazon에서 /52 CIDR을 할당하고 IPAM을 사용하여 계정에서 공유하며 해당 계정에서 VPC를 생성할 수 있습니다.\n",
            "\"category : VPC, question : Amazon에서 인접 CIDR 블록을 제공하나요? 그리고 이는 IPAM과 함께 어떻게 작동하나요?, answer : 예. Amazon에서는 VPC 할당을 위한 인접 IPv6 CIDR 블록을 제공합니다. 인접 CIDR 블록을 사용하면 액세스 제어 목록, 라우팅 테이블, 보안 그룹, 방화벽과 같은 네트워킹 및 보안 구성에서 단일 항목의 CIDR을 집계할 수 있습니다. Amazon IPv6 CIDR을 퍼블릭으로 영역이 제한된 풀로 프로비저닝하고 모든 IPAM 기능을 사용하여 IP 사용량을 관리 및 모니터링할 수 있습니다. 이러한 CIDR 블록 할당은 /52 증분으로 시작되며, 요청 시 더 큰 블록을 사용할 수 있습니다. 예를 들어 Amazon에서 /52 CIDR을 할당하고 IPAM을 사용하여 계정에서 공유하며 해당 계정에서 VPC를 생성할 수 있습니다.\"\n",
            "IPAM 없이 Amazon 제공 인접 IPv6 CIDR 블록을 사용할 수 있나요?\n",
            "아니요. Amazon 제공 인접 IPv6 CIDR 블록은 현재 IPAM에서만 지원됩니다.\n",
            "\"category : VPC, question : IPAM 없이 Amazon 제공 인접 IPv6 CIDR 블록을 사용할 수 있나요?, answer : 아니요. Amazon 제공 인접 IPv6 CIDR 블록은 현재 IPAM에서만 지원됩니다.\"\n",
            "IPAM 풀을 다른 계정과 공유할 수 있나요?\n",
            "AWS Resource Access Manager(RAM)를 사용하여 IPAM 풀을 AWS 조직의 다른 계정과 공유할 수 있습니다. 기본 AWS 조직 외의 계정과도 IPAM 풀을 공유할 수 있습니다. 예를 들어 이러한 계정은 회사의 다른 사업부 또는 다른 AWS 조직에서 사용자 대신 파트너가 호스팅하는 관리형 서비스일 수 있습니다.\n",
            "\"category : VPC, question : IPAM 풀을 다른 계정과 공유할 수 있나요?, answer : AWS Resource Access Manager(RAM)를 사용하여 IPAM 풀을 AWS 조직의 다른 계정과 공유할 수 있습니다. 기본 AWS 조직 외의 계정과도 IPAM 풀을 공유할 수 있습니다. 예를 들어 이러한 계정은 회사의 다른 사업부 또는 다른 AWS 조직에서 사용자 대신 파트너가 호스팅하는 관리형 서비스일 수 있습니다.\"\n",
            "어떤 서브넷이 기본값으로 어떤 게이트웨이를 사용할 것인지를 지정할 수 있나요?\n",
            "예. 각각의 서브넷에 대해 기본 경로를 생성할 수 있습니다. 기본 경로는 인터넷 게이트웨이, 가상 프라이빗 게이트웨이 또는 NAT 게이트웨이를 통해 VPC에 송신되도록 트래픽을 보낼 수 있습니다.\n",
            "\"category : VPC, question : 어떤 서브넷이 기본값으로 어떤 게이트웨이를 사용할 것인지를 지정할 수 있나요?, answer : 예. 각각의 서브넷에 대해 기본 경로를 생성할 수 있습니다. 기본 경로는 인터넷 게이트웨이, 가상 프라이빗 게이트웨이 또는 NAT 게이트웨이를 통해 VPC에 송신되도록 트래픽을 보낼 수 있습니다.\"\n",
            "VPC에서 실행되는 Amazon EC2 인스턴스의 보안을 어떻게 유지하나요?\n",
            "Amazon EC2 보안 그룹을 사용하여 Amazon VPC 내의 인스턴스 보안을 유지할 수 있습니다. VPC 내의 보안 그룹을 통해 각각의 Amazon EC2 인스턴스와의 인바운드 및 아웃바운드 네트워크 트래픽을 지정할 수 있습니다. 인스턴스와 소통하도록 명시적으로 허용되지 않은 트래픽은 자동으로 거부됩니다.\n",
            "보안 그룹에 더하여, 각 서브넷에 출입하는 네트워크 트래픽은 네트워크 ACL(액세스 제어 목록)를 통해 허용하거나 거부할 수 있습니다.\n",
            "\"category : VPC, question : VPC에서 실행되는 Amazon EC2 인스턴스의 보안을 어떻게 유지하나요?, answer : Amazon EC2 보안 그룹을 사용하여 Amazon VPC 내의 인스턴스 보안을 유지할 수 있습니다. VPC 내의 보안 그룹을 통해 각각의 Amazon EC2 인스턴스와의 인바운드 및 아웃바운드 네트워크 트래픽을 지정할 수 있습니다. 인스턴스와 소통하도록 명시적으로 허용되지 않은 트래픽은 자동으로 거부됩니다.\n",
            "보안 그룹에 더하여, 각 서브넷에 출입하는 네트워크 트래픽은 네트워크 ACL(액세스 제어 목록)를 통해 허용하거나 거부할 수 있습니다.\"\n",
            "VPC의 보안 그룹과 VPC의 네트워크 ACL의 차이는 무엇인가요?\n",
            "VPC 내의 보안 그룹은 Amazon EC2 인스턴스 사이에서 어떤 트래픽을 허용할 것인지를 지정합니다. 네트워크 ACL은 서브넷 수준에서 작동하며, 서브넷에 출입하는 트래픽을 평가합니다. 네트워크 ACL을 사용하여 허용 및 거부 규칙을 설정할 수 있습니다. 네트워크 ACL은 동일한 서브넷에 있는 인스턴스 간 트래픽은 필터링하지 않습니다. 또한, 보안 그룹이 상태 저장 필터링을 수행하는 반면 네트워크 ACL은 상태 비 저장 필터링을 수행합니다.\n",
            "\"category : VPC, question : VPC의 보안 그룹과 VPC의 네트워크 ACL의 차이는 무엇인가요?, answer : VPC 내의 보안 그룹은 Amazon EC2 인스턴스 사이에서 어떤 트래픽을 허용할 것인지를 지정합니다. 네트워크 ACL은 서브넷 수준에서 작동하며, 서브넷에 출입하는 트래픽을 평가합니다. 네트워크 ACL을 사용하여 허용 및 거부 규칙을 설정할 수 있습니다. 네트워크 ACL은 동일한 서브넷에 있는 인스턴스 간 트래픽은 필터링하지 않습니다. 또한, 보안 그룹이 상태 저장 필터링을 수행하는 반면 네트워크 ACL은 상태 비 저장 필터링을 수행합니다.\"\n",
            "상태 저장 필터링과 상태 비저장 필터링의 차이는 무엇인가요?\n",
            "상태 저장 필터링은 요청의 오리진을 추적하며, 오리진 컴퓨터로 반환하라는 요청에 대한 회신을 자동으로 허용합니다. 예를 들어, 웹 서버에서 TCP 포트 80으로 인바운드 트래픽을 허용하는 상태 저장 필터는 보통 큰 수의 포트(예: 목적지 TCP 포트 63, 912)에서 반환 트래픽이 클라이언트와 웹 서버 간의 상태 저장 필터를 통과하도록 허용합니다. 이 필터링 디바이스는 소스 포트와 목적지 포트 수 및 IP 주소를 추적하는 상태 테이블을 관리합니다. 필터링 디바이스에 적용되는 유일한 규칙은 웹 서버에서 TCP 포트 80에 대한 인바운드 트래픽을 허용한다는 것입니다.\n",
            "상태 비저장 필터링은 소스 또는 목적지 IP 주소와 목적지 포트만 검사하고, 트래픽이 새로운 요청인지 또는 요청에 대한 응답인지 여부는 무시합니다. 위의 예에서, 두 가지 규칙을 필터링 디바이스에 적용해야 합니다. 하나는 TCP 포트 80에서 웹 서버에 대한 인바운드 트래픽을 허용하는 것이며, 다른 하나는 웹 서버로부터 아웃바운드 트래픽을 허용하는 것입니다(TCP 포트 범위는 49, 65~152, 535).\n",
            "\"category : VPC, question : 상태 저장 필터링과 상태 비저장 필터링의 차이는 무엇인가요?, answer : 상태 저장 필터링은 요청의 오리진을 추적하며, 오리진 컴퓨터로 반환하라는 요청에 대한 회신을 자동으로 허용합니다. 예를 들어, 웹 서버에서 TCP 포트 80으로 인바운드 트래픽을 허용하는 상태 저장 필터는 보통 큰 수의 포트(예: 목적지 TCP 포트 63, 912)에서 반환 트래픽이 클라이언트와 웹 서버 간의 상태 저장 필터를 통과하도록 허용합니다. 이 필터링 디바이스는 소스 포트와 목적지 포트 수 및 IP 주소를 추적하는 상태 테이블을 관리합니다. 필터링 디바이스에 적용되는 유일한 규칙은 웹 서버에서 TCP 포트 80에 대한 인바운드 트래픽을 허용한다는 것입니다.\n",
            "상태 비저장 필터링은 소스 또는 목적지 IP 주소와 목적지 포트만 검사하고, 트래픽이 새로운 요청인지 또는 요청에 대한 응답인지 여부는 무시합니다. 위의 예에서, 두 가지 규칙을 필터링 디바이스에 적용해야 합니다. 하나는 TCP 포트 80에서 웹 서버에 대한 인바운드 트래픽을 허용하는 것이며, 다른 하나는 웹 서버로부터 아웃바운드 트래픽을 허용하는 것입니다(TCP 포트 범위는 49, 65~152, 535).\"\n",
            "VPC에 있는 Amazon EC2 인스턴스가 VPC에 있지 않은 Amazon EC2 인스턴스와 통신할 수 있나요?\n",
            "예. 인터넷 게이트웨이가 구성되면, VPC 외부의 Amazon EC2 인스턴스로 향하는 Amazon VPC 트래픽은 인터넷 게이트웨이를 통과한 후, 퍼블릭 AWS 네트워크를 통해 EC2 인스턴스에 전달됩니다. 인터넷 게이트웨이가 구성되어 있지 않거나 인스턴스가 가상 프라이빗 게이트웨이를 통해 라우팅되도록 구성된 서브넷에 있을 경우, 트래픽은 VPN 연결을 통과하고, 데이터 센터를 벗어난 후, 퍼블릭 AWS 네트워크로 재진입하게 됩니다.\n",
            "\"category : VPC, question : VPC에 있는 Amazon EC2 인스턴스가 VPC에 있지 않은 Amazon EC2 인스턴스와 통신할 수 있나요?, answer : 예. 인터넷 게이트웨이가 구성되면, VPC 외부의 Amazon EC2 인스턴스로 향하는 Amazon VPC 트래픽은 인터넷 게이트웨이를 통과한 후, 퍼블릭 AWS 네트워크를 통해 EC2 인스턴스에 전달됩니다. 인터넷 게이트웨이가 구성되어 있지 않거나 인스턴스가 가상 프라이빗 게이트웨이를 통해 라우팅되도록 구성된 서브넷에 있을 경우, 트래픽은 VPN 연결을 통과하고, 데이터 센터를 벗어난 후, 퍼블릭 AWS 네트워크로 재진입하게 됩니다.\"\n",
            "특정 리전에 있는 VPC의 Amazon EC2 인스턴스가 다른 리전에 있는 VPC의 Amazon EC2 인스턴스와 통신할 수 있나요?\n",
            "예. 한 리전의 인스턴스는 Inter-Region VPC Peering, 퍼블릭 IP 주소, NAT 게이트웨이, NAT 인스턴스, VPN Connections 또는 Direct Connect 연결을 사용하여 서로 통신할 수 있습니다.\n",
            "\"category : VPC, question : 특정 리전에 있는 VPC의 Amazon EC2 인스턴스가 다른 리전에 있는 VPC의 Amazon EC2 인스턴스와 통신할 수 있나요?, answer : 예. 한 리전의 인스턴스는 Inter-Region VPC Peering, 퍼블릭 IP 주소, NAT 게이트웨이, NAT 인스턴스, VPN Connections 또는 Direct Connect 연결을 사용하여 서로 통신할 수 있습니다.\"\n",
            "VPC 내의 Amazon EC2 인스턴스가 Amazon S3와 통신할 수 있나요?\n",
            "예. 사용자의 리소스가 VPC 내에서 Amazon S3와 통신할 수 있는 여러 옵션이 있습니다. S3용 VPC 엔드포인트를 사용하면 모든 트래픽이 Amazon의 네트워크 내에서 유지되도록 하고, 추가적인 액세스 정책을 Amazon S3 트래픽에 적용할 수 있습니다. 인터넷 게이트웨이를 사용하면 VPC에서 인터넷에 액세스할 수 있으며, VPC의 인스턴스가 Amazon S3와 통신할 수 있습니다. 또한 Amazon S3의 모든 트래픽이 Direct Connect 또는 VPN 연결을 트래버스하고 데이터센터에서 벗어나 퍼블릭 AWS 네트워크로 재진입하도록 할 수 있습니다.\n",
            "\"category : VPC, question : VPC 내의 Amazon EC2 인스턴스가 Amazon S3와 통신할 수 있나요?, answer : 예. 사용자의 리소스가 VPC 내에서 Amazon S3와 통신할 수 있는 여러 옵션이 있습니다. S3용 VPC 엔드포인트를 사용하면 모든 트래픽이 Amazon의 네트워크 내에서 유지되도록 하고, 추가적인 액세스 정책을 Amazon S3 트래픽에 적용할 수 있습니다. 인터넷 게이트웨이를 사용하면 VPC에서 인터넷에 액세스할 수 있으며, VPC의 인스턴스가 Amazon S3와 통신할 수 있습니다. 또한 Amazon S3의 모든 트래픽이 Direct Connect 또는 VPN 연결을 트래버스하고 데이터센터에서 벗어나 퍼블릭 AWS 네트워크로 재진입하도록 할 수 있습니다.\"\n",
            "VPC의 네트워크 트래픽을 모니터링할 수 있나요?\n",
            "예. Amazon VPC 트래픽 미러링 및 Amazon VPC 흐름 로그 기능을 사용하여 Amazon VPC의 네트워크 트래픽을 모니터링할 수 있습니다.\n",
            "\"category : VPC, question : VPC의 네트워크 트래픽을 모니터링할 수 있나요?, answer : 예. Amazon VPC 트래픽 미러링 및 Amazon VPC 흐름 로그 기능을 사용하여 Amazon VPC의 네트워크 트래픽을 모니터링할 수 있습니다.\"\n",
            "Amazon VPC 흐름 로그란 무엇인가요?\n",
            "VPC 흐름 로그는 VPC의 네트워크 인터페이스에서 송수신되는 IP 트래픽에 대한 정보를 캡처할 수 있는 기능입니다. 흐름 로그 데이터는 Amazon CloudWatch Logs 또는 Amazon S3에 게시할 수 있습니다. VPC 흐름 로그를 모니터링하여 네트워크 의존성 및 트래픽 패턴에 대한 운영 가시성을 얻고, 이상을 추적하며 데이터 유출을 예방하거나, 또는 네트워크 연결성 및 구성 문제를 해결할 수 있습니다. 흐름 로그의 강화된 메타데이터는 누가 TCP 연결을 시작했는지, NAT 게이트웨이 등의 중간 계층을 통과하는 트래픽의 실제 패킷 수준 출처 및 대상에 대한 추가적인 인사이트를 얻도록 지원합니다. 흐름 로그를 아카이빙하여 일반적인 규정 준수 요건을 충족할 수 있습니다. Amazon VPC 흐름 로그에 대한 자세한 내용은 설명서를 참조하십시오.\n",
            "\"category : VPC, question : Amazon VPC 흐름 로그란 무엇인가요?, answer : VPC 흐름 로그는 VPC의 네트워크 인터페이스에서 송수신되는 IP 트래픽에 대한 정보를 캡처할 수 있는 기능입니다. 흐름 로그 데이터는 Amazon CloudWatch Logs 또는 Amazon S3에 게시할 수 있습니다. VPC 흐름 로그를 모니터링하여 네트워크 의존성 및 트래픽 패턴에 대한 운영 가시성을 얻고, 이상을 추적하며 데이터 유출을 예방하거나, 또는 네트워크 연결성 및 구성 문제를 해결할 수 있습니다. 흐름 로그의 강화된 메타데이터는 누가 TCP 연결을 시작했는지, NAT 게이트웨이 등의 중간 계층을 통과하는 트래픽의 실제 패킷 수준 출처 및 대상에 대한 추가적인 인사이트를 얻도록 지원합니다. 흐름 로그를 아카이빙하여 일반적인 규정 준수 요건을 충족할 수 있습니다. Amazon VPC 흐름 로그에 대한 자세한 내용은 설명서를 참조하십시오.\"\n",
            "VPC 흐름 로그를 사용하려면 어떻게 해야 하나요?\n",
            "VPC, 서브넷 또는 네트워크 인터페이스에 대해 흐름 로그를 생성할 수 있습니다. 서브넷 또는 VPC에 대한 흐름 로그를 생성하면 해당 서브넷 또는 VPC의 각 네트워크 인터페이스가 모니터링됩니다. 흐름 로그 구독을 생성하는 동안 캡처할 메타데이터 필드, 최대 집계 간격 및 기본 로그 대상을 선택할 수 있습니다. 또한 모든 트래픽을 캡처하도록 선택할 수도 있고 승인되거나 거부된 트래픽만 캡처하도록 선택할 수도 있습니다. CloudWatch Log Insights 또는 CloudWatch Contributor Insights와 같은 도구를 사용하여 CloudWatch Logs로 전송되는 VPC 흐름 로그를 분석할 수 있습니다. Amazon Athena 또는 AWS QuickSight와 같은 도구를 사용하여 Amazon S3로 전송되는 VPC 흐름 로그를 쿼리하고 시각화할 수 있습니다. 또한 사용자 지정 다운스트림 애플리케이션을 구축하여 로그를 분석하거나, Splunk, Datadog, Sumo Logic, Cisco StealthWatch, Checkpoint CloudGuard, New Relic 등의 파트너 솔루션을 사용할 수도 있습니다.\n",
            "\"category : VPC, question : VPC 흐름 로그를 사용하려면 어떻게 해야 하나요?, answer : VPC, 서브넷 또는 네트워크 인터페이스에 대해 흐름 로그를 생성할 수 있습니다. 서브넷 또는 VPC에 대한 흐름 로그를 생성하면 해당 서브넷 또는 VPC의 각 네트워크 인터페이스가 모니터링됩니다. 흐름 로그 구독을 생성하는 동안 캡처할 메타데이터 필드, 최대 집계 간격 및 기본 로그 대상을 선택할 수 있습니다. 또한 모든 트래픽을 캡처하도록 선택할 수도 있고 승인되거나 거부된 트래픽만 캡처하도록 선택할 수도 있습니다. CloudWatch Log Insights 또는 CloudWatch Contributor Insights와 같은 도구를 사용하여 CloudWatch Logs로 전송되는 VPC 흐름 로그를 분석할 수 있습니다. Amazon Athena 또는 AWS QuickSight와 같은 도구를 사용하여 Amazon S3로 전송되는 VPC 흐름 로그를 쿼리하고 시각화할 수 있습니다. 또한 사용자 지정 다운스트림 애플리케이션을 구축하여 로그를 분석하거나, Splunk, Datadog, Sumo Logic, Cisco StealthWatch, Checkpoint CloudGuard, New Relic 등의 파트너 솔루션을 사용할 수도 있습니다.\"\n",
            "VPC 흐름 로그는 AWS Transit Gateway를 지원하나요?\n",
            "예. Transit Gateway 또는 개별 Transit Gateway Attachment에 대한 VPC 흐름 로그를 생성할 수 있습니다. 이 기능을 사용하면 Transit Gateway에서 Transit Gateway를 통과하는 네트워크 흐름에 대한 소스/대상 IP, 포트, 프로토콜, 트래픽 카운터, 타임스탬프 및 다양한 메타데이터와 같은 자세한 정보를 내보낼 수 있습니다. Transit Gateway에 대한 Amazon VPC 흐름 로그 지원에 대해 자세히 알아보려면 설명서를 참조하세요.\n",
            "\"category : VPC, question : VPC 흐름 로그는 AWS Transit Gateway를 지원하나요?, answer : 예. Transit Gateway 또는 개별 Transit Gateway Attachment에 대한 VPC 흐름 로그를 생성할 수 있습니다. 이 기능을 사용하면 Transit Gateway에서 Transit Gateway를 통과하는 네트워크 흐름에 대한 소스/대상 IP, 포트, 프로토콜, 트래픽 카운터, 타임스탬프 및 다양한 메타데이터와 같은 자세한 정보를 내보낼 수 있습니다. Transit Gateway에 대한 Amazon VPC 흐름 로그 지원에 대해 자세히 알아보려면 설명서를 참조하세요.\"\n",
            "흐름 로그를 사용하면 네트워크 지연 시간이나 성능에 영향을 미치나요?\n",
            "흐름 로그 데이터는 네트워크 트래픽 경로 외부에서 수집되므로 네트워크 처리량이나 지연 시간에 영향을 미치지 않습니다. 흐름 로그를 생성하거나 삭제더라도 네트워크 성능에 영향을 미치지 않습니다.\n",
            "\"category : VPC, question : 흐름 로그를 사용하면 네트워크 지연 시간이나 성능에 영향을 미치나요?, answer : 흐름 로그 데이터는 네트워크 트래픽 경로 외부에서 수집되므로 네트워크 처리량이나 지연 시간에 영향을 미치지 않습니다. 흐름 로그를 생성하거나 삭제더라도 네트워크 성능에 영향을 미치지 않습니다.\"\n",
            "VPC 흐름 로그 요금은 어떻게 되나요?\n",
            "CloudWatch Logs 또는 Amazon S3에 흐름 로그를 게시할 때 Vended 로그에 대한 데이터 수집 및 아카이빙 요금이 적용됩니다. 자세한 내용과 예는 Amazon CloudWatch 요금을 참조하세요. 또한 비용 할당 태그를 사용하여 흐름 로그를 게시하는 데 따른 요금을 추적할 수도 있습니다.\n",
            "\"category : VPC, question : VPC 흐름 로그 요금은 어떻게 되나요?, answer : CloudWatch Logs 또는 Amazon S3에 흐름 로그를 게시할 때 Vended 로그에 대한 데이터 수집 및 아카이빙 요금이 적용됩니다. 자세한 내용과 예는 Amazon CloudWatch 요금을 참조하세요. 또한 비용 할당 태그를 사용하여 흐름 로그를 게시하는 데 따른 요금을 추적할 수도 있습니다.\"\n",
            "Amazon VPC Traﬃc Mirroring이란 무엇인가요?\n",
            "고객은 Amazon VPC 트래픽 미러링을 사용하여 콘텐츠 검사, 위협 모니터링, 문제 해결 등의 사용 사례를 위해 Amazon EC2 인스턴스의 네트워크 트래픽을 복제하고 대역 외 보안 및 모니터링 어플라이언스로 전달하는 과정을 손쉽게 처리할 수 있습니다. 이러한 어플라이언스는 개별 EC2 인스턴스 또는 UDP(User Datagram Protocol) 리스너가 있는 NLB(Network Load Balancer) 뒤의 인스턴스 플릿에 배포될 수 있습니다.\n",
            "\"category : VPC, question : Amazon VPC Traﬃc Mirroring이란 무엇인가요?, answer : 고객은 Amazon VPC 트래픽 미러링을 사용하여 콘텐츠 검사, 위협 모니터링, 문제 해결 등의 사용 사례를 위해 Amazon EC2 인스턴스의 네트워크 트래픽을 복제하고 대역 외 보안 및 모니터링 어플라이언스로 전달하는 과정을 손쉽게 처리할 수 있습니다. 이러한 어플라이언스는 개별 EC2 인스턴스 또는 UDP(User Datagram Protocol) 리스너가 있는 NLB(Network Load Balancer) 뒤의 인스턴스 플릿에 배포될 수 있습니다.\"\n",
            "Amazon VPC Traﬃc Mirroring을 통해 어떤 리소스를 모니터링할 수 있나요?\n",
            "트래픽 미러링은 EC2 인스턴스용 탄력적 네트워크 인터페이스(ENI)에서 네트워크 패킷 캡처를 지원합니다. Amazon VPC Traﬃc Mirroring을 지원하는 EC2 인스턴스는 트래픽 미러링 설명서를 참조하세요.\n",
            "\"category : VPC, question : Amazon VPC Traﬃc Mirroring을 통해 어떤 리소스를 모니터링할 수 있나요?, answer : 트래픽 미러링은 EC2 인스턴스용 탄력적 네트워크 인터페이스(ENI)에서 네트워크 패킷 캡처를 지원합니다. Amazon VPC Traﬃc Mirroring을 지원하는 EC2 인스턴스는 트래픽 미러링 설명서를 참조하세요.\"\n",
            "Amazon VPC Traﬃc Mirroring에서는 어떤 유형의 어플라이언스가 지원되나요?\n",
            "고객은 오픈 소스 도구를 사용할 수도 있고, AWS Marketplace에서 제공되는 광범위한 모니터링 솔루션 중에서 선택할 수도 있습니다. 트래픽 미러링을 사용하는 고객은 공급업체별 에이전트를 설치할 필요 없이 복제된 트래픽을 네트워크 패킷 수집기/브로커 또는 분석 도구로 스트리밍할 수 있습니다.\n",
            "\"category : VPC, question : Amazon VPC Traﬃc Mirroring에서는 어떤 유형의 어플라이언스가 지원되나요?, answer : 고객은 오픈 소스 도구를 사용할 수도 있고, AWS Marketplace에서 제공되는 광범위한 모니터링 솔루션 중에서 선택할 수도 있습니다. 트래픽 미러링을 사용하는 고객은 공급업체별 에이전트를 설치할 필요 없이 복제된 트래픽을 네트워크 패킷 수집기/브로커 또는 분석 도구로 스트리밍할 수 있습니다.\"\n",
            "Amazon VPC Traﬃc Mirroring은 Amazon VPC 흐름 로그와 어떻게 다른가요?\n",
            "고객은 Amazon VPC 흐름 로그를 사용하여 네트워크 흐름 로그를 수집, 저장 및 분석할 수 있습니다. 흐름 로그에 캡처되는 정보에는 허용 및 거부되는 트래픽, 원본 및 대상 IP 주소, 포트, 프로토콜 번호, 패킷 및 바이트 수, 작업(수락 또는 거절)에 대한 정보가 포함됩니다. 이 기능을 사용하면 연결 및 보안 문제를 해결할 수 있는 것은 물론, 네트워크 액세스 규칙이 예상대로 작동하도록 할 수 있습니다.\n",
            "Amazon VPC 트래픽 미러링을 사용하면 페이로드 같은 실제 트래픽 콘텐츠를 분석하여 네트워크 트래픽을 좀 더 깊이 있게 파악할 수 있습니다. 이 기능은 실제 패킷을 분석하여 성능 문제의 근본 원인을 파악하거나, 정교한 네트워크 공격을 리버스 엔지니어링하거나, 내부자 침해 또는 손상된 워크로드를 감지 및 차단해야 하는 사용 사례를 위한 것입니다.\n",
            "\"category : VPC, question : Amazon VPC Traﬃc Mirroring은 Amazon VPC 흐름 로그와 어떻게 다른가요?, answer : 고객은 Amazon VPC 흐름 로그를 사용하여 네트워크 흐름 로그를 수집, 저장 및 분석할 수 있습니다. 흐름 로그에 캡처되는 정보에는 허용 및 거부되는 트래픽, 원본 및 대상 IP 주소, 포트, 프로토콜 번호, 패킷 및 바이트 수, 작업(수락 또는 거절)에 대한 정보가 포함됩니다. 이 기능을 사용하면 연결 및 보안 문제를 해결할 수 있는 것은 물론, 네트워크 액세스 규칙이 예상대로 작동하도록 할 수 있습니다.\n",
            "Amazon VPC 트래픽 미러링을 사용하면 페이로드 같은 실제 트래픽 콘텐츠를 분석하여 네트워크 트래픽을 좀 더 깊이 있게 파악할 수 있습니다. 이 기능은 실제 패킷을 분석하여 성능 문제의 근본 원인을 파악하거나, 정교한 네트워크 공격을 리버스 엔지니어링하거나, 내부자 침해 또는 손상된 워크로드를 감지 및 차단해야 하는 사용 사례를 위한 것입니다.\"\n",
            "Amazon VPC를 사용할 수 있는 Amazon EC2 리전은 어디인가요?\n",
            "Amazon VPC는 현재 모든 Amazon EC2 리전 내 여러 가용 영역에서 사용할 수 있습니다.\n",
            "\"category : VPC, question : Amazon VPC를 사용할 수 있는 Amazon EC2 리전은 어디인가요?, answer : Amazon VPC는 현재 모든 Amazon EC2 리전 내 여러 가용 영역에서 사용할 수 있습니다.\"\n",
            "한 VPC를 여러 가용 영역으로 확장할 수 있나요?\n",
            "예.\n",
            "\"category : VPC, question : 한 VPC를 여러 가용 영역으로 확장할 수 있나요?, answer : 예.\"\n",
            "한 서브넷을 여러 가용 영역으로 확장할 수 있나요?\n",
            "아니요. 한 서브넷은 하나의 가용 영역 내에서만 상주해야 합니다.\n",
            "\"category : VPC, question : 한 서브넷을 여러 가용 영역으로 확장할 수 있나요?, answer : 아니요. 한 서브넷은 하나의 가용 영역 내에서만 상주해야 합니다.\"\n",
            "Amazon EC2 인스턴스를 시작할 가용 영역을 지정하려면 어떻게 해야 하나요?\n",
            "Amazon EC2 인스턴스를 시작할 때 반드시 인스턴스를 시작할 서브넷을 지정해야 합니다. 인스턴스는 지정한 서브넷과 연결된 가용 영역 내에서 시작합니다.\n",
            "\"category : VPC, question : Amazon EC2 인스턴스를 시작할 가용 영역을 지정하려면 어떻게 해야 하나요?, answer : Amazon EC2 인스턴스를 시작할 때 반드시 인스턴스를 시작할 서브넷을 지정해야 합니다. 인스턴스는 지정한 서브넷과 연결된 가용 영역 내에서 시작합니다.\"\n",
            "서브넷이 어떤 가용 영역에 위치할 것인지를 어떻게 결정하나요?\n",
            "서브넷을 생성할 때, 반드시 서브넷을 배치할 가용 영역을 지정해야 합니다. VPC 마법사를 사용할 때, 마법사 구성 화면에서 서브넷 가용 영역을 선택할 수 있습니다. API 또는 CLI를 사용하여, 서브넷을 생성할 때 서브넷에 대한 가용 영역을 지정할 수 있습니다. 가용 영역을 지정하지 않으면 \"No Preference\" 옵션이 기본적으로 선택되며 리전에서 사용 가능한 가용 영역에 서브넷이 생성됩니다.\n",
            "\"category : VPC, question : 서브넷이 어떤 가용 영역에 위치할 것인지를 어떻게 결정하나요?, answer : 서브넷을 생성할 때, 반드시 서브넷을 배치할 가용 영역을 지정해야 합니다. VPC 마법사를 사용할 때, 마법사 구성 화면에서 서브넷 가용 영역을 선택할 수 있습니다. API 또는 CLI를 사용하여, 서브넷을 생성할 때 서브넷에 대한 가용 영역을 지정할 수 있습니다. 가용 영역을 지정하지 않으면 \"No Preference\" 옵션이 기본적으로 선택되며 리전에서 사용 가능한 가용 영역에 서브넷이 생성됩니다.\"\n",
            "서로 다른 서브넷에 있는 인스턴스 간의 네트워크 대역폭에 대한 요금이 청구되나요?\n",
            "인스턴스가 다른 가용 영역에 있는 서브넷에 상주할 경우, GB당 0.01 USD의 데이터 전송 요금이 부과됩니다.\n",
            "\"category : VPC, question : 서로 다른 서브넷에 있는 인스턴스 간의 네트워크 대역폭에 대한 요금이 청구되나요?, answer : 인스턴스가 다른 가용 영역에 있는 서브넷에 상주할 경우, GB당 0.01 USD의 데이터 전송 요금이 부과됩니다.\"\n",
            "DescribeInstances()를 호출하면 EC2-Classic과 EC2-VPC에 있는 인스턴스를 비롯하여 모든 Amazon EC2 인스턴스를 볼 수 있나요?\n",
            "예. DescribeInstances()는 모든 실행 중인 Amazon EC2 인스턴스를 반환합니다. 서브넷 필드에 있는 항목으로 EC2-VPC 인스턴스와 EC2-Classic 인스턴스를 구별할 수 있습니다. 서브넷 ID의 목록이 있다면, 인스턴스는 VPC 내에 있는 것입니다.\n",
            "\"category : VPC, question : DescribeInstances()를 호출하면 EC2-Classic과 EC2-VPC에 있는 인스턴스를 비롯하여 모든 Amazon EC2 인스턴스를 볼 수 있나요?, answer : 예. DescribeInstances()는 모든 실행 중인 Amazon EC2 인스턴스를 반환합니다. 서브넷 필드에 있는 항목으로 EC2-VPC 인스턴스와 EC2-Classic 인스턴스를 구별할 수 있습니다. 서브넷 ID의 목록이 있다면, 인스턴스는 VPC 내에 있는 것입니다.\"\n",
            "DescribeVolumes()를 호출하면 EC2-Classic과 EC2-VPC에 있는 볼륨을 비롯하여 모든 Amazon EBS 볼륨을 볼 수 있나요?\n",
            "예. DescribeVolumes()는 모든 EBS 볼륨을 반환합니다.\n",
            "\"category : VPC, question : DescribeVolumes()를 호출하면 EC2-Classic과 EC2-VPC에 있는 볼륨을 비롯하여 모든 Amazon EBS 볼륨을 볼 수 있나요?, answer : 예. DescribeVolumes()는 모든 EBS 볼륨을 반환합니다.\"\n",
            "VPC에서 얼마나 많은 Amazon EC2 인스턴스를 사용할 수 있나요?\n",
            "IPv4 주소 지정이 필요한 인스턴스의 경우 VPC 내에서 실행할 수 있는 Amazon EC2 인스턴스 수에 제한이 없습니다. 단, VPC가 각 인스턴스에 할당된 IPv4 주소를 가지도록 적절한 크기로 설정되어야 합니다. 처음에는 한 번에 최대 20개의 Amazon EC2 인스턴스를 시작할 수 있도록 제한되며, VPC의 최대 크기는 /16(65,536 IP)입니다. 이러한 한도를 늘리려면 다음 양식을 작성해 주세요. IPv6 전용 인스턴스의 경우 /56의 VPC 크기는 Amazon EC2 인스턴스를 거의 무제한으로 시작할 수 있는 기능을 제공합니다.\n",
            "\"category : VPC, question : VPC에서 얼마나 많은 Amazon EC2 인스턴스를 사용할 수 있나요?, answer : IPv4 주소 지정이 필요한 인스턴스의 경우 VPC 내에서 실행할 수 있는 Amazon EC2 인스턴스 수에 제한이 없습니다. 단, VPC가 각 인스턴스에 할당된 IPv4 주소를 가지도록 적절한 크기로 설정되어야 합니다. 처음에는 한 번에 최대 20개의 Amazon EC2 인스턴스를 시작할 수 있도록 제한되며, VPC의 최대 크기는 /16(65,536 IP)입니다. 이러한 한도를 늘리려면 다음 양식을 작성해 주세요. IPv6 전용 인스턴스의 경우 /56의 VPC 크기는 Amazon EC2 인스턴스를 거의 무제한으로 시작할 수 있는 기능을 제공합니다.\"\n",
            "Amazon VPC에서 기존 AMI를 사용할 수 있나요?\n",
            "VPC와 같은 리전 내에 등록된 Amazon VPC의 AMI를 사용할 수 있습니다. 예를 들어, us-east-1에 등록된 AMI를 us-east-1에 있는 VPC와 함께 사용할 수 있습니다. 자세한 내용은 Amazon EC2 리전 및 가용 영역 FAQ에서 확인할 수 있습니다.\n",
            "\"category : VPC, question : Amazon VPC에서 기존 AMI를 사용할 수 있나요?, answer : VPC와 같은 리전 내에 등록된 Amazon VPC의 AMI를 사용할 수 있습니다. 예를 들어, us-east-1에 등록된 AMI를 us-east-1에 있는 VPC와 함께 사용할 수 있습니다. 자세한 내용은 Amazon EC2 리전 및 가용 영역 FAQ에서 확인할 수 있습니다.\"\n",
            "기존 Amazon EBS 스냅샷을 사용할 수 있나요?\n",
            "예. VPC와 같은 리전에 위치해 있다면 Amazon EBS 스냅샷을 사용할 수 있습니다. 세부 정보는 Amazon EC2 리전 및 가용 영역 FAQ에서 확인할 수 있습니다.\n",
            "\"category : VPC, question : 기존 Amazon EBS 스냅샷을 사용할 수 있나요?, answer : 예. VPC와 같은 리전에 위치해 있다면 Amazon EBS 스냅샷을 사용할 수 있습니다. 세부 정보는 Amazon EC2 리전 및 가용 영역 FAQ에서 확인할 수 있습니다.\"\n",
            "Amazon VPC에 있는 Amazon EBS 볼륨에서 Amazon EC2 인스턴스를 부팅할 수 있나요?\n",
            "예. 하지만 Amazon EBS-backed AMI를 사용하여 VPC에서 시작한 인스턴스는 중지될 때와 다시 시작할 때 동일한 IP 주소를 유지합니다. VPC 외부에서 시작한 유사한 인스턴스가 새 IP 주소를 얻는 것과는 대조됩니다. 서브넷에서 중지된 인스턴스에 대한 IP 주소는 이용할 수 없는 것으로 간주합니다.\n",
            "\"category : VPC, question : Amazon VPC에 있는 Amazon EBS 볼륨에서 Amazon EC2 인스턴스를 부팅할 수 있나요?, answer : 예. 하지만 Amazon EBS-backed AMI를 사용하여 VPC에서 시작한 인스턴스는 중지될 때와 다시 시작할 때 동일한 IP 주소를 유지합니다. VPC 외부에서 시작한 유사한 인스턴스가 새 IP 주소를 얻는 것과는 대조됩니다. 서브넷에서 중지된 인스턴스에 대한 IP 주소는 이용할 수 없는 것으로 간주합니다.\"\n",
            "Amazon VPC에서 Amazon CloudWatch를 사용할 수 있나요?\n",
            "예.\n",
            "\"category : VPC, question : Amazon VPC에서 Amazon CloudWatch를 사용할 수 있나요?, answer : 예.\"\n",
            "Amazon VPC에서 Auto Scaling 기능을 사용할 수 있나요?\n",
            "예.\n",
            "\"category : VPC, question : Amazon VPC에서 Auto Scaling 기능을 사용할 수 있나요?, answer : 예.\"\n",
            "VPC에서 Amazon EC2 클러스터 인스턴스를 시작할 수 있나요?\n",
            "예. 클러스터 인스턴스는 Amazon VPC에서 지원되지만, 모든 인스턴스 유형을 모든 리전 및 가용 영역에서 사용할 수 있는 것은 아닙니다.\n",
            "\"category : VPC, question : VPC에서 Amazon EC2 클러스터 인스턴스를 시작할 수 있나요?, answer : 예. 클러스터 인스턴스는 Amazon VPC에서 지원되지만, 모든 인스턴스 유형을 모든 리전 및 가용 영역에서 사용할 수 있는 것은 아닙니다.\"\n",
            "인스턴스 호스트 이름이란 무엇인가요?\n",
            "인스턴스를 시작하면 호스트 이름이 할당됩니다. IP 기반 이름 또는 리소스 기반 이름의 두 가지 옵션을 사용할 수 있으며 이 파라미터는 인스턴스 시작 시 구성할 수 있습니다. IP 기반 이름은 프라이빗 IPv4 주소 형식을 사용하는 반면 리소스 기반 이름은 instance-id 형식을 사용합니다.\n",
            "\"category : VPC, question : 인스턴스 호스트 이름이란 무엇인가요?, answer : 인스턴스를 시작하면 호스트 이름이 할당됩니다. IP 기반 이름 또는 리소스 기반 이름의 두 가지 옵션을 사용할 수 있으며 이 파라미터는 인스턴스 시작 시 구성할 수 있습니다. IP 기반 이름은 프라이빗 IPv4 주소 형식을 사용하는 반면 리소스 기반 이름은 instance-id 형식을 사용합니다.\"\n",
            "Amazon EC2 인스턴스의 인스턴스 호스트 이름을 변경할 수 있나요?\n",
            "예, 인스턴스를 중지한 다음 리소스 기반 이름 지정 옵션을 변경하여 인스턴스 양식 IP 기반의 호스트 이름을 리소스 기반으로 또는 그 반대로 변경할 수 있습니다.\n",
            "\"category : VPC, question : Amazon EC2 인스턴스의 인스턴스 호스트 이름을 변경할 수 있나요?, answer : 예, 인스턴스를 중지한 다음 리소스 기반 이름 지정 옵션을 변경하여 인스턴스 양식 IP 기반의 호스트 이름을 리소스 기반으로 또는 그 반대로 변경할 수 있습니다.\"\n",
            "인스턴스 호스트 이름을 DNS 호스트 이름으로 사용할 수 있나요?\n",
            "예, 인스턴스 호스트 이름을 DNS 호스트 이름으로 사용할 수 있습니다. IPv4 전용 또는 이중 스택 서브넷에서 시작된 인스턴스의 경우 IP 기반 이름은 항상 인스턴스의 기본 네트워크 인터페이스에서 프라이빗 IPv4 주소로 확인되며 비활성화할 수 없습니다. 또한 리소스 기반 이름은 기본 네트워크 인터페이스의 프라이빗 IPv4 주소나 기본 네트워크 인터페이스의 첫 번째 IPv6 GUA 또는 둘 다로 확인하도록 구성할 수 있습니다. IPv6 전용 서브넷에서 시작된 인스턴스의 경우 리소스 기반 이름은 기본 네트워크 인터페이스의 첫 번째 IPv6 GUA로 확인되도록 구성됩니다.\n",
            "\"category : VPC, question : 인스턴스 호스트 이름을 DNS 호스트 이름으로 사용할 수 있나요?, answer : 예, 인스턴스 호스트 이름을 DNS 호스트 이름으로 사용할 수 있습니다. IPv4 전용 또는 이중 스택 서브넷에서 시작된 인스턴스의 경우 IP 기반 이름은 항상 인스턴스의 기본 네트워크 인터페이스에서 프라이빗 IPv4 주소로 확인되며 비활성화할 수 없습니다. 또한 리소스 기반 이름은 기본 네트워크 인터페이스의 프라이빗 IPv4 주소나 기본 네트워크 인터페이스의 첫 번째 IPv6 GUA 또는 둘 다로 확인하도록 구성할 수 있습니다. IPv6 전용 서브넷에서 시작된 인스턴스의 경우 리소스 기반 이름은 기본 네트워크 인터페이스의 첫 번째 IPv6 GUA로 확인되도록 구성됩니다.\"\n",
            "기본 VPC란 무엇인가요?\n",
            "기본 VPC는 AWS 클라우드 내에 있는 논리적으로 격리된 가상 네트워크로서, 처음 Amazon EC2 리소스를 프로비저닝할 때 사용자의 AWS 계정에 자동으로 생성됩니다. 서브넷 ID를 지정하지 않고 인스턴스를 시작하면 인스턴스가 기본 VPC에서 시작됩니다.\n",
            "\"category : VPC, question : 기본 VPC란 무엇인가요?, answer : 기본 VPC는 AWS 클라우드 내에 있는 논리적으로 격리된 가상 네트워크로서, 처음 Amazon EC2 리소스를 프로비저닝할 때 사용자의 AWS 계정에 자동으로 생성됩니다. 서브넷 ID를 지정하지 않고 인스턴스를 시작하면 인스턴스가 기본 VPC에서 시작됩니다.\"\n",
            "기본 VPC의 이점은 무엇인가요?\n",
            "기본 VPC에서 리소스를 시작할 경우, Amazon VPC(EC2-VPC)의 고급 네트워킹 기능을 활용할 수 있을 뿐만 아니라 Amazon EC2(EC2-Classic)를 편리하게 사용할 수 있습니다. VPC를 별도로 생성하여 VPC에서 인스턴스를 시작하지 않고도 보안 그룹 회원을 즉각적으로 변경하거나 보안 그룹 송신 필터링, 다중 IP 주소 및 다중 네트워크 인터페이스와 같은 기능을 활용할 수 있습니다.\n",
            "\"category : VPC, question : 기본 VPC의 이점은 무엇인가요?, answer : 기본 VPC에서 리소스를 시작할 경우, Amazon VPC(EC2-VPC)의 고급 네트워킹 기능을 활용할 수 있을 뿐만 아니라 Amazon EC2(EC2-Classic)를 편리하게 사용할 수 있습니다. VPC를 별도로 생성하여 VPC에서 인스턴스를 시작하지 않고도 보안 그룹 회원을 즉각적으로 변경하거나 보안 그룹 송신 필터링, 다중 IP 주소 및 다중 네트워크 인터페이스와 같은 기능을 활용할 수 있습니다.\"\n",
            "어떤 계정에 기본 VPC가 활성화되어 있나요?\n",
            "AWS 계정이 2013년 3월 18일 이후에 생성된 경우, 기본 VPC에서 리소스를 시작할 수 있습니다. 어떤 리전에서 기본 VPC 기능 세트를 사용할 수 있는지 확인하려면 이 포럼 공지 사항을 참조하세요. 또한, 기재된 날짜 이전에 생성된 계정은 EC2 인스턴스를 시작한 적이 없거나 Amazon Elastic Load Balancing, Amazon RDS, Amazon ElastiCache 또는 Amazon Redshift 리소스를 프로비저닝한 적이 없는 리전 중 기본 VPC가 활성된 리전에서 기본 VPC를 사용할 수 있습니다.\n",
            "\"category : VPC, question : 어떤 계정에 기본 VPC가 활성화되어 있나요?, answer : AWS 계정이 2013년 3월 18일 이후에 생성된 경우, 기본 VPC에서 리소스를 시작할 수 있습니다. 어떤 리전에서 기본 VPC 기능 세트를 사용할 수 있는지 확인하려면 이 포럼 공지 사항을 참조하세요. 또한, 기재된 날짜 이전에 생성된 계정은 EC2 인스턴스를 시작한 적이 없거나 Amazon Elastic Load Balancing, Amazon RDS, Amazon ElastiCache 또는 Amazon Redshift 리소스를 프로비저닝한 적이 없는 리전 중 기본 VPC가 활성된 리전에서 기본 VPC를 사용할 수 있습니다.\"\n",
            "기본 VPC를 사용하기 위해 Amazon VPC에 관해 알아야 할 사항이 있나요?\n",
            "AWS Management Console, AWS EC2 CLI 또는 Amazon EC2 API를 사용하여 기본 VPC에서 EC2 인스턴스와 기타 AWS 리소스를 시작하고 관리할 수 있습니다. AWS에서 자동으로 기본 VPC를 생성하고 AWS 리전의 각 가용 영역에 기본 서브넷을 생성합니다. 기본 VPC는 인터넷 게이트웨이에 연결되고 사용자의 인스턴스는 EC2-Classic과 마찬가지로 퍼블릭 IP 주소를 자동으로 부여받습니다.\n",
            "\"category : VPC, question : 기본 VPC를 사용하기 위해 Amazon VPC에 관해 알아야 할 사항이 있나요?, answer : AWS Management Console, AWS EC2 CLI 또는 Amazon EC2 API를 사용하여 기본 VPC에서 EC2 인스턴스와 기타 AWS 리소스를 시작하고 관리할 수 있습니다. AWS에서 자동으로 기본 VPC를 생성하고 AWS 리전의 각 가용 영역에 기본 서브넷을 생성합니다. 기본 VPC는 인터넷 게이트웨이에 연결되고 사용자의 인스턴스는 EC2-Classic과 마찬가지로 퍼블릭 IP 주소를 자동으로 부여받습니다.\"\n",
            "EC2-Classic과 EC2-VPC에서 각각 시작한 인스턴스의 차이점은 무엇인가요?\n",
            "EC2 사용 설명서에서 EC2-Classic과 EC2-VPC 차이점 섹션을 참조하세요.\n",
            "\"category : VPC, question : EC2-Classic과 EC2-VPC에서 각각 시작한 인스턴스의 차이점은 무엇인가요?, answer : EC2 사용 설명서에서 EC2-Classic과 EC2-VPC 차이점 섹션을 참조하세요.\"\n",
            "기본 VPC를 사용하려면 VPN 연결이 필요한가요?\n",
            "기본 VPC는 인터넷에 연결되어 있으며 기본 VPC 내의 기본 서브넷 에서 시작된 모든 인스턴스는 자동으로 퍼블릭 IP 주소를 받게 됩니다. 원할 경우, 사용자의 기본 VPC에 VPN 연결을 추가할 수 있습니다.\n",
            "\"category : VPC, question : 기본 VPC를 사용하려면 VPN 연결이 필요한가요?, answer : 기본 VPC는 인터넷에 연결되어 있으며 기본 VPC 내의 기본 서브넷 에서 시작된 모든 인스턴스는 자동으로 퍼블릭 IP 주소를 받게 됩니다. 원할 경우, 사용자의 기본 VPC에 VPN 연결을 추가할 수 있습니다.\"\n",
            "다른 VPC를 생성하여 기본 VPC 외에 추가로 사용할 수 있나요?\n",
            "예. 기본이 아닌 VPC에서 인스턴스를 시작하려면 인스턴스 시작 과정 중에서 서브넷 ID를 지정하여야 합니다.\n",
            "\"category : VPC, question : 다른 VPC를 생성하여 기본 VPC 외에 추가로 사용할 수 있나요?, answer : 예. 기본이 아닌 VPC에서 인스턴스를 시작하려면 인스턴스 시작 과정 중에서 서브넷 ID를 지정하여야 합니다.\"\n",
            "기본 VPC 안에 프라이빗 서브넷과 같은 추가 서브넷을 생성할 수 있나요?\n",
            "예. 기본 서브넷이 아닌 다른 서브넷에서 시작하려면, 콘솔을 사용하거나 CLI, API 또는 SDK에서 --subnet 옵션을 사용해 시작할 서브넷을 지정할 수 있습니다.\n",
            "\"category : VPC, question : 기본 VPC 안에 프라이빗 서브넷과 같은 추가 서브넷을 생성할 수 있나요?, answer : 예. 기본 서브넷이 아닌 다른 서브넷에서 시작하려면, 콘솔을 사용하거나 CLI, API 또는 SDK에서 --subnet 옵션을 사용해 시작할 서브넷을 지정할 수 있습니다.\"\n",
            "보유할 수 있는 기본 VPC의 수는 몇 개인가요?\n",
            "Supported Platforms 속성이 \"EC2-VPC\"로 설정된 각 AWS 리전에서 1개의 기본 VPC를 보유할 수 있습니다.\n",
            "\"category : VPC, question : 보유할 수 있는 기본 VPC의 수는 몇 개인가요?, answer : Supported Platforms 속성이 \"EC2-VPC\"로 설정된 각 AWS 리전에서 1개의 기본 VPC를 보유할 수 있습니다.\"\n",
            "기본 VPC에는 몇 개의 기본 서브넷이 있나요?\n",
            "사용자의 기본 VPC에 있는 가용 영역당 하나의 기본 서브넷이 생성됩니다.\n",
            "\"category : VPC, question : 기본 VPC에는 몇 개의 기본 서브넷이 있나요?, answer : 사용자의 기본 VPC에 있는 가용 영역당 하나의 기본 서브넷이 생성됩니다.\"\n",
            "기본 VPC로 사용할 VPC를 지정할 수 있나요?\n",
            "현재는 지원되지 않습니다.\n",
            "\"category : VPC, question : 기본 VPC로 사용할 VPC를 지정할 수 있나요?, answer : 현재는 지원되지 않습니다.\"\n",
            "기본 서브넷으로 사용할 서브넷을 지정할 수 있나요?\n",
            "현재는 지원되지 않습니다.\n",
            "\"category : VPC, question : 기본 서브넷으로 사용할 서브넷을 지정할 수 있나요?, answer : 현재는 지원되지 않습니다.\"\n",
            "기본 VPC를 삭제할 수 있습니까?\n",
            "예. 기본 VPC를 삭제할 수 있습니다. 삭제한 후에는 VPC 콘솔에서 바로 또는 CLI를 사용하여 새로운 기본 VPC를 생성할 수 있습니다. 그러면 리전에 새로운 기본 VPC가 생성됩니다. 삭제가 이전 VPC가 복원되지는 않습니다.\n",
            "\"category : VPC, question : 기본 VPC를 삭제할 수 있습니까?, answer : 예. 기본 VPC를 삭제할 수 있습니다. 삭제한 후에는 VPC 콘솔에서 바로 또는 CLI를 사용하여 새로운 기본 VPC를 생성할 수 있습니다. 그러면 리전에 새로운 기본 VPC가 생성됩니다. 삭제가 이전 VPC가 복원되지는 않습니다.\"\n",
            "기본 서브넷을 삭제할 수 있나요?\n",
            "예. 기본 서브넷을 삭제할 수 있습니다. 삭제되고 나면 CLI 또는 SDK를 사용하여 가용 영역에 새로운 기본 서브넷을 생성할 수 있습니다. 그러면 지정된 가용 영역에 새로운 기본 서브넷이 생성되며, 삭제된 이전 서브넷은 복원되지 않습니다.\n",
            "\"category : VPC, question : 기본 서브넷을 삭제할 수 있나요?, answer : 예. 기본 서브넷을 삭제할 수 있습니다. 삭제되고 나면 CLI 또는 SDK를 사용하여 가용 영역에 새로운 기본 서브넷을 생성할 수 있습니다. 그러면 지정된 가용 영역에 새로운 기본 서브넷이 생성되며, 삭제된 이전 서브넷은 복원되지 않습니다.\"\n",
            "저는 기존 EC2-Classic 계정을 갖고 있습니다. 기본 VPC를 가질 수 있나요?\n",
            "기본 VPC를 보유하는 가장 간단한 방법은 기본 VPC가 활성화된 리전에 새 계정을 생성하거나, 이전에 사용한 적이 없는 리전에 있는 기존 계정을 사용하는 것입니다. 그 리전에 있는 해당 계정의 Supported Platforms 속성이 \"EC2-VPC\"로 설정된 경우에 한해 가능합니다.\n",
            "\"category : VPC, question : 저는 기존 EC2-Classic 계정을 갖고 있습니다. 기본 VPC를 가질 수 있나요?, answer : 기본 VPC를 보유하는 가장 간단한 방법은 기본 VPC가 활성화된 리전에 새 계정을 생성하거나, 이전에 사용한 적이 없는 리전에 있는 기존 계정을 사용하는 것입니다. 그 리전에 있는 해당 계정의 Supported Platforms 속성이 \"EC2-VPC\"로 설정된 경우에 한해 가능합니다.\"\n",
            "기존 EC2 계정에 기본 VPC를 갖고 싶습니다. 가능한가요?\n",
            "예. 하지만 그 리전의 해당 계정에 EC2-Classic 리소스가 없는 경우에 한하여 기존 계정에 기본 VPC를 활성화할 수 있습니다. 또한, VPC에 의해 프로비저닝되지 않은 해당 리전의 모든 Elastic Load Balancer, Amazon RDS, Amazon ElastiCache, Amazon Redshift 리소스를 종료해야 합니다. 사용자의 계정에 기본 VPC가 구성된 후에는, Auto Scaling을 통해 시작된 인스턴스를 비롯하여 이후의 모든 리소스 작업은 기본 VPC에서 시작됩니다. 기존 계정을 기본 VPC로 구성하도록 요청하려면 Account and Billing -> Service: Account -> Category: Convert EC2 Classic to VPC로 이동하여 요청을 작성하십시오. 고객의 요청, 기존 AWS 서비스 및 EC2-Classic의 존재 여부를 검토한 후 다음 단계를 안내해 드리겠습니다.\n",
            "\"category : VPC, question : 기존 EC2 계정에 기본 VPC를 갖고 싶습니다. 가능한가요?, answer : 예. 하지만 그 리전의 해당 계정에 EC2-Classic 리소스가 없는 경우에 한하여 기존 계정에 기본 VPC를 활성화할 수 있습니다. 또한, VPC에 의해 프로비저닝되지 않은 해당 리전의 모든 Elastic Load Balancer, Amazon RDS, Amazon ElastiCache, Amazon Redshift 리소스를 종료해야 합니다. 사용자의 계정에 기본 VPC가 구성된 후에는, Auto Scaling을 통해 시작된 인스턴스를 비롯하여 이후의 모든 리소스 작업은 기본 VPC에서 시작됩니다. 기존 계정을 기본 VPC로 구성하도록 요청하려면 Account and Billing -> Service: Account -> Category: Convert EC2 Classic to VPC로 이동하여 요청을 작성하십시오. 고객의 요청, 기존 AWS 서비스 및 EC2-Classic의 존재 여부를 검토한 후 다음 단계를 안내해 드리겠습니다.\"\n",
            "기본 VPC는 IAM 계정에 어떤 영향을 미치나요?\n",
            "사용자의 AWS 계정에 기본 VPC가 있는 경우 사용자의 AWS 계정에 연결된 IAM 계정은 AWS 계정과 동일한 기본 VPC를 사용합니다.\n",
            "\"category : VPC, question : 기본 VPC는 IAM 계정에 어떤 영향을 미치나요?, answer : 사용자의 AWS 계정에 기본 VPC가 있는 경우 사용자의 AWS 계정에 연결된 IAM 계정은 AWS 계정과 동일한 기본 VPC를 사용합니다.\"\n",
            "EC2-Classic이란 무엇인가요?\n",
            "EC2-Classic은 2006년 여름에 EC2와 함께 출시한 플랫 네트워크입니다. EC2-Classic을 사용하면 인스턴스가 다른 고객과 공유하는 단일 플랫 네트워크에서 실행됩니다. 시간이 지나면서 고객의 진화하는 요구에서 아이디어를 얻어 사용자의 AWS 계정과 논리적으로 격리된 가상 프라이빗 클라우드에서 인스턴스를 실행할 수 있는 Amazon Virtual Private Cloud(VPC)를 2009년에 출시했습니다. 현재는 대다수 고객들이 Amazon VPC를 사용하며 소수의 고객들만이 EC2-Classic을 사용합니다.\n",
            "\"category : VPC, question : EC2-Classic이란 무엇인가요?, answer : EC2-Classic은 2006년 여름에 EC2와 함께 출시한 플랫 네트워크입니다. EC2-Classic을 사용하면 인스턴스가 다른 고객과 공유하는 단일 플랫 네트워크에서 실행됩니다. 시간이 지나면서 고객의 진화하는 요구에서 아이디어를 얻어 사용자의 AWS 계정과 논리적으로 격리된 가상 프라이빗 클라우드에서 인스턴스를 실행할 수 있는 Amazon Virtual Private Cloud(VPC)를 2009년에 출시했습니다. 현재는 대다수 고객들이 Amazon VPC를 사용하며 소수의 고객들만이 EC2-Classic을 사용합니다.\"\n",
            "무엇이 변경되나요?\n",
            "2022년 8월 15일에 Amazon EC2-Classic을 사용 중지하므로 이 날짜 전까지 EC2-Classic에서 실행 중인 EC2 인스턴스 및 기타 AWS 리소스를 Amazon VPC로 마이그레이션해야 합니다. 다음 섹션에서는 EC2-Class 사용 중지와 마이그레이션을 지원할 도구 및 리소스에 대해 자세히 설명합니다.\n",
            "\"category : VPC, question : 무엇이 변경되나요?, answer : 2022년 8월 15일에 Amazon EC2-Classic을 사용 중지하므로 이 날짜 전까지 EC2-Classic에서 실행 중인 EC2 인스턴스 및 기타 AWS 리소스를 Amazon VPC로 마이그레이션해야 합니다. 다음 섹션에서는 EC2-Class 사용 중지와 마이그레이션을 지원할 도구 및 리소스에 대해 자세히 설명합니다.\"\n",
            "EC2-Classic 사용 중지로 제 계정에 어떤 영향이 있습니까?\n",
            "AWS 리전 중 하나의 계정에서 EC2-Classic을 사용하도록 설정한 경우에만 이 변경의 영향을 받습니다. 콘솔 또는 describe-account-attributes 명령을 사용하여 AWS 리전에 대해 EC2-Classic을 사용하도록 설정했는지 여부를 확인할 수 있습니다. 자세한 내용은 이 문서를 참조하세요.리전에서 EC2-Classic에 실행 중인 활성 AWS 리소스가 없는 경우 해당 리전에 대해 계정에서 EC2-Classic을 끄십시오. 리전에서 EC2-Classic을 끄면 해당 리전에서 기본 VPC를 시작할 수 있습니다. 이렇게 하려면 AWS Support Center(console.aws.amazon.com/support)로 이동하고 ‘사례 생성’을 선택한 다음 ‘유형’에서 ‘계정 및 결제 지원’, ‘범주’에서 ‘계정’을 선택한 다음 ‘EC2 Classic에서 VPC로 변환’을 선택하고 필요한 기타 세부 정보를 입력하고 ‘제출’을 선택합니다.2021년 1월 1일 이후 EC2-Classic에 AWS 리소스(EC2 인스턴스, Amazon Relational Database, AWS Elastic Beanstalk, Amazon Redshift, AWS Data Pipeline, Amazon EMR, AWS OpsWorks)가 없는 모든 AWS 리전에 대해 2021년 10월 30일에 사용자의 계정에서 EC2-Classic을 자동으로 끕니다.반면 EC2-Classic에서 실행 중인 AWS 리소스가 있는 경우에는 가능한 신속하게 Amazon VPC로의 마이그레이션을 계획해야 합니다. 2022년 8월 15일 이후에는 EC2-Classic 플랫폼에서 인스턴스 또는 AWS 서비스를 시작할 수 없습니다. 실행 중 상태의 모든 워크로드 또는 서비스는 2022년 8월 16일부터 EC2-Classic이 사용 중지되므로 EC2-Classic의 모든 AWS 서비스에 대한 액세스가 점차 손실됩니다.\n",
            "후속 질문에서 AWS 리소스의 마이그레이션 가이드를 확인할 수 있습니다.\n",
            "\"category : VPC, question : EC2-Classic 사용 중지로 제 계정에 어떤 영향이 있습니까?, answer : AWS 리전 중 하나의 계정에서 EC2-Classic을 사용하도록 설정한 경우에만 이 변경의 영향을 받습니다. 콘솔 또는 describe-account-attributes 명령을 사용하여 AWS 리전에 대해 EC2-Classic을 사용하도록 설정했는지 여부를 확인할 수 있습니다. 자세한 내용은 이 문서를 참조하세요.리전에서 EC2-Classic에 실행 중인 활성 AWS 리소스가 없는 경우 해당 리전에 대해 계정에서 EC2-Classic을 끄십시오. 리전에서 EC2-Classic을 끄면 해당 리전에서 기본 VPC를 시작할 수 있습니다. 이렇게 하려면 AWS Support Center(console.aws.amazon.com/support)로 이동하고 ‘사례 생성’을 선택한 다음 ‘유형’에서 ‘계정 및 결제 지원’, ‘범주’에서 ‘계정’을 선택한 다음 ‘EC2 Classic에서 VPC로 변환’을 선택하고 필요한 기타 세부 정보를 입력하고 ‘제출’을 선택합니다.2021년 1월 1일 이후 EC2-Classic에 AWS 리소스(EC2 인스턴스, Amazon Relational Database, AWS Elastic Beanstalk, Amazon Redshift, AWS Data Pipeline, Amazon EMR, AWS OpsWorks)가 없는 모든 AWS 리전에 대해 2021년 10월 30일에 사용자의 계정에서 EC2-Classic을 자동으로 끕니다.반면 EC2-Classic에서 실행 중인 AWS 리소스가 있는 경우에는 가능한 신속하게 Amazon VPC로의 마이그레이션을 계획해야 합니다. 2022년 8월 15일 이후에는 EC2-Classic 플랫폼에서 인스턴스 또는 AWS 서비스를 시작할 수 없습니다. 실행 중 상태의 모든 워크로드 또는 서비스는 2022년 8월 16일부터 EC2-Classic이 사용 중지되므로 EC2-Classic의 모든 AWS 서비스에 대한 액세스가 점차 손실됩니다.\n",
            "후속 질문에서 AWS 리소스의 마이그레이션 가이드를 확인할 수 있습니다.\"\n",
            "EC2-Classic에서 Amazon VPC로 이동하면 어떤 이점이 있습니까?\n",
            "Amazon VPC는 AWS 계정과 논리적으로 격리되어 있어 AWS의 가상 네트워크 환경에 대한 완벽한 제어가 가능합니다. EC2-Classic 환경에서는 워크로드에서 다른 고객과 단일 플랫 네트워크를 공유합니다. Amazon VPC 환경에서는 EC2-Classic 환경과 비교해 자체 IP 주소 공간을 선택할 수 있는 기능, 퍼블릭 및 프라이빗 서브넷 구성, 라우팅 테이블 및 네트워크 게이트웨이 관리 등과 같은 여러 다른 이점을 제공합니다. 현재 EC2-Classic에서 사용할 수 있는 모든 서비스 및 인스턴스에 해당하는 서비스를 Amazon VPC 환경에서 사용할 수 있습니다. 또한 Amazon VPC에서는 EC2-Classic보다 다양한 최신의 인스턴스를 제공합니다. Amazon VPC에 대한 자세한 내용은 이 링크를 참조하세요.\n",
            "\"category : VPC, question : EC2-Classic에서 Amazon VPC로 이동하면 어떤 이점이 있습니까?, answer : Amazon VPC는 AWS 계정과 논리적으로 격리되어 있어 AWS의 가상 네트워크 환경에 대한 완벽한 제어가 가능합니다. EC2-Classic 환경에서는 워크로드에서 다른 고객과 단일 플랫 네트워크를 공유합니다. Amazon VPC 환경에서는 EC2-Classic 환경과 비교해 자체 IP 주소 공간을 선택할 수 있는 기능, 퍼블릭 및 프라이빗 서브넷 구성, 라우팅 테이블 및 네트워크 게이트웨이 관리 등과 같은 여러 다른 이점을 제공합니다. 현재 EC2-Classic에서 사용할 수 있는 모든 서비스 및 인스턴스에 해당하는 서비스를 Amazon VPC 환경에서 사용할 수 있습니다. 또한 Amazon VPC에서는 EC2-Classic보다 다양한 최신의 인스턴스를 제공합니다. Amazon VPC에 대한 자세한 내용은 이 링크를 참조하세요.\"\n",
            "EC2-Classic에서 VPC로 마이그레이션하려면 어떻게 해야 하나요?\n",
            "리소스 마이그레이션에 도움을 주기 위해 아래에서 찾을 수 있는 플레이북과 빌드 솔루션을 게시했습니다. 마이그레이션하려면 VPC에서 EC2-Classic 리소스를 다시 생성해야 합니다. 먼저 이 스크립트를 사용하여 계정의 모든 리전에서 EC2-Classic에 프로비저닝된 모든 리소스를 식별할 수 있습니다. 그런 다음 아래에서 관련 AWS 리소스의 마이그레이션 가이드를 사용할 수 있습니다.\n",
            "\n",
            "Instances and Security Groups\n",
            "Classic Load Balancer\n",
            "Amazon Relational Database Service\n",
            "AWS Elastic Beanstalk\n",
            "Amazon Redshift(DC1 클러스터 마이그레이션용 및 기타 노드 유형 마이그레이션용)\n",
            "AWS Data Pipeline \n",
            "Amazon EMR \n",
            "AWS OpsWorks\n",
            "\n",
            "위의 마이그레이션 가이드와 더불어 애플리케이션 마이그레이션을 간소화, 가속화 및 비용을 절감하는 리프트 앤 시프트(리호스팅) 솔루션인 AWS Application Migration Service(AWS MGN)도 제공합니다. AWS MGN에 대한 관련 리소스는 다음을 참조하십시오.\n",
            "\n",
            "AWS Application Migration Service 시작하기 \n",
            "AWS Application Migration Service 온디맨드 기술 교육\n",
            "AWS Application Migration Service 특징 및 기능을 자세히 살펴보기 위한 설명서\n",
            "서비스 아키텍처 및 네트워크 아키텍처 동영상\n",
            "\n",
            "EC2-Classic에서 VPC로의 단순한 개별 EC2 인스턴스 마이그레이션의 경우 AWS MGN 또는 인스턴스 마이그레이션 가이드 외에 ”AWS Systems Manager > 자동화“에서 “AWSSupport-MigrateEC2 ClassicToVPC“ 런북도 사용할 수 있습니다. 이 런북은 EC2-Classic에서 인스턴스의 AMI를 생성하고, VPC에서 AMI의 새 인스턴스를 생성하고, 선택적으로 EC2-Classic 인스턴스를 종료하여 EC2-Classic에서 VPC로 인스턴스를 마이그레이션하기 위해 필요한 단계를 자동화합니다.\n",
            "질문이 있는 경우 AWS Premium Support를 통해 AWS Support 팀에 문의할 수 있습니다.참고: 여러 AWS 리전에서 EC2-Classic에 실행 중인 AWS 리소스가 있는 경우에는 해당 리전에서 모든 리소스를 VPC로 마이그레이션한 직후 각 리전에서 EC2-Classic을 끄는 것이 좋습니다.\n",
            "\"category : VPC, question : EC2-Classic에서 VPC로 마이그레이션하려면 어떻게 해야 하나요?, answer : 리소스 마이그레이션에 도움을 주기 위해 아래에서 찾을 수 있는 플레이북과 빌드 솔루션을 게시했습니다. 마이그레이션하려면 VPC에서 EC2-Classic 리소스를 다시 생성해야 합니다. 먼저 이 스크립트를 사용하여 계정의 모든 리전에서 EC2-Classic에 프로비저닝된 모든 리소스를 식별할 수 있습니다. 그런 다음 아래에서 관련 AWS 리소스의 마이그레이션 가이드를 사용할 수 있습니다.\n",
            "\n",
            "Instances and Security Groups\n",
            "Classic Load Balancer\n",
            "Amazon Relational Database Service\n",
            "AWS Elastic Beanstalk\n",
            "Amazon Redshift(DC1 클러스터 마이그레이션용 및 기타 노드 유형 마이그레이션용)\n",
            "AWS Data Pipeline \n",
            "Amazon EMR \n",
            "AWS OpsWorks\n",
            "\n",
            "위의 마이그레이션 가이드와 더불어 애플리케이션 마이그레이션을 간소화, 가속화 및 비용을 절감하는 리프트 앤 시프트(리호스팅) 솔루션인 AWS Application Migration Service(AWS MGN)도 제공합니다. AWS MGN에 대한 관련 리소스는 다음을 참조하십시오.\n",
            "\n",
            "AWS Application Migration Service 시작하기 \n",
            "AWS Application Migration Service 온디맨드 기술 교육\n",
            "AWS Application Migration Service 특징 및 기능을 자세히 살펴보기 위한 설명서\n",
            "서비스 아키텍처 및 네트워크 아키텍처 동영상\n",
            "\n",
            "EC2-Classic에서 VPC로의 단순한 개별 EC2 인스턴스 마이그레이션의 경우 AWS MGN 또는 인스턴스 마이그레이션 가이드 외에 ”AWS Systems Manager > 자동화“에서 “AWSSupport-MigrateEC2 ClassicToVPC“ 런북도 사용할 수 있습니다. 이 런북은 EC2-Classic에서 인스턴스의 AMI를 생성하고, VPC에서 AMI의 새 인스턴스를 생성하고, 선택적으로 EC2-Classic 인스턴스를 종료하여 EC2-Classic에서 VPC로 인스턴스를 마이그레이션하기 위해 필요한 단계를 자동화합니다.\n",
            "질문이 있는 경우 AWS Premium Support를 통해 AWS Support 팀에 문의할 수 있습니다.참고: 여러 AWS 리전에서 EC2-Classic에 실행 중인 AWS 리소스가 있는 경우에는 해당 리전에서 모든 리소스를 VPC로 마이그레이션한 직후 각 리전에서 EC2-Classic을 끄는 것이 좋습니다.\"\n",
            "알고 있어야 하는 중요한 날짜는 언제인가요?\n",
            "AWS에서는 2022년 8월 15일 사용 중지 날짜 전에 다음 두 가지 작업을 수행합니다.\n",
            "\n",
            "2021년 10월 30일에 EC2-Classic 환경에 대한 3년 예약 인스턴스(RI) 및 1년 RI 실행을 중지합니다. 이미 EC2-Classic 환경에 있는 RI는 이때 영향을 받지 않습니다. 2022년 8월 15일 후 만료되도록 설정된 RI는 나머지 임대 기간 동안 Amazon VPC 환경을 사용하도록 수정해야 합니다. RI를 수정하는 방법에 대한 자세한 내용은 AWS 문서를 참조하세요.\n",
            "2022년 8월 15일부터 EC2-Classic 환경에서 새 인스턴스(Spot 또는 온디맨드) 생성이 더 이상 허용되지 않습니다. 실행 중 상태의 모든 워크로드 또는 서비스는 2022년 8월 16일부터 EC2-Classic이 사용 중지되므로 EC2-Classic의 모든 AWS 서비스에 대한 액세스가 점차 손실됩니다.\n",
            "\"category : VPC, question : 알고 있어야 하는 중요한 날짜는 언제인가요?, answer : AWS에서는 2022년 8월 15일 사용 중지 날짜 전에 다음 두 가지 작업을 수행합니다.\n",
            "\n",
            "2021년 10월 30일에 EC2-Classic 환경에 대한 3년 예약 인스턴스(RI) 및 1년 RI 실행을 중지합니다. 이미 EC2-Classic 환경에 있는 RI는 이때 영향을 받지 않습니다. 2022년 8월 15일 후 만료되도록 설정된 RI는 나머지 임대 기간 동안 Amazon VPC 환경을 사용하도록 수정해야 합니다. RI를 수정하는 방법에 대한 자세한 내용은 AWS 문서를 참조하세요.\n",
            "2022년 8월 15일부터 EC2-Classic 환경에서 새 인스턴스(Spot 또는 온디맨드) 생성이 더 이상 허용되지 않습니다. 실행 중 상태의 모든 워크로드 또는 서비스는 2022년 8월 16일부터 EC2-Classic이 사용 중지되므로 EC2-Classic의 모든 AWS 서비스에 대한 액세스가 점차 손실됩니다.\"\n",
            "실행 중인 EC2 인스턴스에 하나 이상의 네트워크 인터페이스를 연결하거나 분리할 수 있나요?\n",
            "예.\n",
            "\"category : VPC, question : 실행 중인 EC2 인스턴스에 하나 이상의 네트워크 인터페이스를 연결하거나 분리할 수 있나요?, answer : 예.\"\n",
            "특정 가용 영역에 있는 네트워크 인터페이스를 다른 가용 영역에 있는 인스턴스와 연결할 수 있나요?\n",
            "네트워크 인터페이스는 동일한 가용 영역에 상주하는 인스턴스에만 연결할 수 있습니다.\n",
            "\"category : VPC, question : 특정 가용 영역에 있는 네트워크 인터페이스를 다른 가용 영역에 있는 인스턴스와 연결할 수 있나요?, answer : 네트워크 인터페이스는 동일한 가용 영역에 상주하는 인스턴스에만 연결할 수 있습니다.\"\n",
            "특정 VPC에 있는 네트워크 인터페이스를 다른 VPC에 있는 인스턴스와 연결할 수 있나요?\n",
            "네트워크 인터페이스는 동일한 VPC 내에 있는 인스턴스에만 인터페이스로서 연결할 수 있습니다.\n",
            "\"category : VPC, question : 특정 VPC에 있는 네트워크 인터페이스를 다른 VPC에 있는 인스턴스와 연결할 수 있나요?, answer : 네트워크 인터페이스는 동일한 VPC 내에 있는 인스턴스에만 인터페이스로서 연결할 수 있습니다.\"\n",
            "단일 인스턴스에서 개별 IP 주소를 필요로 하는 여러 웹사이트를 호스팅하는 방법으로 탄력적 네트워크 인터페이스를 사용할 수 있나요?\n",
            "예. 하지만 인터페이스가 여러 개인 경우에는 가장 좋은 방법은 아닙니다. 대신 인스턴스에 프라이빗 IP 주소를 추가로 할당하고 필요에 따라 프라이빗 IP에 EIP를 연결합니다.\n",
            "\"category : VPC, question : 단일 인스턴스에서 개별 IP 주소를 필요로 하는 여러 웹사이트를 호스팅하는 방법으로 탄력적 네트워크 인터페이스를 사용할 수 있나요?, answer : 예. 하지만 인터페이스가 여러 개인 경우에는 가장 좋은 방법은 아닙니다. 대신 인스턴스에 프라이빗 IP 주소를 추가로 할당하고 필요에 따라 프라이빗 IP에 EIP를 연결합니다.\"\n",
            "EC2 인스턴스의 기본 인터페이스(eth0)를 분리할 수 있나요?\n",
            "EC2 인스턴스에 보조 인터페이스(eth1-ethn)를 연결하고 분리할 수는 있으나, eth0 인터페이스를 분리할 수는 없습니다.\n",
            "\"category : VPC, question : EC2 인스턴스의 기본 인터페이스(eth0)를 분리할 수 있나요?, answer : EC2 인스턴스에 보조 인터페이스(eth1-ethn)를 연결하고 분리할 수는 있으나, eth0 인터페이스를 분리할 수는 없습니다.\"\n",
            "다른 리전의 VPC에 대한 피어링 연결을 생성할 수 있나요?\n",
            "예. 다른 리전에서 VPC를 사용하여 피어링 연결을 생성할 수 있습니다. 리전 간 VPC 피어링은 전 세계 모든 상용 리전(중국 제외)에서 사용할 수 있습니다.\n",
            "\"category : VPC, question : 다른 리전의 VPC에 대한 피어링 연결을 생성할 수 있나요?, answer : 예. 다른 리전에서 VPC를 사용하여 피어링 연결을 생성할 수 있습니다. 리전 간 VPC 피어링은 전 세계 모든 상용 리전(중국 제외)에서 사용할 수 있습니다.\"\n",
            "내 VPC를 다른 AWS 계정에 속한 VPC와 피어링할 수 있나요?\n",
            "예. 다른 VPC의 소유자가 피어링 연결 요청을 허용한 경우 가능합니다.\n",
            "\"category : VPC, question : 내 VPC를 다른 AWS 계정에 속한 VPC와 피어링할 수 있나요?, answer : 예. 다른 VPC의 소유자가 피어링 연결 요청을 허용한 경우 가능합니다.\"\n",
            "VPC 피어링 연결 비용은 어떻게 되나요?\n",
            "VPC 피어링 연결 생성에 대한 비용은 없지만, 피어링 연결을 통한 데이터 전송 비용이 청구됩니다. 데이터 전송 요금은 EC2 요금 페이지의 데이터 전송 섹션을 참조하세요.\n",
            "\"category : VPC, question : VPC 피어링 연결 비용은 어떻게 되나요?, answer : VPC 피어링 연결 생성에 대한 비용은 없지만, 피어링 연결을 통한 데이터 전송 비용이 청구됩니다. 데이터 전송 요금은 EC2 요금 페이지의 데이터 전송 섹션을 참조하세요.\"\n",
            "피어링 연결을 사용하려면 인터넷 게이트웨이가 필요하나요?\n",
            "VPC 피어링 연결에는 인터넷 게이트웨이가 필요하지 않습니다.\n",
            "\"category : VPC, question : 피어링 연결을 사용하려면 인터넷 게이트웨이가 필요하나요?, answer : VPC 피어링 연결에는 인터넷 게이트웨이가 필요하지 않습니다.\"\n",
            "리전 내 VPC 피어링 트래픽은 암호화되나요?\n",
            "피어링된 VPC 내 인스턴스 간 트래픽은 프라이빗 상태를 유지하며 격리됩니다. 이는 동일한 VPC 내 두 인스턴스 간 트래픽이 프라이빗 상태를 유지하며 격리되는 것과 유사합니다.\n",
            "\"category : VPC, question : 리전 내 VPC 피어링 트래픽은 암호화되나요?, answer : 피어링된 VPC 내 인스턴스 간 트래픽은 프라이빗 상태를 유지하며 격리됩니다. 이는 동일한 VPC 내 두 인스턴스 간 트래픽이 프라이빗 상태를 유지하며 격리되는 것과 유사합니다.\"\n",
            "내 쪽의 피어링 연결을 삭제해도 다른 쪽에서는 계속 내 VPC에 대한 액세스 권한을 보유하나요?\n",
            "피어링 연결된 양측 모두 언제든지 피어링 연결을 종료할 수 있습니다. 피어링 연결을 종료하면 두 VPC 간 트래픽을 주고받지 않게 됩니다.\n",
            "\"category : VPC, question : 내 쪽의 피어링 연결을 삭제해도 다른 쪽에서는 계속 내 VPC에 대한 액세스 권한을 보유하나요?, answer : 피어링 연결된 양측 모두 언제든지 피어링 연결을 종료할 수 있습니다. 피어링 연결을 종료하면 두 VPC 간 트래픽을 주고받지 않게 됩니다.\"\n",
            "VPC A를 VPC B에 피어링하고 VPC B를 VPC C에 피어링하는 경우 VPC A와 VPC C도 피어링되나요?\n",
            "전이성 피어링 관계는 지원되지 않습니다.\n",
            "\"category : VPC, question : VPC A를 VPC B에 피어링하고 VPC B를 VPC C에 피어링하는 경우 VPC A와 VPC C도 피어링되나요?, answer : 전이성 피어링 관계는 지원되지 않습니다.\"\n",
            "피어링 연결이 중단되면 어떻게 되나요?\n",
            "AWS는 VPC의 기존 인프라를 사용하여 VPC 피어링 연결을 생성합니다. 이는 게이트웨이도, VPN 연결도 아니며 개별적인 물리적 하드웨어에 의존하지 않습니다. 그러므로 통신 또는 대역폭 병목에 대한 단일 지점 장애가 없습니다.\n",
            "리전 간 VPC 피어링은 현재 VPC를 지원하는 것과 동일한 수평적으로 확장되고 중복적이며 가용성이 높은 기술에서 운영됩니다. 리전 간 VPC 피어링 트래픽은 내장된 중복성 및 동적 대역폭 할당 기능을 지원하는 AWS 백본으로 이동합니다. 통신에 대한 단일 실패 지점이 없습니다.\n",
            "Inter-Region Peering 연결이 끊어지는 경우 트래픽이 인터넷을 통해 라우팅되지 않습니다.\n",
            "\"category : VPC, question : 피어링 연결이 중단되면 어떻게 되나요?, answer : AWS는 VPC의 기존 인프라를 사용하여 VPC 피어링 연결을 생성합니다. 이는 게이트웨이도, VPN 연결도 아니며 개별적인 물리적 하드웨어에 의존하지 않습니다. 그러므로 통신 또는 대역폭 병목에 대한 단일 지점 장애가 없습니다.\n",
            "리전 간 VPC 피어링은 현재 VPC를 지원하는 것과 동일한 수평적으로 확장되고 중복적이며 가용성이 높은 기술에서 운영됩니다. 리전 간 VPC 피어링 트래픽은 내장된 중복성 및 동적 대역폭 할당 기능을 지원하는 AWS 백본으로 이동합니다. 통신에 대한 단일 실패 지점이 없습니다.\n",
            "Inter-Region Peering 연결이 끊어지는 경우 트래픽이 인터넷을 통해 라우팅되지 않습니다.\"\n",
            "피어링 연결에 대한 대역폭 제한이 있나요?\n",
            "피어링되는 VPC 내 인스턴스 간 대역폭은 동일한 VPC 내 인스턴스 간 대역폭과 같습니다. 참고: 배치 그룹은 피어링된 여러 VPC를 포괄할 수 있지만, 피어링된 VPC의 인스턴스 간에는 양방향 대역폭이 지원되지 않습니다. 배치 그룹에 대한 자세한 내용을 읽어보세요.\n",
            "\"category : VPC, question : 피어링 연결에 대한 대역폭 제한이 있나요?, answer : 피어링되는 VPC 내 인스턴스 간 대역폭은 동일한 VPC 내 인스턴스 간 대역폭과 같습니다. 참고: 배치 그룹은 피어링된 여러 VPC를 포괄할 수 있지만, 피어링된 VPC의 인스턴스 간에는 양방향 대역폭이 지원되지 않습니다. 배치 그룹에 대한 자세한 내용을 읽어보세요.\"\n",
            "리전 간 VPC 피어링 트래픽은 암호화되나요?\n",
            "트래픽은 최신 AEAD(Authenticated Encryption with Associated Data) 알고리즘을 사용하여 암호화됩니다. AWS에서 키 계약 및 관리를 처리합니다.\n",
            "\"category : VPC, question : 리전 간 VPC 피어링 트래픽은 암호화되나요?, answer : 트래픽은 최신 AEAD(Authenticated Encryption with Associated Data) 알고리즘을 사용하여 암호화됩니다. AWS에서 키 계약 및 관리를 처리합니다.\"\n",
            "DNS 변환은 리전 간 VPC 피어링과 어떻게 연동되나요?\n",
            "기본적으로 다른 리전의 피어링된 VPC에서 인스턴스에 대한 퍼블릭 호스트 이름 쿼리는 퍼블릭 IP 주소로 확인됩니다. 53 프라이빗 DNS 라우팅은 Inter-Region VPC Peering을 사용한 프라이빗 IP 주소를 해결하는 데 사용할 수 있습니다.\n",
            "\"category : VPC, question : DNS 변환은 리전 간 VPC 피어링과 어떻게 연동되나요?, answer : 기본적으로 다른 리전의 피어링된 VPC에서 인스턴스에 대한 퍼블릭 호스트 이름 쿼리는 퍼블릭 IP 주소로 확인됩니다. 53 프라이빗 DNS 라우팅은 Inter-Region VPC Peering을 사용한 프라이빗 IP 주소를 해결하는 데 사용할 수 있습니다.\"\n",
            "리전 간 VPC 피어링 연결 사이에서 보안 그룹을 참조할 수 있나요?\n",
            "리전 간 VPC 피어링 연결 사이에서 보안 그룹을 참조할 수 없습니다.\n",
            "\"category : VPC, question : 리전 간 VPC 피어링 연결 사이에서 보안 그룹을 참조할 수 있나요?, answer : 리전 간 VPC 피어링 연결 사이에서 보안 그룹을 참조할 수 없습니다.\"\n",
            "리전 간 VPC 피어링은 IPv6를 지원하나요?\n",
            "예. 리전 간 VPC 피어링은 IPv6를 지원합니다.\n",
            "\"category : VPC, question : 리전 간 VPC 피어링은 IPv6를 지원하나요?, answer : 예. 리전 간 VPC 피어링은 IPv6를 지원합니다.\"\n",
            "리전 간 VPC 피어링을 EC2-Classic Link와 함께 사용할 수 있나요?\n",
            "리전 간 VPC 피어링은 EC2-ClassicLink와 함께 사용할 수 없습니다.\n",
            "\"category : VPC, question : 리전 간 VPC 피어링을 EC2-Classic Link와 함께 사용할 수 있나요?, answer : 리전 간 VPC 피어링은 EC2-ClassicLink와 함께 사용할 수 없습니다.\"\n",
            "ClassicLink란 무엇인가요?\n",
            "Amazon Virtual Private Cloud(VPC) ClassicLink는 EC2-Classic 플랫폼의 EC2 인스턴스가 프라이빗 IP 주소를 사용해 VPC의 인스턴스와 통신할 수 있도록 지원합니다. ClassicLink를 사용하려면 사용자 계정에서 VPC에 ClassicLink를 활성화한 후 VPC의 보안 그룹과 EC2-Classic의 인스턴스를 연결하십시오. VPC 보안 그룹의 모든 규칙은 EC2-Classic의 인스턴스와 VPC의 인스턴스 간 통신에 적용됩니다.\n",
            "\"category : VPC, question : ClassicLink란 무엇인가요?, answer : Amazon Virtual Private Cloud(VPC) ClassicLink는 EC2-Classic 플랫폼의 EC2 인스턴스가 프라이빗 IP 주소를 사용해 VPC의 인스턴스와 통신할 수 있도록 지원합니다. ClassicLink를 사용하려면 사용자 계정에서 VPC에 ClassicLink를 활성화한 후 VPC의 보안 그룹과 EC2-Classic의 인스턴스를 연결하십시오. VPC 보안 그룹의 모든 규칙은 EC2-Classic의 인스턴스와 VPC의 인스턴스 간 통신에 적용됩니다.\"\n",
            "ClassicLink 요금은 어떻게 되나요?\n",
            "ClassicLink를 사용할 경우 추가 요금이 부과되지 않으며, 기존 가용 영역 간 데이터 전송 요금이 적용됩니다. 자세한 내용은 EC2 요금 페이지를 참조하세요.\n",
            "\"category : VPC, question : ClassicLink 요금은 어떻게 되나요?, answer : ClassicLink를 사용할 경우 추가 요금이 부과되지 않으며, 기존 가용 영역 간 데이터 전송 요금이 적용됩니다. 자세한 내용은 EC2 요금 페이지를 참조하세요.\"\n",
            "ClassicLink를 사용하려면 어떻게 해야 하나요?\n",
            "ClassicLink를 사용하려면 먼저 사용자 계정에 있는 하나 이상의 VPC에서 ClassicLink를 활성화해야 합니다. 그런 다음 VPC의 보안 그룹과 원하는 EC2-Classic 인스턴스를 연결합니다. EC2-Classic 인스턴스가 현재 VPC에 연결되고 VPC에서 선택한 보안 그룹의 멤버가 됩니다. EC2-Classic 인스턴스를 둘 이상의 VPC와 동시에 연결할 수 없습니다.\n",
            "\"category : VPC, question : ClassicLink를 사용하려면 어떻게 해야 하나요?, answer : ClassicLink를 사용하려면 먼저 사용자 계정에 있는 하나 이상의 VPC에서 ClassicLink를 활성화해야 합니다. 그런 다음 VPC의 보안 그룹과 원하는 EC2-Classic 인스턴스를 연결합니다. EC2-Classic 인스턴스가 현재 VPC에 연결되고 VPC에서 선택한 보안 그룹의 멤버가 됩니다. EC2-Classic 인스턴스를 둘 이상의 VPC와 동시에 연결할 수 없습니다.\"\n",
            "EC2-Classic 인스턴스가 VPC 멤버가 되나요?\n",
            "EC2-Classic 인스턴스는 VPC 멤버가 되지 않습니다. 해당 인스턴스와 연결된 VPC 보안 그룹의 멤버가 됩니다. VPC 보안 그룹에 대한 참조와 모든 규칙은 EC2-Classic 인스턴스의 인스턴스와 VPC 내 리소스 간 통신에 적용됩니다.\n",
            "\"category : VPC, question : EC2-Classic 인스턴스가 VPC 멤버가 되나요?, answer : EC2-Classic 인스턴스는 VPC 멤버가 되지 않습니다. 해당 인스턴스와 연결된 VPC 보안 그룹의 멤버가 됩니다. VPC 보안 그룹에 대한 참조와 모든 규칙은 EC2-Classic 인스턴스의 인스턴스와 VPC 내 리소스 간 통신에 적용됩니다.\"\n",
            "프라이빗 IP를 사용하여 통신하기 위해 내 EC2-Classic과 EC2-VPC 인스턴스 간에 서로 통신하는 데 EC2 퍼블릭 DNS 호스트 이름을 사용할 수 있나요?\n",
            "아니요. EC2-Classic 인스턴스에서 쿼리할 때 EC2 퍼블릭 DNS 호스트 이름으로는 EC2-VPC 인스턴스의 프라이빗 IP 주소를 확인할 수 없습니다. 반대의 경우도 마찬가지입니다.\n",
            "\"category : VPC, question : 프라이빗 IP를 사용하여 통신하기 위해 내 EC2-Classic과 EC2-VPC 인스턴스 간에 서로 통신하는 데 EC2 퍼블릭 DNS 호스트 이름을 사용할 수 있나요?, answer : 아니요. EC2-Classic 인스턴스에서 쿼리할 때 EC2 퍼블릭 DNS 호스트 이름으로는 EC2-VPC 인스턴스의 프라이빗 IP 주소를 확인할 수 없습니다. 반대의 경우도 마찬가지입니다.\"\n",
            "ClassicLink를 활성화할 수 없는 VPC가 있나요?\n",
            "예. VPC의 CIDR(Classless Inter-Domain Routing)이 10.0.0.0/8 범위(10.0.0.0/16 및 10.1.0.0/16 제외)에 속하는 경우 ClassicLink를 활성화할 수 없습니다. 또한, 10.0.0.0/8 CIDR 공간에서 '로컬' 외 대상을 가리키는 라우팅 테이블이 있는 VPC에 대해 ClassicLink를 활성화할 수 없습니다.\n",
            "\"category : VPC, question : ClassicLink를 활성화할 수 없는 VPC가 있나요?, answer : 예. VPC의 CIDR(Classless Inter-Domain Routing)이 10.0.0.0/8 범위(10.0.0.0/16 및 10.1.0.0/16 제외)에 속하는 경우 ClassicLink를 활성화할 수 없습니다. 또한, 10.0.0.0/8 CIDR 공간에서 '로컬' 외 대상을 가리키는 라우팅 테이블이 있는 VPC에 대해 ClassicLink를 활성화할 수 없습니다.\"\n",
            "EC2-Classic 인스턴스의 트래픽이 Amazon VPC를 통과하여 인터넷 게이트웨이, 가상 프라이빗 게이트웨이 또는 피어링된 VPC로 전송될 수 있나요?\n",
            "EC2-Classic 인스턴스의 트래픽은 VPC 내 프라이빗 IP 주소로만 라우팅될 수 있습니다. 인터넷 게이트웨이, 가상 프라이빗 게이트웨이 또는 피어링된 VPC 대상을 비롯하여 VPC 외부에 있는 대상으로는 라우팅되지 않습니다.\n",
            "\"category : VPC, question : EC2-Classic 인스턴스의 트래픽이 Amazon VPC를 통과하여 인터넷 게이트웨이, 가상 프라이빗 게이트웨이 또는 피어링된 VPC로 전송될 수 있나요?, answer : EC2-Classic 인스턴스의 트래픽은 VPC 내 프라이빗 IP 주소로만 라우팅될 수 있습니다. 인터넷 게이트웨이, 가상 프라이빗 게이트웨이 또는 피어링된 VPC 대상을 비롯하여 VPC 외부에 있는 대상으로는 라우팅되지 않습니다.\"\n",
            "ClassicLink가 EC2-Classic 인스턴스와 EC2-Classic 플랫폼에 있는 다른 인스턴스 간의 액세스 제어에 영향을 미치나요?\n",
            "ClassicLink는 EC2-Classic 플랫폼의 기존 보안 그룹을 통해 EC2-Classic 인스턴스에 대해 정의된 액세스 제어를 변경하지 않습니다.\n",
            "\"category : VPC, question : ClassicLink가 EC2-Classic 인스턴스와 EC2-Classic 플랫폼에 있는 다른 인스턴스 간의 액세스 제어에 영향을 미치나요?, answer : ClassicLink는 EC2-Classic 플랫폼의 기존 보안 그룹을 통해 EC2-Classic 인스턴스에 대해 정의된 액세스 제어를 변경하지 않습니다.\"\n",
            "내 EC2-Classic 인스턴스의 ClassicLink 설정은 중지/시작 주기 동안 지속되나요?\n",
            "ClassicLink 연결은 EC2-Classic 인스턴스의 중지/시작 주기 동안 지속되지 않습니다. EC2-Classic 인스턴스를 중지하고 시작한 후 다시 VPC에 연결해야 합니다. 그렇지만 인스턴스 재부팅 주기 동안 ClassicLink 연결이 지속됩니다.\n",
            "\"category : VPC, question : 내 EC2-Classic 인스턴스의 ClassicLink 설정은 중지/시작 주기 동안 지속되나요?, answer : ClassicLink 연결은 EC2-Classic 인스턴스의 중지/시작 주기 동안 지속되지 않습니다. EC2-Classic 인스턴스를 중지하고 시작한 후 다시 VPC에 연결해야 합니다. 그렇지만 인스턴스 재부팅 주기 동안 ClassicLink 연결이 지속됩니다.\"\n",
            "ClassicLink를 사용하여 EC2-Classic 보안 그룹 규칙에 VPC 보안 그룹을 참조하거나 그 반대의 경우가 가능한가요?\n",
            "ClassicLink를 사용하여 EC2-Classic 보안 그룹 규칙에 VPC 보안 그룹을 참조할 수 없습니다. 반대의 경우도 같습니다.\n",
            "\"category : VPC, question : ClassicLink를 사용하여 EC2-Classic 보안 그룹 규칙에 VPC 보안 그룹을 참조하거나 그 반대의 경우가 가능한가요?, answer : ClassicLink를 사용하여 EC2-Classic 보안 그룹 규칙에 VPC 보안 그룹을 참조할 수 없습니다. 반대의 경우도 같습니다.\"\n",
            "AWS PrivateLink란 무엇인가요?\n",
            "AWS PrivateLink는 모든 네트워크 트래픽을 AWS 네트워크 내부에 유지하면서 고객이 높은 가용성과 확장성 방식으로 AWS에 호스팅된 서비스에 액세스할 수 있도록 합니다. 고객은 이 기능을 통해 퍼블릭 IP나 인터넷 연결 트래픽 없이도 자신의 온프레미스 또는 Amazon Virtual Private Cloud(VPC)에서 PrivateLink로 구동되는 서비스에 비공개로 액세스할 수 있습니다. 서비스 소유자는 자신의 Network Load Balancers를 PrivateLink 서비스에 등록하고 다른 AWS 고객에게 서비스를 제공할 수 있습니다.\n",
            "\"category : VPC, question : AWS PrivateLink란 무엇인가요?, answer : AWS PrivateLink는 모든 네트워크 트래픽을 AWS 네트워크 내부에 유지하면서 고객이 높은 가용성과 확장성 방식으로 AWS에 호스팅된 서비스에 액세스할 수 있도록 합니다. 고객은 이 기능을 통해 퍼블릭 IP나 인터넷 연결 트래픽 없이도 자신의 온프레미스 또는 Amazon Virtual Private Cloud(VPC)에서 PrivateLink로 구동되는 서비스에 비공개로 액세스할 수 있습니다. 서비스 소유자는 자신의 Network Load Balancers를 PrivateLink 서비스에 등록하고 다른 AWS 고객에게 서비스를 제공할 수 있습니다.\"\n",
            "AWS PrivateLink를 사용하려면 어떻게 해야 하나요?\n",
            "서비스 사용자는 PrivateLink로 구동되는 서비스를 위해 인터페이스 유형 VPC 엔드포인트를 생성해야 합니다. 이러한 서비스 엔드포인트는 VPC 내 프라이빗 IP가 연결된 ENI(Elastic Network Interface)로 표시됩니다. 이러한 엔드포인트가 생성되면, 이 IP로 향하는 모든 트래픽이 비공개로 해당 AWS 서비스로 라우팅됩니다.\n",
            "서비스 사용자는 서비스 앞단에 Network Load Balancer(NLB)를 설정하여 AWS PrivateLink에 서비스를 탑재할 수 있으며 NLB PrivateLink 서비스를 생성하여 NLB에 등록할 수 있습니다. 고객은 자신의 계정 및 IAM 역할을 허용한 후에 VPC 내에 엔드포인트를 설정하여 서비스에 연결할 수 있습니다.\n",
            "\"category : VPC, question : AWS PrivateLink를 사용하려면 어떻게 해야 하나요?, answer : 서비스 사용자는 PrivateLink로 구동되는 서비스를 위해 인터페이스 유형 VPC 엔드포인트를 생성해야 합니다. 이러한 서비스 엔드포인트는 VPC 내 프라이빗 IP가 연결된 ENI(Elastic Network Interface)로 표시됩니다. 이러한 엔드포인트가 생성되면, 이 IP로 향하는 모든 트래픽이 비공개로 해당 AWS 서비스로 라우팅됩니다.\n",
            "서비스 사용자는 서비스 앞단에 Network Load Balancer(NLB)를 설정하여 AWS PrivateLink에 서비스를 탑재할 수 있으며 NLB PrivateLink 서비스를 생성하여 NLB에 등록할 수 있습니다. 고객은 자신의 계정 및 IAM 역할을 허용한 후에 VPC 내에 엔드포인트를 설정하여 서비스에 연결할 수 있습니다.\"\n",
            "현재 AWS PrivateLink에서 사용할 수 있는 서비스는 무엇인가요?\n",
            "Amazon Elastic Compute Cloud(EC2), Elastic Load Balancing(ELB), Kinesis Streams, Service Catalog, EC2 Systems Manager, Amazon SNS 및 AWS DataSync와 같은 AWS 서비스가 이 기능을 지원합니다. 많은 SaaS 솔루션도 이 기능을 지원합니다. AWS Marketplace에서 AWS PrivateLink가 지원되는 SaaS 제품에 대해 더 자세히 알아볼 수 있습니다.\n",
            "\"category : VPC, question : 현재 AWS PrivateLink에서 사용할 수 있는 서비스는 무엇인가요?, answer : Amazon Elastic Compute Cloud(EC2), Elastic Load Balancing(ELB), Kinesis Streams, Service Catalog, EC2 Systems Manager, Amazon SNS 및 AWS DataSync와 같은 AWS 서비스가 이 기능을 지원합니다. 많은 SaaS 솔루션도 이 기능을 지원합니다. AWS Marketplace에서 AWS PrivateLink가 지원되는 SaaS 제품에 대해 더 자세히 알아볼 수 있습니다.\"\n",
            "AWS Direct Connect를 통해 AWS PrivateLink에서 지원하는 서비스에 비공개로 액세스할 수 있나요?\n",
            "예. 온프레미스에 있는 애플리케이션은 AWS Direct Connect를 통해 Amazon VPC에 있는 서비스 엔드포인트에 연결할 수 있습니다. 서비스 엔드포인트에서는 자동으로 AWS PrivateLink에서 지원하는 AWS 서비스로 트래픽을 보냅니다.\n",
            "\"category : VPC, question : AWS Direct Connect를 통해 AWS PrivateLink에서 지원하는 서비스에 비공개로 액세스할 수 있나요?, answer : 예. 온프레미스에 있는 애플리케이션은 AWS Direct Connect를 통해 Amazon VPC에 있는 서비스 엔드포인트에 연결할 수 있습니다. 서비스 엔드포인트에서는 자동으로 AWS PrivateLink에서 지원하는 AWS 서비스로 트래픽을 보냅니다.\"\n",
            "AWS Management Console을 사용하여 Amazon VPC를 제어하고 관리할 수 있나요?\n",
            "예. AWS Management Console을 사용하여 VPC, 서브넷, 라우팅 테이블, 인터넷 게이트웨이, IPSec VPN 연결 등의 Amazon VPC 객체를 관리할 수 있습니다. 또한 간편한 마법사를 사용하여 VPC를 생성할 수도 있습니다.\n",
            "\"category : VPC, question : AWS Management Console을 사용하여 Amazon VPC를 제어하고 관리할 수 있나요?, answer : 예. AWS Management Console을 사용하여 VPC, 서브넷, 라우팅 테이블, 인터넷 게이트웨이, IPSec VPN 연결 등의 Amazon VPC 객체를 관리할 수 있습니다. 또한 간편한 마법사를 사용하여 VPC를 생성할 수도 있습니다.\"\n",
            "얼마나 많은 VPC, 서브넷, 탄력적 IP 주소 및 인터넷 게이트웨이를 생성할 수 있나요?\n",
            "VPC 한도에 대한 내용은 Amazon VPC 사용 설명서를 참조하세요.\n",
            "\"category : VPC, question : 얼마나 많은 VPC, 서브넷, 탄력적 IP 주소 및 인터넷 게이트웨이를 생성할 수 있나요?, answer : VPC 한도에 대한 내용은 Amazon VPC 사용 설명서를 참조하세요.\"\n",
            "Amazon VPC에 대한 AWS Support를 받을 수 있나요?\n",
            "예. AWS Support에 대한 자세한 정보는 여기를 클릭하세요.\n",
            "\"category : VPC, question : Amazon VPC에 대한 AWS Support를 받을 수 있나요?, answer : 예. AWS Support에 대한 자세한 정보는 여기를 클릭하세요.\"\n",
            "Amazon VPC에서 ElasticFox를 사용할 수 있나요?\n",
            "ElasticFox는 공식적으로 더 이상 Amazon VPC를 관리하는 데 지원되지 않습니다. Amazon VPC 지원은 AWS API, 명령줄 도구, AWS Management Console 및 그 외 다양한 타사 유틸리티를 통해 사용할 수 있습니다.\n",
            "\"category : VPC, question : Amazon VPC에서 ElasticFox를 사용할 수 있나요?, answer : ElasticFox는 공식적으로 더 이상 Amazon VPC를 관리하는 데 지원되지 않습니다. Amazon VPC 지원은 AWS API, 명령줄 도구, AWS Management Console 및 그 외 다양한 타사 유틸리티를 통해 사용할 수 있습니다.\"\n",
            "========== RDS  :  https://aws.amazon.com/ko/rds/faqs/ 사이트 크롤링 진행중 ==========\n",
            "182\n",
            "Amazon RDS는 무엇입니까?\n",
            "Amazon Relational Database Service(RDS)는 클라우드에서 관계형 데이터베이스를 쉽게 설정, 운영 및 확장할 수 있는 관리형 서비스입니다. 이 서비스는 시간 소모적인 데이터베이스 관리 작업을 처리하는 한편 비용 효율적이고 크기를 조정할 수 있는 용량을 제공하므로, 고객은 애플리케이션과 비즈니스에 좀 더 집중할 수 있습니다.\n",
            "Amazon RDS를 사용하면 익숙한 RDS for PostgreSQL, RDS for MySQL, RDS for MariaDB, RDS for SQL Server, RDS for Oracle 또는 RDS for Db2 데이터베이스의 기능에 액세스할 수 있습니다. 즉, 기존의 데이터베이스에서 이미 사용하고 있는 코드, 애플리케이션 및 도구가 Amazon RDS에서 원활하게 작동합니다. Amazon RDS는 데이터베이스를 자동으로 백업하고 데이터베이스 소프트웨어를 최신 버전으로 유지할 수 있습니다. 관계형 데이터베이스 인스턴스와 관련된 컴퓨팅 리소스나 스토리지 용량을 확장할 수 있는 유연성의 이점을 누릴 수 있습니다. 또한, Amazon RDS는 복제를 사용하기 쉽게 하여 데이터베이스의 가용성을 향상하거나, 데이터의 내구성을 개선하거나, 읽기 중심의 데이터베이스 워크로드를 위해 단일 데이터베이스 인스턴스의 용량 제한 이상으로 규모를 확장할 수 있습니다. 모든 AWS 서비스와 마찬가지로 선불 투자가 필요하지 않으며 사용한 리소스에 대해서만 요금을 지불하면 됩니다.\n",
            "\"category : RDS, question : Amazon RDS는 무엇입니까?, answer : Amazon Relational Database Service(RDS)는 클라우드에서 관계형 데이터베이스를 쉽게 설정, 운영 및 확장할 수 있는 관리형 서비스입니다. 이 서비스는 시간 소모적인 데이터베이스 관리 작업을 처리하는 한편 비용 효율적이고 크기를 조정할 수 있는 용량을 제공하므로, 고객은 애플리케이션과 비즈니스에 좀 더 집중할 수 있습니다.\n",
            "Amazon RDS를 사용하면 익숙한 RDS for PostgreSQL, RDS for MySQL, RDS for MariaDB, RDS for SQL Server, RDS for Oracle 또는 RDS for Db2 데이터베이스의 기능에 액세스할 수 있습니다. 즉, 기존의 데이터베이스에서 이미 사용하고 있는 코드, 애플리케이션 및 도구가 Amazon RDS에서 원활하게 작동합니다. Amazon RDS는 데이터베이스를 자동으로 백업하고 데이터베이스 소프트웨어를 최신 버전으로 유지할 수 있습니다. 관계형 데이터베이스 인스턴스와 관련된 컴퓨팅 리소스나 스토리지 용량을 확장할 수 있는 유연성의 이점을 누릴 수 있습니다. 또한, Amazon RDS는 복제를 사용하기 쉽게 하여 데이터베이스의 가용성을 향상하거나, 데이터의 내구성을 개선하거나, 읽기 중심의 데이터베이스 워크로드를 위해 단일 데이터베이스 인스턴스의 용량 제한 이상으로 규모를 확장할 수 있습니다. 모든 AWS 서비스와 마찬가지로 선불 투자가 필요하지 않으며 사용한 리소스에 대해서만 요금을 지불하면 됩니다.\"\n",
            "Amazon RDS와 Amazon EC2 관계형 데이터베이스 AMI는 각각 언제 사용해야 합니까?\n",
            "Amazon Web Services는 개발자를 위한 다양한 데이터베이스 솔루션을 제공합니다. Amazon RDS를 사용하면 완전관리형이며 모든 기능을 갖춘 관계형 데이터베이스를 실행하면서 데이터베이스 관리에 대한 부담은 덜 수 있습니다. 또한, Amazon EC2에서 AWS의 다양한 관계형 데이터베이스 AMI 중 하나를 사용하여 클라우드에서 자체 관계형 데이터베이스를 관리할 수 있습니다. 데이터베이스 솔루션마다 중요한 차이점이 있으므로, 차이점을 확인하여 목적에 맞는 데이터베이스를 선택하십시오. 고객에게 가장 적합한 솔루션에 대한 지침은 AWS의 클라우드 데이터베이스를 참조하세요.\n",
            "\"category : RDS, question : Amazon RDS와 Amazon EC2 관계형 데이터베이스 AMI는 각각 언제 사용해야 합니까?, answer : Amazon Web Services는 개발자를 위한 다양한 데이터베이스 솔루션을 제공합니다. Amazon RDS를 사용하면 완전관리형이며 모든 기능을 갖춘 관계형 데이터베이스를 실행하면서 데이터베이스 관리에 대한 부담은 덜 수 있습니다. 또한, Amazon EC2에서 AWS의 다양한 관계형 데이터베이스 AMI 중 하나를 사용하여 클라우드에서 자체 관계형 데이터베이스를 관리할 수 있습니다. 데이터베이스 솔루션마다 중요한 차이점이 있으므로, 차이점을 확인하여 목적에 맞는 데이터베이스를 선택하십시오. 고객에게 가장 적합한 솔루션에 대한 지침은 AWS의 클라우드 데이터베이스를 참조하세요.\"\n",
            "Amazon RDS에 하이브리드 또는 온프레미스 배포 옵션이 있습니까?\n",
            "예. Amazon RDS on Outposts를 사용하여 Amazon RDS를 온프레미스로 실행할 수 있습니다. 자세한 내용은 Amazon RDS on Outposts FAQ를 참조하세요.\n",
            "\"category : RDS, question : Amazon RDS에 하이브리드 또는 온프레미스 배포 옵션이 있습니까?, answer : 예. Amazon RDS on Outposts를 사용하여 Amazon RDS를 온프레미스로 실행할 수 있습니다. 자세한 내용은 Amazon RDS on Outposts FAQ를 참조하세요.\"\n",
            "Amazon RDS에 대해 자세히 알아보고 온보딩하는 데 도움을 받을 수 있습니까?\n",
            "예. Amazon RDS 전문가가 질문에 답변하고 지원을 제공할 수 있습니다. AWS에 문의하면 AWS가 귀사를 도울 방법을 논의하기 위해 영업일 기준 1일 이내에 회신을 드립니다.\n",
            "\"category : RDS, question : Amazon RDS에 대해 자세히 알아보고 온보딩하는 데 도움을 받을 수 있습니까?, answer : 예. Amazon RDS 전문가가 질문에 답변하고 지원을 제공할 수 있습니다. AWS에 문의하면 AWS가 귀사를 도울 방법을 논의하기 위해 영업일 기준 1일 이내에 회신을 드립니다.\"\n",
            "Amazon EC2 컴퓨팅 인스턴스에서 실행되는 애플리케이션 또는 SQL 기반 클라이언트와 Amazon RDS 데이터베이스 인스턴스/클러스터 간의 연결을 설정하려면 어떻게 해야 하나요?\n",
            "Amazon RDS 콘솔을 사용하여 EC2 컴퓨팅 인스턴스와 새로운 Amazon RDS 데이터베이스 간의 연결을 설정할 수 있습니다. ‘Create database(데이터베이스 생성)’ 페이지의 Connectivity Section(연결 섹션)에서 ‘Connect to an EC2 compute resource(EC2 컴퓨팅 리소스에 연결)’ 옵션을 선택합니다. 이 옵션을 선택하면 Amazon RDS가 VPC, 보안 그룹, 서브넷 및 수신/송신 규칙 생성과 같은 수동 네트워킹 설정 작업을 자동화하여 애플리케이션과 데이터베이스 간의 연결을 설정합니다.\n",
            "또한 기존 Amazon RDS 데이터베이스와 EC2 컴퓨팅 인스턴스 간의 연결을 설정할 수 있습니다. 연결을 설정하려면 RDS 콘솔의 데이터베이스 목록 페이지에서 RDS 데이터베이스를 선택하고 ‘Action(작업)’ 메뉴 드롭다운 목록에서 ‘Set up EC2 connection(EC2 연결 설정)’을 선택합니다. Amazon RDS에서는 관련 네트워크 설정을 자동으로 지정하여 선택한 EC2 인스턴스 및 RDS 데이터베이스 간의 보안 연결을 활성화합니다.\n",
            "이 연결 자동화는 신규 사용자 및 애플리케이션 개발자의 생산성을 개선합니다. 이제 EC2 컴퓨팅 인스턴스의 SQL을 사용하는 애플리케이션 또는 클라이언트를 몇 분 안에 빠르고 원활하게 RDS 데이터베이스에 연결할 수 있습니다.\n",
            "\"category : RDS, question : Amazon EC2 컴퓨팅 인스턴스에서 실행되는 애플리케이션 또는 SQL 기반 클라이언트와 Amazon RDS 데이터베이스 인스턴스/클러스터 간의 연결을 설정하려면 어떻게 해야 하나요?, answer : Amazon RDS 콘솔을 사용하여 EC2 컴퓨팅 인스턴스와 새로운 Amazon RDS 데이터베이스 간의 연결을 설정할 수 있습니다. ‘Create database(데이터베이스 생성)’ 페이지의 Connectivity Section(연결 섹션)에서 ‘Connect to an EC2 compute resource(EC2 컴퓨팅 리소스에 연결)’ 옵션을 선택합니다. 이 옵션을 선택하면 Amazon RDS가 VPC, 보안 그룹, 서브넷 및 수신/송신 규칙 생성과 같은 수동 네트워킹 설정 작업을 자동화하여 애플리케이션과 데이터베이스 간의 연결을 설정합니다.\n",
            "또한 기존 Amazon RDS 데이터베이스와 EC2 컴퓨팅 인스턴스 간의 연결을 설정할 수 있습니다. 연결을 설정하려면 RDS 콘솔의 데이터베이스 목록 페이지에서 RDS 데이터베이스를 선택하고 ‘Action(작업)’ 메뉴 드롭다운 목록에서 ‘Set up EC2 connection(EC2 연결 설정)’을 선택합니다. Amazon RDS에서는 관련 네트워크 설정을 자동으로 지정하여 선택한 EC2 인스턴스 및 RDS 데이터베이스 간의 보안 연결을 활성화합니다.\n",
            "이 연결 자동화는 신규 사용자 및 애플리케이션 개발자의 생산성을 개선합니다. 이제 EC2 컴퓨팅 인스턴스의 SQL을 사용하는 애플리케이션 또는 클라이언트를 몇 분 안에 빠르고 원활하게 RDS 데이터베이스에 연결할 수 있습니다.\"\n",
            "서버리스 Lambda 애플리케이션과 내 Amazon RDS 또는 Amazon Aurora 데이터베이스 인스턴스 및 클러스터 간의 연결을 설정하려면 어떻게 해야 하나요?\n",
            "Amazon RDS 콘솔에서 AWS Lambda 함수와 Amazon RDS 또는 Amazon Aurora 데이터베이스 간의 연결을 설정할 수 있습니다. RDS 콘솔의 데이터베이스 목록 페이지에서 RDS 또는 Aurora 데이터베이스를 선택하고 ‘Action(작업)’ 메뉴에서 ‘Set up Lambda connection(Lambda 연결 설정)’을 선택합니다. Amazon RDS에서는 관련 네트워크 설정을 자동으로 지정하여 선택한 Lambda 함수 및 RDS 또는 Aurora 데이터베이스 간의 보안 연결을 활성화합니다.\n",
            "이 연결 설정 중에 RDS 프록시를 사용하는 것이 좋습니다. 기존 RDS 프록시를 사용하거나 연결 중에 자동으로 생성할 수 있는 새 RDS 프록시를 사용하여 이 연결을 설정할 수 있습니다. 이렇게 연결 설정을 자동화하면 신규 사용자 및 애플리케이션 개발자의 생산성이 개선됩니다. 이제 몇 분 내에 서버리스 애플리케이션 또는 Lambda 함수를 RDS 또는 Aurora 데이터베이스에 빠르고 원활하게 연결할 수 있습니다.\n",
            "\"category : RDS, question : 서버리스 Lambda 애플리케이션과 내 Amazon RDS 또는 Amazon Aurora 데이터베이스 인스턴스 및 클러스터 간의 연결을 설정하려면 어떻게 해야 하나요?, answer : Amazon RDS 콘솔에서 AWS Lambda 함수와 Amazon RDS 또는 Amazon Aurora 데이터베이스 간의 연결을 설정할 수 있습니다. RDS 콘솔의 데이터베이스 목록 페이지에서 RDS 또는 Aurora 데이터베이스를 선택하고 ‘Action(작업)’ 메뉴에서 ‘Set up Lambda connection(Lambda 연결 설정)’을 선택합니다. Amazon RDS에서는 관련 네트워크 설정을 자동으로 지정하여 선택한 Lambda 함수 및 RDS 또는 Aurora 데이터베이스 간의 보안 연결을 활성화합니다.\n",
            "이 연결 설정 중에 RDS 프록시를 사용하는 것이 좋습니다. 기존 RDS 프록시를 사용하거나 연결 중에 자동으로 생성할 수 있는 새 RDS 프록시를 사용하여 이 연결을 설정할 수 있습니다. 이렇게 연결 설정을 자동화하면 신규 사용자 및 애플리케이션 개발자의 생산성이 개선됩니다. 이제 몇 분 내에 서버리스 애플리케이션 또는 Lambda 함수를 RDS 또는 Aurora 데이터베이스에 빠르고 원활하게 연결할 수 있습니다.\"\n",
            "데이터베이스 인스턴스(DB 인스턴스)는 무엇입니까?\n",
            "DB 인스턴스는 고객이 지정한 컴퓨팅 및 스토리지 리소스를 제공하는 클라우드의 데이터베이스 환경이라고 생각할 수 있습니다. AWS Management Console, Amazon RDS API 및 AWS 명령줄 인터페이스를 통해 DB 인스턴스를 생성 및 삭제하고, DB 인스턴스의 인프라 속성을 정의/미세 조정하며, 액세스 및 보안을 제어할 수 있습니다. 하나 이상의 DB 인스턴스를 실행할 수 있으며 각 DB 인스턴스는 엔진 유형에 따라 하나 이상의 데이터베이스 또는 데이터베이스 스키마를 지원할 수 있습니다.\n",
            "\"category : RDS, question : 데이터베이스 인스턴스(DB 인스턴스)는 무엇입니까?, answer : DB 인스턴스는 고객이 지정한 컴퓨팅 및 스토리지 리소스를 제공하는 클라우드의 데이터베이스 환경이라고 생각할 수 있습니다. AWS Management Console, Amazon RDS API 및 AWS 명령줄 인터페이스를 통해 DB 인스턴스를 생성 및 삭제하고, DB 인스턴스의 인프라 속성을 정의/미세 조정하며, 액세스 및 보안을 제어할 수 있습니다. 하나 이상의 DB 인스턴스를 실행할 수 있으며 각 DB 인스턴스는 엔진 유형에 따라 하나 이상의 데이터베이스 또는 데이터베이스 스키마를 지원할 수 있습니다.\"\n",
            "DB 인스턴스를 생성하려면 어떻게 해야 하나요?\n",
            "DB 인스턴스는 AWS Management Console, Amazon RDS API 또는 AWS Command Line Interface를 사용하여 간단하게 생성할 수 있습니다. AWS Management Console을 사용하여 DB 인스턴스를 시작하려면 ‘RDS’를 클릭한 다음 Instances(인스턴스) 탭에서 ‘Launch DB Instance(DB 인스턴스 시작)’ 버튼을 클릭하세요. 여기에서 DB 엔진 및 버전, 라이선스 모델, 인스턴스 유형, 스토리지 유형 및 용량, 기본 사용자 보안 인증 정보를 비롯하여 DB 인스턴스에 대한 파라미터를 지정할 수 있습니다.\n",
            "또한, DB 인스턴스의 백업 보존 정책, 기본 백업 기간, 예약된 유지 관리 기간을 변경할 수 있습니다. 또는 CreateDBInstance API 또는 create-db-instance 명령을 사용하여 DB 인스턴스를 생성할 수 있습니다.\n",
            "\"category : RDS, question : DB 인스턴스를 생성하려면 어떻게 해야 하나요?, answer : DB 인스턴스는 AWS Management Console, Amazon RDS API 또는 AWS Command Line Interface를 사용하여 간단하게 생성할 수 있습니다. AWS Management Console을 사용하여 DB 인스턴스를 시작하려면 ‘RDS’를 클릭한 다음 Instances(인스턴스) 탭에서 ‘Launch DB Instance(DB 인스턴스 시작)’ 버튼을 클릭하세요. 여기에서 DB 엔진 및 버전, 라이선스 모델, 인스턴스 유형, 스토리지 유형 및 용량, 기본 사용자 보안 인증 정보를 비롯하여 DB 인스턴스에 대한 파라미터를 지정할 수 있습니다.\n",
            "또한, DB 인스턴스의 백업 보존 정책, 기본 백업 기간, 예약된 유지 관리 기간을 변경할 수 있습니다. 또는 CreateDBInstance API 또는 create-db-instance 명령을 사용하여 DB 인스턴스를 생성할 수 있습니다.\"\n",
            "실행 중인 DB 인스턴스에 액세스하려면 어떻게 해야 합니까?\n",
            "DB 인스턴스를 사용할 수 있게 되면, AWS Management Console, DescribeDBInstances API 또는 describe-db-instances 명령에서 DB 인스턴스 설명을 사용하여 엔드포인트를 검색할 수 있습니다. 이 엔드포인트에서 익숙한 데이터베이스 도구 또는 프로그래밍 언어를 사용해 DB 인스턴스에 직접 연결하는 데 필요한 연결 문자열을 구성할 수 있습니다. 실행 중인 DB 인스턴스에 대한 네트워크 요청을 허용하려면 액세스 권한을 부여해야 합니다. 연결 문자열을 구성하고 시작하는 방법에 대한 상세한 설명은 시작안내서를 참조하세요.\n",
            "\"category : RDS, question : 실행 중인 DB 인스턴스에 액세스하려면 어떻게 해야 합니까?, answer : DB 인스턴스를 사용할 수 있게 되면, AWS Management Console, DescribeDBInstances API 또는 describe-db-instances 명령에서 DB 인스턴스 설명을 사용하여 엔드포인트를 검색할 수 있습니다. 이 엔드포인트에서 익숙한 데이터베이스 도구 또는 프로그래밍 언어를 사용해 DB 인스턴스에 직접 연결하는 데 필요한 연결 문자열을 구성할 수 있습니다. 실행 중인 DB 인스턴스에 대한 네트워크 요청을 허용하려면 액세스 권한을 부여해야 합니다. 연결 문자열을 구성하고 시작하는 방법에 대한 상세한 설명은 시작안내서를 참조하세요.\"\n",
            "Amazon RDS에서 몇 개의 DB 인스턴스를 실행할 수 있나요?\n",
            "기본적으로 고객은 최대 총 40개의 Amazon RDS DB 인스턴스를 보유할 수 있습니다. 이러한 인스턴스 40개 중 최대 10개는 ‘라이선스 포함’ 모델에 따른 RDS for Oracle 또는 RDS for SQL Server DB 인스턴스가 될 수 있습니다. BYOL(개인 소유 라이선스 이용 허용) 모델에 따라 40개 모두 Amazon Aurora, RDS for PostgreSQL, RDS for MySQL, RDS for MariaDB, RDS for Oracle에 사용할 수 있습니다. RDS for SQL Server에서는 단일 DB 인스턴스에 최대 100개의 데이터베이스만 구성할 수 있습니다. 자세한 내용은 Amazon RDS for SQL Server 사용 설명서를 참조하세요.\n",
            "\"category : RDS, question : Amazon RDS에서 몇 개의 DB 인스턴스를 실행할 수 있나요?, answer : 기본적으로 고객은 최대 총 40개의 Amazon RDS DB 인스턴스를 보유할 수 있습니다. 이러한 인스턴스 40개 중 최대 10개는 ‘라이선스 포함’ 모델에 따른 RDS for Oracle 또는 RDS for SQL Server DB 인스턴스가 될 수 있습니다. BYOL(개인 소유 라이선스 이용 허용) 모델에 따라 40개 모두 Amazon Aurora, RDS for PostgreSQL, RDS for MySQL, RDS for MariaDB, RDS for Oracle에 사용할 수 있습니다. RDS for SQL Server에서는 단일 DB 인스턴스에 최대 100개의 데이터베이스만 구성할 수 있습니다. 자세한 내용은 Amazon RDS for SQL Server 사용 설명서를 참조하세요.\"\n",
            "DB 인스턴스 내에서 몇 개의 데이터베이스 또는 스키마를 실행할 수 있나요?\n",
            "Amazon Aurora용 RDS: 소프트웨어에서 지정한 제한 없음\n",
            "MySQL용 RDS: 소프트웨어에서 지정한 제한 없음\n",
            "MariaDB용 RDS: 소프트웨어에서 지정한 제한 없음\n",
            "RDS for Oracle: 인스턴스당 데이터베이스 1개, 데이터베이스당 스키마 수에는 소프트웨어에서 지정한 제한 없음\n",
            "RDS for SQL Server: 인스턴스당 데이터베이스 최대 100개\n",
            "PostgreSQL용 RDS: 소프트웨어에서 지정한 제한 없음\n",
            "RDS for Db2: 인스턴스당 최대 8개의 데이터베이스\n",
            "\"category : RDS, question : DB 인스턴스 내에서 몇 개의 데이터베이스 또는 스키마를 실행할 수 있나요?, answer : Amazon Aurora용 RDS: 소프트웨어에서 지정한 제한 없음\n",
            "MySQL용 RDS: 소프트웨어에서 지정한 제한 없음\n",
            "MariaDB용 RDS: 소프트웨어에서 지정한 제한 없음\n",
            "RDS for Oracle: 인스턴스당 데이터베이스 1개, 데이터베이스당 스키마 수에는 소프트웨어에서 지정한 제한 없음\n",
            "RDS for SQL Server: 인스턴스당 데이터베이스 최대 100개\n",
            "PostgreSQL용 RDS: 소프트웨어에서 지정한 제한 없음\n",
            "RDS for Db2: 인스턴스당 최대 8개의 데이터베이스\"\n",
            "Amazon RDS DB 인스턴스로 데이터를 가져오려면 어떻게 해야 하나요?\n",
            "다음은 Amazon RDS로 데이터를 가져오는 다양한 방법입니다.\n",
            "\n",
            "MySQL: mysqldump 또는 mysqlimport 유틸리티\n",
            "Oracle: Data Pump, 가져오기/내보내기 또는 SQL Loader\n",
            "SQL Server: 가져오기/내보내기 마법사, 전체 백업 파일(.bak) 또는 대량 복사 프로그램(BCP)\n",
            "PostgreSQL: pg_dump\n",
            "\n",
            "데이터 가져오기 및 내보내기에 대한 자세한 내용은 MySQL 데이터 가져오기 설명서, Oracle 데이터 가져오기 설명서, SQL Server 데이터 가져오기 설명서, PostgreSQL 데이터 가져오기 설명서 또는 Db2 데이터 가져오기 설명서를 참조하세요.\n",
            "또한 AWS Database Migration Service를 사용하면 데이터베이스를 AWS로 안전하게 마이그레이션할 수 있습니다.\n",
            "\"category : RDS, question : Amazon RDS DB 인스턴스로 데이터를 가져오려면 어떻게 해야 하나요?, answer : 다음은 Amazon RDS로 데이터를 가져오는 다양한 방법입니다.\n",
            "\n",
            "MySQL: mysqldump 또는 mysqlimport 유틸리티\n",
            "Oracle: Data Pump, 가져오기/내보내기 또는 SQL Loader\n",
            "SQL Server: 가져오기/내보내기 마법사, 전체 백업 파일(.bak) 또는 대량 복사 프로그램(BCP)\n",
            "PostgreSQL: pg_dump\n",
            "\n",
            "데이터 가져오기 및 내보내기에 대한 자세한 내용은 MySQL 데이터 가져오기 설명서, Oracle 데이터 가져오기 설명서, SQL Server 데이터 가져오기 설명서, PostgreSQL 데이터 가져오기 설명서 또는 Db2 데이터 가져오기 설명서를 참조하세요.\n",
            "또한 AWS Database Migration Service를 사용하면 데이터베이스를 AWS로 안전하게 마이그레이션할 수 있습니다.\"\n",
            "유지 관리 기간이란 무엇입니까? 유지 관리 기간 동안 내 DB 인스턴스를 사용할 수 있습니까?\n",
            "Amazon RDS 유지 관리 기간을 사용하여 요청이나 필요에 따라 DB 인스턴스 수정, 데이터베이스 엔진 버전 업그레이드 및 소프트웨어 패치를 적용하는 시기를 조정할 수 있습니다. 유지 관리 이벤트가 특정 주에 예정된 경우 사용자가 지정한 유지 관리 기간에 시작됩니다.\n",
            "Amazon RDS가 DB 인스턴스를 오프라인으로 전환해야 하는 유지 관리 이벤트는 컴퓨팅 조정 작업(일반적으로 시작에서 완료까지 몇 분이면 됨), 데이터베이스 엔진 업그레이드 및 필수 소프트웨어 패치 작업입니다. 필수 소프트웨어 패치 작업은 보안 및 내구성과 관련된 패치에 대해서만 자동으로 예약됩니다. 이러한 패치 적용은 자주 발생하는 것은 아닙니다(일반적으로 몇 달에 한 번). 또한 유지 관리 시간에서 차지하는 비중도 크지 않습니다.\n",
            "DB 인스턴스를 생성할 때 기본 주별 유지 관리 기간을 지정하지 않으면 30분이 기본값으로 지정됩니다. 자동으로 유지 관리를 수행하는 시기를 수정하려면 AWS Management Console, ModifyDBInstance API 또는 modify-db-instance 명령을 사용하여 DB 인스턴스를 수정하면 됩니다. 원하는 경우 DB 인스턴스마다 기본 유지 관리 기간을 다르게 설정할 수 있습니다.\n",
            "DB 인스턴스를 다중 AZ 배포로 실행하면 유지 관리 이벤트의 영향을 더 줄일 수 있습니다. 유지 관리 작업에 대한 자세한 내용은 Amazon RDS 사용 설명서를 참조하세요.\n",
            "\"category : RDS, question : 유지 관리 기간이란 무엇입니까? 유지 관리 기간 동안 내 DB 인스턴스를 사용할 수 있습니까?, answer : Amazon RDS 유지 관리 기간을 사용하여 요청이나 필요에 따라 DB 인스턴스 수정, 데이터베이스 엔진 버전 업그레이드 및 소프트웨어 패치를 적용하는 시기를 조정할 수 있습니다. 유지 관리 이벤트가 특정 주에 예정된 경우 사용자가 지정한 유지 관리 기간에 시작됩니다.\n",
            "Amazon RDS가 DB 인스턴스를 오프라인으로 전환해야 하는 유지 관리 이벤트는 컴퓨팅 조정 작업(일반적으로 시작에서 완료까지 몇 분이면 됨), 데이터베이스 엔진 업그레이드 및 필수 소프트웨어 패치 작업입니다. 필수 소프트웨어 패치 작업은 보안 및 내구성과 관련된 패치에 대해서만 자동으로 예약됩니다. 이러한 패치 적용은 자주 발생하는 것은 아닙니다(일반적으로 몇 달에 한 번). 또한 유지 관리 시간에서 차지하는 비중도 크지 않습니다.\n",
            "DB 인스턴스를 생성할 때 기본 주별 유지 관리 기간을 지정하지 않으면 30분이 기본값으로 지정됩니다. 자동으로 유지 관리를 수행하는 시기를 수정하려면 AWS Management Console, ModifyDBInstance API 또는 modify-db-instance 명령을 사용하여 DB 인스턴스를 수정하면 됩니다. 원하는 경우 DB 인스턴스마다 기본 유지 관리 기간을 다르게 설정할 수 있습니다.\n",
            "DB 인스턴스를 다중 AZ 배포로 실행하면 유지 관리 이벤트의 영향을 더 줄일 수 있습니다. 유지 관리 작업에 대한 자세한 내용은 Amazon RDS 사용 설명서를 참조하세요.\"\n",
            "쿼리 실행 속도가 느리다고 생각되면 어떻게 해야 합니까?\n",
            "프로덕션 데이터베이스의 경우 50개가 넘는 CPU, 메모리, 파일 시스템 및 디스크 I/O 지표에 대한 액세스를 제공하는 향상된 모니터링을 사용하는 것이 좋습니다. 이러한 기능을 인스턴스별로 활성화할 수 있고 세부 수준을 선택할 수 있습니다(최소 1초까지). CPU 사용률이 높으면 쿼리 성능이 저하될 수 있으며 이 경우 DB 인스턴스 클래스를 조정하는 것이 좋습니다. DB 인스턴스 모니터링에 대한 자세한 내용은 Amazon RDS 사용 설명서를 참조하세요.\n",
            "RDS for MySQL 또는 MariaDB를 사용하는 경우, 데이터베이스의 느린 쿼리 로그에 액세스하여 실행 속도가 느린 SQL 쿼리가 있는지 확인하고, 있는 경우 각 쿼리의 성능 특성을 확인할 수 있습니다. 'slow_query_log' DB 파라미터를 설정하고 mysql.slow_log 테이블을 쿼리하여 실행 속도가 느린 SQL 쿼리를 확인할 수 있습니다. 자세한 내용은 Amazon RDS 사용 설명서를 참조하세요.\n",
            "RDS for Oracle을 사용하는 경우, Oracle 추적 파일 데이터를 사용하여 실행 속도가 느린 쿼리를 확인할 수 있습니다. 추적 파일 데이터에 액세스하는 방법에 대한 자세한 내용은 Amazon RDS 사용 설명서를 참조하세요. \n",
            "RDS for SQL Server를 사용하는 경우, 클라이언트 측 SQL Server 추적 기능을 사용하여 실행 속도가 느린 쿼리를 확인할 수 있습니다. 서버 측 추적 파일 데이터에 액세스하는 방법에 대한 자세한 내용은 Amazon RDS 사용 설명서를 참조하세요.\n",
            "\"category : RDS, question : 쿼리 실행 속도가 느리다고 생각되면 어떻게 해야 합니까?, answer : 프로덕션 데이터베이스의 경우 50개가 넘는 CPU, 메모리, 파일 시스템 및 디스크 I/O 지표에 대한 액세스를 제공하는 향상된 모니터링을 사용하는 것이 좋습니다. 이러한 기능을 인스턴스별로 활성화할 수 있고 세부 수준을 선택할 수 있습니다(최소 1초까지). CPU 사용률이 높으면 쿼리 성능이 저하될 수 있으며 이 경우 DB 인스턴스 클래스를 조정하는 것이 좋습니다. DB 인스턴스 모니터링에 대한 자세한 내용은 Amazon RDS 사용 설명서를 참조하세요.\n",
            "RDS for MySQL 또는 MariaDB를 사용하는 경우, 데이터베이스의 느린 쿼리 로그에 액세스하여 실행 속도가 느린 SQL 쿼리가 있는지 확인하고, 있는 경우 각 쿼리의 성능 특성을 확인할 수 있습니다. 'slow_query_log' DB 파라미터를 설정하고 mysql.slow_log 테이블을 쿼리하여 실행 속도가 느린 SQL 쿼리를 확인할 수 있습니다. 자세한 내용은 Amazon RDS 사용 설명서를 참조하세요.\n",
            "RDS for Oracle을 사용하는 경우, Oracle 추적 파일 데이터를 사용하여 실행 속도가 느린 쿼리를 확인할 수 있습니다. 추적 파일 데이터에 액세스하는 방법에 대한 자세한 내용은 Amazon RDS 사용 설명서를 참조하세요. \n",
            "RDS for SQL Server를 사용하는 경우, 클라이언트 측 SQL Server 추적 기능을 사용하여 실행 속도가 느린 쿼리를 확인할 수 있습니다. 서버 측 추적 파일 데이터에 액세스하는 방법에 대한 자세한 내용은 Amazon RDS 사용 설명서를 참조하세요.\"\n",
            "Amazon RDS에서 지원하는 관계형 데이터베이스 엔진 버전은 무엇입니까?\n",
            "지원되는 데이터베이스 엔진 버전 목록은 각 엔진의 설명서를 참조하십시오.\n",
            "\n",
            "Amazon Aurora\n",
            "Amazon RDS for PostgreSQL\n",
            "Amazon RDS for MySQL\n",
            "Amazon RDS for MariaDB\n",
            "Amazon RDS for SQL Server\n",
            "Amazon RDS for Oracle\n",
            "Amazon RDS for Db2\n",
            "\"category : RDS, question : Amazon RDS에서 지원하는 관계형 데이터베이스 엔진 버전은 무엇입니까?, answer : 지원되는 데이터베이스 엔진 버전 목록은 각 엔진의 설명서를 참조하십시오.\n",
            "\n",
            "Amazon Aurora\n",
            "Amazon RDS for PostgreSQL\n",
            "Amazon RDS for MySQL\n",
            "Amazon RDS for MariaDB\n",
            "Amazon RDS for SQL Server\n",
            "Amazon RDS for Oracle\n",
            "Amazon RDS for Db2\"\n",
            "Amazon RDS에서는 어떻게 ‘메이저’와 ‘마이너’ DB 엔진 버전을 구별하나요?\n",
            "버전 번호 지정에 대한 자세한 내용은 Amazon RDS 데이터베이스 엔진별 FAQ 페이지를 참조하세요.\n",
            "\n",
            "Amazon RDS for MySQL\n",
            "Amazon RDS for MariaDB\n",
            "Amazon RDS for PostgreSQL\n",
            "Amazon RDS for Oracle\n",
            "Amazon RDS for SQL Server\n",
            "Amazon Aurora\n",
            "\"category : RDS, question : Amazon RDS에서는 어떻게 ‘메이저’와 ‘마이너’ DB 엔진 버전을 구별하나요?, answer : 버전 번호 지정에 대한 자세한 내용은 Amazon RDS 데이터베이스 엔진별 FAQ 페이지를 참조하세요.\n",
            "\n",
            "Amazon RDS for MySQL\n",
            "Amazon RDS for MariaDB\n",
            "Amazon RDS for PostgreSQL\n",
            "Amazon RDS for Oracle\n",
            "Amazon RDS for SQL Server\n",
            "Amazon Aurora\"\n",
            "Amazon RDS에서는 새로운 DB 엔진 버전 지원에 대한 지침을 제공합니까?\n",
            "시간이 지나면 Amazon RDS에서는 새로운 메이저 및 마이너 데이터베이스 엔진 버전에 대한 지원을 추가합니다. 몇 개의 새 버전을 지원할지는 엔진 공급업체 또는 개발 조직의 릴리스 및 패치 빈도와 내용, 그리고 AWS 데이터베이스 엔지니어링 팀의 철저한 릴리스 및 패치 검사 결과에 따라 달라질 수 있습니다. 그러나 일반 지침으로 상용 버전 출시 후 5개월 이내에 새로운 엔진 버전을 지원하는 것을 목표로 하고 있습니다.\n",
            "\"category : RDS, question : Amazon RDS에서는 새로운 DB 엔진 버전 지원에 대한 지침을 제공합니까?, answer : 시간이 지나면 Amazon RDS에서는 새로운 메이저 및 마이너 데이터베이스 엔진 버전에 대한 지원을 추가합니다. 몇 개의 새 버전을 지원할지는 엔진 공급업체 또는 개발 조직의 릴리스 및 패치 빈도와 내용, 그리고 AWS 데이터베이스 엔지니어링 팀의 철저한 릴리스 및 패치 검사 결과에 따라 달라질 수 있습니다. 그러나 일반 지침으로 상용 버전 출시 후 5개월 이내에 새로운 엔진 버전을 지원하는 것을 목표로 하고 있습니다.\"\n",
            "지원되는 DB 엔진 버전 중 DB 인스턴스를 실행할 DB 엔진 버전은 어떻게 지정하나요?\n",
            "AWS Management Console에서 DB 인스턴스 시작 작업 또는 CreateDBInstance API를 통해 새로운 DB 인스턴스를 생성할 때 현재 지원되는 버전(메이저 및 마이너) 중 원하는 버전을 지정하면 됩니다. AWS 리전별로 지원되지 않는 데이터베이스 엔진 버전이 있을 수 있습니다.\n",
            "\"category : RDS, question : 지원되는 DB 엔진 버전 중 DB 인스턴스를 실행할 DB 엔진 버전은 어떻게 지정하나요?, answer : AWS Management Console에서 DB 인스턴스 시작 작업 또는 CreateDBInstance API를 통해 새로운 DB 인스턴스를 생성할 때 현재 지원되는 버전(메이저 및 마이너) 중 원하는 버전을 지정하면 됩니다. AWS 리전별로 지원되지 않는 데이터베이스 엔진 버전이 있을 수 있습니다.\"\n",
            "내 DB 인스턴스 엔진 버전을 새로운 지원 버전으로 업그레이드할지와 시기를 제어하려면 어떻게 해야 하나요?\n",
            "Amazon RDS는 지원되는 데이터베이스 엔진의 신규 버전을 제공하여 사용자의 데이터베이스 인스턴스를 최신 상태로 유지하기 위해 최선을 다합니다. 공급업체 또는 개발 조직에서 데이터베이스 엔진의 신규 버전을 출시하면 Amazon RDS에서 이를 제공하기 전에 AWS 데이터베이스 엔지니어링 팀에서 철저한 테스트를 수행합니다.\n",
            "최신 버전에는 최신 보안 및 기능 수정 사항이 포함되므로 데이터베이스 인스턴스를 최신 마이너 버전으로 업그레이드하는 것이 좋습니다. 메이저 버전 업그레이드와는 달리 마이너 버전 업그레이드에는 데이터베이스 엔진의 동일한 메이저 버전의 이전 마이너 버전과 호환되는 데이터베이스 변경 사항만 포함되어 있습니다. \n",
            "새 마이너 버전에 Amazon RDS 고객에게 도움이 될 만한 수정 사항이 없는 경우, AWS에서는 이를 Amazon RDS에서 제공하지 않기로 결정할 수도 있습니다. 새로운 마이너 버전을 Amazon RDS에서 사용할 수 있게 되자마자, AWS에서는 이를 새로운 DB 인스턴스의 기본 마이너 버전으로 설정합니다. \n",
            "데이터베이스 인스턴스를 지원되는 엔진 버전으로 수동으로 업그레이드하려면 AWS Management Console에서 Modify DB Instance(DB 인스턴스 수정) 명령 또는 ModifyDBInstance API를 사용하여 DB Engine Version(DB 엔진 버전) 파라미터를 원하는 버전으로 설정합니다. 기본적으로 업그레이드는 다음 유지 관리 기간에 적용됩니다. 콘솔 API에서 Apply Immediately(즉시 적용) 옵션을 선택하여 즉시 업그레이드할 수도 있습니다.\n",
            "이전에 릴리스된 마이너 버전과 비교하여 새로운 엔진 마이너 버전에 상당한 오류 수정 사항이 포함되어 있다고 판단한 경우, AWS에서는 Auto Minor Version Upgrade가 ‘Yes(예)’로 설정되어 있는 DB 인스턴스에 대한 자동 업그레이드 일정을 예약합니다. 이러한 업그레이드는 고객이 지정한 유지 관리 기간에 수행되도록 예약됩니다.\n",
            "다중 AZ 인스턴스라 하더라도 DB 엔진 버전을 업그레이드하려면 가동 중지가 필요하므로, 미리 계획을 세울 수 있도록 이렇게 일정을 잡는 것입니다. 자동 마이너 버전 업그레이드 기능을 비활성화하려면 Auto Minor Version Upgrade를 ‘No(아니요)’로 설정하면 됩니다.\n",
            "RDS for Oracle 및 RDS for SQL Server의 경우, 다음 마이너 버전으로 업그레이드하기 위해서는 다른 에디션으로 변경해야 한다면 사용자가 Auto Minor Version Upgrade 설정을 활성화했더라도 AWS에서 자동 업그레이드 일정을 예약하지 않을 수 있습니다. 이럴 때 자동 업그레이드 일정을 예약할지는 사례별로 결정합니다.\n",
            "메이저 버전 업그레이드에는 호환성 위험이 따르므로, 자동으로 수행되지 않으며 반드시 사용자가 시작해야 합니다(메이저 버전 사용 중단은 예외, 아래 참조). DB 인스턴스를 새로운 DB 엔진 버전으로 업그레이드하는 데 대한 자세한 내용은 Amazon RDS 사용 설명서를 참조하세요.\n",
            "\"category : RDS, question : 내 DB 인스턴스 엔진 버전을 새로운 지원 버전으로 업그레이드할지와 시기를 제어하려면 어떻게 해야 하나요?, answer : Amazon RDS는 지원되는 데이터베이스 엔진의 신규 버전을 제공하여 사용자의 데이터베이스 인스턴스를 최신 상태로 유지하기 위해 최선을 다합니다. 공급업체 또는 개발 조직에서 데이터베이스 엔진의 신규 버전을 출시하면 Amazon RDS에서 이를 제공하기 전에 AWS 데이터베이스 엔지니어링 팀에서 철저한 테스트를 수행합니다.\n",
            "최신 버전에는 최신 보안 및 기능 수정 사항이 포함되므로 데이터베이스 인스턴스를 최신 마이너 버전으로 업그레이드하는 것이 좋습니다. 메이저 버전 업그레이드와는 달리 마이너 버전 업그레이드에는 데이터베이스 엔진의 동일한 메이저 버전의 이전 마이너 버전과 호환되는 데이터베이스 변경 사항만 포함되어 있습니다. \n",
            "새 마이너 버전에 Amazon RDS 고객에게 도움이 될 만한 수정 사항이 없는 경우, AWS에서는 이를 Amazon RDS에서 제공하지 않기로 결정할 수도 있습니다. 새로운 마이너 버전을 Amazon RDS에서 사용할 수 있게 되자마자, AWS에서는 이를 새로운 DB 인스턴스의 기본 마이너 버전으로 설정합니다. \n",
            "데이터베이스 인스턴스를 지원되는 엔진 버전으로 수동으로 업그레이드하려면 AWS Management Console에서 Modify DB Instance(DB 인스턴스 수정) 명령 또는 ModifyDBInstance API를 사용하여 DB Engine Version(DB 엔진 버전) 파라미터를 원하는 버전으로 설정합니다. 기본적으로 업그레이드는 다음 유지 관리 기간에 적용됩니다. 콘솔 API에서 Apply Immediately(즉시 적용) 옵션을 선택하여 즉시 업그레이드할 수도 있습니다.\n",
            "이전에 릴리스된 마이너 버전과 비교하여 새로운 엔진 마이너 버전에 상당한 오류 수정 사항이 포함되어 있다고 판단한 경우, AWS에서는 Auto Minor Version Upgrade가 ‘Yes(예)’로 설정되어 있는 DB 인스턴스에 대한 자동 업그레이드 일정을 예약합니다. 이러한 업그레이드는 고객이 지정한 유지 관리 기간에 수행되도록 예약됩니다.\n",
            "다중 AZ 인스턴스라 하더라도 DB 엔진 버전을 업그레이드하려면 가동 중지가 필요하므로, 미리 계획을 세울 수 있도록 이렇게 일정을 잡는 것입니다. 자동 마이너 버전 업그레이드 기능을 비활성화하려면 Auto Minor Version Upgrade를 ‘No(아니요)’로 설정하면 됩니다.\n",
            "RDS for Oracle 및 RDS for SQL Server의 경우, 다음 마이너 버전으로 업그레이드하기 위해서는 다른 에디션으로 변경해야 한다면 사용자가 Auto Minor Version Upgrade 설정을 활성화했더라도 AWS에서 자동 업그레이드 일정을 예약하지 않을 수 있습니다. 이럴 때 자동 업그레이드 일정을 예약할지는 사례별로 결정합니다.\n",
            "메이저 버전 업그레이드에는 호환성 위험이 따르므로, 자동으로 수행되지 않으며 반드시 사용자가 시작해야 합니다(메이저 버전 사용 중단은 예외, 아래 참조). DB 인스턴스를 새로운 DB 엔진 버전으로 업그레이드하는 데 대한 자세한 내용은 Amazon RDS 사용 설명서를 참조하세요.\"\n",
            "업그레이드하기 전에 내 DB 인스턴스에서 새로운 버전을 테스트할 수 있습니까?\n",
            "예. 기존의 DB 인스턴스의 DB 스냅샷을 생성하고, DB 스냅샷에서 복원하여 새 DB 인스턴스를 만든 다음, 새로운 DB 인스턴스에 대한 버전 업그레이드를 시작할 수 있습니다. 그런 다음 업그레이드된 DB 인스턴스 복제본에서 안전하게 테스트한 후 원래의 DB 인스턴스를 업그레이드할지를 결정할 수 있습니다.\n",
            "DB 스냅샷 복원에 대한 자세한 내용은 Amazon RDS 사용 설명서를 참조하세요.\n",
            "\"category : RDS, question : 업그레이드하기 전에 내 DB 인스턴스에서 새로운 버전을 테스트할 수 있습니까?, answer : 예. 기존의 DB 인스턴스의 DB 스냅샷을 생성하고, DB 스냅샷에서 복원하여 새 DB 인스턴스를 만든 다음, 새로운 DB 인스턴스에 대한 버전 업그레이드를 시작할 수 있습니다. 그런 다음 업그레이드된 DB 인스턴스 복제본에서 안전하게 테스트한 후 원래의 DB 인스턴스를 업그레이드할지를 결정할 수 있습니다.\n",
            "DB 스냅샷 복원에 대한 자세한 내용은 Amazon RDS 사용 설명서를 참조하세요.\"\n",
            "Amazon RDS에서는 현재 지원되는 데이터베이스 엔진 버전의 폐지에 대한 지침을 제공합니까?\n",
            "AWS에서는 메이저 버전 릴리스(예: MySQL 5.6, PostgreSQL 9.6)는 Amazon RDS에서 처음으로 지원된 후 최소 3년 동안 지원하고,\n",
            "마이너 버전(예: MySQL 5.6.37, PostgreSQL 9.6.1)은 Amazon RDS에서 처음으로 지원한 후 최소 1년 동안 지원하려고 합니다.\n",
            "\n",
            "AWS에서는 정기적으로 메이저 또는 마이너 엔진 버전을 폐지합니다. 메이저 버전은 해당 커뮤니티 버전에 대한 커뮤니티 수명이 종료되거나 버전이 더 이상 소프트웨어 수정 또는 보안 업데이트를 받지 못할 때까지 사용할 수 있습니다. 마이너 버전의 경우, 마이너 버전에 상당한 버그 또는 보안 문제가 있고 이러한 문제가 이후 마이너 버전에서 해결되었을 때 이를 폐지합니다.\n",
            "이 지침을 준수하도록 노력하고는 있지만 보안 문제가 우려되는 등의 경우에는 특정 주 버전이나 부 버전을 계획보다 일찍 폐기할 수도 있습니다. 예기치 않게 이러한 경우가 발생하는 경우 Amazon RDS는 이러한 문제를 해결하기 위해 데이터베이스 엔진을 자동으로 업그레이드합니다. 해결해야 할 문제에 따라 상황별로 업그레이드 시기가 달라질 수 있습니다.\n",
            "\"category : RDS, question : Amazon RDS에서는 현재 지원되는 데이터베이스 엔진 버전의 폐지에 대한 지침을 제공합니까?, answer : AWS에서는 메이저 버전 릴리스(예: MySQL 5.6, PostgreSQL 9.6)는 Amazon RDS에서 처음으로 지원된 후 최소 3년 동안 지원하고,\n",
            "마이너 버전(예: MySQL 5.6.37, PostgreSQL 9.6.1)은 Amazon RDS에서 처음으로 지원한 후 최소 1년 동안 지원하려고 합니다.\n",
            "\n",
            "AWS에서는 정기적으로 메이저 또는 마이너 엔진 버전을 폐지합니다. 메이저 버전은 해당 커뮤니티 버전에 대한 커뮤니티 수명이 종료되거나 버전이 더 이상 소프트웨어 수정 또는 보안 업데이트를 받지 못할 때까지 사용할 수 있습니다. 마이너 버전의 경우, 마이너 버전에 상당한 버그 또는 보안 문제가 있고 이러한 문제가 이후 마이너 버전에서 해결되었을 때 이를 폐지합니다.\n",
            "이 지침을 준수하도록 노력하고는 있지만 보안 문제가 우려되는 등의 경우에는 특정 주 버전이나 부 버전을 계획보다 일찍 폐기할 수도 있습니다. 예기치 않게 이러한 경우가 발생하는 경우 Amazon RDS는 이러한 문제를 해결하기 위해 데이터베이스 엔진을 자동으로 업그레이드합니다. 해결해야 할 문제에 따라 상황별로 업그레이드 시기가 달라질 수 있습니다.\"\n",
            "Amazon RDS DB 엔진 버전이 폐지되면 어떤 일이 발생합니까?\n",
            "Amazon RDS에서 데이터베이스 엔진의 마이너 버전이 중요도가 떨어져 폐지되는 경우, AWS에서는 자동 업그레이드를 시작하기 전에 폐지 공지 후 3개월의 기간을 제공합니다. 이 기간이 끝나면 여전히 폐지되는 마이너 버전을 실행하고 있는 모든 인스턴스를 대상으로 자동 업그레이드 일정이 예약되고, 예약된 유지 관리 기간 동안 지원되는 최신 마이너 버전으로 업그레이드됩니다.\n",
            "Amazon RDS에서 데이터베이스 엔진의 메이저 버전이 폐지되는 경우, AWS에서는 고객이 지원되는 메이저 버전으로 업그레이드할 수 있도록 폐지 공지 후 최소한 6개월의 기간을 제공합니다. 이 기간이 끝나면 여전히 폐지된 버전을 실행하고 있는 모든 인스턴스를 대상으로 예약된 유지 관리 기간 동안 다음 메이저 버전으로 자동 업그레이드가 진행됩니다.\n",
            "\"category : RDS, question : Amazon RDS DB 엔진 버전이 폐지되면 어떤 일이 발생합니까?, answer : Amazon RDS에서 데이터베이스 엔진의 마이너 버전이 중요도가 떨어져 폐지되는 경우, AWS에서는 자동 업그레이드를 시작하기 전에 폐지 공지 후 3개월의 기간을 제공합니다. 이 기간이 끝나면 여전히 폐지되는 마이너 버전을 실행하고 있는 모든 인스턴스를 대상으로 자동 업그레이드 일정이 예약되고, 예약된 유지 관리 기간 동안 지원되는 최신 마이너 버전으로 업그레이드됩니다.\n",
            "Amazon RDS에서 데이터베이스 엔진의 메이저 버전이 폐지되는 경우, AWS에서는 고객이 지원되는 메이저 버전으로 업그레이드할 수 있도록 폐지 공지 후 최소한 6개월의 기간을 제공합니다. 이 기간이 끝나면 여전히 폐지된 버전을 실행하고 있는 모든 인스턴스를 대상으로 예약된 유지 관리 기간 동안 다음 메이저 버전으로 자동 업그레이드가 진행됩니다.\"\n",
            "특정 버전을 생성할 수 없는 이유는 무엇입니까?\n",
            "일부 경우에는 사전 공지 없이 특정 메이저 또는 마이너 버전을 폐지할 수 있습니다(예: 버전이 고품질, 성능 또는 보안 기준을 충족하지 않음을 발견한 경우). 이러한 경우 Amazon RDS에서는 이러한 버전의 새 데이터베이스 인스턴스 및 클러스터 생성을 중단합니다. 기존 고객은 계속해서 데이터베이스를 실행할 수 있습니다. 해결해야 할 문제에 따라 상황별로 업그레이드 시기가 달라질 수 있습니다.\n",
            "\"category : RDS, question : 특정 버전을 생성할 수 없는 이유는 무엇입니까?, answer : 일부 경우에는 사전 공지 없이 특정 메이저 또는 마이너 버전을 폐지할 수 있습니다(예: 버전이 고품질, 성능 또는 보안 기준을 충족하지 않음을 발견한 경우). 이러한 경우 Amazon RDS에서는 이러한 버전의 새 데이터베이스 인스턴스 및 클러스터 생성을 중단합니다. 기존 고객은 계속해서 데이터베이스를 실행할 수 있습니다. 해결해야 할 문제에 따라 상황별로 업그레이드 시기가 달라질 수 있습니다.\"\n",
            "Amazon RDS 사용료는 어떻게 과금되어 비용이 청구됩니까?\n",
            "최소 비용이나 설정 비용이 없으며, 사용한 만큼만 지불하면 됩니다. 다음을 기준으로 요금이 청구됩니다.\n",
            "\n",
            "DB 인스턴스 시간 – 사용한 DB 인스턴스의 클래스(예: db.t2.micro, db.m4.large) 기준. 부분적으로 사용된 DB 인스턴스의 경우 DB 인스턴스 클래스 생성, 시작 또는 수정 같은 청구 가능한 상태 변경에 따라 1초 단위로 청구되며 최소 10분의 요금이 부과됩니다. 자세한 내용은 새로운 소식 발표를 참조하세요.\n",
            "스토리지(월별 GB당) – DB 인스턴스에 프로비저닝한 스토리지 용량. 월 중간에 프로비저닝된 스토리지 용량을 조정하는 경우, 비례 할당으로 계산된 요금이 적용됩니다.\n",
            "월별 I/O 요청 – 사용자가 요청한 총 스토리지 I/O 요청 수(Amazon RDS 마그네틱 스토리지와 Amazon Aurora에만 해당)\n",
            "월별 프로비저닝된 IOPS - 프로비저닝된 IOPS 속도이며 사용된 IOPS와 무관합니다[Amazon RDS의 프로비저닝된 IOPS(SSD) 스토리지에만 해당].\n",
            "백업 스토리지 – 백업 스토리지는 자동화된 데이터베이스 백업 및 사용자가 생성한 데이터베이스 스냅샷과 연결되어 있습니다. 백업 보존 기간을 연장하거나 추가 데이터베이스 스냅샷을 생성하면 데이터베이스가 사용하는 백업 스토리지가 증가합니다.\n",
            "데이터 전송 – DB 인스턴스에서 인터넷을 통한 데이터 송수신.\n",
            "\n",
            "Amazon RDS의 요금 정보는 Amazon RDS 제품 페이지의 요금 섹션을 참조하세요.\n",
            "\"category : RDS, question : Amazon RDS 사용료는 어떻게 과금되어 비용이 청구됩니까?, answer : 최소 비용이나 설정 비용이 없으며, 사용한 만큼만 지불하면 됩니다. 다음을 기준으로 요금이 청구됩니다.\n",
            "\n",
            "DB 인스턴스 시간 – 사용한 DB 인스턴스의 클래스(예: db.t2.micro, db.m4.large) 기준. 부분적으로 사용된 DB 인스턴스의 경우 DB 인스턴스 클래스 생성, 시작 또는 수정 같은 청구 가능한 상태 변경에 따라 1초 단위로 청구되며 최소 10분의 요금이 부과됩니다. 자세한 내용은 새로운 소식 발표를 참조하세요.\n",
            "스토리지(월별 GB당) – DB 인스턴스에 프로비저닝한 스토리지 용량. 월 중간에 프로비저닝된 스토리지 용량을 조정하는 경우, 비례 할당으로 계산된 요금이 적용됩니다.\n",
            "월별 I/O 요청 – 사용자가 요청한 총 스토리지 I/O 요청 수(Amazon RDS 마그네틱 스토리지와 Amazon Aurora에만 해당)\n",
            "월별 프로비저닝된 IOPS - 프로비저닝된 IOPS 속도이며 사용된 IOPS와 무관합니다[Amazon RDS의 프로비저닝된 IOPS(SSD) 스토리지에만 해당].\n",
            "백업 스토리지 – 백업 스토리지는 자동화된 데이터베이스 백업 및 사용자가 생성한 데이터베이스 스냅샷과 연결되어 있습니다. 백업 보존 기간을 연장하거나 추가 데이터베이스 스냅샷을 생성하면 데이터베이스가 사용하는 백업 스토리지가 증가합니다.\n",
            "데이터 전송 – DB 인스턴스에서 인터넷을 통한 데이터 송수신.\n",
            "\n",
            "Amazon RDS의 요금 정보는 Amazon RDS 제품 페이지의 요금 섹션을 참조하세요.\"\n",
            "Amazon RDS DB 인스턴스의 경우 언제부터 언제까지 사용한 요금이 청구되나요?\n",
            "DB 인스턴스를 사용할 수 있게 되면 즉시 DB 인스턴스에 대한 청구가 시작됩니다. 청구는 인스턴스가 삭제되거나 인스턴스 장애가 발생하여 DB 인스턴스가 종료될 때까지 계속됩니다.\n",
            "\"category : RDS, question : Amazon RDS DB 인스턴스의 경우 언제부터 언제까지 사용한 요금이 청구되나요?, answer : DB 인스턴스를 사용할 수 있게 되면 즉시 DB 인스턴스에 대한 청구가 시작됩니다. 청구는 인스턴스가 삭제되거나 인스턴스 장애가 발생하여 DB 인스턴스가 종료될 때까지 계속됩니다.\"\n",
            "청구 가능한 Amazon RDS 인스턴스 시간을 결정하는 기준은 무엇인가요?\n",
            "DB 인스턴스 시간은 DB 인스턴스가 가용 상태로 실행되고 있는 각 시간에 대해 비용이 청구됩니다. DB 인스턴스에 대한 요금이 더 이상 청구되지 않도록 하려면 추가 인스턴스 시간당 비용이 청구되지 않도록 DB 인스턴스를 중지하거나 삭제해야 합니다. 부분적으로 사용된 DB 인스턴스의 경우 DB 인스턴스 클래스 생성, 시작 또는 수정 같은 청구 가능한 상태 변경에 따라 1초 단위로 청구되며 최소 10분의 요금이 부과됩니다.\n",
            "\"category : RDS, question : 청구 가능한 Amazon RDS 인스턴스 시간을 결정하는 기준은 무엇인가요?, answer : DB 인스턴스 시간은 DB 인스턴스가 가용 상태로 실행되고 있는 각 시간에 대해 비용이 청구됩니다. DB 인스턴스에 대한 요금이 더 이상 청구되지 않도록 하려면 추가 인스턴스 시간당 비용이 청구되지 않도록 DB 인스턴스를 중지하거나 삭제해야 합니다. 부분적으로 사용된 DB 인스턴스의 경우 DB 인스턴스 클래스 생성, 시작 또는 수정 같은 청구 가능한 상태 변경에 따라 1초 단위로 청구되며 최소 10분의 요금이 부과됩니다.\"\n",
            "중지한 DB 인스턴스에 대한 비용은 어떻게 청구되나요?\n",
            "데이터베이스 인스턴스를 중지하면 DB 인스턴스 시간이 아니라 프로비저닝된 스토리지(프로비저닝된 IOPS 포함) 및 백업 스토리지(지정된 보존 기간 내의 수동 스냅샷 및 자동화된 백업 포함)에 대해 비용이 청구됩니다.\n",
            "\"category : RDS, question : 중지한 DB 인스턴스에 대한 비용은 어떻게 청구되나요?, answer : 데이터베이스 인스턴스를 중지하면 DB 인스턴스 시간이 아니라 프로비저닝된 스토리지(프로비저닝된 IOPS 포함) 및 백업 스토리지(지정된 보존 기간 내의 수동 스냅샷 및 자동화된 백업 포함)에 대해 비용이 청구됩니다.\"\n",
            "백업 스토리지 요금은 어떻게 청구되나요?\n",
            "계정에서 전체 리전에 프로비저닝한 데이터베이스 스토리지의 총량에 대해서는 백업 스토리지가 무료로 제공됩니다. 예를 들어 동일 리전과 동일 계정에서 MySQL DB 인스턴스에 월 100GB의 스토리지를 프로비저닝했고 PostgreSQL DB 인스턴스에 월 150GB의 스토리지를 프로비저닝한 경우 추가 요금 없이 250GB의 백업 스토리지가 이 계정과 리전에 제공됩니다. 이 양을 초과하는 백업 스토리지에 대한 요금만 부과됩니다.\n",
            "계정에서 리전에 프로비저닝한 데이터베이스 스토리지의 총량을 해당 리전의 총 백업 스토리지와 매일 비교하여 초과 백업 스토리지에 대한 요금만 부과합니다. 예를 들어 매일 정확히 10GB의 백업 스토리지가 초과된 경우 해당 월에 10GB-월의 백업 스토리지 요금이 부과됩니다. 또는 매일 300GB의 스토리지를 프로비저닝하고 15일 동안만 매일 500GB의 백업 스토리지를 프로비저닝한 경우 100GB-월의 백업 스토리지 요금만 부과됩니다(200GB-월 요금이 부과되지 않음). 요금이 일 단위(비례 배분)로 계산되고 한 달 내내 백업을 사용한 것이 아니기 때문입니다. 무료 백업 스토리지는 계정 및 리전별로 적용된다는 점을 참고하시기 바랍니다.\n",
            "백업의 크기는 인스턴스에 있는 데이터의 양과 직접적으로 비례합니다. 예를 들어 DB 인스턴스에 100GB의 스토리지를 프로비저닝하고 5GB의 데이터만 저장한 경우 첫 번째 백업은 100GB가 아니라 약 5GB가 됩니다. 후속 백업은 증분 백업이며 DB 인스턴스의 변경된 데이터만 저장됩니다. 참고로 백업 스토리지 크기는 RDS 콘솔이나 DescribeDBSnapshots API 응답에 표시되지 않습니다.\n",
            "\"category : RDS, question : 백업 스토리지 요금은 어떻게 청구되나요?, answer : 계정에서 전체 리전에 프로비저닝한 데이터베이스 스토리지의 총량에 대해서는 백업 스토리지가 무료로 제공됩니다. 예를 들어 동일 리전과 동일 계정에서 MySQL DB 인스턴스에 월 100GB의 스토리지를 프로비저닝했고 PostgreSQL DB 인스턴스에 월 150GB의 스토리지를 프로비저닝한 경우 추가 요금 없이 250GB의 백업 스토리지가 이 계정과 리전에 제공됩니다. 이 양을 초과하는 백업 스토리지에 대한 요금만 부과됩니다.\n",
            "계정에서 리전에 프로비저닝한 데이터베이스 스토리지의 총량을 해당 리전의 총 백업 스토리지와 매일 비교하여 초과 백업 스토리지에 대한 요금만 부과합니다. 예를 들어 매일 정확히 10GB의 백업 스토리지가 초과된 경우 해당 월에 10GB-월의 백업 스토리지 요금이 부과됩니다. 또는 매일 300GB의 스토리지를 프로비저닝하고 15일 동안만 매일 500GB의 백업 스토리지를 프로비저닝한 경우 100GB-월의 백업 스토리지 요금만 부과됩니다(200GB-월 요금이 부과되지 않음). 요금이 일 단위(비례 배분)로 계산되고 한 달 내내 백업을 사용한 것이 아니기 때문입니다. 무료 백업 스토리지는 계정 및 리전별로 적용된다는 점을 참고하시기 바랍니다.\n",
            "백업의 크기는 인스턴스에 있는 데이터의 양과 직접적으로 비례합니다. 예를 들어 DB 인스턴스에 100GB의 스토리지를 프로비저닝하고 5GB의 데이터만 저장한 경우 첫 번째 백업은 100GB가 아니라 약 5GB가 됩니다. 후속 백업은 증분 백업이며 DB 인스턴스의 변경된 데이터만 저장됩니다. 참고로 백업 스토리지 크기는 RDS 콘솔이나 DescribeDBSnapshots API 응답에 표시되지 않습니다.\"\n",
            "추가 백업 스토리지 비용이 할당된 DB 인스턴스 스토리지보다 높은 이유는 무엇인가요?\n",
            "기본 데이터 용도로 DB 인스턴스에 프로비저닝된 스토리지는 단일 가용 영역 내에 있습니다. 데이터베이스를 백업하면 백업 데이터(트랜잭션 로그 포함)는 여러 가용 영역에 지리적으로 이중화되어 복제됩니다. 따라서 데이터 내구성 수준이 훨씬 강화됩니다. 무료 할당량을 초과하는 백업 스토리지 가격에는 중요한 백업의 내구성을 극대화하기 위한 이 추가 복제 비용이 반영되어 있습니다.\n",
            "\"category : RDS, question : 추가 백업 스토리지 비용이 할당된 DB 인스턴스 스토리지보다 높은 이유는 무엇인가요?, answer : 기본 데이터 용도로 DB 인스턴스에 프로비저닝된 스토리지는 단일 가용 영역 내에 있습니다. 데이터베이스를 백업하면 백업 데이터(트랜잭션 로그 포함)는 여러 가용 영역에 지리적으로 이중화되어 복제됩니다. 따라서 데이터 내구성 수준이 훨씬 강화됩니다. 무료 할당량을 초과하는 백업 스토리지 가격에는 중요한 백업의 내구성을 극대화하기 위한 이 추가 복제 비용이 반영되어 있습니다.\"\n",
            "다중 AZ DB 인스턴스 배포 비용은 어떻게 청구됩니까?\n",
            "DB 인스턴스를 다중 AZ 배포로 지정하면 Amazon RDS 요금 페이지에 게시된 다중 AZ 요금에 따라 비용이 청구됩니다. 다중 AZ 비용은 다음에 따라 청구됩니다.\n",
            "\n",
            "다중 AZ DB 인스턴스 시간 - 사용한 DB 인스턴스의 클래스(예: db.t2.micro, db.m4.large) 기준 단일 가용 영역에서 표준 배포와 마찬가지로, 부분적으로 사용된 DB 인스턴스 시간의 경우 DB 인스턴스 클래스 생성, 시작 또는 수정 같은 청구 가능한 상태 변경에 따라 1초 단위로 청구되며 최소 10분의 요금이 부과됩니다. 한 시간 이내에 DB 인스턴스 배포를 표준과 다중 AZ 배포 간에 전환하는 경우 해당 시간에 대해 표준 배포 요금과 다중 AZ 배포 요금이 모두 청구됩니다.\n",
            "다중 AZ DB 인스턴스용으로 프로비저닝된 스토리지 – 한 시간 이내에 DB 인스턴스 배포를 표준과 다중 AZ 배포 간에 전환하는 경우 해당 시간에 대해 표준 배포 스토리지 요금과 다중 AZ 배포 스토리지 요금 중 높은 금액이 청구됩니다.\n",
            "월별 I/O 요청 – 사용자가 요청한 총 스토리지 I/O 요청 수. 다중 AZ 배포는 데이터베이스 읽기/쓰기 비율에 따라 표준 DB 인스턴스 배포보다 많은 수의 I/O 요청을 사용합니다. 데이터베이스 업데이트에 따른 쓰기 I/O 사용량은 Amazon RDS가 데이터를 예비 DB 인스턴스에 동기 복제하는 경우 2배가 됩니다. 읽기 I/O 사용량은 변하지 않습니다.\n",
            "백업 스토리지 – 백업 스토리지 사용량은 DB 인스턴스가 표준이든 다중 AZ 배포든 변하지 않습니다. 기본 DB 인스턴스에서 I/O가 일시적으로 중단되지 않도록 예비 복제본에서 백업이 수행됩니다.\n",
            "데이터 전송 – 기본 복제본과 예비 복제본 간에 데이터를 복제할 때 발생하는 데이터 전송에 대해서는 요금이 부과되지 않습니다. DB 인스턴스에서 송수신되는 인터넷 데이터 전송 요금은 표준 배포와 같은 요금이 청구됩니다.\n",
            "\"category : RDS, question : 다중 AZ DB 인스턴스 배포 비용은 어떻게 청구됩니까?, answer : DB 인스턴스를 다중 AZ 배포로 지정하면 Amazon RDS 요금 페이지에 게시된 다중 AZ 요금에 따라 비용이 청구됩니다. 다중 AZ 비용은 다음에 따라 청구됩니다.\n",
            "\n",
            "다중 AZ DB 인스턴스 시간 - 사용한 DB 인스턴스의 클래스(예: db.t2.micro, db.m4.large) 기준 단일 가용 영역에서 표준 배포와 마찬가지로, 부분적으로 사용된 DB 인스턴스 시간의 경우 DB 인스턴스 클래스 생성, 시작 또는 수정 같은 청구 가능한 상태 변경에 따라 1초 단위로 청구되며 최소 10분의 요금이 부과됩니다. 한 시간 이내에 DB 인스턴스 배포를 표준과 다중 AZ 배포 간에 전환하는 경우 해당 시간에 대해 표준 배포 요금과 다중 AZ 배포 요금이 모두 청구됩니다.\n",
            "다중 AZ DB 인스턴스용으로 프로비저닝된 스토리지 – 한 시간 이내에 DB 인스턴스 배포를 표준과 다중 AZ 배포 간에 전환하는 경우 해당 시간에 대해 표준 배포 스토리지 요금과 다중 AZ 배포 스토리지 요금 중 높은 금액이 청구됩니다.\n",
            "월별 I/O 요청 – 사용자가 요청한 총 스토리지 I/O 요청 수. 다중 AZ 배포는 데이터베이스 읽기/쓰기 비율에 따라 표준 DB 인스턴스 배포보다 많은 수의 I/O 요청을 사용합니다. 데이터베이스 업데이트에 따른 쓰기 I/O 사용량은 Amazon RDS가 데이터를 예비 DB 인스턴스에 동기 복제하는 경우 2배가 됩니다. 읽기 I/O 사용량은 변하지 않습니다.\n",
            "백업 스토리지 – 백업 스토리지 사용량은 DB 인스턴스가 표준이든 다중 AZ 배포든 변하지 않습니다. 기본 DB 인스턴스에서 I/O가 일시적으로 중단되지 않도록 예비 복제본에서 백업이 수행됩니다.\n",
            "데이터 전송 – 기본 복제본과 예비 복제본 간에 데이터를 복제할 때 발생하는 데이터 전송에 대해서는 요금이 부과되지 않습니다. DB 인스턴스에서 송수신되는 인터넷 데이터 전송 요금은 표준 배포와 같은 요금이 청구됩니다.\"\n",
            "요금에 세금이 포함되어 있습니까?\n",
            "달리 명시하지 않는 한 가격에는 VAT 및 해당 판매세를 포함한 관련 조세 공과가 포함되지 않습니다. 청구지 주소가 일본으로 되어 있는 고객의 경우 AWS 서비스 사용 시 일본 소비세의 적용을 받게 됩니다. 자세히 알아보세요.\n",
            "\"category : RDS, question : 요금에 세금이 포함되어 있습니까?, answer : 달리 명시하지 않는 한 가격에는 VAT 및 해당 판매세를 포함한 관련 조세 공과가 포함되지 않습니다. 청구지 주소가 일본으로 되어 있는 고객의 경우 AWS 서비스 사용 시 일본 소비세의 적용을 받게 됩니다. 자세히 알아보세요.\"\n",
            "Amazon RDS용 AWS 프리 티어는 어떤 혜택을 제공하나요?\n",
            "Amazon RDS용 AWS 프리 티어는 MySQL, MariaDB, PostgreSQL, SQL Server Express Edition을 실행하는 단일 AZ Micro DB 인스턴스를 무료로 사용할 수 있는 혜택을 제공합니다. 프리 티어는 월별 최대 750 인스턴스 시간까지 사용할 수 있습니다. 또한, 고객은 매달 20GB의 범용(SSD) 데이터베이스 스토리지와 20GB의 백업 스토리지를 무료로 제공받습니다.\n",
            "\"category : RDS, question : Amazon RDS용 AWS 프리 티어는 어떤 혜택을 제공하나요?, answer : Amazon RDS용 AWS 프리 티어는 MySQL, MariaDB, PostgreSQL, SQL Server Express Edition을 실행하는 단일 AZ Micro DB 인스턴스를 무료로 사용할 수 있는 혜택을 제공합니다. 프리 티어는 월별 최대 750 인스턴스 시간까지 사용할 수 있습니다. 또한, 고객은 매달 20GB의 범용(SSD) 데이터베이스 스토리지와 20GB의 백업 스토리지를 무료로 제공받습니다.\"\n",
            "Amazon RDS용 AWS 프리 티어를 사용할 수 있는 기간은 얼마나 되나요?\n",
            "AWS 계정을 새로 생성하면 12개월 동안 AWS 프리 티어 액세스 권한을 가지게 됩니다. 자세한 내용은 AWS 프리 티어 FAQ를 참조하세요.\n",
            "\"category : RDS, question : Amazon RDS용 AWS 프리 티어를 사용할 수 있는 기간은 얼마나 되나요?, answer : AWS 계정을 새로 생성하면 12개월 동안 AWS 프리 티어 액세스 권한을 가지게 됩니다. 자세한 내용은 AWS 프리 티어 FAQ를 참조하세요.\"\n",
            "Amazon RDS용 AWS 프리 티어에서 DB 인스턴스를 두 개 이상 실행할 수 있습니까?\n",
            "예. AWS 고객은 동시에 두 개 이상의 단일 AZ 마이크로 DB 인스턴스를 실행할 수 있으며 그 사용량은 Amazon RDS용 AWS 프리 티어 사용량 계산에 포함됩니다. 하지만 모든 Amazon RDS 단일 AZ 마이크로 DB 인스턴스 및 모든 해당되는 데이터베이스 엔진과 리전의 사용량을 합쳐 750 인스턴스 시간을 초과하는 경우, 초과 사용량에 대해서는 표준 Amazon RDS 요금이 부과됩니다.\n",
            "예를 들어 한 달 동안 두 개의 단일 AZ 마이크로 DB 인스턴스를 각각 400시간 실행한 경우 총 누적 사용량은 800 인스턴스 시간이며, 이 중 750시간은 무료로 제공됩니다. 나머지 50시간에 대해서는 표준 Amazon RDS 요금이 청구됩니다.\n",
            "\"category : RDS, question : Amazon RDS용 AWS 프리 티어에서 DB 인스턴스를 두 개 이상 실행할 수 있습니까?, answer : 예. AWS 고객은 동시에 두 개 이상의 단일 AZ 마이크로 DB 인스턴스를 실행할 수 있으며 그 사용량은 Amazon RDS용 AWS 프리 티어 사용량 계산에 포함됩니다. 하지만 모든 Amazon RDS 단일 AZ 마이크로 DB 인스턴스 및 모든 해당되는 데이터베이스 엔진과 리전의 사용량을 합쳐 750 인스턴스 시간을 초과하는 경우, 초과 사용량에 대해서는 표준 Amazon RDS 요금이 부과됩니다.\n",
            "예를 들어 한 달 동안 두 개의 단일 AZ 마이크로 DB 인스턴스를 각각 400시간 실행한 경우 총 누적 사용량은 800 인스턴스 시간이며, 이 중 750시간은 무료로 제공됩니다. 나머지 50시간에 대해서는 표준 Amazon RDS 요금이 청구됩니다.\"\n",
            "AWS 프리 티어를 통해 MySQL, MariaDB, PostgreSQL, SQL Server Micro DB 인스턴스 각각에 대해 750 인스턴스 시간이 제공되나요?\n",
            "아니요. AWS 프리 티어 액세스 권한이 있는 고객은 MySQL, PostgreSQL 또는 SQL Server Express 버전을 구동하는 마이크로 인스턴스 모두를 합쳐 최대 750 인스턴스 시간을 사용할 수 있습니다. 모든 Amazon RDS 단일 AZ Micro DB 인스턴스 및 모든 해당되는 데이터베이스 엔진과 리전의 사용량을 합쳐 750 인스턴스 시간을 초과하는 경우, 초과 사용량에 대해서는 표준 Amazon RDS 요금이 부과됩니다.\n",
            "\"category : RDS, question : AWS 프리 티어를 통해 MySQL, MariaDB, PostgreSQL, SQL Server Micro DB 인스턴스 각각에 대해 750 인스턴스 시간이 제공되나요?, answer : 아니요. AWS 프리 티어 액세스 권한이 있는 고객은 MySQL, PostgreSQL 또는 SQL Server Express 버전을 구동하는 마이크로 인스턴스 모두를 합쳐 최대 750 인스턴스 시간을 사용할 수 있습니다. 모든 Amazon RDS 단일 AZ Micro DB 인스턴스 및 모든 해당되는 데이터베이스 엔진과 리전의 사용량을 합쳐 750 인스턴스 시간을 초과하는 경우, 초과 사용량에 대해서는 표준 Amazon RDS 요금이 부과됩니다.\"\n",
            "인스턴스 시간 사용량이 프리 티어 혜택을 초과한 경우 요금은 어떻게 청구되나요?\n",
            "프리 티어에서 제공하는 인스턴스 시간을 초과하는 경우 표준 Amazon RDS 요금이 청구됩니다. 자세한 내용은 Amazon RDS 요금 페이지를 참조하세요.\n",
            "\"category : RDS, question : 인스턴스 시간 사용량이 프리 티어 혜택을 초과한 경우 요금은 어떻게 청구되나요?, answer : 프리 티어에서 제공하는 인스턴스 시간을 초과하는 경우 표준 Amazon RDS 요금이 청구됩니다. 자세한 내용은 Amazon RDS 요금 페이지를 참조하세요.\"\n",
            "예약형 인스턴스(RI)란 무엇입니까?\n",
            "Amazon RDS 예약형 인스턴스는 1년 또는 3년의 약정 기간에 DB 인스턴스를 예약할 수 있는 옵션을 제공하므로 온디맨드 인스턴스 요금보다 DB 인스턴스의 시간당 요금을 대폭 할인받을 수 있습니다. RI 결제 옵션에는 선결제 없음, 부분 선결제, 전체 선결제 등 3가지가 있으며 선결제 금액으로 실질 시간당 요금을 지불할 수 있습니다.\n",
            "\"category : RDS, question : 예약형 인스턴스(RI)란 무엇입니까?, answer : Amazon RDS 예약형 인스턴스는 1년 또는 3년의 약정 기간에 DB 인스턴스를 예약할 수 있는 옵션을 제공하므로 온디맨드 인스턴스 요금보다 DB 인스턴스의 시간당 요금을 대폭 할인받을 수 있습니다. RI 결제 옵션에는 선결제 없음, 부분 선결제, 전체 선결제 등 3가지가 있으며 선결제 금액으로 실질 시간당 요금을 지불할 수 있습니다.\"\n",
            "예약형 인스턴스는 온디맨드 DB 인스턴스와 어떻게 다른가요?\n",
            "예약형 인스턴스와 온디맨드 DB 인스턴스는 기능 면에서 전혀 차이가 없습니다. 유일한 차이점은 DB 인스턴스가 청구되는 방식입니다. 예약형 인스턴스에서는 1년 또는 3년 예약을 구매하고 해당 기간에 온디맨드 DB 인스턴스와 비교하여 더 낮은 실질 시간당 사용 요금을 지불하게 됩니다. 리전 내 예약 인스턴스를 구매하지 않는 한, 모든 DB 인스턴스는 온디맨드 시간당 요금으로 청구됩니다.\n",
            "\"category : RDS, question : 예약형 인스턴스는 온디맨드 DB 인스턴스와 어떻게 다른가요?, answer : 예약형 인스턴스와 온디맨드 DB 인스턴스는 기능 면에서 전혀 차이가 없습니다. 유일한 차이점은 DB 인스턴스가 청구되는 방식입니다. 예약형 인스턴스에서는 1년 또는 3년 예약을 구매하고 해당 기간에 온디맨드 DB 인스턴스와 비교하여 더 낮은 실질 시간당 사용 요금을 지불하게 됩니다. 리전 내 예약 인스턴스를 구매하지 않는 한, 모든 DB 인스턴스는 온디맨드 시간당 요금으로 청구됩니다.\"\n",
            "예약형 인스턴스는 어떻게 구입하고 생성할 수 있나요?\n",
            "Amazon RDS용 AWS Management Console의 ‘Reserved Instance(예약형 인스턴스)’ 섹션에서 예약형 인스턴스를 구매할 수 있습니다. 또는 Amazon RDS API 또는 AWS Command Line Interface를 사용하여 구매할 수 있는 예약형 인스턴스를 나열한 후, 원하는 DB 인스턴스 예약을 구매할 수도 있습니다.\n",
            "일단 예약 구매를 완료한 후에는 사용 면에서 예약 DB 인스턴스와 온디맨드 DB 인스턴스는 차이가 없습니다. 예약했던 것과 같은 인스턴스 클래스, 엔진 및 리전을 사용하여 DB 인스턴스를 시작합니다. 예약 구매가 활성화되어 있는 한, Amazon RDS는 해당하는 할인된 시간당 요금을 새 DB 인스턴스에 적용합니다.\n",
            "\"category : RDS, question : 예약형 인스턴스는 어떻게 구입하고 생성할 수 있나요?, answer : Amazon RDS용 AWS Management Console의 ‘Reserved Instance(예약형 인스턴스)’ 섹션에서 예약형 인스턴스를 구매할 수 있습니다. 또는 Amazon RDS API 또는 AWS Command Line Interface를 사용하여 구매할 수 있는 예약형 인스턴스를 나열한 후, 원하는 DB 인스턴스 예약을 구매할 수도 있습니다.\n",
            "일단 예약 구매를 완료한 후에는 사용 면에서 예약 DB 인스턴스와 온디맨드 DB 인스턴스는 차이가 없습니다. 예약했던 것과 같은 인스턴스 클래스, 엔진 및 리전을 사용하여 DB 인스턴스를 시작합니다. 예약 구매가 활성화되어 있는 한, Amazon RDS는 해당하는 할인된 시간당 요금을 새 DB 인스턴스에 적용합니다.\"\n",
            "예약형 인스턴스에 용량 예약이 포함됩니까?\n",
            "Amazon RDS 예약 인스턴스는 특정 가용 영역이 아니라 리전에 대해 구매하게 됩니다. RI가 한 가용 영역에 종속되지 않으므로 용량 예약은 아닙니다. 즉, 특정 가용 영역에 용량이 제한된 경우에도 해당 리전에서 예약 인스턴스를 구매할 수 있으며 할인은 해당 리전 내에서 사용량에 일치하는 모든 가용 영역에 적용됩니다.\n",
            "\"category : RDS, question : 예약형 인스턴스에 용량 예약이 포함됩니까?, answer : Amazon RDS 예약 인스턴스는 특정 가용 영역이 아니라 리전에 대해 구매하게 됩니다. RI가 한 가용 영역에 종속되지 않으므로 용량 예약은 아닙니다. 즉, 특정 가용 영역에 용량이 제한된 경우에도 해당 리전에서 예약 인스턴스를 구매할 수 있으며 할인은 해당 리전 내에서 사용량에 일치하는 모든 가용 영역에 적용됩니다.\"\n",
            "예약 인스턴스를 몇 개까지 구매할 수 있나요?\n",
            "예약 DB 인스턴스는 최대 40개까지 구매할 수 있습니다. 41개 이상의 DB 인스턴스를 실행하려면 Amazon RDS DB 인스턴스 요청 양식을 작성하세요.\n",
            "\"category : RDS, question : 예약 인스턴스를 몇 개까지 구매할 수 있나요?, answer : 예약 DB 인스턴스는 최대 40개까지 구매할 수 있습니다. 41개 이상의 DB 인스턴스를 실행하려면 Amazon RDS DB 인스턴스 요청 양식을 작성하세요.\"\n",
            "기존 DB 인스턴스에 예약형 인스턴스를 적용하려면 어떻게 해야 합니까?\n",
            "동일 리전 내에서 현재 실행 중이며 예약하려는 DB 인스턴스와 동일한 DB 인스턴스 클래스, DB 엔진, 다중 AZ 옵션 및 라이선스 모델의 DB 인스턴스 예약을 구매하면 됩니다. 예약 인스턴스를 구매하면 Amazon RDS가 기존 DB 인스턴스에 새로운 시간당 사용 요금을 자동으로 적용합니다.\n",
            "\"category : RDS, question : 기존 DB 인스턴스에 예약형 인스턴스를 적용하려면 어떻게 해야 합니까?, answer : 동일 리전 내에서 현재 실행 중이며 예약하려는 DB 인스턴스와 동일한 DB 인스턴스 클래스, DB 엔진, 다중 AZ 옵션 및 라이선스 모델의 DB 인스턴스 예약을 구매하면 됩니다. 예약 인스턴스를 구매하면 Amazon RDS가 기존 DB 인스턴스에 새로운 시간당 사용 요금을 자동으로 적용합니다.\"\n",
            "예약형 인스턴스에 가입하면 언제부터 기간이 시작됩니까? 기간이 종료되면 내 DB 인스턴스는 어떻게 되나요?\n",
            "지불 인증을 처리하는 동안 귀하의 요청을 받을 경우 예약형 인스턴스와 관련된 요금 변경 내용이 적용됩니다. AWS Account Activity 페이지, DescribeReservedDBInstances API 또는 describe-reserved-db-instances 명령을 사용하여 예약 상태를 확인할 수 있습니다. 다음 청구 기간에 일회성 결제가 승인되지 않는 경우는 할인 요금이 적용되지 않습니다.\n",
            "예약 기간이 지나면 예약 인스턴스는 DB 인스턴스 클래스와 리전에 해당하는 온디맨드 시간당 사용 요금으로 전환됩니다.\n",
            "\"category : RDS, question : 예약형 인스턴스에 가입하면 언제부터 기간이 시작됩니까? 기간이 종료되면 내 DB 인스턴스는 어떻게 되나요?, answer : 지불 인증을 처리하는 동안 귀하의 요청을 받을 경우 예약형 인스턴스와 관련된 요금 변경 내용이 적용됩니다. AWS Account Activity 페이지, DescribeReservedDBInstances API 또는 describe-reserved-db-instances 명령을 사용하여 예약 상태를 확인할 수 있습니다. 다음 청구 기간에 일회성 결제가 승인되지 않는 경우는 할인 요금이 적용되지 않습니다.\n",
            "예약 기간이 지나면 예약 인스턴스는 DB 인스턴스 클래스와 리전에 해당하는 온디맨드 시간당 사용 요금으로 전환됩니다.\"\n",
            "어느 DB 인스턴스에 예약형 인스턴스 요금을 적용할지 어떻게 관리합니까?\n",
            "Amazon RDS의 DB 인스턴스 생성, 수정 및 삭제 작업은 온디맨드 인스턴스와 예약 인스턴스에 차이가 없습니다. 요금을 계산할 때 시스템이 사용자의 예약 상태를 자동으로 적용하고 해당하는 모든 DB 인스턴스에 저렴한 시간당 예약 DB 인스턴스 요금을 청구합니다.\n",
            "\"category : RDS, question : 어느 DB 인스턴스에 예약형 인스턴스 요금을 적용할지 어떻게 관리합니까?, answer : Amazon RDS의 DB 인스턴스 생성, 수정 및 삭제 작업은 온디맨드 인스턴스와 예약 인스턴스에 차이가 없습니다. 요금을 계산할 때 시스템이 사용자의 예약 상태를 자동으로 적용하고 해당하는 모든 DB 인스턴스에 저렴한 시간당 예약 DB 인스턴스 요금을 청구합니다.\"\n",
            "DB 인스턴스 클래스를 확장하거나 축소하면 내 예약은 어떻게 됩니까?\n",
            "각 예약은 DB 엔진, DB 인스턴스 클래스, 다중 AZ 배포 옵션, 라이선스 모델 및 리전과 같은 속성과 관련이 있습니다.\n",
            "크기 유연성의 대상이 되는 DB 엔진 및 라이선스 모델(MySQL, MariaDB, PostgreSQL, Amazon Aurora 또는 Oracle ‘기존 보유 라이선스 사용’)에 대한 예약은 같은 데이터베이스 엔진 및 리전의 인스턴스 패밀리(M4, T2, R3 등) 내에서 실행 중인 모든 크기의 DB 인스턴스에 자동으로 적용됩니다. 또한, 예약은 단일 AZ 또는 다중 AZ 베포 옵션으로 실행되는 DB 인스턴스에도 적용됩니다.\n",
            "예를 들어 db.m4.2xlarge MySQL 예약을 구매했다고 가정해 보겠습니다. 실행 중인 DB 인스턴스를 db.m4.4xlarge로 확장하기로 하면, 이 RI의 할인율이 더 큰 DB 인스턴스 사용량의 1/2에 적용됩니다.\n",
            "크기 유연성의 대상이 아닌 DB 엔진이나 라이선스 모델(Microsoft SQL Server 또는 Oracle '라이선스 포함')을 실행 중인 경우에는 각 예약이 약정 기간 동안 같은 속성을 가진 DB 인스턴스에만 적용될 수 있습니다. 예약 기간이 끝나기 전에 실행 중인 DB 인스턴스의 이러한 속성을 수정하면, 해당 DB 인스턴스의 시간당 사용 요금이 온디맨드 시간당 사용 요금으로 전환됩니다.\n",
            "크기 유연성에 대한 자세한 내용은 Amazon RDS 사용 설명서를 참조하세요.\n",
            "\"category : RDS, question : DB 인스턴스 클래스를 확장하거나 축소하면 내 예약은 어떻게 됩니까?, answer : 각 예약은 DB 엔진, DB 인스턴스 클래스, 다중 AZ 배포 옵션, 라이선스 모델 및 리전과 같은 속성과 관련이 있습니다.\n",
            "크기 유연성의 대상이 되는 DB 엔진 및 라이선스 모델(MySQL, MariaDB, PostgreSQL, Amazon Aurora 또는 Oracle ‘기존 보유 라이선스 사용’)에 대한 예약은 같은 데이터베이스 엔진 및 리전의 인스턴스 패밀리(M4, T2, R3 등) 내에서 실행 중인 모든 크기의 DB 인스턴스에 자동으로 적용됩니다. 또한, 예약은 단일 AZ 또는 다중 AZ 베포 옵션으로 실행되는 DB 인스턴스에도 적용됩니다.\n",
            "예를 들어 db.m4.2xlarge MySQL 예약을 구매했다고 가정해 보겠습니다. 실행 중인 DB 인스턴스를 db.m4.4xlarge로 확장하기로 하면, 이 RI의 할인율이 더 큰 DB 인스턴스 사용량의 1/2에 적용됩니다.\n",
            "크기 유연성의 대상이 아닌 DB 엔진이나 라이선스 모델(Microsoft SQL Server 또는 Oracle '라이선스 포함')을 실행 중인 경우에는 각 예약이 약정 기간 동안 같은 속성을 가진 DB 인스턴스에만 적용될 수 있습니다. 예약 기간이 끝나기 전에 실행 중인 DB 인스턴스의 이러한 속성을 수정하면, 해당 DB 인스턴스의 시간당 사용 요금이 온디맨드 시간당 사용 요금으로 전환됩니다.\n",
            "크기 유연성에 대한 자세한 내용은 Amazon RDS 사용 설명서를 참조하세요.\"\n",
            "한 리전 또는 가용 영역에서 다른 리전 또는 가용 영역으로 예약형 인스턴스를 옮길 수 있습니까?\n",
            "각 예약 인스턴스는 특정 리전과 연결되어 있으며 이는 예약의 수명 기간 동안 고정되어 변경할 수 없습니다. 그러나 각 예약은 연결된 리전 내의 모든 가용 영역에서 사용할 수 있습니다.\n",
            "\"category : RDS, question : 한 리전 또는 가용 영역에서 다른 리전 또는 가용 영역으로 예약형 인스턴스를 옮길 수 있습니까?, answer : 각 예약 인스턴스는 특정 리전과 연결되어 있으며 이는 예약의 수명 기간 동안 고정되어 변경할 수 없습니다. 그러나 각 예약은 연결된 리전 내의 모든 가용 영역에서 사용할 수 있습니다.\"\n",
            "예약형 인스턴스를 다중 AZ 배포에 사용할 수 있습니까?\n",
            "예. 예약 인스턴스를 구매할 때 DB 인스턴스 구성에서 다중 AZ 옵션을 선택하여 구매할 수 있습니다. 그뿐 아니라 예약형 인스턴스 크기의 유연성을 지원하는 DB 엔진 및 라이선스 모델을 사용하는 경우 다중 AZ 예약형 인스턴스는 두 개의 단일 AZ DB 인스턴스를 사용할 수 있습니다.\n",
            "\"category : RDS, question : 예약형 인스턴스를 다중 AZ 배포에 사용할 수 있습니까?, answer : 예. 예약 인스턴스를 구매할 때 DB 인스턴스 구성에서 다중 AZ 옵션을 선택하여 구매할 수 있습니다. 그뿐 아니라 예약형 인스턴스 크기의 유연성을 지원하는 DB 엔진 및 라이선스 모델을 사용하는 경우 다중 AZ 예약형 인스턴스는 두 개의 단일 AZ DB 인스턴스를 사용할 수 있습니다.\"\n",
            "예약형 인스턴스를 읽기 복제본에 사용할 수 있습니까?\n",
            "DB 인스턴스 예약은 DB 인스턴스 클래스와 리전이 동일한 경우에 한해 읽기 전용 복제본에 사용할 수 있습니다. 요금을 계산할 때 시스템이 사용자의 예약 상태를 자동으로 적용하고 해당하는 모든 DB 인스턴스에 저렴한 시간당 예약 인스턴스 요금을 청구합니다.\n",
            "\"category : RDS, question : 예약형 인스턴스를 읽기 복제본에 사용할 수 있습니까?, answer : DB 인스턴스 예약은 DB 인스턴스 클래스와 리전이 동일한 경우에 한해 읽기 전용 복제본에 사용할 수 있습니다. 요금을 계산할 때 시스템이 사용자의 예약 상태를 자동으로 적용하고 해당하는 모든 DB 인스턴스에 저렴한 시간당 예약 인스턴스 요금을 청구합니다.\"\n",
            "예약을 취소할 수 있습니까?\n",
            "아니요. 예약 DB 인스턴스는 취소할 수 없으며 일회성 요금(해당하는 경우)은 환불되지 않습니다. 예약 DB 인스턴스 기간에는 사용량과 관계없이 시간당 요금을 계속해서 지불하게 됩니다.\n",
            "\"category : RDS, question : 예약을 취소할 수 있습니까?, answer : 아니요. 예약 DB 인스턴스는 취소할 수 없으며 일회성 요금(해당하는 경우)은 환불되지 않습니다. 예약 DB 인스턴스 기간에는 사용량과 관계없이 시간당 요금을 계속해서 지불하게 됩니다.\"\n",
            "결제 옵션은 청구서에 어떤 영향을 미칩니까?\n",
            "전체 선결제 옵션으로 RI를 구매하는 경우 RI의 전체 계약 기간에 대한 요금을 한 번에 선결제하게 됩니다. 선결제 없음 옵션을 선택하면 선결제 금액을 지불하지 않습니다. 이 경우 RI의 전체 요금은 계약 기간 내 각 시간으로 분배되며 사용량에 관계없이 기간 내 시간에 대해 요금이 청구됩니다. 부분 선결제 옵션은 전체 선결제와 선결제 없음 옵션에 대한 하이브리드 옵션입니다. 소정의 선결제 금액을 지불하고 사용량에 관계없이 기간 내 시간에 대해 저렴한 시간당 요금을 지불하는 옵션입니다.\n",
            "\"category : RDS, question : 결제 옵션은 청구서에 어떤 영향을 미칩니까?, answer : 전체 선결제 옵션으로 RI를 구매하는 경우 RI의 전체 계약 기간에 대한 요금을 한 번에 선결제하게 됩니다. 선결제 없음 옵션을 선택하면 선결제 금액을 지불하지 않습니다. 이 경우 RI의 전체 요금은 계약 기간 내 각 시간으로 분배되며 사용량에 관계없이 기간 내 시간에 대해 요금이 청구됩니다. 부분 선결제 옵션은 전체 선결제와 선결제 없음 옵션에 대한 하이브리드 옵션입니다. 소정의 선결제 금액을 지불하고 사용량에 관계없이 기간 내 시간에 대해 저렴한 시간당 요금을 지불하는 옵션입니다.\"\n",
            "내 요구에 적합한 초기 DB 인스턴스 클래스 및 스토리지 용량을 어떻게 결정합니까?\n",
            "초기 DB 인스턴스 클래스와 스토리지 용량을 선택하려면 애플리케이션의 컴퓨팅, 메모리 및 스토리지 요구 사항을 확인하세요. 사용 가능한 DB 인스턴스 클래스에 대한 정보는 Amazon RDS 사용 설명서를 참조하세요.\n",
            "\"category : RDS, question : 내 요구에 적합한 초기 DB 인스턴스 클래스 및 스토리지 용량을 어떻게 결정합니까?, answer : 초기 DB 인스턴스 클래스와 스토리지 용량을 선택하려면 애플리케이션의 컴퓨팅, 메모리 및 스토리지 요구 사항을 확인하세요. 사용 가능한 DB 인스턴스 클래스에 대한 정보는 Amazon RDS 사용 설명서를 참조하세요.\"\n",
            "내 Amazon RDS 데이터베이스 인스턴스에 연결된 컴퓨팅 리소스 및/또는 스토리지 용량을 확장하려면 어떻게 해야 하나요?\n",
            "AWS Management Console(원하는 DB 인스턴스를 선택하고 Modify(수정) 버튼 클릭), Amazon RDS API 또는 AWS Command Line Interface를 통해 DB 인스턴스에 할당된 컴퓨팅 리소스와 스토리지 용량을 조정할 수 있습니다. 메모리와 CPU 리소스는 DB 인스턴스 클래스를 변경하여 수정하고, 사용 가능한 스토리지는 스토리지 할당을 수정하면 변경됩니다. \n",
            "DB 인스턴스 클래스 또는 할당된 스토리지를 수정하면 지정된 유지 관리 기간에 요청된 변경 내용이 적용된다는 점에 유의하십시오. 또는 “apply-immediately” 플래그를 사용하여 확장 요청을 즉시 적용할 수 있습니다. 다른 처리되지 않은 시스템 변경 내용도 적용됩니다.\n",
            "일부 구형 SQL Server용 RDS 인스턴스는 확장된 스토리지에 적합하지 않을 수 있습니다. 자세한 내용은 RDS for SQL Server FAQ를 참조하세요.\n",
            "\"category : RDS, question : 내 Amazon RDS 데이터베이스 인스턴스에 연결된 컴퓨팅 리소스 및/또는 스토리지 용량을 확장하려면 어떻게 해야 하나요?, answer : AWS Management Console(원하는 DB 인스턴스를 선택하고 Modify(수정) 버튼 클릭), Amazon RDS API 또는 AWS Command Line Interface를 통해 DB 인스턴스에 할당된 컴퓨팅 리소스와 스토리지 용량을 조정할 수 있습니다. 메모리와 CPU 리소스는 DB 인스턴스 클래스를 변경하여 수정하고, 사용 가능한 스토리지는 스토리지 할당을 수정하면 변경됩니다. \n",
            "DB 인스턴스 클래스 또는 할당된 스토리지를 수정하면 지정된 유지 관리 기간에 요청된 변경 내용이 적용된다는 점에 유의하십시오. 또는 “apply-immediately” 플래그를 사용하여 확장 요청을 즉시 적용할 수 있습니다. 다른 처리되지 않은 시스템 변경 내용도 적용됩니다.\n",
            "일부 구형 SQL Server용 RDS 인스턴스는 확장된 스토리지에 적합하지 않을 수 있습니다. 자세한 내용은 RDS for SQL Server FAQ를 참조하세요.\"\n",
            "Amazon RDS 스토리지의 하드웨어는 어떻게 구성되어 있습니까?\n",
            "Amazon RDS는 데이터베이스와 로그를 저장하는 데 EBS 볼륨을 사용합니다. Amazon RDS는 필요한 스토리지의 크기에 따라 자동으로 데이터를 여러 EBS 볼륨에 나누어 저장하여 IOPS 성능을 강화합니다. MySQL 및 Oracle의 기존 DB 인스턴스의 경우 스토리지를 스케일 업하면 I/O 용량이 일부 개선될 수 있습니다. AWS Management Console, ModifyDBInstance API 또는 modify-db-instance 명령을 사용하여 DB 인스턴스에 할당된 스토리지 용량을 조정할 수 있습니다.\n",
            "자세한 내용은 Amazon RDS용 스토리지를 참조하세요.\n",
            "\"category : RDS, question : Amazon RDS 스토리지의 하드웨어는 어떻게 구성되어 있습니까?, answer : Amazon RDS는 데이터베이스와 로그를 저장하는 데 EBS 볼륨을 사용합니다. Amazon RDS는 필요한 스토리지의 크기에 따라 자동으로 데이터를 여러 EBS 볼륨에 나누어 저장하여 IOPS 성능을 강화합니다. MySQL 및 Oracle의 기존 DB 인스턴스의 경우 스토리지를 스케일 업하면 I/O 용량이 일부 개선될 수 있습니다. AWS Management Console, ModifyDBInstance API 또는 modify-db-instance 명령을 사용하여 DB 인스턴스에 할당된 스토리지 용량을 조정할 수 있습니다.\n",
            "자세한 내용은 Amazon RDS용 스토리지를 참조하세요.\"\n",
            "DB 인스턴스는 확장 중에 사용할 수 있습니까?\n",
            "DB 인스턴스 가용성을 유지하면서 DB 인스턴스에 할당된 스토리지 용량을 늘릴 수 있습니다. 그러나 DB 인스턴스에서 사용하는 컴퓨팅 리소스를 스케일 업하거나 스케일 다운할 경우 DB 인스턴스 클래스를 수정하는 동안 데이터베이스를 일시적으로 사용할 수 없게 됩니다. 사용이 불가능한 시간은 일반적으로 몇 분 정도이며, 수정 사항을 즉시 적용하도록 지정하지 않은 이상, DB 인스턴스의 유지 관리 기간에 수정 사항이 적용됩니다.\n",
            "\"category : RDS, question : DB 인스턴스는 확장 중에 사용할 수 있습니까?, answer : DB 인스턴스 가용성을 유지하면서 DB 인스턴스에 할당된 스토리지 용량을 늘릴 수 있습니다. 그러나 DB 인스턴스에서 사용하는 컴퓨팅 리소스를 스케일 업하거나 스케일 다운할 경우 DB 인스턴스 클래스를 수정하는 동안 데이터베이스를 일시적으로 사용할 수 없게 됩니다. 사용이 불가능한 시간은 일반적으로 몇 분 정도이며, 수정 사항을 즉시 적용하도록 지정하지 않은 이상, DB 인스턴스의 유지 관리 기간에 수정 사항이 적용됩니다.\"\n",
            "최대 DB 인스턴스 클래스와 최대 스토리지 용량을 초과하여 DB 인스턴스를 어떻게 확장할 수 있습니까?\n",
            "Amazon RDS는 여러 애플리케이션 요구를 충족하기 위해 다양한 DB 인스턴스 클래스와 스토리지 할당을 지원합니다. 애플리케이션에 최대 DB 인스턴스 클래스보다 더 많은 컴퓨팅 리소스가 필요하거나 최대 할당보다 더 많은 스토리지가 필요할 경우, 파티셔닝을 구현하여 여러 DB 인스턴스에 데이터를 분산할 수 있습니다.\n",
            "\"category : RDS, question : 최대 DB 인스턴스 클래스와 최대 스토리지 용량을 초과하여 DB 인스턴스를 어떻게 확장할 수 있습니까?, answer : Amazon RDS는 여러 애플리케이션 요구를 충족하기 위해 다양한 DB 인스턴스 클래스와 스토리지 할당을 지원합니다. 애플리케이션에 최대 DB 인스턴스 클래스보다 더 많은 컴퓨팅 리소스가 필요하거나 최대 할당보다 더 많은 스토리지가 필요할 경우, 파티셔닝을 구현하여 여러 DB 인스턴스에 데이터를 분산할 수 있습니다.\"\n",
            "Amazon RDS의 범용(SSD) 스토리지란 무엇입니까?\n",
            "Amazon RDS의 범용(SSD) 스토리지는 I/O 요구 수준이 보통인 광범위한 데이터베이스 워크로드에 적합합니다. 기준 성능이 IOPS 3회/GB이고 순간 최고 성능이 IPOS 3,000회인 이 스토리지 옵션은 대다수 애플리케이션의 요구 사항을 충족하는 예측 가능한 성능을 제공합니다.\n",
            "\"category : RDS, question : Amazon RDS의 범용(SSD) 스토리지란 무엇입니까?, answer : Amazon RDS의 범용(SSD) 스토리지는 I/O 요구 수준이 보통인 광범위한 데이터베이스 워크로드에 적합합니다. 기준 성능이 IOPS 3회/GB이고 순간 최고 성능이 IPOS 3,000회인 이 스토리지 옵션은 대다수 애플리케이션의 요구 사항을 충족하는 예측 가능한 성능을 제공합니다.\"\n",
            "Amazon RDS의 프로비저닝된 IOPS(SSD) 스토리지란 무엇입니까?\n",
            "Amazon RDS의 프로비저닝된 IOPS(SSD) 스토리지는 빠르고 예측 가능하며 일관성 있는 I/O 성능을 제공하기 위해 설계된 SSD 지원 스토리지 옵션입니다. Amazon RDS의 프로비저닝된 IOPS(SSD) 스토리지의 경우, DB 인스턴스 생성 시 IOPS 속도를 지정하면 Amazon RDS에서는 DB 인스턴스의 수명 기간에 해당 IOPS 속도를 프로비저닝합니다. Amazon RDS의 프로비저닝된 IOPS(SSD) 스토리지는 I/O 집중형 트랜잭션(OLTP) 데이터베이스 워크로드를 위해 최적화되어 있습니다. 자세한 내용은 Amazon RDS 사용 설명서를 참조하세요.\n",
            "\"category : RDS, question : Amazon RDS의 프로비저닝된 IOPS(SSD) 스토리지란 무엇입니까?, answer : Amazon RDS의 프로비저닝된 IOPS(SSD) 스토리지는 빠르고 예측 가능하며 일관성 있는 I/O 성능을 제공하기 위해 설계된 SSD 지원 스토리지 옵션입니다. Amazon RDS의 프로비저닝된 IOPS(SSD) 스토리지의 경우, DB 인스턴스 생성 시 IOPS 속도를 지정하면 Amazon RDS에서는 DB 인스턴스의 수명 기간에 해당 IOPS 속도를 프로비저닝합니다. Amazon RDS의 프로비저닝된 IOPS(SSD) 스토리지는 I/O 집중형 트랜잭션(OLTP) 데이터베이스 워크로드를 위해 최적화되어 있습니다. 자세한 내용은 Amazon RDS 사용 설명서를 참조하세요.\"\n",
            "Amazon RDS 마그네틱 스토리지란 무엇입니까?\n",
            "Amazon RDS 마그네틱 스토리지는 데이터에 대한 액세스 빈도가 낮은 소규모 데이터베이스 워크로드에 유용합니다. 마그네틱 스토리지는 프로덕션 데이터베이스 인스턴스용으로는 권장되지 않습니다.\n",
            "\"category : RDS, question : Amazon RDS 마그네틱 스토리지란 무엇입니까?, answer : Amazon RDS 마그네틱 스토리지는 데이터에 대한 액세스 빈도가 낮은 소규모 데이터베이스 워크로드에 유용합니다. 마그네틱 스토리지는 프로덕션 데이터베이스 인스턴스용으로는 권장되지 않습니다.\"\n",
            "Amazon RDS 스토리지 유형을 선택하려면 어떻게 해야 합니까?\n",
            "워크로드에 가장 적합한 스토리지 유형을 선택합니다.\n",
            "\n",
            "고성능 OLTP 워크로드: Amazon RDS의 프로비저닝된 IOPS(SSD) 스토리지\n",
            "I/O 요구 사항이 보통인 데이터베이스 워크로드: Amazon RDS의 범용(SSD) 스토리지\n",
            "\"category : RDS, question : Amazon RDS 스토리지 유형을 선택하려면 어떻게 해야 합니까?, answer : 워크로드에 가장 적합한 스토리지 유형을 선택합니다.\n",
            "\n",
            "고성능 OLTP 워크로드: Amazon RDS의 프로비저닝된 IOPS(SSD) 스토리지\n",
            "I/O 요구 사항이 보통인 데이터베이스 워크로드: Amazon RDS의 범용(SSD) 스토리지\"\n",
            "Amazon RDS가 지원하는 최대 및 최소 IOPS는 무엇입니까?\n",
            "Amazon RDS가 지원하는 IOPS는 데이터베이스 엔진에 따라 다릅니다. 자세한 내용은 Amazon RDS 사용 설명서를 참조하세요.\n",
            "\"category : RDS, question : Amazon RDS가 지원하는 최대 및 최소 IOPS는 무엇입니까?, answer : Amazon RDS가 지원하는 IOPS는 데이터베이스 엔진에 따라 다릅니다. 자세한 내용은 Amazon RDS 사용 설명서를 참조하세요.\"\n",
            "자동 백업과 DB 스냅샷의 차이점은 무엇입니까?\n",
            "Amazon RDS는 DB 인스턴스 백업 및 복구를 위한 두 가지 방법, 즉 자동 백업 및 데이터베이스 스냅샷(DB 스냅샷)을 제공합니다.\n",
            "Amazon RDS의 자동 백업 기능을 사용해 DB 인스턴스를 특정 시점으로 복구할 수 있습니다. DB 인스턴스의 자동 백업을 활성화하면 Amazon RDS가 매일 자동으로 데이터에 대한 전체 스냅샷을 만들고(기본 백업 기간 내에), 트랜잭션 로그를 캡처(DB 인스턴스를 업데이트할 때)합니다. 특정 시점으로 복구를 시작할 때, DB 인스턴스를 사용자가 요청한 특정 시점으로 복구하기 위해 가장 적합한 일일 백업에 트랜잭션 로그가 적용됩니다. \n",
            "Amazon RDS는 사용자가 지정한 일정 기간(보존 기간) 동안 DB 인스턴스의 백업을 보관합니다. 보존 기관은 기본적으로 1일이지만 최대 35일까지 설정할 수 있습니다. 특정 시점으로 복구를 시작할 수 있으며, 복구 가능한 최근 시간까지 보존 기간을 초 단위로 지정할 수 있습니다. DescribeDBInstances API를 사용하여 DB 인스턴스의 복구 가능한 최근 시간(일반적으로 최근 5분)으로 돌아갈 수 있습니다. \n",
            "또는 AWS Management Console에서 DB 인스턴스를 선택하고 콘솔의 아래쪽 창에 있는 ‘Description(설명)’ 탭에서 복구 가능한 최근 시간을 찾을 수 있습니다.\n",
            "사용자가 시작하는 DB 스냅샷에서는 원하는 빈도로 DB 인스턴스를 일관되게 백업한 다음 언제든지 해당 상태로 복구할 수 있습니다. DB 스냅샷은 AWS Management Console, CreateDBSnapshot API 또는 create-db-snapshot 명령을 사용하여 생성할 수 있으며 사용자가 명시적으로 삭제할 때까지 유지됩니다.\n",
            "사용자는 Amazon RDS가 자동 백업을 지원하기 위해 수행하는 스냅샷을 복사(AWS Console 또는 copy-db-snapshot 명령 사용)하거나 스냅샷 복구 기능을 사용할 수 있습니다. 스냅샷은 \"automated\" 스냅샷 유형을 사용하여 찾을 수 있습니다. 또 \"Snapshot Created Time\" 필드에서 스냅샷이 촬영된 시간을 확인할 수 있습니다. \n",
            "또는 \"automated\" 스냅샷 식별자에도 스냅샷이 만들어진 시간(단위: UTC)이 포함되어 있습니다.\n",
            "참고: 특정 시점으로 복구 또는 DB 스냅샷에서 복구 작업을 수행하면 새로운 엔드포인트를 가지는 새 DB 인스턴스가 생성됩니다(필요한 경우 기존 DB 인스턴스를 삭제할 수 있음). 이 작업을 완료하면 특정 DB 스냅샷 또는 시점에서 여러 DB 인스턴스를 만들 수 있습니다.\n",
            "\"category : RDS, question : 자동 백업과 DB 스냅샷의 차이점은 무엇입니까?, answer : Amazon RDS는 DB 인스턴스 백업 및 복구를 위한 두 가지 방법, 즉 자동 백업 및 데이터베이스 스냅샷(DB 스냅샷)을 제공합니다.\n",
            "Amazon RDS의 자동 백업 기능을 사용해 DB 인스턴스를 특정 시점으로 복구할 수 있습니다. DB 인스턴스의 자동 백업을 활성화하면 Amazon RDS가 매일 자동으로 데이터에 대한 전체 스냅샷을 만들고(기본 백업 기간 내에), 트랜잭션 로그를 캡처(DB 인스턴스를 업데이트할 때)합니다. 특정 시점으로 복구를 시작할 때, DB 인스턴스를 사용자가 요청한 특정 시점으로 복구하기 위해 가장 적합한 일일 백업에 트랜잭션 로그가 적용됩니다. \n",
            "Amazon RDS는 사용자가 지정한 일정 기간(보존 기간) 동안 DB 인스턴스의 백업을 보관합니다. 보존 기관은 기본적으로 1일이지만 최대 35일까지 설정할 수 있습니다. 특정 시점으로 복구를 시작할 수 있으며, 복구 가능한 최근 시간까지 보존 기간을 초 단위로 지정할 수 있습니다. DescribeDBInstances API를 사용하여 DB 인스턴스의 복구 가능한 최근 시간(일반적으로 최근 5분)으로 돌아갈 수 있습니다. \n",
            "또는 AWS Management Console에서 DB 인스턴스를 선택하고 콘솔의 아래쪽 창에 있는 ‘Description(설명)’ 탭에서 복구 가능한 최근 시간을 찾을 수 있습니다.\n",
            "사용자가 시작하는 DB 스냅샷에서는 원하는 빈도로 DB 인스턴스를 일관되게 백업한 다음 언제든지 해당 상태로 복구할 수 있습니다. DB 스냅샷은 AWS Management Console, CreateDBSnapshot API 또는 create-db-snapshot 명령을 사용하여 생성할 수 있으며 사용자가 명시적으로 삭제할 때까지 유지됩니다.\n",
            "사용자는 Amazon RDS가 자동 백업을 지원하기 위해 수행하는 스냅샷을 복사(AWS Console 또는 copy-db-snapshot 명령 사용)하거나 스냅샷 복구 기능을 사용할 수 있습니다. 스냅샷은 \"automated\" 스냅샷 유형을 사용하여 찾을 수 있습니다. 또 \"Snapshot Created Time\" 필드에서 스냅샷이 촬영된 시간을 확인할 수 있습니다. \n",
            "또는 \"automated\" 스냅샷 식별자에도 스냅샷이 만들어진 시간(단위: UTC)이 포함되어 있습니다.\n",
            "참고: 특정 시점으로 복구 또는 DB 스냅샷에서 복구 작업을 수행하면 새로운 엔드포인트를 가지는 새 DB 인스턴스가 생성됩니다(필요한 경우 기존 DB 인스턴스를 삭제할 수 있음). 이 작업을 완료하면 특정 DB 스냅샷 또는 시점에서 여러 DB 인스턴스를 만들 수 있습니다.\"\n",
            "DB 인스턴스 백업을 활성화해야 합니까, 아니면 자동으로 수행됩니까?\n",
            "기본적으로 Amazon RDS는 DB 인스턴스를 자동으로 백업합니다. 이때 보존 기간은 7일입니다. 백업 보존 기간을 수정하려면 RDS 콘솔이나 CreateDBInstance API(새 DB 인스턴스 생성 시) 또는 ModifyDBInstance API(기존 인스턴스의 경우)를 사용하면 됩니다. 이러한 메서드를 사용하여 RetentionPeriod 파라미터를 0(자동 백업 비활성화)부터 원하는 일수(최대 35일) 사이의 임의 숫자로 변경할 수 있습니다. DB 인스턴스가 읽기 전용 복제본에 대한 소스인 경우 값을 0으로 설정할 수 없습니다. 자동 백업에 대한 자세한 내용은 Amazon RDS 사용 설명서를 참조하세요.\n",
            "\"category : RDS, question : DB 인스턴스 백업을 활성화해야 합니까, 아니면 자동으로 수행됩니까?, answer : 기본적으로 Amazon RDS는 DB 인스턴스를 자동으로 백업합니다. 이때 보존 기간은 7일입니다. 백업 보존 기간을 수정하려면 RDS 콘솔이나 CreateDBInstance API(새 DB 인스턴스 생성 시) 또는 ModifyDBInstance API(기존 인스턴스의 경우)를 사용하면 됩니다. 이러한 메서드를 사용하여 RetentionPeriod 파라미터를 0(자동 백업 비활성화)부터 원하는 일수(최대 35일) 사이의 임의 숫자로 변경할 수 있습니다. DB 인스턴스가 읽기 전용 복제본에 대한 소스인 경우 값을 0으로 설정할 수 없습니다. 자동 백업에 대한 자세한 내용은 Amazon RDS 사용 설명서를 참조하세요.\"\n",
            "백업 기간은 무엇이고 왜 필요합니까? 백업 기간에 데이터베이스를 사용할 수 있습니까?\n",
            "기본 백업 기간은 DB 인스턴스를 백업하기 위해 사용자가 지정하는 기간입니다. Amazon RDS는 이러한 정기 데이터 백업과 트랜잭션 로그를 함께 사용하여 최대 복구 가능한 최근 시간(일반적으로 최근 몇 분)까지 DB 인스턴스를 보존 기간 중 원하는 시점(단위: 초)으로 복구할 수 있도록 지원합니다. 백업 기간 중에 백업 프로세스가 시작될 때 스토리지 I/O가 일시적으로 중단(일반적으로 몇 초 미만)될 수 있으며, 일시적으로 지연 시간이 증가하는 것을 경험할 수도 있습니다. 다중 AZ DB 배포를 사용하면 백업이 예비 복제본에서 수행되므로, I/O가 중단되지 않습니다.\n",
            "\"category : RDS, question : 백업 기간은 무엇이고 왜 필요합니까? 백업 기간에 데이터베이스를 사용할 수 있습니까?, answer : 기본 백업 기간은 DB 인스턴스를 백업하기 위해 사용자가 지정하는 기간입니다. Amazon RDS는 이러한 정기 데이터 백업과 트랜잭션 로그를 함께 사용하여 최대 복구 가능한 최근 시간(일반적으로 최근 몇 분)까지 DB 인스턴스를 보존 기간 중 원하는 시점(단위: 초)으로 복구할 수 있도록 지원합니다. 백업 기간 중에 백업 프로세스가 시작될 때 스토리지 I/O가 일시적으로 중단(일반적으로 몇 초 미만)될 수 있으며, 일시적으로 지연 시간이 증가하는 것을 경험할 수도 있습니다. 다중 AZ DB 배포를 사용하면 백업이 예비 복제본에서 수행되므로, I/O가 중단되지 않습니다.\"\n",
            "자동 백업과 DB 스냅샷은 어디에 저장되며 어떻게 보존하면 됩니까?\n",
            "Amazon RDS DB 스냅샷과 자동 백업은 S3에 저장됩니다.\n",
            "AWS Management Console, ModifyDBInstance API 또는 modify-db-instance 명령을 사용해 RetentionPeriod 파라미터를 수정하여 자동 백업이 보관되는 기간을 관리할 수 있습니다. 자동 백업을 완전히 비활성화하려는 경우 보존 기간을 0으로 설정하면 됩니다(권장하지 않음). Amazon RDS 콘솔의 ‘Snapshots(스냅샷)’ 섹션을 사용하여 사용자가 만든 DB 스냅샷을 관리할 수 있습니다. DescribeDBSnapshots API 또는 describe-db-snapshots 명령을 사용하여 특정 DB 인스턴스에 대해 사용자가 작성한 DB 스냅샷 목록을 확인하고, DeleteDBSnapshot API 또는 delete-db-snapshot 명령을 사용해 스냅샷을 삭제할 수 있습니다.\n",
            "\"category : RDS, question : 자동 백업과 DB 스냅샷은 어디에 저장되며 어떻게 보존하면 됩니까?, answer : Amazon RDS DB 스냅샷과 자동 백업은 S3에 저장됩니다.\n",
            "AWS Management Console, ModifyDBInstance API 또는 modify-db-instance 명령을 사용해 RetentionPeriod 파라미터를 수정하여 자동 백업이 보관되는 기간을 관리할 수 있습니다. 자동 백업을 완전히 비활성화하려는 경우 보존 기간을 0으로 설정하면 됩니다(권장하지 않음). Amazon RDS 콘솔의 ‘Snapshots(스냅샷)’ 섹션을 사용하여 사용자가 만든 DB 스냅샷을 관리할 수 있습니다. DescribeDBSnapshots API 또는 describe-db-snapshots 명령을 사용하여 특정 DB 인스턴스에 대해 사용자가 작성한 DB 스냅샷 목록을 확인하고, DeleteDBSnapshot API 또는 delete-db-snapshot 명령을 사용해 스냅샷을 삭제할 수 있습니다.\"\n",
            "내 DB 인스턴스 보존 기간의 날짜 수보다 자동 DB 스냅샷 수가 더 많은 이유는 무엇인가요?\n",
            "보존 기간의 날짜 수보다 자동 DB 스냅샷 수가 1 또는 2개 더 많은 것이 정상입니다. 보존 기간 중 원하는 시점으로 특정 시점 복원을 수행할 수 있도록 보장하기 위해 자동 스냅샷 하나가 추가로 유지됩니다.\n",
            "예를 들어 백업 기간이 1일로 설정된 경우, 이전 24시간 내 원하는 시점으로 복원할 수 있으려면 2개의 자동 스냅샷이 필요합니다. 또한, 언제나 가장 오래된 자동 스냅샷이 삭제되기 전에 새로운 자동 스냅샷이 생성되므로 자동 스냅샷이 하나 더 보일 수도 있습니다.\n",
            "\"category : RDS, question : 내 DB 인스턴스 보존 기간의 날짜 수보다 자동 DB 스냅샷 수가 더 많은 이유는 무엇인가요?, answer : 보존 기간의 날짜 수보다 자동 DB 스냅샷 수가 1 또는 2개 더 많은 것이 정상입니다. 보존 기간 중 원하는 시점으로 특정 시점 복원을 수행할 수 있도록 보장하기 위해 자동 스냅샷 하나가 추가로 유지됩니다.\n",
            "예를 들어 백업 기간이 1일로 설정된 경우, 이전 24시간 내 원하는 시점으로 복원할 수 있으려면 2개의 자동 스냅샷이 필요합니다. 또한, 언제나 가장 오래된 자동 스냅샷이 삭제되기 전에 새로운 자동 스냅샷이 생성되므로 자동 스냅샷이 하나 더 보일 수도 있습니다.\"\n",
            "DB 인스턴스를 삭제하면 백업과 DB 스냅샷은 어떻게 되나요?\n",
            "DB 인스턴스를 삭제할 때 삭제 시에 최종 DB 스냅샷을 생성할 수 있고 그렇게 하면 이 DB 스냅샷을 사용하여 나중에 삭제된 DB 인스턴스를 복구할 수 있습니다. Amazon RDS는 이 최종 사용자 생성 DB 스냅샷을 DB 인스턴스 삭제 후에 수동으로 생성한 모든 다른 DB 스냅샷과 함께 보관합니다. 백업 스토리지 비용에 대한 자세한 내용은 요금 페이지를 참조하세요.\n",
            "자동 백업은 DB 인스턴스가 삭제될 때 삭제됩니다. 수동으로 생성된 DB 스냅샷만 DB 인스턴스가 삭제된 후 보관됩니다.\n",
            "\"category : RDS, question : DB 인스턴스를 삭제하면 백업과 DB 스냅샷은 어떻게 되나요?, answer : DB 인스턴스를 삭제할 때 삭제 시에 최종 DB 스냅샷을 생성할 수 있고 그렇게 하면 이 DB 스냅샷을 사용하여 나중에 삭제된 DB 인스턴스를 복구할 수 있습니다. Amazon RDS는 이 최종 사용자 생성 DB 스냅샷을 DB 인스턴스 삭제 후에 수동으로 생성한 모든 다른 DB 스냅샷과 함께 보관합니다. 백업 스토리지 비용에 대한 자세한 내용은 요금 페이지를 참조하세요.\n",
            "자동 백업은 DB 인스턴스가 삭제될 때 삭제됩니다. 수동으로 생성된 DB 스냅샷만 DB 인스턴스가 삭제된 후 보관됩니다.\"\n",
            "Amazon Virtual Private Cloud(VPC)란 무엇이며 Amazon RDS와 어떻게 연동되나요?\n",
            "Amazon VPC를 사용하면 AWS 클라우드의 격리된 프라이빗 공간에 가상 네트워킹 환경을 생성하고 프라이빗 IP 주소 범위, 서브넷, 라우팅 테이블, 네트워크 게이트웨이 등 다양한 항목을 완벽하게 제어할 수 있습니다. Amazon VPC를 사용하여 가상 네트워크 토폴로지를 정의하고 자체 데이터 센터에서 운영하는 IP 네트워크와 흡사하게 네트워크 설정을 사용자 지정할 수 있습니다.\n",
            "예를 들어, 프라이빗 서브넷에서 공개적으로 액세스할 수 없는 백엔드 서버를 유지 관리하면서 퍼블릭 측 웹 애플리케이션을 실행하고자 할 때 VPC를 활용할 수 있습니다. 인터넷에 액세스할 수 있는 웹 서버에 퍼블릭 서브넷을 만들고 인터넷에 액세스할 수 없는 프라이빗 서브넷에 백 엔드 Amazon RDS DB 인스턴스를 배포할 수 있습니다. Amazon VPC에 대한 자세한 내용은 Amazon Virtual Private Cloud 사용 설명서를 참조하세요.\n",
            "\"category : RDS, question : Amazon Virtual Private Cloud(VPC)란 무엇이며 Amazon RDS와 어떻게 연동되나요?, answer : Amazon VPC를 사용하면 AWS 클라우드의 격리된 프라이빗 공간에 가상 네트워킹 환경을 생성하고 프라이빗 IP 주소 범위, 서브넷, 라우팅 테이블, 네트워크 게이트웨이 등 다양한 항목을 완벽하게 제어할 수 있습니다. Amazon VPC를 사용하여 가상 네트워크 토폴로지를 정의하고 자체 데이터 센터에서 운영하는 IP 네트워크와 흡사하게 네트워크 설정을 사용자 지정할 수 있습니다.\n",
            "예를 들어, 프라이빗 서브넷에서 공개적으로 액세스할 수 없는 백엔드 서버를 유지 관리하면서 퍼블릭 측 웹 애플리케이션을 실행하고자 할 때 VPC를 활용할 수 있습니다. 인터넷에 액세스할 수 있는 웹 서버에 퍼블릭 서브넷을 만들고 인터넷에 액세스할 수 없는 프라이빗 서브넷에 백 엔드 Amazon RDS DB 인스턴스를 배포할 수 있습니다. Amazon VPC에 대한 자세한 내용은 Amazon Virtual Private Cloud 사용 설명서를 참조하세요.\"\n",
            "Amazon RDS를 VPC 내에서 사용하는 것과 VPC가 아닌 EC2-Classic 플랫폼에서 사용하는 것은 어떻게 다릅니까?\n",
            "AWS 계정이 2013년 12월 4일 이전에 생성되었다면 Amazon Elastic Compute Cloud(EC2)-Classic 환경에서 Amazon RDS를 실행할 수도 있습니다. Amazon RDS의 기본 기능은 EC2-Classic을 사용하든 EC2-VPC를 사용하든 관계없이 동일합니다. Amazon RDS는 DB 인스턴스를 VPC 내부 또는 외부 중 어디에 배포하든 관계없이 백업, 소프트웨어 패치 적용, 자동 장애 감지, 읽기 전용 복제본 및 복구를 관리합니다. EC2-Classic과 EC2-VPC의 차이점에 대한 자세한 내용은 EC2 설명서를 참조하세요.\n",
            "\"category : RDS, question : Amazon RDS를 VPC 내에서 사용하는 것과 VPC가 아닌 EC2-Classic 플랫폼에서 사용하는 것은 어떻게 다릅니까?, answer : AWS 계정이 2013년 12월 4일 이전에 생성되었다면 Amazon Elastic Compute Cloud(EC2)-Classic 환경에서 Amazon RDS를 실행할 수도 있습니다. Amazon RDS의 기본 기능은 EC2-Classic을 사용하든 EC2-VPC를 사용하든 관계없이 동일합니다. Amazon RDS는 DB 인스턴스를 VPC 내부 또는 외부 중 어디에 배포하든 관계없이 백업, 소프트웨어 패치 적용, 자동 장애 감지, 읽기 전용 복제본 및 복구를 관리합니다. EC2-Classic과 EC2-VPC의 차이점에 대한 자세한 내용은 EC2 설명서를 참조하세요.\"\n",
            "DB 서브넷 그룹은 무엇이며 왜 필요한가요?\n",
            "DB 서브넷 그룹은 VPC 내의 Amazon RDS DB 인스턴스에 대해 지정할 수 있는 서브넷의 모음입니다. 각 DB 서브넷 그룹에는 특정 리전의 가용 영역마다 하나 이상의 서브넷이 있어야 합니다. VPC에 DB 인스턴스를 만들 때, DB 서브넷 그룹을 선택해야 합니다. 그러면 Amazon RDS가 DB 서브넷 그룹과 기본 가용 영역을 사용하여 서브넷과 서브넷의 IP 주소를 선택합니다. Amazon RDS가 엘라스틱 네트워크 인터페이스를 만든 다음 해당 IP 주소를 가진 DB 인스턴스에 연결합니다.\n",
            "기본 IP 주소가 변경될 수 있으므로(예: 장애 조치 도중) 사용자의 DB 인스턴스에 연결할 때 DNS 이름을 사용하는 것이 좋습니다.\n",
            "다중 AZ 배포의 경우, 특정 리전의 모든 가용 영역에 서브넷을 정의하면 Amazon RDS가 필요한 경우 다른 가용 영역에 새로운 예비 복제본을 만들 수 있습니다. 단일 AZ 배포의 경우도 어느 시점에 단일 AZ 배포를 다중 AZ 배포로 변환하려면 이 작업을 수행해야 합니다.\n",
            "\"category : RDS, question : DB 서브넷 그룹은 무엇이며 왜 필요한가요?, answer : DB 서브넷 그룹은 VPC 내의 Amazon RDS DB 인스턴스에 대해 지정할 수 있는 서브넷의 모음입니다. 각 DB 서브넷 그룹에는 특정 리전의 가용 영역마다 하나 이상의 서브넷이 있어야 합니다. VPC에 DB 인스턴스를 만들 때, DB 서브넷 그룹을 선택해야 합니다. 그러면 Amazon RDS가 DB 서브넷 그룹과 기본 가용 영역을 사용하여 서브넷과 서브넷의 IP 주소를 선택합니다. Amazon RDS가 엘라스틱 네트워크 인터페이스를 만든 다음 해당 IP 주소를 가진 DB 인스턴스에 연결합니다.\n",
            "기본 IP 주소가 변경될 수 있으므로(예: 장애 조치 도중) 사용자의 DB 인스턴스에 연결할 때 DNS 이름을 사용하는 것이 좋습니다.\n",
            "다중 AZ 배포의 경우, 특정 리전의 모든 가용 영역에 서브넷을 정의하면 Amazon RDS가 필요한 경우 다른 가용 영역에 새로운 예비 복제본을 만들 수 있습니다. 단일 AZ 배포의 경우도 어느 시점에 단일 AZ 배포를 다중 AZ 배포로 변환하려면 이 작업을 수행해야 합니다.\"\n",
            "VPC에 Amazon RDS DB 인스턴스를 어떻게 만듭니까?\n",
            "이 프로세스를 진행하는 절차는 Amazon RDS 사용 설명서의 VPC에서 DB 인스턴스 만들기를 참조하세요.\n",
            "\"category : RDS, question : VPC에 Amazon RDS DB 인스턴스를 어떻게 만듭니까?, answer : 이 프로세스를 진행하는 절차는 Amazon RDS 사용 설명서의 VPC에서 DB 인스턴스 만들기를 참조하세요.\"\n",
            "DB 인스턴스에 대한 네트워크 액세스를 어떻게 제어합니까?\n",
            "Amazon RDS 사용 설명서의 보안 그룹 섹션을 참조하여 DB 인스턴스에 대한 액세스를 제어하는 다양한 방법에 대해 알아보세요.\n",
            "\"category : RDS, question : DB 인스턴스에 대한 네트워크 액세스를 어떻게 제어합니까?, answer : Amazon RDS 사용 설명서의 보안 그룹 섹션을 참조하여 DB 인스턴스에 대한 액세스를 제어하는 다양한 방법에 대해 알아보세요.\"\n",
            "VPC의 Amazon RDS DB 인스턴스에 어떻게 연결합니까?\n",
            "VPC 내에 배포된 DB 인스턴스는 동일한 VPC에 배포된 EC2 인스턴스에서 액세스할 수 있습니다. 이러한 EC2 인스턴스가 엘라스틱 IP가 연결된 공인 서브넷에 배포된 경우 인터넷을 통해 EC2 인스턴스에 액세스할 수 있습니다. VPC 내에 배포된 DB 인스턴스는 인터넷에서 액세스하거나 VPN 또는 퍼블릭 서브넷에서 실행할 수 있는 Bastion Host를 통해 VPC 외부의 EC2 인스턴스에서 액세스하거나 Amazon RDS의 공용 액세스 옵션을 사용하여 액세스할 수 있습니다.\n",
            "\n",
            "Bastion Host를 사용하려면 SSH Bastion 역할을 하는 EC2 인스턴스를 사용하여 퍼블릭 서브넷을 설정해야 합니다. 이 퍼블릭 서브넷은 SSH 호스트를 통해 트래픽을 제어할 수 있는 인터넷 게이트웨이 또는 라우팅 규칙이 필요합니다. 또한, SSH 호스트에서 Amazon RDS DB 인스턴스의 프라이빗 IP 주소로 요청을 전달할 수 있어야 합니다.\n",
            "공용 연결을 사용하려면 Publicly Accessible 옵션을 yes로 설정하여 DB 인스턴스를 만듭니다. 공용 액세스가 활성화되면 기본적으로 VPC 내부의 DB 인스턴스를 VPC 외부에서 완벽하게 액세스할 수 있게 됩니다. 따라서 인스턴스에 대한 액세스를 허용하기 위해 VPN이나 배스천 호스트를 설정할 필요가 없습니다.\n",
            "\n",
            "VPN 게이트웨이를 설정해 사내 네트워크를 VPC로 확장하여 해당 VPC의 Amazon RDS DB 인스턴스에 액세스할 수 있도록 하는 방법도 있습니다. 자세한 내용은 Amazon VPC 사용 설명서를 참조하세요.\n",
            "기본 IP 주소가 변경될 수 있으므로(예: 장애 조치 도중) DB 인스턴스에 연결할 때 DNS 이름을 사용하는 것이 좋습니다.\n",
            "\"category : RDS, question : VPC의 Amazon RDS DB 인스턴스에 어떻게 연결합니까?, answer : VPC 내에 배포된 DB 인스턴스는 동일한 VPC에 배포된 EC2 인스턴스에서 액세스할 수 있습니다. 이러한 EC2 인스턴스가 엘라스틱 IP가 연결된 공인 서브넷에 배포된 경우 인터넷을 통해 EC2 인스턴스에 액세스할 수 있습니다. VPC 내에 배포된 DB 인스턴스는 인터넷에서 액세스하거나 VPN 또는 퍼블릭 서브넷에서 실행할 수 있는 Bastion Host를 통해 VPC 외부의 EC2 인스턴스에서 액세스하거나 Amazon RDS의 공용 액세스 옵션을 사용하여 액세스할 수 있습니다.\n",
            "\n",
            "Bastion Host를 사용하려면 SSH Bastion 역할을 하는 EC2 인스턴스를 사용하여 퍼블릭 서브넷을 설정해야 합니다. 이 퍼블릭 서브넷은 SSH 호스트를 통해 트래픽을 제어할 수 있는 인터넷 게이트웨이 또는 라우팅 규칙이 필요합니다. 또한, SSH 호스트에서 Amazon RDS DB 인스턴스의 프라이빗 IP 주소로 요청을 전달할 수 있어야 합니다.\n",
            "공용 연결을 사용하려면 Publicly Accessible 옵션을 yes로 설정하여 DB 인스턴스를 만듭니다. 공용 액세스가 활성화되면 기본적으로 VPC 내부의 DB 인스턴스를 VPC 외부에서 완벽하게 액세스할 수 있게 됩니다. 따라서 인스턴스에 대한 액세스를 허용하기 위해 VPN이나 배스천 호스트를 설정할 필요가 없습니다.\n",
            "\n",
            "VPN 게이트웨이를 설정해 사내 네트워크를 VPC로 확장하여 해당 VPC의 Amazon RDS DB 인스턴스에 액세스할 수 있도록 하는 방법도 있습니다. 자세한 내용은 Amazon VPC 사용 설명서를 참조하세요.\n",
            "기본 IP 주소가 변경될 수 있으므로(예: 장애 조치 도중) DB 인스턴스에 연결할 때 DNS 이름을 사용하는 것이 좋습니다.\"\n",
            "VPC 외부의 기존 DB 인스턴스를 내 VPC로 이동할 수 있습니까?\n",
            "DB 인스턴스가 VPC 내에 있지 않은 경우 AWS Management Console을 사용하여 손쉽게 DB 인스턴스를 VPC로 이동할 수 있습니다. 자세한 내용은 Amazon RDS 사용 설명서를 참조하세요. 또한, VPC 외부에 있는 DB 인스턴스의 스냅샷을 생성한 다음, 사용할 DB 서브넷 그룹을 지정함으로써 이를 VPC 내에 복원할 수도 있습니다. 또는 “지정 시간으로 복원” 작업도 가능합니다.\n",
            "\"category : RDS, question : VPC 외부의 기존 DB 인스턴스를 내 VPC로 이동할 수 있습니까?, answer : DB 인스턴스가 VPC 내에 있지 않은 경우 AWS Management Console을 사용하여 손쉽게 DB 인스턴스를 VPC로 이동할 수 있습니다. 자세한 내용은 Amazon RDS 사용 설명서를 참조하세요. 또한, VPC 외부에 있는 DB 인스턴스의 스냅샷을 생성한 다음, 사용할 DB 서브넷 그룹을 지정함으로써 이를 VPC 내에 복원할 수도 있습니다. 또는 “지정 시간으로 복원” 작업도 가능합니다.\"\n",
            "기존 DB 인스턴스를 VPC 내부에서 외부로 이동할 수 있습니까?\n",
            "DB 인스턴스를 VPC 내부에서 VPC 외부로 마이그레이션하는 기능은 지원되지 않습니다. 보안상의 이유로 VPC 내부에 있는 DB 인스턴스의 DB 스냅샷은 VPC 외부로 복원할 수 없습니다. 마찬가지로 “특정 시점으로 복원” 기능도 지원되지 않습니다.\n",
            "\"category : RDS, question : 기존 DB 인스턴스를 VPC 내부에서 외부로 이동할 수 있습니까?, answer : DB 인스턴스를 VPC 내부에서 VPC 외부로 마이그레이션하는 기능은 지원되지 않습니다. 보안상의 이유로 VPC 내부에 있는 DB 인스턴스의 DB 스냅샷은 VPC 외부로 복원할 수 없습니다. 마찬가지로 “특정 시점으로 복원” 기능도 지원되지 않습니다.\"\n",
            "애플리케이션이 VPC에 있는 DB 인스턴스에 액세스할 수 있도록 하려면 어떤 점에 주의해야 합니까?\n",
            "사용자는 VPC의 라우팅 테이블과 네트워킹 ACL을 수정하여 VPC의 클라이언트 인스턴스에서 DB 인스턴스에 도달할 수 있도록 해야 합니다. 다중 AZ 배포의 경우 장애 조치 후 클라이언트 EC2 인스턴스와 Amazon RDS DB 인스턴스가 서로 다른 가용 영역에 속할 수 있습니다. 따라서 AZ 사이의 통신이 가능하도록 네트워킹 ACL을 구성해야 합니다.\n",
            "\"category : RDS, question : 애플리케이션이 VPC에 있는 DB 인스턴스에 액세스할 수 있도록 하려면 어떤 점에 주의해야 합니까?, answer : 사용자는 VPC의 라우팅 테이블과 네트워킹 ACL을 수정하여 VPC의 클라이언트 인스턴스에서 DB 인스턴스에 도달할 수 있도록 해야 합니다. 다중 AZ 배포의 경우 장애 조치 후 클라이언트 EC2 인스턴스와 Amazon RDS DB 인스턴스가 서로 다른 가용 영역에 속할 수 있습니다. 따라서 AZ 사이의 통신이 가능하도록 네트워킹 ACL을 구성해야 합니다.\"\n",
            "DB 인스턴스의 DB 서브넷 그룹을 변경할 수 있습니까?\n",
            "기존 DB 서브넷 그룹을 업데이트하여 기존 가용 영역 또는 DB 인스턴스를 만든 후 추가된 새로운 가용 영역에 서브넷을 추가할 수 있습니다. 기존 DB 서브넷 그룹에서 서브넷을 제거하면 이 서브넷 그룹에서 제거된 특정 AZ에서 실행 중이던 인스턴스를 사용하지 못할 수도 있습니다. 자세한 내용은 Amazon RDS 사용 설명서를 참조하세요.\n",
            "\"category : RDS, question : DB 인스턴스의 DB 서브넷 그룹을 변경할 수 있습니까?, answer : 기존 DB 서브넷 그룹을 업데이트하여 기존 가용 영역 또는 DB 인스턴스를 만든 후 추가된 새로운 가용 영역에 서브넷을 추가할 수 있습니다. 기존 DB 서브넷 그룹에서 서브넷을 제거하면 이 서브넷 그룹에서 제거된 특정 AZ에서 실행 중이던 인스턴스를 사용하지 못할 수도 있습니다. 자세한 내용은 Amazon RDS 사용 설명서를 참조하세요.\"\n",
            "Amazon RDS 기본 사용자 계정이란 무엇이며, AWS 계정과 어떻게 다릅니까?\n",
            "Amazon RDS를 사용하려면 AWS 개발자 계정이 필요합니다. Amazon RDS에 가입하기 전에 이 계정을 가지고 있지 않은 경우, 가입 프로세스를 시작할 때 계정을 만들라는 메시지가 표시됩니다. 기본 사용자 계정은 AWS 개발자 계정과 달라야 하고, Amazon RDS에서 DB 인스턴스에 대한 액세스를 제어하기 위한 용도로만 사용합니다. 기본 사용자 계정은 DB 인스턴스에 액세스하는 데 사용할 수 있는 네이티브 데이터베이스 사용자 계정입니다. \n",
            "DB 인스턴스를 만들 때 각 DB 인스턴스에 연결할 기본 사용자 이름과 암호를 지정할 수 있습니다. DB 인스턴스를 만들면 기본 사용자 자격 증명을 사용하여 데이터베이스에 연결할 수 있습니다. 나중에 추가 사용자 계정을 만들어 DB 인스턴스에 액세스할 수 있는 사용자를 제한할 수도 있습니다.\n",
            "\"category : RDS, question : Amazon RDS 기본 사용자 계정이란 무엇이며, AWS 계정과 어떻게 다릅니까?, answer : Amazon RDS를 사용하려면 AWS 개발자 계정이 필요합니다. Amazon RDS에 가입하기 전에 이 계정을 가지고 있지 않은 경우, 가입 프로세스를 시작할 때 계정을 만들라는 메시지가 표시됩니다. 기본 사용자 계정은 AWS 개발자 계정과 달라야 하고, Amazon RDS에서 DB 인스턴스에 대한 액세스를 제어하기 위한 용도로만 사용합니다. 기본 사용자 계정은 DB 인스턴스에 액세스하는 데 사용할 수 있는 네이티브 데이터베이스 사용자 계정입니다. \n",
            "DB 인스턴스를 만들 때 각 DB 인스턴스에 연결할 기본 사용자 이름과 암호를 지정할 수 있습니다. DB 인스턴스를 만들면 기본 사용자 자격 증명을 사용하여 데이터베이스에 연결할 수 있습니다. 나중에 추가 사용자 계정을 만들어 DB 인스턴스에 액세스할 수 있는 사용자를 제한할 수도 있습니다.\"\n",
            "내 DB 인스턴스의 기본 사용자에게 어떤 권한이 부여됩니까?\n",
            "MySQL의 기본 사용자의 기본 권한은 create, drop, references, event, alter, delete, index, insert, select, update, create temporary tables, lock tables, trigger, create view, show view, alter routine, create routine, execute, trigger, create user, process, show databases, grant option입니다.\n",
            "Oracle의 기본 사용자에게는 ‘dba’ 역할이 부여됩니다. 기본 사용자는 해당 역할과 관련된 권한을 대부분 상속합니다. 권한의 제한 목록과 이러한 권한이 필요한 관리 작업을 수행할 수 있는 방법은 Amazon RDS 사용 설명서를 참조하세요.\n",
            "SQL Server에서는 데이터베이스를 만든 사용자에게 \"db_owner\" 역할이 부여됩니다. 권한의 제한 목록과 이러한 권한이 필요한 관리 작업을 수행할 수 있는 방법은 Amazon RDS 사용 설명서를 참조하세요.\n",
            "\"category : RDS, question : 내 DB 인스턴스의 기본 사용자에게 어떤 권한이 부여됩니까?, answer : MySQL의 기본 사용자의 기본 권한은 create, drop, references, event, alter, delete, index, insert, select, update, create temporary tables, lock tables, trigger, create view, show view, alter routine, create routine, execute, trigger, create user, process, show databases, grant option입니다.\n",
            "Oracle의 기본 사용자에게는 ‘dba’ 역할이 부여됩니다. 기본 사용자는 해당 역할과 관련된 권한을 대부분 상속합니다. 권한의 제한 목록과 이러한 권한이 필요한 관리 작업을 수행할 수 있는 방법은 Amazon RDS 사용 설명서를 참조하세요.\n",
            "SQL Server에서는 데이터베이스를 만든 사용자에게 \"db_owner\" 역할이 부여됩니다. 권한의 제한 목록과 이러한 권한이 필요한 관리 작업을 수행할 수 있는 방법은 Amazon RDS 사용 설명서를 참조하세요.\"\n",
            "Amazon RDS를 통한 사용자 관리에 차이점이 있습니까?\n",
            "아니요, 관계형 데이터베이스를 사용하여 사용자가 직접 관리하는 것과 동일한 익숙한 방법으로 모두 작동합니다.\n",
            "\"category : RDS, question : Amazon RDS를 통한 사용자 관리에 차이점이 있습니까?, answer : 아니요, 관계형 데이터베이스를 사용하여 사용자가 직접 관리하는 것과 동일한 익숙한 방법으로 모두 작동합니다.\"\n",
            "데이터 센터의 서버에서 실행되는 프로그램이 Amazon RDS 데이터베이스에 액세스할 수 있습니까?\n",
            "예. 보안 그룹을 구성하여 인터넷을 통해 데이터베이스에 액세스할 수 있는 기능을 활성화해야 합니다. 자체 데이터 센터의 서버에 해당하는 특정 IP, IP 범위 또는 서브넷에 대한 액세스만 허용할 수 있습니다.\n",
            "\"category : RDS, question : 데이터 센터의 서버에서 실행되는 프로그램이 Amazon RDS 데이터베이스에 액세스할 수 있습니까?, answer : 예. 보안 그룹을 구성하여 인터넷을 통해 데이터베이스에 액세스할 수 있는 기능을 활성화해야 합니다. 자체 데이터 센터의 서버에 해당하는 특정 IP, IP 범위 또는 서브넷에 대한 액세스만 허용할 수 있습니다.\"\n",
            "SSL/TLS를 사용하여 내 애플리케이션과 내 DB 인스턴스 간의 연결을 암호화할 수 있습니까?\n",
            "예. 이 옵션은 모든 Amazon RDS 엔진에서 지원됩니다. Amazon RDS는 각 DB 인스턴스에 대해 SSL/TLS 인증서를 생성합니다. 암호화된 연결이 설정되면 DB 인스턴스와 애플리케이션 간에 전송되는 데이터는 전송 중에 암호화됩니다.\n",
            "SSL/TLS는 보안상 장점이 있지만, SSL 암호화는 컴퓨팅 중심 작업이며 데이터베이스 연결의 지연 시간을 늘린다는 점에 유의해야 합니다. Amazon RDS에서의 SSL/TLS 지원은 애플리케이션과 DB 인스턴스 간의 연결을 암호화하기 위한 것이므로 DB 인스턴스 자체를 인증하는 데 사용해서는 안 됩니다.\n",
            "Amazon RDS를 통한 암호화된 연결 설정에 대한 자세한 내용은 Amazon RDS의 MySQL 사용 설명서, MariaDB 사용 설명서, PostgreSQL 사용 설명서 또는 Oracle 사용 설명서를 참조하세요. SSL/TLS와 이러한 엔진의 연동에 대해 자세히 알아보려면 MySQL 설명서, MariaDB 설명서, MSDN SQL Server 설명서, PostgreSQL 설명서 또는Oracle 설명서를 직접 참조하시기 바랍니다.\n",
            "\"category : RDS, question : SSL/TLS를 사용하여 내 애플리케이션과 내 DB 인스턴스 간의 연결을 암호화할 수 있습니까?, answer : 예. 이 옵션은 모든 Amazon RDS 엔진에서 지원됩니다. Amazon RDS는 각 DB 인스턴스에 대해 SSL/TLS 인증서를 생성합니다. 암호화된 연결이 설정되면 DB 인스턴스와 애플리케이션 간에 전송되는 데이터는 전송 중에 암호화됩니다.\n",
            "SSL/TLS는 보안상 장점이 있지만, SSL 암호화는 컴퓨팅 중심 작업이며 데이터베이스 연결의 지연 시간을 늘린다는 점에 유의해야 합니다. Amazon RDS에서의 SSL/TLS 지원은 애플리케이션과 DB 인스턴스 간의 연결을 암호화하기 위한 것이므로 DB 인스턴스 자체를 인증하는 데 사용해서는 안 됩니다.\n",
            "Amazon RDS를 통한 암호화된 연결 설정에 대한 자세한 내용은 Amazon RDS의 MySQL 사용 설명서, MariaDB 사용 설명서, PostgreSQL 사용 설명서 또는 Oracle 사용 설명서를 참조하세요. SSL/TLS와 이러한 엔진의 연동에 대해 자세히 알아보려면 MySQL 설명서, MariaDB 설명서, MSDN SQL Server 설명서, PostgreSQL 설명서 또는Oracle 설명서를 직접 참조하시기 바랍니다.\"\n",
            "Amazon RDS 데이터베이스에 저장된 데이터를 암호화할 수 있나요?\n",
            "Amazon RDS는 AWS Key Management Service(KMS)를 통해 관리하는 키를 사용하여 모든 데이터베이스 엔진에 대한 저장 중 암호화를 지원합니다. Amazon RDS 암호화를 실행 중인 데이터베이스 인스턴스에서는 자동 백업, 읽기 전용 복제본 및 스냅샷과 마찬가지로 기본 스토리지에 저장된 데이터가 암호화됩니다. 암호화와 복호화는 투명하게 처리됩니다. Amazon RDS와 KMS 사용에 관한 자세한 내용은 Amazon RDS 사용 설명서를 참조하세요.\n",
            "또한, DB 스냅샷을 생성한 후 해당 스냅샷의 사본을 만들어 KMS 암호화 키를 지정함으로써 이전에 암호화되지 않은 DB 인스턴스나 DB 클러스터도 암호화할 수 있습니다. 그런 다음 암호화된 스냅샷에서 암호화된 DB 인스턴스 또는 DB 클러스터를 복원할 수 있습니다.\n",
            "Amazon RDS for Oracle 및 SQL Server는 이러한 엔진의 TDE(Transparent Data Encryption) 기술을 지원합니다. 자세한 내용은 Oracle 및 SQL Server 관련 Amazon RDS 사용 설명서를 참조하세요.\n",
            "\"category : RDS, question : Amazon RDS 데이터베이스에 저장된 데이터를 암호화할 수 있나요?, answer : Amazon RDS는 AWS Key Management Service(KMS)를 통해 관리하는 키를 사용하여 모든 데이터베이스 엔진에 대한 저장 중 암호화를 지원합니다. Amazon RDS 암호화를 실행 중인 데이터베이스 인스턴스에서는 자동 백업, 읽기 전용 복제본 및 스냅샷과 마찬가지로 기본 스토리지에 저장된 데이터가 암호화됩니다. 암호화와 복호화는 투명하게 처리됩니다. Amazon RDS와 KMS 사용에 관한 자세한 내용은 Amazon RDS 사용 설명서를 참조하세요.\n",
            "또한, DB 스냅샷을 생성한 후 해당 스냅샷의 사본을 만들어 KMS 암호화 키를 지정함으로써 이전에 암호화되지 않은 DB 인스턴스나 DB 클러스터도 암호화할 수 있습니다. 그런 다음 암호화된 스냅샷에서 암호화된 DB 인스턴스 또는 DB 클러스터를 복원할 수 있습니다.\n",
            "Amazon RDS for Oracle 및 SQL Server는 이러한 엔진의 TDE(Transparent Data Encryption) 기술을 지원합니다. 자세한 내용은 Oracle 및 SQL Server 관련 Amazon RDS 사용 설명서를 참조하세요.\"\n",
            "내 시스템 및 사용자가 특정 Amazon RDS 리소스에서 수행할 수 있는 작업을 제어하려면 어떻게 해야 합니까?\n",
            "AWS IAM 사용자 및 그룹이 Amazon RDS 리소스에서 수행할 수 있는 작업을 제어할 수 있습니다. 사용자 및 그룹에 적용할 AWS IAM 정책에서 Amazon RDS 리소스를 참조하면 이 작업을 수행할 수 있습니다. AWS IAM 정책에서 참조할 수 있는 Amazon RDS 리소스에는 DB 인스턴스, DB 스냅샷, 읽기 전용 복제본, DB 보안 그룹, DB 옵션 그룹, DB 파라미터 그룹, 이벤트 구독 및 DB 서브넷 그룹이 포함됩니다. \n",
            "또한 이러한 리소스에 태그를 지정하여 메타데이터를 추가할 수 있습니다. 태그를 지정하여 리소스를 분류(예: ‘Development’ DB 인스턴스, ‘Production’ DB 인스턴스, ‘Test’ DB 인스턴스)하고 동일한 태그를 가진 리소스에서 수행할 수 있는 권한(즉, 작업)을 나열하는 AWS IAM 정책을 작성할 수 있습니다. 자세한 내용은 Amazon RDS 리소스 태깅을 참조하세요.\n",
            "\"category : RDS, question : 내 시스템 및 사용자가 특정 Amazon RDS 리소스에서 수행할 수 있는 작업을 제어하려면 어떻게 해야 합니까?, answer : AWS IAM 사용자 및 그룹이 Amazon RDS 리소스에서 수행할 수 있는 작업을 제어할 수 있습니다. 사용자 및 그룹에 적용할 AWS IAM 정책에서 Amazon RDS 리소스를 참조하면 이 작업을 수행할 수 있습니다. AWS IAM 정책에서 참조할 수 있는 Amazon RDS 리소스에는 DB 인스턴스, DB 스냅샷, 읽기 전용 복제본, DB 보안 그룹, DB 옵션 그룹, DB 파라미터 그룹, 이벤트 구독 및 DB 서브넷 그룹이 포함됩니다. \n",
            "또한 이러한 리소스에 태그를 지정하여 메타데이터를 추가할 수 있습니다. 태그를 지정하여 리소스를 분류(예: ‘Development’ DB 인스턴스, ‘Production’ DB 인스턴스, ‘Test’ DB 인스턴스)하고 동일한 태그를 가진 리소스에서 수행할 수 있는 권한(즉, 작업)을 나열하는 AWS IAM 정책을 작성할 수 있습니다. 자세한 내용은 Amazon RDS 리소스 태깅을 참조하세요.\"\n",
            "Amazon RDS 배포에 대해 보안 분석 또는 운영 문제 해결을 수행하고 싶습니다. 내 계정에서 이루어진 모든 Amazon RDS API 호출 기록을 얻을 수 있습니까?\n",
            "예. AWS CloudTrail은 계정에 대한 AWS API 호출을 기록하고 로그 파일을 사용자에게 전달하는 웹 서비스입니다. CloudTrail에서 작성되는 AWS API 호출 내역을 통해 보안 분석, 리소스 변경 사항 추적 및 규정 준수 감사를 수행할 수 있습니다.\n",
            "\"category : RDS, question : Amazon RDS 배포에 대해 보안 분석 또는 운영 문제 해결을 수행하고 싶습니다. 내 계정에서 이루어진 모든 Amazon RDS API 호출 기록을 얻을 수 있습니까?, answer : 예. AWS CloudTrail은 계정에 대한 AWS API 호출을 기록하고 로그 파일을 사용자에게 전달하는 웹 서비스입니다. CloudTrail에서 작성되는 AWS API 호출 내역을 통해 보안 분석, 리소스 변경 사항 추적 및 규정 준수 감사를 수행할 수 있습니다.\"\n",
            "HIPAA 규정을 준수해야 하는 애플리케이션에 Amazon RDS를 사용할 수 있나요?\n",
            "예. 모든 Amazon RDS 데이터베이스 엔진은 HIPAA 적격 서비스입니다. 따라서 AWS와 체결한 비즈니스 제휴 계약(BAA)에 따라 HIPAA 규정 준수 애플리케이션을 구축하고 개인 건강 정보(PHI)를 비롯한 의료 서비스 관련 정보를 저장하는 데 이를 사용할 수 있습니다.\n",
            "이미 BAA를 체결한 경우, 다른 작업 없이 BAA에 포함된 계정에서 이러한 서비스 사용을 시작할 수 있습니다. AWS와 BAA를 아직 체결하지 않았거나, AWS의 HIPAA 규정 준수 애플리케이션에 대한 질문이 있는 경우 해당하는 고객 관리자에게 문의하십시오.\n",
            "\"category : RDS, question : HIPAA 규정을 준수해야 하는 애플리케이션에 Amazon RDS를 사용할 수 있나요?, answer : 예. 모든 Amazon RDS 데이터베이스 엔진은 HIPAA 적격 서비스입니다. 따라서 AWS와 체결한 비즈니스 제휴 계약(BAA)에 따라 HIPAA 규정 준수 애플리케이션을 구축하고 개인 건강 정보(PHI)를 비롯한 의료 서비스 관련 정보를 저장하는 데 이를 사용할 수 있습니다.\n",
            "이미 BAA를 체결한 경우, 다른 작업 없이 BAA에 포함된 계정에서 이러한 서비스 사용을 시작할 수 있습니다. AWS와 BAA를 아직 체결하지 않았거나, AWS의 HIPAA 규정 준수 애플리케이션에 대한 질문이 있는 경우 해당하는 고객 관리자에게 문의하십시오.\"\n",
            "DB 인스턴스에 적합한 구성 파라미터를 선택하려면 어떻게 해야 합니까?\n",
            "기본적으로 Amazon RDS는 인스턴스 클래스와 스토리지 용량을 고려하여 DB 인스턴스에 최적인 구성 파라미터를 선택합니다. 하지만 원하는 경우 AWS Management Console, Amazon RDS API 또는 AWS 명령줄 인터페이스를 사용하여 이를 변경할 수 있습니다. 다만 구성 파라미터를 권장값에서 변경하면 성능 저하에서 시스템 작동 중단에 이르기까지 예기치 않은 결과가 발생할 수 있으므로 이러한 위험을 충분히 인식하고 있는 고급 사용자만 이 작업을 시도하는 게 좋습니다.\n",
            "\"category : RDS, question : DB 인스턴스에 적합한 구성 파라미터를 선택하려면 어떻게 해야 합니까?, answer : 기본적으로 Amazon RDS는 인스턴스 클래스와 스토리지 용량을 고려하여 DB 인스턴스에 최적인 구성 파라미터를 선택합니다. 하지만 원하는 경우 AWS Management Console, Amazon RDS API 또는 AWS 명령줄 인터페이스를 사용하여 이를 변경할 수 있습니다. 다만 구성 파라미터를 권장값에서 변경하면 성능 저하에서 시스템 작동 중단에 이르기까지 예기치 않은 결과가 발생할 수 있으므로 이러한 위험을 충분히 인식하고 있는 고급 사용자만 이 작업을 시도하는 게 좋습니다.\"\n",
            "DB 파라미터 그룹은 무엇입니까? 어떤 면에서 유용합니까?\n",
            "데이터베이스 파라미터 그룹(DB 파라미터그룹)은 하나 이상의 DB 인스턴스에 적용 가능한 엔진 설정값의 “컨테이너” 역할을 합니다. DB 파라미터 그룹을 지정하지 않고 DB 인스턴스를 만드는 경우 기본 DB 파라미터 그룹이 사용됩니다. 이 기본값 그룹에는 실행하는 DB 인스턴스에 최적화된 엔진 기본값과 Amazon RDS 시스템 기본 값이 포함됩니다.\n",
            "그러나 사용자 지정 엔진 설정값을 사용해 DB 인스턴스를 실행하려면 새 DB 파라미터 그룹을 만들고, 필요한 파라미터를 수정하고, 새 DB 파라미터 그룹을 사용하기 위해 DB 인스턴스를 수정하기만 하면 됩니다. 연결이 이루어지면 특정 DB 파라미터 그룹을 사용하는 모든 DB 인스턴스가 해당 DB 파라미터 그룹에 대한 모든 파라미터 업데이트를 가져옵니다.\n",
            "DB 파라미터 그룹 설정에 대한 자세한 내용은 Amazon RDS 사용 설명서를 참조하세요.\n",
            "\"category : RDS, question : DB 파라미터 그룹은 무엇입니까? 어떤 면에서 유용합니까?, answer : 데이터베이스 파라미터 그룹(DB 파라미터그룹)은 하나 이상의 DB 인스턴스에 적용 가능한 엔진 설정값의 “컨테이너” 역할을 합니다. DB 파라미터 그룹을 지정하지 않고 DB 인스턴스를 만드는 경우 기본 DB 파라미터 그룹이 사용됩니다. 이 기본값 그룹에는 실행하는 DB 인스턴스에 최적화된 엔진 기본값과 Amazon RDS 시스템 기본 값이 포함됩니다.\n",
            "그러나 사용자 지정 엔진 설정값을 사용해 DB 인스턴스를 실행하려면 새 DB 파라미터 그룹을 만들고, 필요한 파라미터를 수정하고, 새 DB 파라미터 그룹을 사용하기 위해 DB 인스턴스를 수정하기만 하면 됩니다. 연결이 이루어지면 특정 DB 파라미터 그룹을 사용하는 모든 DB 인스턴스가 해당 DB 파라미터 그룹에 대한 모든 파라미터 업데이트를 가져옵니다.\n",
            "DB 파라미터 그룹 설정에 대한 자세한 내용은 Amazon RDS 사용 설명서를 참조하세요.\"\n",
            "Amazon RDS 리소스 구성을 모니터링하려면 어떻게 해야 하나요?\n",
            "AWS Config를 사용하여 Amazon RDS DB 인스턴스, DB 서브넷 그룹, DB 스냅샷, DB 보안 그룹 및 이벤트 구독에 대한 구성 변경 사항을 지속적으로 기록하고 Amazon Simple Notification Service(SNS)를 통해 변경 사항에 대한 알림을 받을 수 있습니다. 또한 AWS Config 규칙을 생성하여 해당 Amazon RDS 리소스가 원하는 구성을 사용하고 있는지 평가할 수 있습니다.\n",
            "\"category : RDS, question : Amazon RDS 리소스 구성을 모니터링하려면 어떻게 해야 하나요?, answer : AWS Config를 사용하여 Amazon RDS DB 인스턴스, DB 서브넷 그룹, DB 스냅샷, DB 보안 그룹 및 이벤트 구독에 대한 구성 변경 사항을 지속적으로 기록하고 Amazon Simple Notification Service(SNS)를 통해 변경 사항에 대한 알림을 받을 수 있습니다. 또한 AWS Config 규칙을 생성하여 해당 Amazon RDS 리소스가 원하는 구성을 사용하고 있는지 평가할 수 있습니다.\"\n",
            "DB 인스턴스를 다중 AZ 배포로 실행한다는 것은 무엇을 의미합니까?\n",
            "다중 AZ 배포로 실행되도록 DB 인스턴스를 생성 또는 수정하면 Amazon RDS가 다른 가용 영역에 동기식 ‘예비’ 복제본을 자동으로 프로비저닝하고 유지합니다. DB 인스턴스에 대한 업데이트는 가용 영역 전체에서 예비 복제본에 동기식으로 복제됩니다. 이는 양쪽의 동기화를 유지하고 DB 인스턴스 장애로부터 최신 데이터베이스 업데이트를 보호하기 위해서입니다. \n",
            "특정 유형의 계획된 유지 관리를 수행하는 도중에, 또는 예기치 않은 DB 인스턴스 장애나 가용 영역 장애가 발생할 경우 Amazon RDS가 자동으로 예비 복제본으로 장애 조치하므로 예비 복제본이 승격되자마자 데이터베이스 쓰기 및 읽기를 재개할 수 있습니다. DB 인스턴스의 이름 레코드는 변경되지 않으므로 관리자가 직접 개입할 필요 없이 애플리케이션이 데이터베이스 작업을 재개할 수 있습니다. 다중 AZ 배포에서 복제본은 투명합니다. 사용자는 예비와 직접 상호 작용하지 않으며, 예비는 읽기 트래픽을 처리하는 데 사용될 수 없습니다. 다중 AZ 배포에 대한 자세한 내용은 Amazon RDS 사용 설명서를 참조하세요.\n",
            "\"category : RDS, question : DB 인스턴스를 다중 AZ 배포로 실행한다는 것은 무엇을 의미합니까?, answer : 다중 AZ 배포로 실행되도록 DB 인스턴스를 생성 또는 수정하면 Amazon RDS가 다른 가용 영역에 동기식 ‘예비’ 복제본을 자동으로 프로비저닝하고 유지합니다. DB 인스턴스에 대한 업데이트는 가용 영역 전체에서 예비 복제본에 동기식으로 복제됩니다. 이는 양쪽의 동기화를 유지하고 DB 인스턴스 장애로부터 최신 데이터베이스 업데이트를 보호하기 위해서입니다. \n",
            "특정 유형의 계획된 유지 관리를 수행하는 도중에, 또는 예기치 않은 DB 인스턴스 장애나 가용 영역 장애가 발생할 경우 Amazon RDS가 자동으로 예비 복제본으로 장애 조치하므로 예비 복제본이 승격되자마자 데이터베이스 쓰기 및 읽기를 재개할 수 있습니다. DB 인스턴스의 이름 레코드는 변경되지 않으므로 관리자가 직접 개입할 필요 없이 애플리케이션이 데이터베이스 작업을 재개할 수 있습니다. 다중 AZ 배포에서 복제본은 투명합니다. 사용자는 예비와 직접 상호 작용하지 않으며, 예비는 읽기 트래픽을 처리하는 데 사용될 수 없습니다. 다중 AZ 배포에 대한 자세한 내용은 Amazon RDS 사용 설명서를 참조하세요.\"\n",
            "가용 영역은 무엇입니까?\n",
            "가용 영역은 다른 가용 영역에서 발생한 장애를 격리시키기 위해 만들어진 리전 내의 개별 장소입니다. 각 가용 영역은 물리적으로 분리된 자체 독립 인프라에서 실행되며 높은 안정성을 제공하도록 설계되었습니다. 발전기 및 냉각 장비와 같은 일반적인 장애 지점은 가용 영역 전체에서 공유되지 않습니다. 또한 물리적으로 분리되어 있어 화재, 태풍 또는 홍수와 같이 예기치 않은 자연 재해가 발생할 경우 단일 가용 영역만 영향을 받게 됩니다. 같은 지역에 있는 가용 영역은 지연 시간이 짧은 네트워크 연결을 제공합니다.\n",
            "\"category : RDS, question : 가용 영역은 무엇입니까?, answer : 가용 영역은 다른 가용 영역에서 발생한 장애를 격리시키기 위해 만들어진 리전 내의 개별 장소입니다. 각 가용 영역은 물리적으로 분리된 자체 독립 인프라에서 실행되며 높은 안정성을 제공하도록 설계되었습니다. 발전기 및 냉각 장비와 같은 일반적인 장애 지점은 가용 영역 전체에서 공유되지 않습니다. 또한 물리적으로 분리되어 있어 화재, 태풍 또는 홍수와 같이 예기치 않은 자연 재해가 발생할 경우 단일 가용 영역만 영향을 받게 됩니다. 같은 지역에 있는 가용 영역은 지연 시간이 짧은 네트워크 연결을 제공합니다.\"\n",
            "다중 AZ 배포에서 ‘기본’과 ‘예비’는 무엇을 의미합니까?\n",
            "DB 인스턴스를 다중 AZ 배포로 실행할 경우 “기본 복제본”이 데이터베이스 쓰기 및 읽기를 처리합니다. 또한 Amazon RDS는 백그라운드에서 기본 복제본의 최신 복제본인 \"예비 복제본\"을 프로비저닝하고 관리합니다. 예비 복제본은 장애 조치 시, “승격”됩니다. 장애 조치 후 예비 복제본이 기본 복제본이 되어 데이터베이스 작업을 수락합니다. 승격되기 전에는 어떠한 경우에도 예비를 사용할 수 없습니다(예: 읽기 작업의 경우). 단일 DB 인스턴스의 용량 한도 이상으로 읽기 트래픽을 확장하려는 경우 읽기 전용 복제본 FAQ를 참조하세요.\n",
            "\"category : RDS, question : 다중 AZ 배포에서 ‘기본’과 ‘예비’는 무엇을 의미합니까?, answer : DB 인스턴스를 다중 AZ 배포로 실행할 경우 “기본 복제본”이 데이터베이스 쓰기 및 읽기를 처리합니다. 또한 Amazon RDS는 백그라운드에서 기본 복제본의 최신 복제본인 \"예비 복제본\"을 프로비저닝하고 관리합니다. 예비 복제본은 장애 조치 시, “승격”됩니다. 장애 조치 후 예비 복제본이 기본 복제본이 되어 데이터베이스 작업을 수락합니다. 승격되기 전에는 어떠한 경우에도 예비를 사용할 수 없습니다(예: 읽기 작업의 경우). 단일 DB 인스턴스의 용량 한도 이상으로 읽기 트래픽을 확장하려는 경우 읽기 전용 복제본 FAQ를 참조하세요.\"\n",
            "다중 AZ 배포의 이점은 무엇입니까?\n",
            "DB 인스턴스를 다중 AZ 배포로 실행할 경우 얻을 수 있는 주요 이점은 데이터베이스 내구성과 가용성의 향상입니다. 다중 AZ 배포가 제공하는 향상된 가용성과 내결함성 덕분에 다중 AZ 배포는 프로덕션 환경에 최적입니다.  \n",
            "DB 인스턴스를 다중 AZ 배포로 실행하면 DB 인스턴스 구성요소 장애 또는 한 가용 영역의 가용성 손실 등 예기치 않은 이벤트 발생 시 데이터를 보호할 수 있습니다. 예를 들어, 기본 복제본의 스토리지 볼륨에 장애가 발생할 경우 Amazon RDS가 자동으로 예비 복제본에 장애 조치를 수행하고 데이터베이스 업데이트는 예비 복제본에서 모두 완전한 상태로 유지됩니다. 이것은 단일 AZ의 표준 배포와 관련하여 추가 데이터 내구성을 제공합니다. 단일 AZ의 표준 배포에서는 사용자가 복구 작업을 시작해야 하고 복구 가능한 최근 시간(일반적으로 최근 5분 내) 후 발생한 업데이트는 사용할 수 없습니다.  DB 인스턴스를 다중 AZ 배포로 실행할 경우 향상된 데이터베이스 가용성을 활용할 수 있습니다. 가용 영역 장애 또는 DB 인스턴스 장애가 발생하면 자동 장애 조치가 완료되는 동안에만 가용성이 영향을 받습니다. 계획된 유지 관리에서도 다중 AZ의 가용성 이점을 활용할 수 있습니다.  예를 들어, 자동 백업을 사용하면 백업이 예비 복제본에서 수행되므로 기본 백업 기간 동안 기본 복제본에서 I/O 작업이 더 이상 중단되지 않습니다. 패치 적용 또는 DB 인스턴스 클래스를 확장하는 경우 이러한 작업은 자동 장애 조치 전에 예비 복제본에 먼저 적용됩니다. 결과적으로 가용성에 미치는 영향은 자동 장애 조치를 완료하는 데 필요한 시간으로 제한됩니다.  DB 인스턴스를 다중 AZ 배포로 실행할 경우 얻을 수 있는 또 다른 이점은 DB 인스턴스 장애 조치가 자동으로 수행되므로 관리가 필요하지 않다는 것입니다. Amazon RDS의 맥락에서 이는 가용 영역 장애 또는 DB 인스턴스 장애가 발생할 경우 DB 인스턴스 이벤트를 모니터링하고 수동으로 DB 인스턴스 복구를 수행(RestoreDBInstanceToPointInTime 또는 RestoreDBInstanceFromSnapshot API를 사용하여)할 필요가 없음을 의미합니다.\n",
            "\"category : RDS, question : 다중 AZ 배포의 이점은 무엇입니까?, answer : DB 인스턴스를 다중 AZ 배포로 실행할 경우 얻을 수 있는 주요 이점은 데이터베이스 내구성과 가용성의 향상입니다. 다중 AZ 배포가 제공하는 향상된 가용성과 내결함성 덕분에 다중 AZ 배포는 프로덕션 환경에 최적입니다.  \n",
            "DB 인스턴스를 다중 AZ 배포로 실행하면 DB 인스턴스 구성요소 장애 또는 한 가용 영역의 가용성 손실 등 예기치 않은 이벤트 발생 시 데이터를 보호할 수 있습니다. 예를 들어, 기본 복제본의 스토리지 볼륨에 장애가 발생할 경우 Amazon RDS가 자동으로 예비 복제본에 장애 조치를 수행하고 데이터베이스 업데이트는 예비 복제본에서 모두 완전한 상태로 유지됩니다. 이것은 단일 AZ의 표준 배포와 관련하여 추가 데이터 내구성을 제공합니다. 단일 AZ의 표준 배포에서는 사용자가 복구 작업을 시작해야 하고 복구 가능한 최근 시간(일반적으로 최근 5분 내) 후 발생한 업데이트는 사용할 수 없습니다.  DB 인스턴스를 다중 AZ 배포로 실행할 경우 향상된 데이터베이스 가용성을 활용할 수 있습니다. 가용 영역 장애 또는 DB 인스턴스 장애가 발생하면 자동 장애 조치가 완료되는 동안에만 가용성이 영향을 받습니다. 계획된 유지 관리에서도 다중 AZ의 가용성 이점을 활용할 수 있습니다.  예를 들어, 자동 백업을 사용하면 백업이 예비 복제본에서 수행되므로 기본 백업 기간 동안 기본 복제본에서 I/O 작업이 더 이상 중단되지 않습니다. 패치 적용 또는 DB 인스턴스 클래스를 확장하는 경우 이러한 작업은 자동 장애 조치 전에 예비 복제본에 먼저 적용됩니다. 결과적으로 가용성에 미치는 영향은 자동 장애 조치를 완료하는 데 필요한 시간으로 제한됩니다.  DB 인스턴스를 다중 AZ 배포로 실행할 경우 얻을 수 있는 또 다른 이점은 DB 인스턴스 장애 조치가 자동으로 수행되므로 관리가 필요하지 않다는 것입니다. Amazon RDS의 맥락에서 이는 가용 영역 장애 또는 DB 인스턴스 장애가 발생할 경우 DB 인스턴스 이벤트를 모니터링하고 수동으로 DB 인스턴스 복구를 수행(RestoreDBInstanceToPointInTime 또는 RestoreDBInstanceFromSnapshot API를 사용하여)할 필요가 없음을 의미합니다.\"\n",
            "DB 인스턴스를 다중 AZ 배포로 실행할 경우 얻을 수 있는 성능 이점이 있습니까?\n",
            "고객을 대신해 수행하는 동기식 데이터 복제로 인해 단일 가용 영역에서 표준 DB 인스턴스 배포와 관련된 지연 시간이 증가할 수 있습니다.\n",
            "\"category : RDS, question : DB 인스턴스를 다중 AZ 배포로 실행할 경우 얻을 수 있는 성능 이점이 있습니까?, answer : 고객을 대신해 수행하는 동기식 데이터 복제로 인해 단일 가용 영역에서 표준 DB 인스턴스 배포와 관련된 지연 시간이 증가할 수 있습니다.\"\n",
            "DB 인스턴스를 다중 AZ 배포로 실행하는 경우, 읽기 또는 쓰기 작업에 예비 복제본을 사용할 수 있습니까?\n",
            "아니요. 다중 AZ 예비 복제본은 읽기 요청을 처리할 수 업습니다. 다중 AZ 배포는 읽기 기능 확장의 이점보다는 향상된 데이터베이스 가용성과 내구성을 제공하도록 설계되었습니다. 따라서 이 기능은 기본 복제본과 예비 복제본 간의 동기 복제를 사용합니다. 당사의 구현 기술은 기본 및 예비가 항상 동기화되지만 읽기 또는 쓰기 작업에 예비를 사용할 수 없도록 합니다. 읽기 확장 솔루션에 관심이 있다면 읽기 전용 복제본의 FAQ를 참조하세요.\n",
            "\"category : RDS, question : DB 인스턴스를 다중 AZ 배포로 실행하는 경우, 읽기 또는 쓰기 작업에 예비 복제본을 사용할 수 있습니까?, answer : 아니요. 다중 AZ 예비 복제본은 읽기 요청을 처리할 수 업습니다. 다중 AZ 배포는 읽기 기능 확장의 이점보다는 향상된 데이터베이스 가용성과 내구성을 제공하도록 설계되었습니다. 따라서 이 기능은 기본 복제본과 예비 복제본 간의 동기 복제를 사용합니다. 당사의 구현 기술은 기본 및 예비가 항상 동기화되지만 읽기 또는 쓰기 작업에 예비를 사용할 수 없도록 합니다. 읽기 확장 솔루션에 관심이 있다면 읽기 전용 복제본의 FAQ를 참조하세요.\"\n",
            "다중 AZ DB 인스턴스 배포 설정은 어떻게 하나요?\n",
            "다중 AZ DB 인스턴스 배포를 만들려면 AWS Management Console에서 DB 인스턴스를 시작할 때 ‘다중 AZ 배포’에서 ‘예’ 옵션을 클릭하면 됩니다.\n",
            "또는 Amazon RDS API를 사용하는 경우 CreateDBInstance API를 호출하여 ‘다중 AZ(Multi-AZ)’ 파라미터 값을 ‘true’로 설정합니다. 기존의 표준(단일 AZ) DB 인스턴스를 다중 AZ로 전환하려면 AWS 관리 콘솔에서 DB 인스턴스를 수정하거나 ModifyDBInstance API를 사용하여 다중 AZ(Multi-AZ) 파라미터를 true로 설정합니다.\n",
            "\"category : RDS, question : 다중 AZ DB 인스턴스 배포 설정은 어떻게 하나요?, answer : 다중 AZ DB 인스턴스 배포를 만들려면 AWS Management Console에서 DB 인스턴스를 시작할 때 ‘다중 AZ 배포’에서 ‘예’ 옵션을 클릭하면 됩니다.\n",
            "또는 Amazon RDS API를 사용하는 경우 CreateDBInstance API를 호출하여 ‘다중 AZ(Multi-AZ)’ 파라미터 값을 ‘true’로 설정합니다. 기존의 표준(단일 AZ) DB 인스턴스를 다중 AZ로 전환하려면 AWS 관리 콘솔에서 DB 인스턴스를 수정하거나 ModifyDBInstance API를 사용하여 다중 AZ(Multi-AZ) 파라미터를 true로 설정합니다.\"\n",
            "Amazon RDS 인스턴스를 단일 AZ에서 다중 AZ로 변경하면 어떻게 되나요?\n",
            "RDS for PostgreSQL, RDS for MySQL, RDS for MariaDB, RDS for SQL Server, RDS for Oracle 및 RDS for Db2 데이터베이스 엔진의 경우 Amazon RDS 인스턴스를 단일 AZ에서 다중 AZ로 변환하도록 선택하면 다음과 같은 프로세스가 수행됩니다.\n",
            "\n",
            "기본 인스턴스의 스냅샷이 생성됩니다.\n",
            "다른 가용 영역에서 위의 스냅샷으로부터 새로운 대기 인스턴스가 생성됩니다.\n",
            "기본 인스턴스와 대기 인스턴스 간에 동기식 복제가 구성됩니다.\n",
            "\n",
            " \n",
            "이에 따라 인스턴스가 단일 AZ에서 다중 AZ로 전환될 때 가동 중지 시간이 발생하지 않습니다. 하지만 예비 인스턴스의 데이터가 기본 인스턴스의 데이터와 일치하도록 업데이트되는 동안 지연 시간이 증가할 수 있습니다.\n",
            "\"category : RDS, question : Amazon RDS 인스턴스를 단일 AZ에서 다중 AZ로 변경하면 어떻게 되나요?, answer : RDS for PostgreSQL, RDS for MySQL, RDS for MariaDB, RDS for SQL Server, RDS for Oracle 및 RDS for Db2 데이터베이스 엔진의 경우 Amazon RDS 인스턴스를 단일 AZ에서 다중 AZ로 변환하도록 선택하면 다음과 같은 프로세스가 수행됩니다.\n",
            "\n",
            "기본 인스턴스의 스냅샷이 생성됩니다.\n",
            "다른 가용 영역에서 위의 스냅샷으로부터 새로운 대기 인스턴스가 생성됩니다.\n",
            "기본 인스턴스와 대기 인스턴스 간에 동기식 복제가 구성됩니다.\n",
            "\n",
            " \n",
            "이에 따라 인스턴스가 단일 AZ에서 다중 AZ로 전환될 때 가동 중지 시간이 발생하지 않습니다. 하지만 예비 인스턴스의 데이터가 기본 인스턴스의 데이터와 일치하도록 업데이트되는 동안 지연 시간이 증가할 수 있습니다.\"\n",
            "Amazon RDS가 예비 복제본에 장애 조치를 실행하도록 하는 이벤트는 무엇입니까?\n",
            "Amazon RDS는 다중 AZ 배포에 대한 가장 빈번한 오류를 감지해 자동으로 복구하므로 관리자의 개입 없이 데이터베이스 작업을 최대한 빨리 재개할 수 있습니다. Amazon RDS는 다음과 같은 이벤트가 발생하는 경우 장애 조치를 자동으로 수행합니다.\n",
            "\n",
            "기본 가용 영역의 가용성 손실\n",
            "기본 복제본에 대한 네트워크 연결 상실\n",
            "기본 복제본의 컴퓨팅 장치 장애\n",
            "기본 복제본의 스토리지 장애\n",
            "\n",
            "참고: 다중 AZ 배포에서 DB 인스턴스 조정 또는 OS 패치와 같은 시스템 업그레이드 작업을 하는 경우 가용성을 향상하기 위해 자동 장애 조치 전에 먼저 예비 복제본에 해당 작업이 적용됩니다. 따라서 자동 장애 조치를 완료하는 데 걸리는 시간 동안만 가용성에 영향을 주게 됩니다. Amazon RDS 다중 AZ 배포는 장시간 동작 쿼리, 교착 상태 또는 데이터베이스 손상 오류 같은 데이터베이스 동작에 대해서는 자동으로 장애 조치를 수행하지 않습니다.\n",
            "\"category : RDS, question : Amazon RDS가 예비 복제본에 장애 조치를 실행하도록 하는 이벤트는 무엇입니까?, answer : Amazon RDS는 다중 AZ 배포에 대한 가장 빈번한 오류를 감지해 자동으로 복구하므로 관리자의 개입 없이 데이터베이스 작업을 최대한 빨리 재개할 수 있습니다. Amazon RDS는 다음과 같은 이벤트가 발생하는 경우 장애 조치를 자동으로 수행합니다.\n",
            "\n",
            "기본 가용 영역의 가용성 손실\n",
            "기본 복제본에 대한 네트워크 연결 상실\n",
            "기본 복제본의 컴퓨팅 장치 장애\n",
            "기본 복제본의 스토리지 장애\n",
            "\n",
            "참고: 다중 AZ 배포에서 DB 인스턴스 조정 또는 OS 패치와 같은 시스템 업그레이드 작업을 하는 경우 가용성을 향상하기 위해 자동 장애 조치 전에 먼저 예비 복제본에 해당 작업이 적용됩니다. 따라서 자동 장애 조치를 완료하는 데 걸리는 시간 동안만 가용성에 영향을 주게 됩니다. Amazon RDS 다중 AZ 배포는 장시간 동작 쿼리, 교착 상태 또는 데이터베이스 손상 오류 같은 데이터베이스 동작에 대해서는 자동으로 장애 조치를 수행하지 않습니다.\"\n",
            "자동 장애 조치가 발생할 때 알림을 받을 수 있습니까?\n",
            "예, Amazon RDS가 DB 인스턴스 이벤트를 내보내 자동 장애 조치가 발생했음을 알립니다. Amazon RDS 콘솔의 \"Events\" 섹션을 클릭하거나 DescribeEvents API를 사용하여 DB 인스턴스와 관련된 이벤트 정보를 반환할 수 있습니다. 또한 Amazon RDS 이벤트 알림을 사용하면 특정 DB 이벤트가 발생할 때 알림을 받을 수 있습니다.\n",
            "\"category : RDS, question : 자동 장애 조치가 발생할 때 알림을 받을 수 있습니까?, answer : 예, Amazon RDS가 DB 인스턴스 이벤트를 내보내 자동 장애 조치가 발생했음을 알립니다. Amazon RDS 콘솔의 \"Events\" 섹션을 클릭하거나 DescribeEvents API를 사용하여 DB 인스턴스와 관련된 이벤트 정보를 반환할 수 있습니다. 또한 Amazon RDS 이벤트 알림을 사용하면 특정 DB 이벤트가 발생할 때 알림을 받을 수 있습니다.\"\n",
            "다중 AZ 장애 조치 도중 어떤 일이 발생하며 얼마나 오래 걸립니까?\n",
            "장애 조치는 Amazon RDS가 자동으로 처리하므로 관리자가 개입하지 않고 최대한 신속하게 데이터베이스 작업을 재개할 수 있습니다. 장애 조치 시, Amazon RDS는 DB 인스턴스의 정식 이름 레코드(CNAME)가 예비 복제본을 가리키도록 변경합니다. 그러면 이 예비 복제본이 승격되어 새 기본 복제본이 됩니다. 모범 사례에 따라 애플리케이션 계층에서 데이터베이스 연결을 다시 시도하는 것이 좋습니다.\n",
            "기본 복제본에서 장애를 감지하고 예비 복제본에서 트랜잭션을 재개하는 데 걸리는 시간 간격으로 정의되는 장애 조치는 일반적으로 1~2분 내에 완료됩니다. 장애 조치 시간은 활발하게 사용되지 않는 대규모 트랜잭션의 복구 필요 여부에도 영향을 받습니다. 최상의 결과를 위해서는 다중 AZ와 함께 적당히 큰 인스턴스 유형을 사용하는 것이 좋습니다. 또한, 빠르고 예측 가능하며 일관된 처리량 성능을 위해 다중 AZ 인스턴스와 함께 프로비저닝된 IOPS를 사용하는 것이 좋습니다.\n",
            "\"category : RDS, question : 다중 AZ 장애 조치 도중 어떤 일이 발생하며 얼마나 오래 걸립니까?, answer : 장애 조치는 Amazon RDS가 자동으로 처리하므로 관리자가 개입하지 않고 최대한 신속하게 데이터베이스 작업을 재개할 수 있습니다. 장애 조치 시, Amazon RDS는 DB 인스턴스의 정식 이름 레코드(CNAME)가 예비 복제본을 가리키도록 변경합니다. 그러면 이 예비 복제본이 승격되어 새 기본 복제본이 됩니다. 모범 사례에 따라 애플리케이션 계층에서 데이터베이스 연결을 다시 시도하는 것이 좋습니다.\n",
            "기본 복제본에서 장애를 감지하고 예비 복제본에서 트랜잭션을 재개하는 데 걸리는 시간 간격으로 정의되는 장애 조치는 일반적으로 1~2분 내에 완료됩니다. 장애 조치 시간은 활발하게 사용되지 않는 대규모 트랜잭션의 복구 필요 여부에도 영향을 받습니다. 최상의 결과를 위해서는 다중 AZ와 함께 적당히 큰 인스턴스 유형을 사용하는 것이 좋습니다. 또한, 빠르고 예측 가능하며 일관된 처리량 성능을 위해 다중 AZ 인스턴스와 함께 프로비저닝된 IOPS를 사용하는 것이 좋습니다.\"\n",
            "다중 AZ DB 인스턴스 배포에 ‘강제 장애 조치’를 실행할 수 있습니까?\n",
            "Amazon RDS는 다양한 장애 조건에 대해 사용자의 개입 없이 자동으로 장애 조치를 수행합니다. 또한 Amazon RDS는 인스턴스를 재시작할 때 장애 조치를 시작하는 옵션을 제공합니다. AWS Management Console을 사용하거나 RebootDBInstance API를 호출하여 이 기능에 액세스할 수 있습니다.\n",
            "\"category : RDS, question : 다중 AZ DB 인스턴스 배포에 ‘강제 장애 조치’를 실행할 수 있습니까?, answer : Amazon RDS는 다양한 장애 조건에 대해 사용자의 개입 없이 자동으로 장애 조치를 수행합니다. 또한 Amazon RDS는 인스턴스를 재시작할 때 장애 조치를 시작하는 옵션을 제공합니다. AWS Management Console을 사용하거나 RebootDBInstance API를 호출하여 이 기능에 액세스할 수 있습니다.\"\n",
            "다중 AZ 동기 복제는 어떻게 제어/구성합니까?\n",
            "다중 AZ 배포를 사용할 경우, “Multi-AZ” 매개 변수를 true로 설정하면 됩니다. 예비 복제본, 동기 복제 및 장애 조치 생성은 모두 자동으로 처리됩니다. 즉, 예비 복제본이 배포되는 가용 영역을 선택하거나 사용 가능한 예비 복제본의 수를 변경할 수 없습니다(Amazon RDS는 기본 DB 인스턴스당 하나의 전용 예비 복제본을 프로비저닝함). 예비 복제본이 데이터베이스 읽기 작업을 허용하도록 설정할 수도 없습니다. 다중 AZ 구성에 대해 자세히 알아보십시오.\n",
            "\"category : RDS, question : 다중 AZ 동기 복제는 어떻게 제어/구성합니까?, answer : 다중 AZ 배포를 사용할 경우, “Multi-AZ” 매개 변수를 true로 설정하면 됩니다. 예비 복제본, 동기 복제 및 장애 조치 생성은 모두 자동으로 처리됩니다. 즉, 예비 복제본이 배포되는 가용 영역을 선택하거나 사용 가능한 예비 복제본의 수를 변경할 수 없습니다(Amazon RDS는 기본 DB 인스턴스당 하나의 전용 예비 복제본을 프로비저닝함). 예비 복제본이 데이터베이스 읽기 작업을 허용하도록 설정할 수도 없습니다. 다중 AZ 구성에 대해 자세히 알아보십시오.\"\n",
            "예비는 기본과 같은 리전에 배치됩니까?\n",
            "예. 예비 인스턴스는 기본 DB 인스턴스와 동일한 리전의 다른 가용 영역에 자동으로 프로비저닝됩니다.\n",
            "\"category : RDS, question : 예비는 기본과 같은 리전에 배치됩니까?, answer : 예. 예비 인스턴스는 기본 DB 인스턴스와 동일한 리전의 다른 가용 영역에 자동으로 프로비저닝됩니다.\"\n",
            "현재 프라이머리가 위치한 가용 영역을 알 수 있나요?\n",
            "예. AWS Management Console 또는 DescribeDBInstances API를 사용하여 현재 기본 인스턴스의 위치를 알 수 있습니다.\n",
            "\"category : RDS, question : 현재 프라이머리가 위치한 가용 영역을 알 수 있나요?, answer : 예. AWS Management Console 또는 DescribeDBInstances API를 사용하여 현재 기본 인스턴스의 위치를 알 수 있습니다.\"\n",
            "장애 조치 후에 내 기본 복제본이 다른 AWS 리소스(예: EC2 인스턴스)와 다른 가용 영역에 위치합니다. 지연 시간을 고려해야 합니까?\n",
            "가용 영역은 같은 리전의 다른 가용 영역에 지연 시간이 짧은 네트워크 연결을 제공하도록 설계되었습니다. 또한 하나의 가용 영역에서 서비스 장애 발생 시, 애플리케이션이 회복력을 가질 수 있도록 여러 가용 영역 전체에 애플리케이션과 기타 AWS 리소스를 중복하여 저장할 수 있습니다. 다중 AZ 배포는 사용자의 관리 작업 없이 데이터베이스에서 이러한 요구를 해결합니다.\n",
            "\"category : RDS, question : 장애 조치 후에 내 기본 복제본이 다른 AWS 리소스(예: EC2 인스턴스)와 다른 가용 영역에 위치합니다. 지연 시간을 고려해야 합니까?, answer : 가용 영역은 같은 리전의 다른 가용 영역에 지연 시간이 짧은 네트워크 연결을 제공하도록 설계되었습니다. 또한 하나의 가용 영역에서 서비스 장애 발생 시, 애플리케이션이 회복력을 가질 수 있도록 여러 가용 영역 전체에 애플리케이션과 기타 AWS 리소스를 중복하여 저장할 수 있습니다. 다중 AZ 배포는 사용자의 관리 작업 없이 데이터베이스에서 이러한 요구를 해결합니다.\"\n",
            "DB 스냅샷과 자동 백업이 다중 AZ 배포에서 어떻게 작동합니까?\n",
            "표준 배포를 단일 AZ 배포로 실행하든, 다중 AZ 배포로 실행하든 관계없이 동일한 방식으로 자동 백업과 DB 스냅샷 기능을 사용할 수 있습니다. 다중 AZ 배포를 실행할 경우 I/O가 기본 복제본에서 중단되지 않도록 자동 백업 및 DB 스냅샷이 예비 복제본에서 수행됩니다. 단일 또는 다중 AZ 배포의 백업 도중에 I/O 지연 시간(일반적으로 몇 분 덩인 지속)이 길어질 수 있습니다.\n",
            "다중 AZ 배포를 사용할 경우 복원 작업(지정 시간 복구 또는 DB 스냅샷에서 복구) 실행도 표준 단일 AZ 배포와 동일하게 작동합니다. RestoreDBInstanceFromSnapshot 또는 RestoreDBInstanceToPointInTime API를 사용해 새로운 DB 인스턴스 배포를 만들 수 있습니다. 이 새 DB 인스턴스 배포는 표준 또는 다중 AZ가 될 수 있으며, 원본 백업이 표준 또는 다중 AZ 배포에서 실행되었는지는 관계없습니다.\n",
            "\"category : RDS, question : DB 스냅샷과 자동 백업이 다중 AZ 배포에서 어떻게 작동합니까?, answer : 표준 배포를 단일 AZ 배포로 실행하든, 다중 AZ 배포로 실행하든 관계없이 동일한 방식으로 자동 백업과 DB 스냅샷 기능을 사용할 수 있습니다. 다중 AZ 배포를 실행할 경우 I/O가 기본 복제본에서 중단되지 않도록 자동 백업 및 DB 스냅샷이 예비 복제본에서 수행됩니다. 단일 또는 다중 AZ 배포의 백업 도중에 I/O 지연 시간(일반적으로 몇 분 덩인 지속)이 길어질 수 있습니다.\n",
            "다중 AZ 배포를 사용할 경우 복원 작업(지정 시간 복구 또는 DB 스냅샷에서 복구) 실행도 표준 단일 AZ 배포와 동일하게 작동합니다. RestoreDBInstanceFromSnapshot 또는 RestoreDBInstanceToPointInTime API를 사용해 새로운 DB 인스턴스 배포를 만들 수 있습니다. 이 새 DB 인스턴스 배포는 표준 또는 다중 AZ가 될 수 있으며, 원본 백업이 표준 또는 다중 AZ 배포에서 실행되었는지는 관계없습니다.\"\n",
            "DB 인스턴스를 읽기 복제본으로 실행한다는 것은 무엇을 의미하나요?\n",
            "읽기 전용 복제본은 지원되는 엔진에 내장된 복제 기능을 통해 단일 DB 인스턴스의 용량 제한을 탄력적으로 스케일 아웃하여 읽기 중심의 데이터베이스 워크로드를 손쉽게 처리하도록 지원합니다.\n",
            "AWS Management Console을 몇 번 클릭하거나 CreateDBInstanceReadReplica API를 사용하여 읽기 전용 복제본을 생성할 수 있습니다. 읽기 전용 복제본이 생성되면 지원되는 엔진의 기본 비동기식 복제 기능을 사용하여 소스 DB 인스턴스에 대한 데이터베이스 업데이트가 복제됩니다. 해당 원본 DB 인스턴스에 대한 여러 읽기 전용 복제본을 만들어 애플리케이션의 읽기 트래픽을 분산할 수 있습니다.\n",
            "읽기 전용 복제본은 지원되는 엔진에 내장된 복제 기능을 사용하므로 강점과 약점을 갖고 있습니다. 특히 소스 DB 인스턴스에 업데이트가 발생한 후 읽기 전용 복제본에 업데이트가 적용됩니다. 즉, 복제 지연 시간은 상당히 다를 수 있습니다. 읽기 전용 복제본을 다중 AZ 배포에 연결하여 다중 AZ 배포가 제공하는 향상된 데이터베이스 쓰기 가용성과 데이터 내구성 외에 읽기 크기 조정의 이점을 얻을 수 있습니다.\n",
            "\"category : RDS, question : DB 인스턴스를 읽기 복제본으로 실행한다는 것은 무엇을 의미하나요?, answer : 읽기 전용 복제본은 지원되는 엔진에 내장된 복제 기능을 통해 단일 DB 인스턴스의 용량 제한을 탄력적으로 스케일 아웃하여 읽기 중심의 데이터베이스 워크로드를 손쉽게 처리하도록 지원합니다.\n",
            "AWS Management Console을 몇 번 클릭하거나 CreateDBInstanceReadReplica API를 사용하여 읽기 전용 복제본을 생성할 수 있습니다. 읽기 전용 복제본이 생성되면 지원되는 엔진의 기본 비동기식 복제 기능을 사용하여 소스 DB 인스턴스에 대한 데이터베이스 업데이트가 복제됩니다. 해당 원본 DB 인스턴스에 대한 여러 읽기 전용 복제본을 만들어 애플리케이션의 읽기 트래픽을 분산할 수 있습니다.\n",
            "읽기 전용 복제본은 지원되는 엔진에 내장된 복제 기능을 사용하므로 강점과 약점을 갖고 있습니다. 특히 소스 DB 인스턴스에 업데이트가 발생한 후 읽기 전용 복제본에 업데이트가 적용됩니다. 즉, 복제 지연 시간은 상당히 다를 수 있습니다. 읽기 전용 복제본을 다중 AZ 배포에 연결하여 다중 AZ 배포가 제공하는 향상된 데이터베이스 쓰기 가용성과 데이터 내구성 외에 읽기 크기 조정의 이점을 얻을 수 있습니다.\"\n",
            "Amazon RDS 읽기 복제본은 언제 사용하는 것이 좋은가요?\n",
            "특정 소스 DB 인스턴스가 제대로 작동하기 위해 읽기 전용 복제본을 배포하는 위치는 다양합니다. 읽기 전용 복제본을 배포하는 일반적인 이유는 다음과 같습니다.\n",
            "\n",
            "읽기 중심의 데이터베이스 워크로드를 위해 단일 DB 인스턴스의 컴퓨팅 파워 또는 I/O 용량을 확장합니다. 이 과도한 읽기 트래픽을 하나 이상의 읽기 전용 복제본으로 이동할 수 있습니다.\n",
            "원본 DB 인스턴스를 사용할 수 없는 동안 읽기 트래픽을 처리합니다. 소스 DB 인스턴스가 I/O 요청을 처리하지 못할 경우(예: 백업 또는 예약된 유지 관리를 위한 I/O 중단) 읽기 트래픽을 읽기 전용 복제본으로 이동할 수 있습니다. 이 사용 사례의 경우 소스 DB 인스턴스를 사용할 수 없으므로 읽기 전용 복제본의 데이터가 ‘무효’일 수 있다는 점에 유의하세요.\n",
            "비즈니스 보고 또는 데이터 웨어하우징 시나리오 기본 프로덕션 DB 인스턴스가 아니라 읽기 전용 복제본에 대해 비즈니스 보고 쿼리를 실행하려고 할 수 있습니다.\n",
            "동일한 AWS 리전에 있거나 다른 리전에 있는 소스 DB 인스턴스의 재해 복구를 위해 읽기 전용 복제본을 사용할 수 있습니다.\n",
            "\"category : RDS, question : Amazon RDS 읽기 복제본은 언제 사용하는 것이 좋은가요?, answer : 특정 소스 DB 인스턴스가 제대로 작동하기 위해 읽기 전용 복제본을 배포하는 위치는 다양합니다. 읽기 전용 복제본을 배포하는 일반적인 이유는 다음과 같습니다.\n",
            "\n",
            "읽기 중심의 데이터베이스 워크로드를 위해 단일 DB 인스턴스의 컴퓨팅 파워 또는 I/O 용량을 확장합니다. 이 과도한 읽기 트래픽을 하나 이상의 읽기 전용 복제본으로 이동할 수 있습니다.\n",
            "원본 DB 인스턴스를 사용할 수 없는 동안 읽기 트래픽을 처리합니다. 소스 DB 인스턴스가 I/O 요청을 처리하지 못할 경우(예: 백업 또는 예약된 유지 관리를 위한 I/O 중단) 읽기 트래픽을 읽기 전용 복제본으로 이동할 수 있습니다. 이 사용 사례의 경우 소스 DB 인스턴스를 사용할 수 없으므로 읽기 전용 복제본의 데이터가 ‘무효’일 수 있다는 점에 유의하세요.\n",
            "비즈니스 보고 또는 데이터 웨어하우징 시나리오 기본 프로덕션 DB 인스턴스가 아니라 읽기 전용 복제본에 대해 비즈니스 보고 쿼리를 실행하려고 할 수 있습니다.\n",
            "동일한 AWS 리전에 있거나 다른 리전에 있는 소스 DB 인스턴스의 재해 복구를 위해 읽기 전용 복제본을 사용할 수 있습니다.\"\n",
            "읽기 전용 복제본을 만들려면 내 DB 인스턴스에서 자동 백업을 활성화해야 합니까?\n",
            "예. 백업 보존 기간을 0이 아닌 값으로 설정하여 읽기 전용 복제본을 추가하기 전에 소스 DB 인스턴스에서 자동 백업을 활성화하세요. 읽기 전용 복제본이 작동하려면 백업이 활성화된 상태로 유지되어야 합니다.\n",
            "\"category : RDS, question : 읽기 전용 복제본을 만들려면 내 DB 인스턴스에서 자동 백업을 활성화해야 합니까?, answer : 예. 백업 보존 기간을 0이 아닌 값으로 설정하여 읽기 전용 복제본을 추가하기 전에 소스 DB 인스턴스에서 자동 백업을 활성화하세요. 읽기 전용 복제본이 작동하려면 백업이 활성화된 상태로 유지되어야 합니다.\"\n",
            "Amazon RDS 읽기 전용 복제본을 지원하는 데이터베이스 엔진 버전은 무엇입니까?\n",
            "Amazon Aurora: 모든 DB 클러스터\n",
            "Amazon RDS for MySQL: 모든 DB 인스턴스가 읽기 전용 복제본의 생성을 지원합니다. 읽기 전용 복제본이 작동하려면 소스 DB 인스턴스에 자동 백업이 활성화된 상태로 유지되어야 합니다. 복제본에 대한 자동 백업은 MySQL 5.6 이상(5.5는 해당 안 됨)을 실행하는 Amazon RDS 읽기 전용 복제본에 대해서만 지원됩니다.\n",
            "Amazon RDS for PostgreSQL: PostgreSQL 버전이 9.3.5 이상인 DB 인스턴스만 읽기 전용 복제본 생성을 지원합니다. Amazon RDS 읽기 전용 복제본을 활용하려면 9.3.5 이전 버전의 기존 PostgreSQL 인스턴스를 PostgreSQL 9.3.5 버전으로 업그레이드해야 합니다.\n",
            "Amazon RDS for MariaDB: 모든 DB 인스턴스가 읽기 전용 복제본 생성을 지원합니다. 읽기 전용 복제본이 작동하려면 소스 DB 인스턴스에 자동 백업이 활성화된 상태로 유지되어야 합니다.\n",
            "Amazon RDS for Oracle: Oracle Database Enterprise Edition에서 BYOL(기존 보유 라이선스 사용) 모델을 사용하고 Active Data Guard 옵션에 대한 라이선스를 보유한 Oracle 버전 12.1.0.2.v12 이상 및 모든 12.2 버전에 지원됩니다.\n",
            "Amazon RDS for SQL Server: 기본 복제 기술이 SQL Server 버전 2016 및 2017용 Always On 가용성 그룹을 사용하는 경우 다중 AZ 구성의 엔터프라이즈 에디션에서 읽기 전용 복제본이 지원됩니다.\n",
            "\"category : RDS, question : Amazon RDS 읽기 전용 복제본을 지원하는 데이터베이스 엔진 버전은 무엇입니까?, answer : Amazon Aurora: 모든 DB 클러스터\n",
            "Amazon RDS for MySQL: 모든 DB 인스턴스가 읽기 전용 복제본의 생성을 지원합니다. 읽기 전용 복제본이 작동하려면 소스 DB 인스턴스에 자동 백업이 활성화된 상태로 유지되어야 합니다. 복제본에 대한 자동 백업은 MySQL 5.6 이상(5.5는 해당 안 됨)을 실행하는 Amazon RDS 읽기 전용 복제본에 대해서만 지원됩니다.\n",
            "Amazon RDS for PostgreSQL: PostgreSQL 버전이 9.3.5 이상인 DB 인스턴스만 읽기 전용 복제본 생성을 지원합니다. Amazon RDS 읽기 전용 복제본을 활용하려면 9.3.5 이전 버전의 기존 PostgreSQL 인스턴스를 PostgreSQL 9.3.5 버전으로 업그레이드해야 합니다.\n",
            "Amazon RDS for MariaDB: 모든 DB 인스턴스가 읽기 전용 복제본 생성을 지원합니다. 읽기 전용 복제본이 작동하려면 소스 DB 인스턴스에 자동 백업이 활성화된 상태로 유지되어야 합니다.\n",
            "Amazon RDS for Oracle: Oracle Database Enterprise Edition에서 BYOL(기존 보유 라이선스 사용) 모델을 사용하고 Active Data Guard 옵션에 대한 라이선스를 보유한 Oracle 버전 12.1.0.2.v12 이상 및 모든 12.2 버전에 지원됩니다.\n",
            "Amazon RDS for SQL Server: 기본 복제 기술이 SQL Server 버전 2016 및 2017용 Always On 가용성 그룹을 사용하는 경우 다중 AZ 구성의 엔터프라이즈 에디션에서 읽기 전용 복제본이 지원됩니다.\"\n",
            "특정 DB 인스턴스의 읽기 복제본을 배포하려면 어떻게 해야 하나요?\n",
            "표준 CreateDBInstanceReadReplica API를 사용하거나 AWS Management Console에서 몇 단계를 수행하여 몇 분 만에 읽기 전용 복제본을 만들 수 있습니다. 읽기 전용 복제본을 만들 때 SourceDBInstanceIdentifier를 지정하여 읽기 전용 복제본을 확인할 수 있습니다. SourceDBInstanceIdentifier는 복제할 “소스” DB 인스턴스의 DB 인스턴스 식별자입니다. 표준 DB 인스턴스와 마찬가지로 가용 영역, DB 인스턴스 클래스 및 기본 유지 관리 기간을 지정할 수 있습니다. 엔진 버전(예: PostgreSQL 9.3.5)과 읽기 전용 복제본의 스토리지 할당은 소스 DB 인스턴스에서 상속됩니다. \n",
            "읽기 전용 복제본 생성을 시작하면 Amazon RDS가 소스 DB 인스턴스의 스냅샷을 만들고 복제를 시작합니다. 그 결과, 스냅샷을 찍는 동안 소스 DB 인스턴스에 가벼운 I/O 중단이 발생합니다. I/O 중단은 일반적으로 1분 정도입니다. 소스 DB 인스턴스가 다중 AZ 배포(다중 AZ 배포의 경우 스냅샷이 예비 복제본에서 생성됨)인 경우에는 발생하지 않습니다.\n",
            "Amazon RDS는 현재 최적화가 이루어지고 있습니다(곧 릴리스될 예정). 여러 읽기 전용 복제본을 30분 내에 생성할 경우 모든 읽기 전용 복제본이 동일한 소스 스냅샷을 사용해 I/O 영향을 최소화합니다(각 읽기 전용 복제본이 생성된 후 “추적” 복제가 시작됨).\n",
            "\"category : RDS, question : 특정 DB 인스턴스의 읽기 복제본을 배포하려면 어떻게 해야 하나요?, answer : 표준 CreateDBInstanceReadReplica API를 사용하거나 AWS Management Console에서 몇 단계를 수행하여 몇 분 만에 읽기 전용 복제본을 만들 수 있습니다. 읽기 전용 복제본을 만들 때 SourceDBInstanceIdentifier를 지정하여 읽기 전용 복제본을 확인할 수 있습니다. SourceDBInstanceIdentifier는 복제할 “소스” DB 인스턴스의 DB 인스턴스 식별자입니다. 표준 DB 인스턴스와 마찬가지로 가용 영역, DB 인스턴스 클래스 및 기본 유지 관리 기간을 지정할 수 있습니다. 엔진 버전(예: PostgreSQL 9.3.5)과 읽기 전용 복제본의 스토리지 할당은 소스 DB 인스턴스에서 상속됩니다. \n",
            "읽기 전용 복제본 생성을 시작하면 Amazon RDS가 소스 DB 인스턴스의 스냅샷을 만들고 복제를 시작합니다. 그 결과, 스냅샷을 찍는 동안 소스 DB 인스턴스에 가벼운 I/O 중단이 발생합니다. I/O 중단은 일반적으로 1분 정도입니다. 소스 DB 인스턴스가 다중 AZ 배포(다중 AZ 배포의 경우 스냅샷이 예비 복제본에서 생성됨)인 경우에는 발생하지 않습니다.\n",
            "Amazon RDS는 현재 최적화가 이루어지고 있습니다(곧 릴리스될 예정). 여러 읽기 전용 복제본을 30분 내에 생성할 경우 모든 읽기 전용 복제본이 동일한 소스 스냅샷을 사용해 I/O 영향을 최소화합니다(각 읽기 전용 복제본이 생성된 후 “추적” 복제가 시작됨).\"\n",
            "읽기 전용 복제본에 어떻게 연결하나요?\n",
            "표준 DB 인스턴스에 연결할 때와 마찬가지로 DescribeDBInstance API 또는 AWS 관리 콘솔을 사용하여 읽기 전용 복제본의 엔드포인트를 검색하여 읽기 전용 복제본에 연결할 수 있습니다. 읽기 전용 복제본이 여러 개인 경우 애플리케이션이 읽기 전용 복제본에 읽기 트래픽을 분산하는 방법을 결정합니다.\n",
            "\"category : RDS, question : 읽기 전용 복제본에 어떻게 연결하나요?, answer : 표준 DB 인스턴스에 연결할 때와 마찬가지로 DescribeDBInstance API 또는 AWS 관리 콘솔을 사용하여 읽기 전용 복제본의 엔드포인트를 검색하여 읽기 전용 복제본에 연결할 수 있습니다. 읽기 전용 복제본이 여러 개인 경우 애플리케이션이 읽기 전용 복제본에 읽기 트래픽을 분산하는 방법을 결정합니다.\"\n",
            "특정 소스 DB 인스턴스에 대한 읽기 복제본을 몇 개까지 생성할 수 있나요?\n",
            "Amazon RDS for MySQL, Amazon RDS for MariaDB, Amazon RDS for PostgreSQL을 사용하면 특정 소스 DB 인스턴스에 대한 읽기 전용 복제본을 최대 15개까지 생성할 수 있습니다. Amazon RDS for Oracle 및 Amazon RDS for SQL Server를 사용하면 특정 소스 DB 인스턴스에 대한 읽기 전용 복제본을 최대 5개까지 생성할 수 있습니다.\n",
            "\"category : RDS, question : 특정 소스 DB 인스턴스에 대한 읽기 복제본을 몇 개까지 생성할 수 있나요?, answer : Amazon RDS for MySQL, Amazon RDS for MariaDB, Amazon RDS for PostgreSQL을 사용하면 특정 소스 DB 인스턴스에 대한 읽기 전용 복제본을 최대 15개까지 생성할 수 있습니다. Amazon RDS for Oracle 및 Amazon RDS for SQL Server를 사용하면 특정 소스 DB 인스턴스에 대한 읽기 전용 복제본을 최대 5개까지 생성할 수 있습니다.\"\n",
            "소스 DB 인스턴스와 다른 AWS 리전에서 읽기 전용 복제본을 생성할 수 있나요?\n",
            "예. Amazon RDS(RDS for SQL Server 제외)는 리전 간 읽기 전용 복제본을 지원합니다. 데이터를 원본 DB 인스턴스에 쓰는 시점과 데이터가 읽기 복제본에서 사용 가능한 시점 사이의 시간은 두 리전 간의 네트워크 지연 시간에 따라 다릅니다.\n",
            "\"category : RDS, question : 소스 DB 인스턴스와 다른 AWS 리전에서 읽기 전용 복제본을 생성할 수 있나요?, answer : 예. Amazon RDS(RDS for SQL Server 제외)는 리전 간 읽기 전용 복제본을 지원합니다. 데이터를 원본 DB 인스턴스에 쓰는 시점과 데이터가 읽기 복제본에서 사용 가능한 시점 사이의 시간은 두 리전 간의 네트워크 지연 시간에 따라 다릅니다.\"\n",
            "Amazon RDS 읽기 전용 복제본이 동기식 복제를 지원합니까?\n",
            "아니요. Amazon RDS for MySQL, MariaDB, PostgreSQL, Oracle 및 SQL Server의 읽기 전용 복제본은 이러한 엔진의 기본 비동기식 복제를 사용하여 구현됩니다. Amazon Aurora는 다른 복제 메커니즘을 사용하지만, 여전히 비동기식입니다.\n",
            "\"category : RDS, question : Amazon RDS 읽기 전용 복제본이 동기식 복제를 지원합니까?, answer : 아니요. Amazon RDS for MySQL, MariaDB, PostgreSQL, Oracle 및 SQL Server의 읽기 전용 복제본은 이러한 엔진의 기본 비동기식 복제를 사용하여 구현됩니다. Amazon Aurora는 다른 복제 메커니즘을 사용하지만, 여전히 비동기식입니다.\"\n",
            "읽기 복제본을 사용해 데이터베이스 쓰기 가용성을 개선하거나 내 소스 DB 인스턴스의 데이터를 장애로부터 보호할 수 있습니까?\n",
            "복제를 사용해 데이터베이스 쓰기 가용성을 높이고 최근 데이터베이스 업데이트를 다양한 장애 조건으로부터 보호하려면 DB 인스턴스를 다중 AZ 배포로 실행하는 것이 좋습니다. Amazon RDS 읽기 전용 복제본과 지원되는 엔진의 기본 비동기식 복제를 사용하면 데이터베이스 쓰기가 소스 DB 인스턴스에서 발생한 다음 읽기 전용 복제본에서 발생합니다. 이 복제 “지연 시간”은 상당히 다를 수 있습니다. \n",
            "반면 다중 AZ 배포에서 사용되는 복제는 동기식이므로 모든 데이터베이스 쓰기가 기본 복제본과 예비 복제본에서 동시에 이루어집니다. 이렇게 최신 데이터베이스 업데이트를 보호하여 장애 조치 시, 예비 복제본에서 최신 데이터베이스 업데이트를 사용할 수 있습니다. \n",
            "또한 다중 AZ 배포를 통해 복제가 완벽하게 관리됩니다. Amazon RDS는 자동으로 DB 인스턴스 장애 상태 또는 가용 영역 장애를 모니터링하고, 가동 중단이 발생하면 예비 복제본(Amazon Aurora에서는 읽기 전용 복제본)으로 자동 장애 조치를 수행합니다.\n",
            "\"category : RDS, question : 읽기 복제본을 사용해 데이터베이스 쓰기 가용성을 개선하거나 내 소스 DB 인스턴스의 데이터를 장애로부터 보호할 수 있습니까?, answer : 복제를 사용해 데이터베이스 쓰기 가용성을 높이고 최근 데이터베이스 업데이트를 다양한 장애 조건으로부터 보호하려면 DB 인스턴스를 다중 AZ 배포로 실행하는 것이 좋습니다. Amazon RDS 읽기 전용 복제본과 지원되는 엔진의 기본 비동기식 복제를 사용하면 데이터베이스 쓰기가 소스 DB 인스턴스에서 발생한 다음 읽기 전용 복제본에서 발생합니다. 이 복제 “지연 시간”은 상당히 다를 수 있습니다. \n",
            "반면 다중 AZ 배포에서 사용되는 복제는 동기식이므로 모든 데이터베이스 쓰기가 기본 복제본과 예비 복제본에서 동시에 이루어집니다. 이렇게 최신 데이터베이스 업데이트를 보호하여 장애 조치 시, 예비 복제본에서 최신 데이터베이스 업데이트를 사용할 수 있습니다. \n",
            "또한 다중 AZ 배포를 통해 복제가 완벽하게 관리됩니다. Amazon RDS는 자동으로 DB 인스턴스 장애 상태 또는 가용 영역 장애를 모니터링하고, 가동 중단이 발생하면 예비 복제본(Amazon Aurora에서는 읽기 전용 복제본)으로 자동 장애 조치를 수행합니다.\"\n",
            "다중 AZ DB 인스턴스 배포를 소스로 사용해 읽기 복제본을 만들 수 있습니까?\n",
            "예. 다중 AZ DB 인스턴스는 읽기 전용 복제본과 다른 요구를 해결하므로 제품 배포 시 이 둘을 함께 사용하고 읽기 전용 복제본과 다중 AZ DB 인스턴스 배포를 연결하는 것이 좋습니다. “소스” 다중 AZ-DB 인스턴스는 향상된 쓰기 가용성과 데이터 내구성을 제공하며 연결된 읽기 전용 복제본은 읽기 트래픽 확장성을 향상합니다.\n",
            "\"category : RDS, question : 다중 AZ DB 인스턴스 배포를 소스로 사용해 읽기 복제본을 만들 수 있습니까?, answer : 예. 다중 AZ DB 인스턴스는 읽기 전용 복제본과 다른 요구를 해결하므로 제품 배포 시 이 둘을 함께 사용하고 읽기 전용 복제본과 다중 AZ DB 인스턴스 배포를 연결하는 것이 좋습니다. “소스” 다중 AZ-DB 인스턴스는 향상된 쓰기 가용성과 데이터 내구성을 제공하며 연결된 읽기 전용 복제본은 읽기 트래픽 확장성을 향상합니다.\"\n",
            "Amazon RDS 읽기 전용 복제본을 다중 AZ로 구성할 수 있습니까?\n",
            "예. Amazon RDS for MySQL, MariaDB, PostgreSQL 및 Oracle을 사용하면 읽기 전용 복제본에서 다중 AZ 구성을 활성화하여 재해 복구를 지원하고 엔진 업그레이드에 따른 가동 중단을 최소화할 수 있습니다.\n",
            "\"category : RDS, question : Amazon RDS 읽기 전용 복제본을 다중 AZ로 구성할 수 있습니까?, answer : 예. Amazon RDS for MySQL, MariaDB, PostgreSQL 및 Oracle을 사용하면 읽기 전용 복제본에서 다중 AZ 구성을 활성화하여 재해 복구를 지원하고 엔진 업그레이드에 따른 가동 중단을 최소화할 수 있습니다.\"\n",
            "내 읽기 복제본이 다중 AZ DB 인스턴스 배포를 소스로 사용하는 경우 다중 AZ 장애 조치가 발생하면 어떻게 됩니까?\n",
            "다중 AZ 장애 조치가 발생할 경우 장애 조치가 완료되면 연결된 사용 가능한 읽기 전용 복제본이 복제를 자동으로 재개합니다(새로 승격된 기본 복제본에서 업데이트를 가져옴).\n",
            "\"category : RDS, question : 내 읽기 복제본이 다중 AZ DB 인스턴스 배포를 소스로 사용하는 경우 다중 AZ 장애 조치가 발생하면 어떻게 됩니까?, answer : 다중 AZ 장애 조치가 발생할 경우 장애 조치가 완료되면 연결된 사용 가능한 읽기 전용 복제본이 복제를 자동으로 재개합니다(새로 승격된 기본 복제본에서 업데이트를 가져옴).\"\n",
            "다른 읽기 전용 복제본의 읽기 전용 복제본을 생성할 수 있나요?\n",
            "Amazon Aurora, Amazon RDS for MySQL, Amazon RDS for MariaDB의 경우 읽기 전용 복제본의 계층을 3개 생성할 수 있습니다. 기존의 첫 번째 계층 읽기 전용 복제본으로부터 두 번째 계층 읽기 전용 복제본을 생성할 수 있으며, 두 번째 계층 읽기 전용 복제본으로부터 세 번째 계층 읽기 전용 복제본을 생성할 수 있습니다. 두 번째 계층 및 세 번째 계층 읽기 전용 복제본을 생성함으로써, 사용자의 애플리케이션 요구 사항에 따라 기본 데이터베이스 인스턴스에서 복제 로드 중 일부를 읽기 전용 복제본의 다른 계층으로 이동할 수 있습니다.\n",
            "트랜잭션이 기본 복제본에서 첫 번째 계층 복제본으로 복제된 다음 두 번째 계층 복제본으로 복제될 때 발생하는 추가 복제 지연 시간으로 인해 두 번째 계층 읽기 전용 복제본에서 기본 복제본보다 더 오랜 지연이 발생할 수 있습니다. 마찬가지로, 세 번째 계층 복제본에서 두 번째 계층 읽기 전용 복제본보다 더 오랜 지연이 발생할 수 있습니다.\n",
            "Amazon RDS for Oracle 및 Amazon RDS for SQL Server의 경우 읽기 전용 복제본의 읽기 전용 복제본 생성은 현재 지원되지 않습니다.\n",
            "\"category : RDS, question : 다른 읽기 전용 복제본의 읽기 전용 복제본을 생성할 수 있나요?, answer : Amazon Aurora, Amazon RDS for MySQL, Amazon RDS for MariaDB의 경우 읽기 전용 복제본의 계층을 3개 생성할 수 있습니다. 기존의 첫 번째 계층 읽기 전용 복제본으로부터 두 번째 계층 읽기 전용 복제본을 생성할 수 있으며, 두 번째 계층 읽기 전용 복제본으로부터 세 번째 계층 읽기 전용 복제본을 생성할 수 있습니다. 두 번째 계층 및 세 번째 계층 읽기 전용 복제본을 생성함으로써, 사용자의 애플리케이션 요구 사항에 따라 기본 데이터베이스 인스턴스에서 복제 로드 중 일부를 읽기 전용 복제본의 다른 계층으로 이동할 수 있습니다.\n",
            "트랜잭션이 기본 복제본에서 첫 번째 계층 복제본으로 복제된 다음 두 번째 계층 복제본으로 복제될 때 발생하는 추가 복제 지연 시간으로 인해 두 번째 계층 읽기 전용 복제본에서 기본 복제본보다 더 오랜 지연이 발생할 수 있습니다. 마찬가지로, 세 번째 계층 복제본에서 두 번째 계층 읽기 전용 복제본보다 더 오랜 지연이 발생할 수 있습니다.\n",
            "Amazon RDS for Oracle 및 Amazon RDS for SQL Server의 경우 읽기 전용 복제본의 읽기 전용 복제본 생성은 현재 지원되지 않습니다.\"\n",
            "읽기 복제본은 데이터베이스 읽기 작업만 처리할 수 있나요?\n",
            "읽기 전용 복제본은 읽기 트래픽을 처리하도록 설계되었습니다. 그러나 고급 사용자가 읽기 전용 복제본에 대해 데이터 정의 언어(DDL) SQL 문을 작성하려는 경우가 있을 수 있습니다. 예를 들어, 동일한 색인을 해당 소스 DB 인스턴스에 추가하지 않고, 비즈니스 보고에 사용되는 읽기 전용 복제본에 데이터베이스 인덱스를 추가할 수 있습니다.\n",
            "Amazon RDS for MySQL은 읽기 전용 복제본에 대해 DDL SQL 문을 허용하도록 구성할 수 있습니다. 특정 읽기 전용 복제본에 읽기 작업이 아닌 다른 작업을 활성화하려면 읽기 전용 복제본의 활성 DB 파라미터 그룹을 변경하여 ‘read_only’ 파라미터를 ‘0’으로 설정합니다.\n",
            "Amazon RDS for PostgreSQL은 현재 읽기 전용 복제본에 대한 DDL SQL 문 실행을 지원하지 않습니다.\n",
            "\"category : RDS, question : 읽기 복제본은 데이터베이스 읽기 작업만 처리할 수 있나요?, answer : 읽기 전용 복제본은 읽기 트래픽을 처리하도록 설계되었습니다. 그러나 고급 사용자가 읽기 전용 복제본에 대해 데이터 정의 언어(DDL) SQL 문을 작성하려는 경우가 있을 수 있습니다. 예를 들어, 동일한 색인을 해당 소스 DB 인스턴스에 추가하지 않고, 비즈니스 보고에 사용되는 읽기 전용 복제본에 데이터베이스 인덱스를 추가할 수 있습니다.\n",
            "Amazon RDS for MySQL은 읽기 전용 복제본에 대해 DDL SQL 문을 허용하도록 구성할 수 있습니다. 특정 읽기 전용 복제본에 읽기 작업이 아닌 다른 작업을 활성화하려면 읽기 전용 복제본의 활성 DB 파라미터 그룹을 변경하여 ‘read_only’ 파라미터를 ‘0’으로 설정합니다.\n",
            "Amazon RDS for PostgreSQL은 현재 읽기 전용 복제본에 대한 DDL SQL 문 실행을 지원하지 않습니다.\"\n",
            "읽기 전용 복제본을 ‘독립 실행형’ DB 인스턴스로 승격할 수 있습니까?\n",
            "예. 자세한 내용은 Amazon RDS 사용 설명서를 참조하세요.\n",
            "\"category : RDS, question : 읽기 전용 복제본을 ‘독립 실행형’ DB 인스턴스로 승격할 수 있습니까?, answer : 예. 자세한 내용은 Amazon RDS 사용 설명서를 참조하세요.\"\n",
            "읽기 복제본이 해당 소스 DB 인스턴스와 함께 최신 상태로 유지됩니까?\n",
            "소스 DB 인스턴스 업데이트는 모든 관련 읽기 전용 복제본에 자동으로 복제됩니다. 그러나 지원되는 엔진의 비동기식 복제 기술을 사용하면 다양한 이유로 읽기 전용 복제본이 소스 DB 인스턴스보다 지연될 수 있습니다. 대표적인 이유는 다음과 같습니다.\n",
            "\n",
            "소스 DB 인스턴스에 대한 쓰기 I/O 볼륨이 읽기 전용 복제본에 변경 내용이 적용될 수 있는 비율을 초과하는 경우(이 문제는 특히 읽기 전용 복제본의 컴퓨팅 파워가 소스 DB 인스턴스보다 낮은 경우에 발생할 수 있음)\n",
            "소스 DB 인스턴스에 대한 복잡하거나 오랫동안 실행되는 트랜잭션이 읽기 전용 복제본에 대한 복제를 보류하는 경우\n",
            "네트워크 파티션이나 소스 DB 인스턴스와 읽기 전용 복제본 간의 지연 시간\n",
            "\n",
            "읽기 전용 복제본은 지원되는 엔진의 기본 복제 기능의 장점과 단점을 가지고 있습니다. 읽기 전용 복제본을 사용하는 경우는 읽기 전용 복제본과 소스 DB 인스턴스 사이에 지연이나 ‘불일치’가 발생할 수 있다는 점을 인식해야 합니다.\n",
            "\"category : RDS, question : 읽기 복제본이 해당 소스 DB 인스턴스와 함께 최신 상태로 유지됩니까?, answer : 소스 DB 인스턴스 업데이트는 모든 관련 읽기 전용 복제본에 자동으로 복제됩니다. 그러나 지원되는 엔진의 비동기식 복제 기술을 사용하면 다양한 이유로 읽기 전용 복제본이 소스 DB 인스턴스보다 지연될 수 있습니다. 대표적인 이유는 다음과 같습니다.\n",
            "\n",
            "소스 DB 인스턴스에 대한 쓰기 I/O 볼륨이 읽기 전용 복제본에 변경 내용이 적용될 수 있는 비율을 초과하는 경우(이 문제는 특히 읽기 전용 복제본의 컴퓨팅 파워가 소스 DB 인스턴스보다 낮은 경우에 발생할 수 있음)\n",
            "소스 DB 인스턴스에 대한 복잡하거나 오랫동안 실행되는 트랜잭션이 읽기 전용 복제본에 대한 복제를 보류하는 경우\n",
            "네트워크 파티션이나 소스 DB 인스턴스와 읽기 전용 복제본 간의 지연 시간\n",
            "\n",
            "읽기 전용 복제본은 지원되는 엔진의 기본 복제 기능의 장점과 단점을 가지고 있습니다. 읽기 전용 복제본을 사용하는 경우는 읽기 전용 복제본과 소스 DB 인스턴스 사이에 지연이나 ‘불일치’가 발생할 수 있다는 점을 인식해야 합니다.\"\n",
            "활성 읽기 전용 복제본의 상태를 보려면 어떻게 해야 하나요?\n",
            "표준 DescribeDBInstances API를 사용하여 배포한 모든 DB 인스턴스의 목록(읽기 전용 복제본 포함)을 반환하거나 Amazon RDS 콘솔의 ‘인스턴스(Instances)’ 탭을 클릭하면 됩니다.\n",
            "Amazon RDS를 사용하면 읽기 전용 복제본이 소스 DB 인스턴스보다 얼마나 지연되었는지 볼 수 있습니다. 읽기 전용 복제본이 프라이머리보다 지연된 시간(단위: 초)이 AWS Management Console 또는 Amazon CloudWatch API를 통해 사용 가능한 Amazon CloudWatch 지표(‘복제 지연’)로 게시됩니다.\n",
            "Amazon RDS for MySQL의 경우 이 정보의 소스가 읽기 전용 복제본에 대해 표준 ‘Show Replica Status(복제본 상태 보기)’ MySQL 명령을 실행하여 표시된 소스와 동일합니다. Amazon RDS for PostgreSQL의 경우 소스 DB 인스턴스에서 pg_stat_replication 보기를 사용하여 복제 지표를 탐색할 수 있습니다.\n",
            "Amazon RDS는 읽기 전용 복제본의 복제 상태를 모니터링하고 복제가 어떤 이유로든지 정지된 경우(예: 프라이머리 데이터베이스 인스턴스에 업데이트된 내용과 충돌되는 DML 쿼리가 복제본에서 실행되면 복제 오류가 발생할 수 있음) AWS Management Console의 ‘Replication State(복제 상태)’ 필드를 ‘Error(오류)’로 업데이트합니다. 사용자는 복제 오류(Replication Error) 필드를 확인하여 MySQL 엔진에서 보낸 관련 오류의 상세 정보를 검토하고 오류를 복구하기 위한 적절한 조치를 취할 수 있습니다. 복제 문제 해결에 대해 자세히 알아보려면 Amazon RDS for MySQL 또는 Amazon RDS for PostgreSQL의 사용 설명서에 있는 읽기 전용 복제본 문제 해결 섹션을 참조하세요. \n",
            "복제 문제가 해결되면 Replication State가 Replicating으로 변경됩니다.\n",
            "\"category : RDS, question : 활성 읽기 전용 복제본의 상태를 보려면 어떻게 해야 하나요?, answer : 표준 DescribeDBInstances API를 사용하여 배포한 모든 DB 인스턴스의 목록(읽기 전용 복제본 포함)을 반환하거나 Amazon RDS 콘솔의 ‘인스턴스(Instances)’ 탭을 클릭하면 됩니다.\n",
            "Amazon RDS를 사용하면 읽기 전용 복제본이 소스 DB 인스턴스보다 얼마나 지연되었는지 볼 수 있습니다. 읽기 전용 복제본이 프라이머리보다 지연된 시간(단위: 초)이 AWS Management Console 또는 Amazon CloudWatch API를 통해 사용 가능한 Amazon CloudWatch 지표(‘복제 지연’)로 게시됩니다.\n",
            "Amazon RDS for MySQL의 경우 이 정보의 소스가 읽기 전용 복제본에 대해 표준 ‘Show Replica Status(복제본 상태 보기)’ MySQL 명령을 실행하여 표시된 소스와 동일합니다. Amazon RDS for PostgreSQL의 경우 소스 DB 인스턴스에서 pg_stat_replication 보기를 사용하여 복제 지표를 탐색할 수 있습니다.\n",
            "Amazon RDS는 읽기 전용 복제본의 복제 상태를 모니터링하고 복제가 어떤 이유로든지 정지된 경우(예: 프라이머리 데이터베이스 인스턴스에 업데이트된 내용과 충돌되는 DML 쿼리가 복제본에서 실행되면 복제 오류가 발생할 수 있음) AWS Management Console의 ‘Replication State(복제 상태)’ 필드를 ‘Error(오류)’로 업데이트합니다. 사용자는 복제 오류(Replication Error) 필드를 확인하여 MySQL 엔진에서 보낸 관련 오류의 상세 정보를 검토하고 오류를 복구하기 위한 적절한 조치를 취할 수 있습니다. 복제 문제 해결에 대해 자세히 알아보려면 Amazon RDS for MySQL 또는 Amazon RDS for PostgreSQL의 사용 설명서에 있는 읽기 전용 복제본 문제 해결 섹션을 참조하세요. \n",
            "복제 문제가 해결되면 Replication State가 Replicating으로 변경됩니다.\"\n",
            "소스 DB 인스턴스의 컴퓨팅 용량 및/또는 스토리지 용량을 확장한 경우, 관련 읽기 전용 복제본의 리소스도 확장해야 합니까?\n",
            "복제가 제대로 작동하기 위해서는 읽기 전용 복제본이 해당 소스 DB 인스턴스와 동등하거나 그 이상의 컴퓨팅 파워 및 스토리지 리소스를 갖도록 하는 것이 좋습니다. 그렇지 않으면 복제 지연이 증가하거나 읽기 전용 복제본이 복제된 업데이트를 저장할 공간이 부족할 수 있습니다.\n",
            "\"category : RDS, question : 소스 DB 인스턴스의 컴퓨팅 용량 및/또는 스토리지 용량을 확장한 경우, 관련 읽기 전용 복제본의 리소스도 확장해야 합니까?, answer : 복제가 제대로 작동하기 위해서는 읽기 전용 복제본이 해당 소스 DB 인스턴스와 동등하거나 그 이상의 컴퓨팅 파워 및 스토리지 리소스를 갖도록 하는 것이 좋습니다. 그렇지 않으면 복제 지연이 증가하거나 읽기 전용 복제본이 복제된 업데이트를 저장할 공간이 부족할 수 있습니다.\"\n",
            "읽기 전용 복제본은 어떻게 삭제합니까? 소스 DB 인스턴스를 삭제하면 자동으로 삭제되나요?\n",
            "AWS Management Console에서 몇 단계를 수행하거나 DB 인스턴스 식별자를 DeleteDBInstance API에 전달하여 읽기 전용 복제본을 삭제할 수 있습니다. \n",
            "해당 소스 DB 인스턴스가 삭제된 후에도 Amazon Aurora 복제본은 활성 상태로 유지되어 읽기 트래픽을 계속 수신합니다. 클러스터의 복제본 하나가 새로운 기본 복제본으로 자동으로 승격되어 쓰기 트래픽을 수신하기 시작합니다.\n",
            "해당 소스 DB 인스턴스가 삭제된 후에도 MySQL 또는 MariaDB용 Amazon RDS 읽기 전용 복제본은 활성 상태로 유지되어 읽기 트래픽을 계속 수신합니다. 소스 DB 인스턴스와 함께 읽기 전용 복제본을 삭제하려면 DeleteDBInstance API 또는 AWS Management Console을 사용하여 읽기 전용 복제본을 확실히 삭제해야 합니다.\n",
            "읽기 전용 복제본이 있는 PostgreSQL DB 인스턴스용 Amazon RDS를 삭제하면 모든 읽기 전용 복제본이 독립 실행형 DB 인스턴스로 승격되고 읽기 및 쓰기 트래픽을 모두 수신할 수 있습니다. 새로 승격된 DB 인스턴스가 서로 독립적으로 작동합니다. 원본 소스 DB 인스턴스와 함께 이러한 DB 인스턴스를 삭제하려면 DeleteDBInstance API 또는 AWS Management Console을 사용하여 인스턴스를 확실히 삭제해야 합니다.\n",
            "\"category : RDS, question : 읽기 전용 복제본은 어떻게 삭제합니까? 소스 DB 인스턴스를 삭제하면 자동으로 삭제되나요?, answer : AWS Management Console에서 몇 단계를 수행하거나 DB 인스턴스 식별자를 DeleteDBInstance API에 전달하여 읽기 전용 복제본을 삭제할 수 있습니다. \n",
            "해당 소스 DB 인스턴스가 삭제된 후에도 Amazon Aurora 복제본은 활성 상태로 유지되어 읽기 트래픽을 계속 수신합니다. 클러스터의 복제본 하나가 새로운 기본 복제본으로 자동으로 승격되어 쓰기 트래픽을 수신하기 시작합니다.\n",
            "해당 소스 DB 인스턴스가 삭제된 후에도 MySQL 또는 MariaDB용 Amazon RDS 읽기 전용 복제본은 활성 상태로 유지되어 읽기 트래픽을 계속 수신합니다. 소스 DB 인스턴스와 함께 읽기 전용 복제본을 삭제하려면 DeleteDBInstance API 또는 AWS Management Console을 사용하여 읽기 전용 복제본을 확실히 삭제해야 합니다.\n",
            "읽기 전용 복제본이 있는 PostgreSQL DB 인스턴스용 Amazon RDS를 삭제하면 모든 읽기 전용 복제본이 독립 실행형 DB 인스턴스로 승격되고 읽기 및 쓰기 트래픽을 모두 수신할 수 있습니다. 새로 승격된 DB 인스턴스가 서로 독립적으로 작동합니다. 원본 소스 DB 인스턴스와 함께 이러한 DB 인스턴스를 삭제하려면 DeleteDBInstance API 또는 AWS Management Console을 사용하여 인스턴스를 확실히 삭제해야 합니다.\"\n",
            "읽기 전용 복제본의 요금은 얼마입니까? 언제부터 언제까지 사용한 요금이 청구됩니까?\n",
            "읽기 전용 복제본은 표준 DB 인스턴스와 동일한 요금이 청구됩니다. 표준 DB 인스턴스와 마찬가지로 읽기 전용 복제본의 ‘DB 인스턴스 시간’당 요금은 읽기 전용 복제본의 DB 인스턴스 클래스에 의해 결정됩니다. 최신 요금 정보는 요금 페이지를 참조하세요. 동일한 AWS 리전 내의 소스 DB 인스턴스와 읽기 전용 복제본 간에 데이터를 복제할 때 발생한 데이터 전송에는 요금이 부과되지 않습니다.\n",
            "읽기 전용 복제본의 요금 청구는 복제본이 성공적으로 생성된 직후에 시작됩니다(상태가 ‘활성’으로 표시되는 경우). 사용자가 삭제 명령을 실행할 때까지 읽기 전용 복제본에는 표준 Amazon RDS DB 인스턴스 시간 요금이 계속 청구됩니다.\n",
            "\"category : RDS, question : 읽기 전용 복제본의 요금은 얼마입니까? 언제부터 언제까지 사용한 요금이 청구됩니까?, answer : 읽기 전용 복제본은 표준 DB 인스턴스와 동일한 요금이 청구됩니다. 표준 DB 인스턴스와 마찬가지로 읽기 전용 복제본의 ‘DB 인스턴스 시간’당 요금은 읽기 전용 복제본의 DB 인스턴스 클래스에 의해 결정됩니다. 최신 요금 정보는 요금 페이지를 참조하세요. 동일한 AWS 리전 내의 소스 DB 인스턴스와 읽기 전용 복제본 간에 데이터를 복제할 때 발생한 데이터 전송에는 요금이 부과되지 않습니다.\n",
            "읽기 전용 복제본의 요금 청구는 복제본이 성공적으로 생성된 직후에 시작됩니다(상태가 ‘활성’으로 표시되는 경우). 사용자가 삭제 명령을 실행할 때까지 읽기 전용 복제본에는 표준 Amazon RDS DB 인스턴스 시간 요금이 계속 청구됩니다.\"\n",
            "Amazon RDS의 향상된 모니터링이란 무엇입니까?\n",
            "Amazon RDS용 향상된 모니터링은 Amazon RDS 인스턴스 상태에 대해 좀 더 세부적인 가시성을 제공합니다. Amazon RDS DB 인스턴스에 대한 ‘Enhanced Monitoring(향상된 모니터링)’ 옵션을 활성화하고 시간 단위를 설정하기만 하면, 향상된 모니터링에서 정의된 시간 단위에 따라 주요 운영 체제 지표를 수집하고 정보를 처리합니다.\n",
            "데이터베이스 로드에 대해 진단 및 시각화를 심화하고 데이터 보존 기간을 늘리기 위해 성능 개선 도우미를 사용해 보세요.\n",
            "\"category : RDS, question : Amazon RDS의 향상된 모니터링이란 무엇입니까?, answer : Amazon RDS용 향상된 모니터링은 Amazon RDS 인스턴스 상태에 대해 좀 더 세부적인 가시성을 제공합니다. Amazon RDS DB 인스턴스에 대한 ‘Enhanced Monitoring(향상된 모니터링)’ 옵션을 활성화하고 시간 단위를 설정하기만 하면, 향상된 모니터링에서 정의된 시간 단위에 따라 주요 운영 체제 지표를 수집하고 정보를 처리합니다.\n",
            "데이터베이스 로드에 대해 진단 및 시각화를 심화하고 데이터 보존 기간을 늘리기 위해 성능 개선 도우미를 사용해 보세요.\"\n",
            "향상된 모니터링에서 모니터링할 수 있는 지표와 프로세스에는 어떤 것이 있습니까?\n",
            "Enhanced Monitoring은 CPU, 메모리, 파일 시스템, 디스크 I/O 등과 같은 Amazon RDS 인스턴스 시스템 수준의 지표를 캡처합니다. 지표의 전체 목록은 설명서에서 확인할 수 있습니다.\n",
            "\"category : RDS, question : 향상된 모니터링에서 모니터링할 수 있는 지표와 프로세스에는 어떤 것이 있습니까?, answer : Enhanced Monitoring은 CPU, 메모리, 파일 시스템, 디스크 I/O 등과 같은 Amazon RDS 인스턴스 시스템 수준의 지표를 캡처합니다. 지표의 전체 목록은 설명서에서 확인할 수 있습니다.\"\n",
            "향상된 모니터링에서 지원하는 엔진에는 어떤 것이 있습니까?\n",
            "Enhanced Monitoring은 모든 Amazon RDS 데이터베이스 엔진을 지원합니다.\n",
            "\"category : RDS, question : 향상된 모니터링에서 지원하는 엔진에는 어떤 것이 있습니까?, answer : Enhanced Monitoring은 모든 Amazon RDS 데이터베이스 엔진을 지원합니다.\"\n",
            "향상된 모니터링에서 지원하는 인스턴스 유형에는 어떤 것이 있습니까?\n",
            "Enhanced Monitoring에서는 t1.micro와 m1.small을 제외한 모든 인스턴스 유형을 지원합니다. 이 소프트웨어는 약간의 CPU, 메모리 및 I/O를 사용합니다. 범용 모니터링을 위해서는 미디엄 규모 이상의 인스턴스에서 시간 단위를 크게 설정하는 것이 좋습니다. 비프로덕션 DB 인스턴스의 경우 Enhanced Monitoring이 ‘off’로 기본 설정되어 있습니다. 비활성 상태로 두거나 활성 상태가 될 때 시간 단위를 변경할 수 있습니다.\n",
            "\"category : RDS, question : 향상된 모니터링에서 지원하는 인스턴스 유형에는 어떤 것이 있습니까?, answer : Enhanced Monitoring에서는 t1.micro와 m1.small을 제외한 모든 인스턴스 유형을 지원합니다. 이 소프트웨어는 약간의 CPU, 메모리 및 I/O를 사용합니다. 범용 모니터링을 위해서는 미디엄 규모 이상의 인스턴스에서 시간 단위를 크게 설정하는 것이 좋습니다. 비프로덕션 DB 인스턴스의 경우 Enhanced Monitoring이 ‘off’로 기본 설정되어 있습니다. 비활성 상태로 두거나 활성 상태가 될 때 시간 단위를 변경할 수 있습니다.\"\n",
            "Amazon RDS 대시보드에서 볼 수 있는 정보에는 어떤 것이 있습니까?\n",
            "콘솔에서는 Amazon RDS DB 인스턴스에 대한 모든 시스템 지표와 프로세스 정보를 그래프 형식으로 볼 수 있습니다. 인스턴스별로 모니터링하려는 지표를 관리하고, 요구 사항에 따라 대시보드를 변경할 수 있습니다.\n",
            "\"category : RDS, question : Amazon RDS 대시보드에서 볼 수 있는 정보에는 어떤 것이 있습니까?, answer : 콘솔에서는 Amazon RDS DB 인스턴스에 대한 모든 시스템 지표와 프로세스 정보를 그래프 형식으로 볼 수 있습니다. 인스턴스별로 모니터링하려는 지표를 관리하고, 요구 사항에 따라 대시보드를 변경할 수 있습니다.\"\n",
            "Amazon RDS 계정의 모든 인스턴스에 대해 같은 시간 단위로 지표를 수집합니까?\n",
            "Amazon RDS 계정의 DB 인스턴스별로 시간 단위를 다르게 설정할 수 있습니다. 또한, Enhanced Monitoring을 활성화할 인스턴스를 선택할 수 있을 뿐 아니라 어떤 인스턴스든 언제든지 원할 때 시간 단위를 변경할 수 있습니다.\n",
            "\"category : RDS, question : Amazon RDS 계정의 모든 인스턴스에 대해 같은 시간 단위로 지표를 수집합니까?, answer : Amazon RDS 계정의 DB 인스턴스별로 시간 단위를 다르게 설정할 수 있습니다. 또한, Enhanced Monitoring을 활성화할 인스턴스를 선택할 수 있을 뿐 아니라 어떤 인스턴스든 언제든지 원할 때 시간 단위를 변경할 수 있습니다.\"\n",
            "Amazon RDS 콘솔에서 확인할 수 있는 기록 지표의 기간은 어떻게 됩니까?\n",
            "설정에 따라 최대 1초의 세부 수준으로 최대 1시간 전까지 모든 지표의 성능 값을 확인할 수 있습니다.\n",
            "\"category : RDS, question : Amazon RDS 콘솔에서 확인할 수 있는 기록 지표의 기간은 어떻게 됩니까?, answer : 설정에 따라 최대 1초의 세부 수준으로 최대 1시간 전까지 모든 지표의 성능 값을 확인할 수 있습니다.\"\n",
            "Amazon RDS 향상된 모니터링에서 생성한 지표를 CloudWatch에서 시각화하려면 어떻게 해야 합니까?\n",
            "Amazon RDS Enhanced Monitoring의 지표는 CloudWatch Logs 계정으로 전송됩니다. CloudWatch에서 CloudWatch Logs의 지표 필터를 생성하고 CloudWatch 대시보드에서 그래프를 표시할 수 있습니다. 자세한 내용은 Amazon CloudWatch 페이지를 참조하세요.\n",
            "\"category : RDS, question : Amazon RDS 향상된 모니터링에서 생성한 지표를 CloudWatch에서 시각화하려면 어떻게 해야 합니까?, answer : Amazon RDS Enhanced Monitoring의 지표는 CloudWatch Logs 계정으로 전송됩니다. CloudWatch에서 CloudWatch Logs의 지표 필터를 생성하고 CloudWatch 대시보드에서 그래프를 표시할 수 있습니다. 자세한 내용은 Amazon CloudWatch 페이지를 참조하세요.\"\n",
            "Amazon RDS 콘솔 대시보드 대신, CloudWatch를 사용해야 하는 경우는 언제입니까?\n",
            "Amazon RDS 콘솔 대시보드에서 제공하는 기록 데이터보다 오래된 데이터를 보려면 CloudWatch를 사용해야 합니다. CloudWatch에서 Amazon RDS 인스턴스를 모니터링하여 단일 위치의 전체 AWS 스택의 상태를 진단할 수 있습니다. 현재 CloudWatch에서는 최대 1분의 시간 단위를 지원하며, 1분 미만의 시간 단위에 대한 값은 평균으로 계산됩니다.\n",
            "\"category : RDS, question : Amazon RDS 콘솔 대시보드 대신, CloudWatch를 사용해야 하는 경우는 언제입니까?, answer : Amazon RDS 콘솔 대시보드에서 제공하는 기록 데이터보다 오래된 데이터를 보려면 CloudWatch를 사용해야 합니다. CloudWatch에서 Amazon RDS 인스턴스를 모니터링하여 단일 위치의 전체 AWS 스택의 상태를 진단할 수 있습니다. 현재 CloudWatch에서는 최대 1분의 시간 단위를 지원하며, 1분 미만의 시간 단위에 대한 값은 평균으로 계산됩니다.\"\n",
            "특정 지표를 기준으로 경보 및 알림을 설정할 수 있습니까?\n",
            "예. CloudWatch에서 경보를 생성하여 경보 상태가 변경될 때 알림을 전송하도록 할 수 있습니다. 경보는 지정한 기간 동안 단일 지표를 감시하고, 여러 기간에 걸쳐 지정된 임계값과 관련된 지표 값을 기준으로 하나 이상의 작업을 수행합니다. CloudWatch 경보에 대한 자세한 내용은 Amazon CloudWatch 개발자 안내서를 참조하세요.\n",
            "\"category : RDS, question : 특정 지표를 기준으로 경보 및 알림을 설정할 수 있습니까?, answer : 예. CloudWatch에서 경보를 생성하여 경보 상태가 변경될 때 알림을 전송하도록 할 수 있습니다. 경보는 지정한 기간 동안 단일 지표를 감시하고, 여러 기간에 걸쳐 지정된 임계값과 관련된 지표 값을 기준으로 하나 이상의 작업을 수행합니다. CloudWatch 경보에 대한 자세한 내용은 Amazon CloudWatch 개발자 안내서를 참조하세요.\"\n",
            "향상된 모니터링을 현재 내가 사용하는 도구와 통합하려면 어떻게 해야 합니까?\n",
            "A: Amazon RDS Enhanced Monitoring에서는 JSON 페이로드로 구성된 지표 세트를 제공하며, 페이로드는 CloudWatch Logs 계정으로 전송됩니다. JSON 페이로드는 Amazon RDS 인스턴스에 대해 마지막으로 구성된 시간 단위에 따라 전송됩니다.\n",
            "서드 파티 대시보드 또는 애플리케이션을 통해 지표를 사용할 수 있는 방법에는 두 가지가 있습니다. 모니터링 도구는 CloudWatch Logs 구독을 사용해 지표에 대한 거의 실시간 피드를 설정할 수 있습니다. 또는 CloudWatch Logs의 필터를 사용해 지표를 CloudWatch에 연결하고 애플리케이션을 CloudWatch와 통합할 수 있습니다. 자세한 내용은 Amazon CloudWatch 설명서를 참조하세요.\n",
            "\"category : RDS, question : 향상된 모니터링을 현재 내가 사용하는 도구와 통합하려면 어떻게 해야 합니까?, answer : A: Amazon RDS Enhanced Monitoring에서는 JSON 페이로드로 구성된 지표 세트를 제공하며, 페이로드는 CloudWatch Logs 계정으로 전송됩니다. JSON 페이로드는 Amazon RDS 인스턴스에 대해 마지막으로 구성된 시간 단위에 따라 전송됩니다.\n",
            "서드 파티 대시보드 또는 애플리케이션을 통해 지표를 사용할 수 있는 방법에는 두 가지가 있습니다. 모니터링 도구는 CloudWatch Logs 구독을 사용해 지표에 대한 거의 실시간 피드를 설정할 수 있습니다. 또는 CloudWatch Logs의 필터를 사용해 지표를 CloudWatch에 연결하고 애플리케이션을 CloudWatch와 통합할 수 있습니다. 자세한 내용은 Amazon CloudWatch 설명서를 참조하세요.\"\n",
            "기록 데이터를 삭제하려면 어떻게 해야 합니까?\n",
            "Enhanced Monitoring에서는 JSON 페이로드를 CloudWatch Logs 계정으로 전송하므로, 다른 CloudWatch Logs 스트림과 마찬가지로 보존 기간을 제어할 수 있습니다. CloudWatch Logs의 Enhanced Monitoring에 구성된 기본 보존 기간은 30일입니다. 보존 기간 설정을 변경하는 방법에 대한 자세한 내용은 Amazon CloudWatch 개발자 안내서를 참조하세요.\n",
            "\"category : RDS, question : 기록 데이터를 삭제하려면 어떻게 해야 합니까?, answer : Enhanced Monitoring에서는 JSON 페이로드를 CloudWatch Logs 계정으로 전송하므로, 다른 CloudWatch Logs 스트림과 마찬가지로 보존 기간을 제어할 수 있습니다. CloudWatch Logs의 Enhanced Monitoring에 구성된 기본 보존 기간은 30일입니다. 보존 기간 설정을 변경하는 방법에 대한 자세한 내용은 Amazon CloudWatch 개발자 안내서를 참조하세요.\"\n",
            "향상된 모니터링은 내 월별 청구서에 어떤 영향을 줍니까?\n",
            "지표가 CloudWatch Logs로 수집되므로, CloudWatch Logs 프리 티어를 초과하면 CloudWatch Logs 데이터 전송 및 스토리지 요금을 기준으로 요금이 부과됩니다. 요금 내역은 여기에서 확인할 수 있습니다. Amazon RDS 인스턴스에 대해 전송된 정보의 양은 Enhanced Monitoring 기능에 정의된 시간 단위에 정비례합니다. 관리자는 계정 내의 여러 인스턴스에 각기 다른 시간 단위를 설정하여 비용을 관리할 수 있습니다.\n",
            "인스턴스에 대한 Enhanced Monitoring에 의해 CloudWatch Logs로 수집된 대략적인 데이터 볼륨은 다음과 같습니다.\n",
            "\n",
            "\n",
            "\n",
            "세부 단위\n",
            "60초\n",
            "30초\n",
            "15초\n",
            "10초\n",
            "5초\n",
            "1초\n",
            "\n",
            "\n",
            " CloudWatch Logs에 수집된 데이터*(월별 GB) \n",
            " 0.27 \n",
            " 0.53 \n",
            " 1.07 \n",
            " 1.61 \n",
            " 3.21 \n",
            " 16.07\n",
            "\"category : RDS, question : 향상된 모니터링은 내 월별 청구서에 어떤 영향을 줍니까?, answer : 지표가 CloudWatch Logs로 수집되므로, CloudWatch Logs 프리 티어를 초과하면 CloudWatch Logs 데이터 전송 및 스토리지 요금을 기준으로 요금이 부과됩니다. 요금 내역은 여기에서 확인할 수 있습니다. Amazon RDS 인스턴스에 대해 전송된 정보의 양은 Enhanced Monitoring 기능에 정의된 시간 단위에 정비례합니다. 관리자는 계정 내의 여러 인스턴스에 각기 다른 시간 단위를 설정하여 비용을 관리할 수 있습니다.\n",
            "인스턴스에 대한 Enhanced Monitoring에 의해 CloudWatch Logs로 수집된 대략적인 데이터 볼륨은 다음과 같습니다.\n",
            "\n",
            "\n",
            "\n",
            "세부 단위\n",
            "60초\n",
            "30초\n",
            "15초\n",
            "10초\n",
            "5초\n",
            "1초\n",
            "\n",
            "\n",
            " CloudWatch Logs에 수집된 데이터*(월별 GB) \n",
            " 0.27 \n",
            " 0.53 \n",
            " 1.07 \n",
            " 1.61 \n",
            " 3.21 \n",
            " 16.07\"\n",
            "Amazon RDS 프록시는 무엇입니까?\n",
            "Amazon RDS 프록시는 Amazon RDS의 프록시 기능으로, 완전관리형의 고가용성 데이터베이스 프록시입니다. RDS Proxy를 사용하면 애플리케이션의 확장성, 데이터베이스 오류에 대한 복원력 및 보안을 개선할 수 있습니다.\n",
            "\"category : RDS, question : Amazon RDS 프록시는 무엇입니까?, answer : Amazon RDS 프록시는 Amazon RDS의 프록시 기능으로, 완전관리형의 고가용성 데이터베이스 프록시입니다. RDS Proxy를 사용하면 애플리케이션의 확장성, 데이터베이스 오류에 대한 복원력 및 보안을 개선할 수 있습니다.\"\n",
            "Amazon RDS 프록시를 사용해야 하는 이유는 무엇입니까?\n",
            "Amazon RDS 프록시는 Amazon RDS의 프록시 기능으로, 완전관리형의 고가용성이며 사용이 쉬운 데이터베이스 프록시입니다. 이 프록시는 다음과 같은 이점을 제공합니다. 1) 데이터베이스 연결을 풀링하고 공유하여 애플리케이션의 확장성을 개선할 수 있습니다. 2) 데이터베이스 장애 조치 시간을 66%까지 줄이고 장애 조치 중에 애플리케이션 연결을 보존하여 애플리케이션의 가용성을 개선할 수 있습니다. 3) 데이터베이스에 AWS IAM 인증을 필요에 따라 적용하고 AWS Secrets Manager에 보안 인증 정보를 안전하게 저장하여 애플리케이션 보안을 개선할 수 있습니다.\n",
            "\"category : RDS, question : Amazon RDS 프록시를 사용해야 하는 이유는 무엇입니까?, answer : Amazon RDS 프록시는 Amazon RDS의 프록시 기능으로, 완전관리형의 고가용성이며 사용이 쉬운 데이터베이스 프록시입니다. 이 프록시는 다음과 같은 이점을 제공합니다. 1) 데이터베이스 연결을 풀링하고 공유하여 애플리케이션의 확장성을 개선할 수 있습니다. 2) 데이터베이스 장애 조치 시간을 66%까지 줄이고 장애 조치 중에 애플리케이션 연결을 보존하여 애플리케이션의 가용성을 개선할 수 있습니다. 3) 데이터베이스에 AWS IAM 인증을 필요에 따라 적용하고 AWS Secrets Manager에 보안 인증 정보를 안전하게 저장하여 애플리케이션 보안을 개선할 수 있습니다.\"\n",
            "Amazon RDS 프록시를 사용하는 경우와 데이터베이스에 직접 연결하는 경우는 어떻게 다릅니까?\n",
            "Amazon RDS 프록시를 사용할 경우 워크로드에 따라 쿼리 또는 트랜잭션 응답 시간에 평균 5밀리초의 네트워크 지연 시간이 추가됩니다. 애플리케이션이 5밀리초의 지연 시간을 허용할 수 없거나 연결 관리 및 RDS로 지원되는 기타 기능이 필요하지 않다면 애플리케이션에서 데이터베이스 엔드포인트에 직접 연결하는 것이 좋을 수 있습니다.\n",
            "\"category : RDS, question : Amazon RDS 프록시를 사용하는 경우와 데이터베이스에 직접 연결하는 경우는 어떻게 다릅니까?, answer : Amazon RDS 프록시를 사용할 경우 워크로드에 따라 쿼리 또는 트랜잭션 응답 시간에 평균 5밀리초의 네트워크 지연 시간이 추가됩니다. 애플리케이션이 5밀리초의 지연 시간을 허용할 수 없거나 연결 관리 및 RDS로 지원되는 기타 기능이 필요하지 않다면 애플리케이션에서 데이터베이스 엔드포인트에 직접 연결하는 것이 좋을 수 있습니다.\"\n",
            "Amazon RDS 프록시가 서버리스 애플리케이션에 제공하는 이점은 무엇입니까?\n",
            "Amazon RDS 프록시를 사용하면 관계형 데이터베이스의 기능과 단순함을 활용하는 최신의 서버리스 애플리케이션을 구축할 수 있습니다. 첫째, RDS 프록시를 사용하면 데이터베이스 연결을 풀링하고 재사용하여 서버리스 애플리케이션을 효율적으로 조정할 수 있습니다. 둘째, RDS 프록시를 사용하면 데이터베이스 자격 증명을 Lambda 코드로 처리할 필요가 없습니다. Lambda 함수에 연결된 IAM 실행 역할을 RDS 프록시 및 데이터베이스 인증에 사용할 수 있습니다. 셋째, 새로운 인프라 또는 코드를 관리하지 않고도 관계형 데이터베이스로 지원되는 서버리스 애플리케이션의 모든 기능을 활용할 수 있습니다. RDS 프록시는 완전관리형이며 애플리케이션 수요에 따라 자동으로 용량이 조정됩니다.\n",
            "\"category : RDS, question : Amazon RDS 프록시가 서버리스 애플리케이션에 제공하는 이점은 무엇입니까?, answer : Amazon RDS 프록시를 사용하면 관계형 데이터베이스의 기능과 단순함을 활용하는 최신의 서버리스 애플리케이션을 구축할 수 있습니다. 첫째, RDS 프록시를 사용하면 데이터베이스 연결을 풀링하고 재사용하여 서버리스 애플리케이션을 효율적으로 조정할 수 있습니다. 둘째, RDS 프록시를 사용하면 데이터베이스 자격 증명을 Lambda 코드로 처리할 필요가 없습니다. Lambda 함수에 연결된 IAM 실행 역할을 RDS 프록시 및 데이터베이스 인증에 사용할 수 있습니다. 셋째, 새로운 인프라 또는 코드를 관리하지 않고도 관계형 데이터베이스로 지원되는 서버리스 애플리케이션의 모든 기능을 활용할 수 있습니다. RDS 프록시는 완전관리형이며 애플리케이션 수요에 따라 자동으로 용량이 조정됩니다.\"\n",
            "Amazon RDS 프록시는 어떤 데이터베이스 엔진을 지원하나요?\n",
            "RDS 프록시는 Amazon Aurora(MySQL 호환), Amazon Aurora(PostgreSQL 호환), Amazon RDS for MariaDB, Amazon RDS for MySQL, Amazon RDS for PostgreSQL 및 Amazon RDS for SQL Server에 사용할 수 있습니다. 지원되는 엔진 버전 목록은 Amazon Aurora 사용 설명서 또는 Amazon RDS 사용 설명서를 참조하세요.\n",
            "\"category : RDS, question : Amazon RDS 프록시는 어떤 데이터베이스 엔진을 지원하나요?, answer : RDS 프록시는 Amazon Aurora(MySQL 호환), Amazon Aurora(PostgreSQL 호환), Amazon RDS for MariaDB, Amazon RDS for MySQL, Amazon RDS for PostgreSQL 및 Amazon RDS for SQL Server에 사용할 수 있습니다. 지원되는 엔진 버전 목록은 Amazon Aurora 사용 설명서 또는 Amazon RDS 사용 설명서를 참조하세요.\"\n",
            "Amazon RDS 프록시를 활성화하려면 어떻게 해야 하나요?\n",
            "Amazon RDS 콘솔에서 클릭 몇 번으로 Amazon RDS 데이터베이스에 대한 Amazon RDS 프록시를 활성화할 수 있습니다. RDS 프록시를 활성화한 다음 RDS 프록시에서 액세스할 VPC 및 서브넷을 지정할 수 있습니다. Lambda 사용자의 경우 Lambda 콘솔에서 클릭 몇 번으로 Amazon RDS 데이터베이스에 대한 Amazon RDS 프록시를 활성화하고 프록시에 액세스할 Lambda 함수를 설정할 수 있습니다.\n",
            "\"category : RDS, question : Amazon RDS 프록시를 활성화하려면 어떻게 해야 하나요?, answer : Amazon RDS 콘솔에서 클릭 몇 번으로 Amazon RDS 데이터베이스에 대한 Amazon RDS 프록시를 활성화할 수 있습니다. RDS 프록시를 활성화한 다음 RDS 프록시에서 액세스할 VPC 및 서브넷을 지정할 수 있습니다. Lambda 사용자의 경우 Lambda 콘솔에서 클릭 몇 번으로 Amazon RDS 데이터베이스에 대한 Amazon RDS 프록시를 활성화하고 프록시에 액세스할 Lambda 함수를 설정할 수 있습니다.\"\n",
            "API를 사용하여 Amazon RDS 프록시에 액세스할 수 있나요?\n",
            "예. Amazon RDS 프록시 API를 사용하여 프록시를 생성한 다음 대상 그룹을 정의하여 프록시를 특정 데이터베이스 인스턴스 또는 클러스터에 연결할 수 있습니다. 예:\n",
            "aws rds create-db-proxy \n",
            "        --db-proxy-name '…' \n",
            "        --engine-family <mysql|postgresql>       \n",
            "        --auth [{}, {}] \n",
            "        --role-arn '…'\n",
            "        --subnet-ids {}\n",
            "        --require-tls <true|false>\n",
            "        --tags {}\n",
            "aws rds register-db-proxy-targets \n",
            "        --target-group-name '…'\n",
            "        --db-cluster-identifier  '…'\n",
            "        --db-instance-identifier '…'\n",
            "\"category : RDS, question : API를 사용하여 Amazon RDS 프록시에 액세스할 수 있나요?, answer : 예. Amazon RDS 프록시 API를 사용하여 프록시를 생성한 다음 대상 그룹을 정의하여 프록시를 특정 데이터베이스 인스턴스 또는 클러스터에 연결할 수 있습니다. 예:\n",
            "aws rds create-db-proxy \n",
            "        --db-proxy-name '…' \n",
            "        --engine-family <mysql|postgresql>       \n",
            "        --auth [{}, {}] \n",
            "        --role-arn '…'\n",
            "        --subnet-ids {}\n",
            "        --require-tls <true|false>\n",
            "        --tags {}\n",
            "aws rds register-db-proxy-targets \n",
            "        --target-group-name '…'\n",
            "        --db-cluster-identifier  '…'\n",
            "        --db-instance-identifier '…'\"\n",
            "Trusted Language Extensions for PostgreSQL을 사용해야 하는 이유는 무엇인가요?\n",
            "Trusted Language Extensions(TLE) for PostgreSQL을 사용하면 고성능 PostgreSQL 확장 프로그램을 구축하고 Amazon Aurora 및 Amazon RDS에서 안전하게 실행할 수 있습니다. 이를 통해 TLE는 출시 시간을 단축하고, 프로덕션 데이터베이스 워크로드에 사용할 사용자 지정 및 서드 파티 코드를 인증해야 하는 데이터베이스 관리자의 부담을 덜어줍니다. 확장이 요구 사항을 충족한다고 판단되는 즉시 사용을 시작할 수 있습니다. 독립 소프트웨어 개발 판매 회사(ISV)는 TLE를 사용하여 Aurora 및 Amazon RDS 실행 고객에게 새로운 PostgreSQL 확장 프로그램을 제공할 수 있습니다.\n",
            "\"category : RDS, question : Trusted Language Extensions for PostgreSQL을 사용해야 하는 이유는 무엇인가요?, answer : Trusted Language Extensions(TLE) for PostgreSQL을 사용하면 고성능 PostgreSQL 확장 프로그램을 구축하고 Amazon Aurora 및 Amazon RDS에서 안전하게 실행할 수 있습니다. 이를 통해 TLE는 출시 시간을 단축하고, 프로덕션 데이터베이스 워크로드에 사용할 사용자 지정 및 서드 파티 코드를 인증해야 하는 데이터베이스 관리자의 부담을 덜어줍니다. 확장이 요구 사항을 충족한다고 판단되는 즉시 사용을 시작할 수 있습니다. 독립 소프트웨어 개발 판매 회사(ISV)는 TLE를 사용하여 Aurora 및 Amazon RDS 실행 고객에게 새로운 PostgreSQL 확장 프로그램을 제공할 수 있습니다.\"\n",
            "PostgreSQL에서 확장 프로그램을 실행할 때 기존에 있는 위험은 무엇이며, TLE for PostgreSQL은 이 위험을 어떻게 완화하나요?\n",
            "PostgreSQL 확장 프로그램은 고성능을 제공하기 위해 동일한 프로세스 공간에서 실행됩니다. 그러나 확장 프로그램에 소프트웨어 결함이 있는 경우 데이터베이스가 충돌할 수 있습니다.  TLE for PostgreSQL은 여러 계층의 보호를 통해 이 위험을 완화합니다. TLE는 시스템 리소스에 대한 액세스를 제한하도록 설계되었습니다. rds_superuser 역할은 특정 확장 프로그램을 설치할 수 있는 사용자를 결정할 수 있습니다. 그러나 이러한 변경 사항은 TLE API를 통해서만 수행될 수 있습니다. TLE는 확장 프로그램의 결함이 미치는 영향을 단일 데이터베이스 연결로 제한합니다. 이러한 보호 장치에 더해 TLE는 rds_superuser 역할의 DBA에게 세분화된 온라인 제어를 제공하도록 설계되었습니다. 이 DBA는 확장 프로그램을 설치할 수 있는 사용자를 제어할 수 있고 확장 프로그램의 실행을 위한 권한 모델을 생성할 수 있습니다.  충분한 권한이 있는 사용자만 TLE 확장 프로그램에서 ‘CREATE EXTENSION’ 명령을 사용하여 확장 프로그램을 실행하고 생성할 수 있습니다. 또한 DBA는 데이터베이스의 내부 동작을 수정하고 일반적으로 권한 상승이 요구되는 보다 정교한 확장 프로그램에 필요한 ‘PostgreSQL 후크’를 허용 목록에 추가할 수 있습니다.\n",
            "\"category : RDS, question : PostgreSQL에서 확장 프로그램을 실행할 때 기존에 있는 위험은 무엇이며, TLE for PostgreSQL은 이 위험을 어떻게 완화하나요?, answer : PostgreSQL 확장 프로그램은 고성능을 제공하기 위해 동일한 프로세스 공간에서 실행됩니다. 그러나 확장 프로그램에 소프트웨어 결함이 있는 경우 데이터베이스가 충돌할 수 있습니다.  TLE for PostgreSQL은 여러 계층의 보호를 통해 이 위험을 완화합니다. TLE는 시스템 리소스에 대한 액세스를 제한하도록 설계되었습니다. rds_superuser 역할은 특정 확장 프로그램을 설치할 수 있는 사용자를 결정할 수 있습니다. 그러나 이러한 변경 사항은 TLE API를 통해서만 수행될 수 있습니다. TLE는 확장 프로그램의 결함이 미치는 영향을 단일 데이터베이스 연결로 제한합니다. 이러한 보호 장치에 더해 TLE는 rds_superuser 역할의 DBA에게 세분화된 온라인 제어를 제공하도록 설계되었습니다. 이 DBA는 확장 프로그램을 설치할 수 있는 사용자를 제어할 수 있고 확장 프로그램의 실행을 위한 권한 모델을 생성할 수 있습니다.  충분한 권한이 있는 사용자만 TLE 확장 프로그램에서 ‘CREATE EXTENSION’ 명령을 사용하여 확장 프로그램을 실행하고 생성할 수 있습니다. 또한 DBA는 데이터베이스의 내부 동작을 수정하고 일반적으로 권한 상승이 요구되는 보다 정교한 확장 프로그램에 필요한 ‘PostgreSQL 후크’를 허용 목록에 추가할 수 있습니다.\"\n",
            "TLE for PostgreSQL은 다른 AWS 서비스와 어떻게 연관되고 연동되나요?\n",
            "TLE for PostgreSQL은 Amazon Aurora PostgreSQL 호환 버전과 Amazon RDS on PostgreSQL 버전 14.5 이상에서 사용할 수 있습니다. TLE는 PostgreSQL 확장 프로그램 자체로 구현되며 Aurora 및 Amazon RDS에서 지원되는 다른 확장 프로그램과 마찬가지로 rds_superuser 역할에서 활성화할 수 있습니다.\n",
            "\"category : RDS, question : TLE for PostgreSQL은 다른 AWS 서비스와 어떻게 연관되고 연동되나요?, answer : TLE for PostgreSQL은 Amazon Aurora PostgreSQL 호환 버전과 Amazon RDS on PostgreSQL 버전 14.5 이상에서 사용할 수 있습니다. TLE는 PostgreSQL 확장 프로그램 자체로 구현되며 Aurora 및 Amazon RDS에서 지원되는 다른 확장 프로그램과 마찬가지로 rds_superuser 역할에서 활성화할 수 있습니다.\"\n",
            "TLE for PostgreSQL을 실행할 수 있는 PostgreSQL 버전은 무엇인가요?\n",
            "Amazon Aurora 및 Amazon RDS의 PostgreSQL 14.5 이상에서 TLE for PostgreSQL을 실행할 수 있습니다.\n",
            "\"category : RDS, question : TLE for PostgreSQL을 실행할 수 있는 PostgreSQL 버전은 무엇인가요?, answer : Amazon Aurora 및 Amazon RDS의 PostgreSQL 14.5 이상에서 TLE for PostgreSQL을 실행할 수 있습니다.\"\n",
            "Trusted Language Extensions for PostgreSQL은 어느 리전에서 사용할 수 있나요?\n",
            "TLE for PostgreSQL은 현재 AWS 중국 리전을 제외한 모든 AWS 리전과 AWS GovCloud 리전에서 사용할 수 있습니다.\n",
            "\"category : RDS, question : Trusted Language Extensions for PostgreSQL은 어느 리전에서 사용할 수 있나요?, answer : TLE for PostgreSQL은 현재 AWS 중국 리전을 제외한 모든 AWS 리전과 AWS GovCloud 리전에서 사용할 수 있습니다.\"\n",
            "TLE를 실행하는 데 드는 비용은 얼마인가요?\n",
            "TLE for PostgreSQL은 Aurora 및 Amazon RDS 고객에게 추가 비용 없이 제공됩니다.\n",
            "\"category : RDS, question : TLE를 실행하는 데 드는 비용은 얼마인가요?, answer : TLE for PostgreSQL은 Aurora 및 Amazon RDS 고객에게 추가 비용 없이 제공됩니다.\"\n",
            "TLE for PostgreSQL은 현재 Amazon Aurora 및 Amazon RDS에서 사용할 수 있는 확장 프로그램과 어떻게 다른가요?\n",
            "Aurora 및 Amazon RDS는 85개가 넘는 엄선된 PostgreSQL 확장 프로그램 세트를 지원합니다. AWS는 AWS Shared Responsibility Model에 따라 이러한 각 확장 프로그램의 보안 위험을 관리합니다. TLE for PostgreSQL을 구현하는 확장 프로그램은 이 세트에 포함됩니다. 서드 파티 소스를 사용하여 작성하거나 서드 파티 소스에서 가져와서 TLE에 설치하는 확장 프로그램은 사용자 애플리케이션 코드의 일부로 간주됩니다. TLE 확장 프로그램을 사용하는 애플리케이션의 보안은 사용자의 책임입니다.\n",
            "\"category : RDS, question : TLE for PostgreSQL은 현재 Amazon Aurora 및 Amazon RDS에서 사용할 수 있는 확장 프로그램과 어떻게 다른가요?, answer : Aurora 및 Amazon RDS는 85개가 넘는 엄선된 PostgreSQL 확장 프로그램 세트를 지원합니다. AWS는 AWS Shared Responsibility Model에 따라 이러한 각 확장 프로그램의 보안 위험을 관리합니다. TLE for PostgreSQL을 구현하는 확장 프로그램은 이 세트에 포함됩니다. 서드 파티 소스를 사용하여 작성하거나 서드 파티 소스에서 가져와서 TLE에 설치하는 확장 프로그램은 사용자 애플리케이션 코드의 일부로 간주됩니다. TLE 확장 프로그램을 사용하는 애플리케이션의 보안은 사용자의 책임입니다.\"\n",
            "TLE for PostgreSQL과 함께 실행할 수 있는 확장 프로그램의 예로는 어떤 것이 있나요?\n",
            "비트맵 압축 및 차등 개인 정보 보호(예: 개인의 개인 정보를 보호하는 공개 액세스 가능한 통계 쿼리)와 같은 개발자 함수를 구축할 수 있습니다.\n",
            "\"category : RDS, question : TLE for PostgreSQL과 함께 실행할 수 있는 확장 프로그램의 예로는 어떤 것이 있나요?, answer : 비트맵 압축 및 차등 개인 정보 보호(예: 개인의 개인 정보를 보호하는 공개 액세스 가능한 통계 쿼리)와 같은 개발자 함수를 구축할 수 있습니다.\"\n",
            "TLE for PostgreSQL을 개발할 때 사용할 수 있는 프로그래밍 언어는 무엇인가요?\n",
            "TLE for PostgreSQL은 현재 JavaScript, PL/pgSQL, Perl 및 SQL을 지원합니다.\n",
            "\"category : RDS, question : TLE for PostgreSQL을 개발할 때 사용할 수 있는 프로그래밍 언어는 무엇인가요?, answer : TLE for PostgreSQL은 현재 JavaScript, PL/pgSQL, Perl 및 SQL을 지원합니다.\"\n",
            "TLE for PostgreSQL 확장 프로그램을 배포하려면 어떻게 해야 하나요?\n",
            "rds_superuser 역할이 TLE for PostgreSQL을 활성화하면, psql과 같은 모든 PostgreSQL 클라이언트에서 SQL CREATE EXTENSION 명령을 사용하여 TLE 확장 프로그램을 배포할 수 있습니다. 이 방법은 프로시저 언어(예: PL/pgSQL 또는 PL/Perl)로 작성된 사용자 정의 함수를 생성하는 방법과 유사합니다. 관리자는 TLE 확장 프로그램을 배포하고 특정 확장 프로그램을 사용할 권한이 있는 사용자를 제어할 수 있습니다.\n",
            "\"category : RDS, question : TLE for PostgreSQL 확장 프로그램을 배포하려면 어떻게 해야 하나요?, answer : rds_superuser 역할이 TLE for PostgreSQL을 활성화하면, psql과 같은 모든 PostgreSQL 클라이언트에서 SQL CREATE EXTENSION 명령을 사용하여 TLE 확장 프로그램을 배포할 수 있습니다. 이 방법은 프로시저 언어(예: PL/pgSQL 또는 PL/Perl)로 작성된 사용자 정의 함수를 생성하는 방법과 유사합니다. 관리자는 TLE 확장 프로그램을 배포하고 특정 확장 프로그램을 사용할 권한이 있는 사용자를 제어할 수 있습니다.\"\n",
            "TLE for PostgreSQL 확장 프로그램은 PostgreSQL 데이터베이스와 어떻게 통신하나요?\n",
            "TLE for PostgreSQL은 TLE API를 통해서만 PostgreSQL 데이터베이스에 액세스합니다. TLE가 지원하는 신뢰할 수 있는 언어에는 PostgreSQL 서버 프로그래밍 인터페이스(SPI)의 모든 함수가 포함되며 암호 확인 후크를 포함한 PostgreSQL 후크를 지원합니다.\n",
            "\"category : RDS, question : TLE for PostgreSQL 확장 프로그램은 PostgreSQL 데이터베이스와 어떻게 통신하나요?, answer : TLE for PostgreSQL은 TLE API를 통해서만 PostgreSQL 데이터베이스에 액세스합니다. TLE가 지원하는 신뢰할 수 있는 언어에는 PostgreSQL 서버 프로그래밍 인터페이스(SPI)의 모든 함수가 포함되며 암호 확인 후크를 포함한 PostgreSQL 후크를 지원합니다.\"\n",
            "TLE for PostgreSQL 오픈 소스 프로젝트에 대해 자세히 알아보려면 어디로 가야 하나요?\n",
            "TLE for PostgreSQL 프로젝트에 대해서는 공식 TLE GitHub 페이지에서 자세히 알아볼 수 있습니다.\n",
            "\"category : RDS, question : TLE for PostgreSQL 오픈 소스 프로젝트에 대해 자세히 알아보려면 어디로 가야 하나요?, answer : TLE for PostgreSQL 프로젝트에 대해서는 공식 TLE GitHub 페이지에서 자세히 알아볼 수 있습니다.\"\n",
            "Amazon RDS 블루/그린 배포는 어떤 엔진을 지원하나요?\n",
            "Amazon RDS 블루/그린 배포는 Amazon Aurora MySQL 호환 에디션, Amazon Aurora PostgreSQL 호환 에디션, Amazon RDS for MySQL, Amazon RDS for MariaDB 및 Amazon RDS for PostgreSQL에서 사용할 수 있습니다.\n",
            "\"category : RDS, question : Amazon RDS 블루/그린 배포는 어떤 엔진을 지원하나요?, answer : Amazon RDS 블루/그린 배포는 Amazon Aurora MySQL 호환 에디션, Amazon Aurora PostgreSQL 호환 에디션, Amazon RDS for MySQL, Amazon RDS for MariaDB 및 Amazon RDS for PostgreSQL에서 사용할 수 있습니다.\"\n",
            "Amazon RDS 블루/그린 배포는 어떤 버전을 지원하나요?\n",
            "Amazon RDS 블루/그린 배포는 Amazon Aurora MySQL 호환 버전 5.6 이상, RDS for MySQL 버전 5.7 이상 및 버전 MariaDB 버전 10.2 이상의 RDS에서 사용할 수 있습니다. 블루/그린 배포는 Amazon Aurora PostgreSQL 호환 에디션과 Amazon RDS for PostgreSQL 버전 11.21 이상, 12.16 이상, 13.12 이상, 14.9 이상 및 15.4 이상에 대해서도 지원됩니다. Amazon Aurora 및 Amazon RDS 설명서에서 사용 가능한 버전에 대해 자세히 알아보세요.\n",
            "\"category : RDS, question : Amazon RDS 블루/그린 배포는 어떤 버전을 지원하나요?, answer : Amazon RDS 블루/그린 배포는 Amazon Aurora MySQL 호환 버전 5.6 이상, RDS for MySQL 버전 5.7 이상 및 버전 MariaDB 버전 10.2 이상의 RDS에서 사용할 수 있습니다. 블루/그린 배포는 Amazon Aurora PostgreSQL 호환 에디션과 Amazon RDS for PostgreSQL 버전 11.21 이상, 12.16 이상, 13.12 이상, 14.9 이상 및 15.4 이상에 대해서도 지원됩니다. Amazon Aurora 및 Amazon RDS 설명서에서 사용 가능한 버전에 대해 자세히 알아보세요.\"\n",
            "Amazon RDS 블루/그린 배포는 어느 리전을 지원하나요?\n",
            "Amazon RDS 블루/그린 배포는 모든 AWS 리전과 AWS GovCloud 리전에서 사용할 수 있습니다.\n",
            "\"category : RDS, question : Amazon RDS 블루/그린 배포는 어느 리전을 지원하나요?, answer : Amazon RDS 블루/그린 배포는 모든 AWS 리전과 AWS GovCloud 리전에서 사용할 수 있습니다.\"\n",
            "Amazon RDS 블루/그린 배포는 언제 사용해야 하나요?\n",
            "Amazon RDS 블루/그린 배포를 사용하면 더 안전하고 단순하며 빠르게 데이터베이스를 변경할 수 있습니다. 블루/그린 배포는 메이저 또는 마이너 버전 데이터베이스 엔진 업그레이드, 운영 체제 업데이트, 논리적 복제를 중단하지 않는 그린 환경의 스키마 변경(예: 테이블 끝에 새 열 추가) 또는 데이터베이스 파라미터 설정 변경과 같은 사용 사례에 적합합니다. 블루/그린 배포를 사용하면 한 번의 전환으로 여러 데이터베이스를 동시에 업데이트할 수 있습니다. 이를 통해, 예측 가능한 짧은 가동 중지 시간으로 보안 패치를 최신 상태로 유지하고, 데이터베이스 성능을 개선하고, 최신 데이터베이스 기능에 액세스할 수 있습니다.\n",
            "\"category : RDS, question : Amazon RDS 블루/그린 배포는 언제 사용해야 하나요?, answer : Amazon RDS 블루/그린 배포를 사용하면 더 안전하고 단순하며 빠르게 데이터베이스를 변경할 수 있습니다. 블루/그린 배포는 메이저 또는 마이너 버전 데이터베이스 엔진 업그레이드, 운영 체제 업데이트, 논리적 복제를 중단하지 않는 그린 환경의 스키마 변경(예: 테이블 끝에 새 열 추가) 또는 데이터베이스 파라미터 설정 변경과 같은 사용 사례에 적합합니다. 블루/그린 배포를 사용하면 한 번의 전환으로 여러 데이터베이스를 동시에 업데이트할 수 있습니다. 이를 통해, 예측 가능한 짧은 가동 중지 시간으로 보안 패치를 최신 상태로 유지하고, 데이터베이스 성능을 개선하고, 최신 데이터베이스 기능에 액세스할 수 있습니다.\"\n",
            "Amazon RDS 블루/그린 배포의 사용 요금은 얼마인가요?\n",
            "블루 인스턴스에서 워크로드를 실행할 때와 동일한 비용이 그린 인스턴스에 대해 발생합니다. 블루 및 그린 인스턴스에서 실행할 때의 비용에는 db.instances에 대한 현재 표준 요금, 스토리지 비용, 읽기/쓰기 I/O 비용 및 사용된 기능에 대한 비용(예: 백업 비용 및 Amazon RDS 성능 개선 도우미 비용)이 포함됩니다. 실질적으로 블루 및 그린 배포가 진행되는 동안 db.instance에서 워크로드를 실행할 때의 약 2배에 해당하는 비용이 발생합니다.\n",
            "예제: us-east-1 AWS 리전에서 다중 AZ(MAZ) 구성으로 RDS for MySQL 5.7 데이터베이스를 2개의 r5.2xlarge DB 인스턴스(기본 데이터베이스 인스턴스와 읽기 전용 복제본)에서 실행하고 있습니다. 각 r5.2xlarge DB 인스턴스는 20GiB 범용 Amazon Elastic Block Storge(EBS)에 대해 구성되어 있습니다. Amazon RDS 블루/그린 배포를 사용하여 블루 인스턴스 토폴로지의 클론을 생성하고 15일(360시간) 동안 실행한 다음 성공적인 전환 후에 블루 인스턴스를 삭제합니다. 블루 인스턴스의 비용은 시간당 1.926 USD의 온디맨드 요금으로 15일간 1,387 USD입니다(인스턴스 + EBS 비용). 이 15일간 블루/그린 배포를 사용한 데 대한 총 비용은 2,774 USD입니다. 이는 해당 기간에 블루 인스턴스를 실행하는 비용의 2배입니다.\n",
            "\"category : RDS, question : Amazon RDS 블루/그린 배포의 사용 요금은 얼마인가요?, answer : 블루 인스턴스에서 워크로드를 실행할 때와 동일한 비용이 그린 인스턴스에 대해 발생합니다. 블루 및 그린 인스턴스에서 실행할 때의 비용에는 db.instances에 대한 현재 표준 요금, 스토리지 비용, 읽기/쓰기 I/O 비용 및 사용된 기능에 대한 비용(예: 백업 비용 및 Amazon RDS 성능 개선 도우미 비용)이 포함됩니다. 실질적으로 블루 및 그린 배포가 진행되는 동안 db.instance에서 워크로드를 실행할 때의 약 2배에 해당하는 비용이 발생합니다.\n",
            "예제: us-east-1 AWS 리전에서 다중 AZ(MAZ) 구성으로 RDS for MySQL 5.7 데이터베이스를 2개의 r5.2xlarge DB 인스턴스(기본 데이터베이스 인스턴스와 읽기 전용 복제본)에서 실행하고 있습니다. 각 r5.2xlarge DB 인스턴스는 20GiB 범용 Amazon Elastic Block Storge(EBS)에 대해 구성되어 있습니다. Amazon RDS 블루/그린 배포를 사용하여 블루 인스턴스 토폴로지의 클론을 생성하고 15일(360시간) 동안 실행한 다음 성공적인 전환 후에 블루 인스턴스를 삭제합니다. 블루 인스턴스의 비용은 시간당 1.926 USD의 온디맨드 요금으로 15일간 1,387 USD입니다(인스턴스 + EBS 비용). 이 15일간 블루/그린 배포를 사용한 데 대한 총 비용은 2,774 USD입니다. 이는 해당 기간에 블루 인스턴스를 실행하는 비용의 2배입니다.\"\n",
            "Amazon RDS 블루/그린 배포에서는 어떤 종류의 변경을 수행할 수 있나요?\n",
            "Amazon RDS 블루/그린 배포에서는 메이저 또는 마이너 버전 업그레이드, 스키마 변경, 인스턴스 크기 조정, 엔진 파라미터 변경 및 유지 관리 업데이트와 같은 데이터베이스 변경을 더 안전하고 더 간단하며 더 빠르게 수행할 수 있습니다.\n",
            "\"category : RDS, question : Amazon RDS 블루/그린 배포에서는 어떤 종류의 변경을 수행할 수 있나요?, answer : Amazon RDS 블루/그린 배포에서는 메이저 또는 마이너 버전 업그레이드, 스키마 변경, 인스턴스 크기 조정, 엔진 파라미터 변경 및 유지 관리 업데이트와 같은 데이터베이스 변경을 더 안전하고 더 간단하며 더 빠르게 수행할 수 있습니다.\"\n",
            "Amazon RDS 블루/그린 배포에서 ‘블루 환경’은 무엇이고, ‘그린 환경’은 무엇인가요?\n",
            "Amazon RDS 블루/그린 배포에서 블루 환경은 현재 프로덕션 환경입니다. 그린 환경은 전환 후에 새로운 프로덕션 환경이 될 스테이징 환경입니다.\n",
            "\"category : RDS, question : Amazon RDS 블루/그린 배포에서 ‘블루 환경’은 무엇이고, ‘그린 환경’은 무엇인가요?, answer : Amazon RDS 블루/그린 배포에서 블루 환경은 현재 프로덕션 환경입니다. 그린 환경은 전환 후에 새로운 프로덕션 환경이 될 스테이징 환경입니다.\"\n",
            "Amazon RDS 블루/그린 배포에서 전환은 어떻게 작동하나요?\n",
            "Amazon RDS 블루/그린 배포에서 전환이 시작되면, 전환이 완료될 때까지 블루 환경과 그린 환경 모두에 대한 쓰기가 차단됩니다. 전환 중에 스테이징 환경 또는 그린 환경은 프로덕션 시스템과 동기화하여 스테이징 환경과 프로덕션 환경의 데이터가 일치할 수 있도록 합니다. 프로덕션과 스테이징 환경이 완벽하게 동기화되면 블루/그린 배포는 새롭게 승격된 프로덕션 환경으로 트래픽을 리디렉션하여 스테이징 환경을 새 프로덕션 환경으로 승격합니다. 블루/그린 배포는 전환이 완료된 후 그린 환경에 대한 쓰기를 지원하여 전환 프로세스 중에 데이터 손실이 발생하지 않도록 합니다.\n",
            "\"category : RDS, question : Amazon RDS 블루/그린 배포에서 전환은 어떻게 작동하나요?, answer : Amazon RDS 블루/그린 배포에서 전환이 시작되면, 전환이 완료될 때까지 블루 환경과 그린 환경 모두에 대한 쓰기가 차단됩니다. 전환 중에 스테이징 환경 또는 그린 환경은 프로덕션 시스템과 동기화하여 스테이징 환경과 프로덕션 환경의 데이터가 일치할 수 있도록 합니다. 프로덕션과 스테이징 환경이 완벽하게 동기화되면 블루/그린 배포는 새롭게 승격된 프로덕션 환경으로 트래픽을 리디렉션하여 스테이징 환경을 새 프로덕션 환경으로 승격합니다. 블루/그린 배포는 전환이 완료된 후 그린 환경에 대한 쓰기를 지원하여 전환 프로세스 중에 데이터 손실이 발생하지 않도록 합니다.\"\n",
            "자체 관리형 논리적 복제본의 구독자/게시자로 블루 환경을 사용하는 경우에도 블루/그린 배포를 사용할 수 있나요?\n",
            "블루 환경이 자체 관리형 논리적 복제본 또는 구독자인 경우 전환이 차단됩니다. 먼저 블루 환경으로의 복제를 중단하고, 전환을 진행한 다음 복제를 재개하는 것이 좋습니다. 반대로 블루 환경이 자체 관리형 논리적 복제본이나 게시자의 소스인 경우에는 계속 전환할 수 있습니다. 하지만 전환 후 친환경 환경에서 복제하려면 자체 관리형 복제본을 업데이트해야 합니다.\n",
            "\"category : RDS, question : 자체 관리형 논리적 복제본의 구독자/게시자로 블루 환경을 사용하는 경우에도 블루/그린 배포를 사용할 수 있나요?, answer : 블루 환경이 자체 관리형 논리적 복제본 또는 구독자인 경우 전환이 차단됩니다. 먼저 블루 환경으로의 복제를 중단하고, 전환을 진행한 다음 복제를 재개하는 것이 좋습니다. 반대로 블루 환경이 자체 관리형 논리적 복제본이나 게시자의 소스인 경우에는 계속 전환할 수 있습니다. 하지만 전환 후 친환경 환경에서 복제하려면 자체 관리형 복제본을 업데이트해야 합니다.\"\n",
            "Amazon RDS 블루/그린 배포가 전환된 후 이전 프로덕션 환경은 어떻게 되나요?\n",
            "Amazon RDS 블루/그린 배포는 이전 프로덕션 환경을 삭제하지 않습니다. 필요한 경우 추가 검증 및 성능/회귀 테스트를 위해 이전 환경에 액세스할 수 있습니다. 이전 프로덕션 환경이 더 이상 필요하지 않다면 삭제해도 됩니다. 이전 프로덕션 인스턴스에는 인스턴스를 삭제하기 전까지 표준 결제 요금이 적용됩니다.\n",
            "\"category : RDS, question : Amazon RDS 블루/그린 배포가 전환된 후 이전 프로덕션 환경은 어떻게 되나요?, answer : Amazon RDS 블루/그린 배포는 이전 프로덕션 환경을 삭제하지 않습니다. 필요한 경우 추가 검증 및 성능/회귀 테스트를 위해 이전 환경에 액세스할 수 있습니다. 이전 프로덕션 환경이 더 이상 필요하지 않다면 삭제해도 됩니다. 이전 프로덕션 인스턴스에는 인스턴스를 삭제하기 전까지 표준 결제 요금이 적용됩니다.\"\n",
            "Amazon RDS 블루/그린 배포 전환 가드레일은 무엇을 확인하나요?\n",
            "Amazon RDS 블루 및 그린 배포의 전환 가드레일은 전환 전에 그린 환경이 캐치업될 때까지 블루 및 그린 환경에 대한 쓰기를 차단합니다. 블루 및 그린 배포는 블루 및 그린 환경에 있는 프라이머리 및 복제본의 상태 확인도 수행합니다. 또한 복제 상태 확인도 수행하는데, 예를 들어 복제가 중지되었는지, 오류가 있는지 여부를 확인합니다. 블루 환경과 그린 환경 사이에 오래 실행되는 트랜잭션이 있는 경우 이를 감지합니다. 사용자는 허용 가능한 최대 가동 중단 시간을 최소 30초로 지정할 수 있으며, 진행 중인 트랜잭션이 이 값을 초과할 경우 전환 제한 시간이 초과됩니다.\n",
            "\"category : RDS, question : Amazon RDS 블루/그린 배포 전환 가드레일은 무엇을 확인하나요?, answer : Amazon RDS 블루 및 그린 배포의 전환 가드레일은 전환 전에 그린 환경이 캐치업될 때까지 블루 및 그린 환경에 대한 쓰기를 차단합니다. 블루 및 그린 배포는 블루 및 그린 환경에 있는 프라이머리 및 복제본의 상태 확인도 수행합니다. 또한 복제 상태 확인도 수행하는데, 예를 들어 복제가 중지되었는지, 오류가 있는지 여부를 확인합니다. 블루 환경과 그린 환경 사이에 오래 실행되는 트랜잭션이 있는 경우 이를 감지합니다. 사용자는 허용 가능한 최대 가동 중단 시간을 최소 30초로 지정할 수 있으며, 진행 중인 트랜잭션이 이 값을 초과할 경우 전환 제한 시간이 초과됩니다.\"\n",
            "Amazon RDS 블루/그린 배포는 글로벌 데이터베이스, Amazon RDS 프록시, 교차 리전 읽기 전용 복제본 또는 캐스케이드된 읽기 전용 복제본을 지원하나요?\n",
            "아니요. Amazon RDS 블루/그린 배포는 글로벌 데이터베이스, Amazon RDS 프록시, 크로스 리전 읽기 전용 복제본 또는 캐스케이드된 읽기 전용 복제본을 지원하지 않습니다.\n",
            "\"category : RDS, question : Amazon RDS 블루/그린 배포는 글로벌 데이터베이스, Amazon RDS 프록시, 교차 리전 읽기 전용 복제본 또는 캐스케이드된 읽기 전용 복제본을 지원하나요?, answer : 아니요. Amazon RDS 블루/그린 배포는 글로벌 데이터베이스, Amazon RDS 프록시, 크로스 리전 읽기 전용 복제본 또는 캐스케이드된 읽기 전용 복제본을 지원하지 않습니다.\"\n",
            "Amazon RDS 블루/그린 배포를 사용하여 변경 사항을 롤백할 수 있나요?\n",
            "아니요. 지금은 Amazon RDS 블루/그린 배포를 사용하여 변경 사항을 롤백할 수 없습니다.\n",
            "\"category : RDS, question : Amazon RDS 블루/그린 배포를 사용하여 변경 사항을 롤백할 수 있나요?, answer : 아니요. 지금은 Amazon RDS 블루/그린 배포를 사용하여 변경 사항을 롤백할 수 없습니다.\"\n",
            "Amazon RDS 최적화된 쓰기는 데이터 파일을 쓰는 방식이 MySQL과 어떻게 다른가요?\n",
            "MySQL은 데이터를 16KiB 페이지의 메모리로 내구성 높은 스토리지에 두 번 기록하는 방법으로 데이터 손실로부터 사용자를 보호합니다. 처음에 ‘이중 쓰기 버퍼’에 기록한 다음 테이블 스토리지에 기록합니다. Amazon RDS 최적화된 쓰기는 AWS Nitro System의 Torn Write Prevention 기능을 사용하여 한 번에 안정적이고 지속적으로 16KiB 데이터 페이지를 데이터 파일에 직접 기록합니다.\n",
            "\"category : RDS, question : Amazon RDS 최적화된 쓰기는 데이터 파일을 쓰는 방식이 MySQL과 어떻게 다른가요?, answer : MySQL은 데이터를 16KiB 페이지의 메모리로 내구성 높은 스토리지에 두 번 기록하는 방법으로 데이터 손실로부터 사용자를 보호합니다. 처음에 ‘이중 쓰기 버퍼’에 기록한 다음 테이블 스토리지에 기록합니다. Amazon RDS 최적화된 쓰기는 AWS Nitro System의 Torn Write Prevention 기능을 사용하여 한 번에 안정적이고 지속적으로 16KiB 데이터 페이지를 데이터 파일에 직접 기록합니다.\"\n",
            "Amazon RDS 최적화된 쓰기를 지원하는 RDS for MySQL 데이터베이스 버전은 무엇인가요?\n",
            "Amazon RDS 최적화된 쓰기는 MySQL 메이저 버전 8.0.30 이상에서 사용할 수 있습니다.\n",
            "\"category : RDS, question : Amazon RDS 최적화된 쓰기를 지원하는 RDS for MySQL 데이터베이스 버전은 무엇인가요?, answer : Amazon RDS 최적화된 쓰기는 MySQL 메이저 버전 8.0.30 이상에서 사용할 수 있습니다.\"\n",
            "Amazon RDS 최적화된 쓰기는 어떤 데이터베이스 인스턴스 유형을 지원하나요? 어느 리전에서 사용할 수 있나요?\n",
            "Amazon RDS Optimized Writes는 db.r6i 및 db.r5b 인스턴스에서 사용할 수 있습니다. AWS 중국 리전을 제외하고 이러한 인스턴스가 제공되는 모든 리전에서 사용할 수 있습니다.\n",
            "\"category : RDS, question : Amazon RDS 최적화된 쓰기는 어떤 데이터베이스 인스턴스 유형을 지원하나요? 어느 리전에서 사용할 수 있나요?, answer : Amazon RDS Optimized Writes는 db.r6i 및 db.r5b 인스턴스에서 사용할 수 있습니다. AWS 중국 리전을 제외하고 이러한 인스턴스가 제공되는 모든 리전에서 사용할 수 있습니다.\"\n",
            "Amazon RDS Optimized Writes는 Amazon Aurora MySQL 호환 버전에서 지원되나요?\n",
            "아니요. Amazon Aurora MySQL 호환 버전에서는 이미 ‘이중 쓰기 버퍼’의 사용을 방지하고 있습니다. 대신, Amazon Aurora는 3개 가용 영역에 걸쳐 6가지 방법으로 데이터를 복제하고 쿼럼 기반 접근 방식을 사용하여 데이터를 내구성 있는 방식으로 쓰고 이후에 올바르게 읽습니다.\n",
            "\"category : RDS, question : Amazon RDS Optimized Writes는 Amazon Aurora MySQL 호환 버전에서 지원되나요?, answer : 아니요. Amazon Aurora MySQL 호환 버전에서는 이미 ‘이중 쓰기 버퍼’의 사용을 방지하고 있습니다. 대신, Amazon Aurora는 3개 가용 영역에 걸쳐 6가지 방법으로 데이터를 복제하고 쿼럼 기반 접근 방식을 사용하여 데이터를 내구성 있는 방식으로 쓰고 이후에 올바르게 읽습니다.\"\n",
            "고객은 Amazon RDS 최적화된 쓰기를 사용하도록 기존 Amazon RDS 데이터베이스를 전환할 수 있나요?\n",
            "현재 이 초기 릴리스는 인스턴스 클래스가 최적화된 쓰기를 지원하더라도 Amazon RDS 최적화된 쓰기를 기존 데이터베이스 인스턴스에 사용하는 것을 지원하지 않습니다.\n",
            "\"category : RDS, question : 고객은 Amazon RDS 최적화된 쓰기를 사용하도록 기존 Amazon RDS 데이터베이스를 전환할 수 있나요?, answer : 현재 이 초기 릴리스는 인스턴스 클래스가 최적화된 쓰기를 지원하더라도 Amazon RDS 최적화된 쓰기를 기존 데이터베이스 인스턴스에 사용하는 것을 지원하지 않습니다.\"\n",
            "Amazon RDS 최적화된 쓰기의 비용은 얼마인가요?\n",
            "Amazon RDS 최적화된 쓰기는 RDS for MySQL 고객에게 추가 비용 없이 제공됩니다.\n",
            "\"category : RDS, question : Amazon RDS 최적화된 쓰기의 비용은 얼마인가요?, answer : Amazon RDS 최적화된 쓰기는 RDS for MySQL 고객에게 추가 비용 없이 제공됩니다.\"\n",
            "Amazon RDS Optimized Reads는 쿼리 성능을 어떻게 개선하나요?\n",
            "MySQL 및 MariaDB의 임시 객체를 쿼리 처리에 사용하는 워크로드의 경우 Amazon RDS Optimized Reads의 이점을 누릴 수 있습니다. Optimized Reads는 Amazon Elastic Block Store 볼륨 대신, 데이터베이스 인스턴스의 NVMe 기반 인스턴스 스토리지에 임시 객체를 배치합니다. 이렇게 하면 복잡한 쿼리의 처리 속도가 최대 2배 더 빨라집니다.\n",
            "\"category : RDS, question : Amazon RDS Optimized Reads는 쿼리 성능을 어떻게 개선하나요?, answer : MySQL 및 MariaDB의 임시 객체를 쿼리 처리에 사용하는 워크로드의 경우 Amazon RDS Optimized Reads의 이점을 누릴 수 있습니다. Optimized Reads는 Amazon Elastic Block Store 볼륨 대신, 데이터베이스 인스턴스의 NVMe 기반 인스턴스 스토리지에 임시 객체를 배치합니다. 이렇게 하면 복잡한 쿼리의 처리 속도가 최대 2배 더 빨라집니다.\"\n",
            "Amazon RDS Optimized Reads를 지원하는 RDS for MySQL 및 RDS for MariaDB 데이터베이스 버전은 무엇인가요?\n",
            "Amazon RDS 최적화된 읽기는 RDS for MySQL의 경우 MySQL 버전 8.0.28 이상, RDS for MariaDB의 경우 MariaDB 버전 10.4.25, 10.5.16, 10.6.7 이상에서 사용 가능합니다.\n",
            "\"category : RDS, question : Amazon RDS Optimized Reads를 지원하는 RDS for MySQL 및 RDS for MariaDB 데이터베이스 버전은 무엇인가요?, answer : Amazon RDS 최적화된 읽기는 RDS for MySQL의 경우 MySQL 버전 8.0.28 이상, RDS for MariaDB의 경우 MariaDB 버전 10.4.25, 10.5.16, 10.6.7 이상에서 사용 가능합니다.\"\n",
            "Amazon RDS Optimized Reads는 어떤 데이터베이스 인스턴스 유형을 지원하나요? 그리고 어떤 리전에서 사용할 수 있나요?\n",
            "Amazon RDS 최적화된 읽기는 db.r5d, db.m5d, db.r6gd, db.m6gd, X2idn, X2iedn 인스턴스가 제공되는 모든 리전에서 사용할 수 있습니다. 자세한 내용은 Amazon RDS DB 인스턴스 클래스 설명서를 참조하세요.\n",
            "\"category : RDS, question : Amazon RDS Optimized Reads는 어떤 데이터베이스 인스턴스 유형을 지원하나요? 그리고 어떤 리전에서 사용할 수 있나요?, answer : Amazon RDS 최적화된 읽기는 db.r5d, db.m5d, db.r6gd, db.m6gd, X2idn, X2iedn 인스턴스가 제공되는 모든 리전에서 사용할 수 있습니다. 자세한 내용은 Amazon RDS DB 인스턴스 클래스 설명서를 참조하세요.\"\n",
            "Amazon RDS 최적화된 읽기는 언제 사용해야 하나요?\n",
            "복잡한 쿼리 또는 범용 분석이 필요한 워크로드 또는 복잡한 그룹, 정렬, 해시 집계, 높은 로드 조인 및 공통 테이블 표현식(CTE)이 필요한 워크로드가 있는 경우 고객은 Amazon RDS 최적화된 읽기를 사용해야 합니다. 이러한 사용 사례에서는 임시 테이블이 생성되므로 최적화된 읽기를 통해 워크로드의 쿼리 처리 속도를 높일 수 있습니다.\n",
            "\"category : RDS, question : Amazon RDS 최적화된 읽기는 언제 사용해야 하나요?, answer : 복잡한 쿼리 또는 범용 분석이 필요한 워크로드 또는 복잡한 그룹, 정렬, 해시 집계, 높은 로드 조인 및 공통 테이블 표현식(CTE)이 필요한 워크로드가 있는 경우 고객은 Amazon RDS 최적화된 읽기를 사용해야 합니다. 이러한 사용 사례에서는 임시 테이블이 생성되므로 최적화된 읽기를 통해 워크로드의 쿼리 처리 속도를 높일 수 있습니다.\"\n",
            "고객이 Amazon RDS 최적화된 읽기를 사용하도록 자신의 기존 Amazon RDS 데이터베이스를 전환할 수 있나요?\n",
            "예. 워크로드를 최적화된 읽기 지원 인스턴스로 이동하는 방법으로 Amazon RDS 최적화된 읽기를 사용하도록 기존 Amazon RDS 데이터베이스를 전환할 수 있습니다. 또한, Optimized Reads는 지원되는 모든 인스턴스 클래스에서 기본적으로 사용할 수 있습니다. db.r5d, db.m5d, db.r6gd, db.m6gd, X2idn, X2iedn 인스턴스에서 워크로드를 실행하고 있다면 이미 Optimized Reads의 이점을 누리고 있는 것입니다.\n",
            "\"category : RDS, question : 고객이 Amazon RDS 최적화된 읽기를 사용하도록 자신의 기존 Amazon RDS 데이터베이스를 전환할 수 있나요?, answer : 예. 워크로드를 최적화된 읽기 지원 인스턴스로 이동하는 방법으로 Amazon RDS 최적화된 읽기를 사용하도록 기존 Amazon RDS 데이터베이스를 전환할 수 있습니다. 또한, Optimized Reads는 지원되는 모든 인스턴스 클래스에서 기본적으로 사용할 수 있습니다. db.r5d, db.m5d, db.r6gd, db.m6gd, X2idn, X2iedn 인스턴스에서 워크로드를 실행하고 있다면 이미 Optimized Reads의 이점을 누리고 있는 것입니다.\"\n",
            "========== Redshift  :  https://aws.amazon.com/ko/redshift/faqs/ 사이트 크롤링 진행중 ==========\n",
            "69\n",
            "Amazon Redshift란 무엇입니까?\n",
            "수만 명의 고객이 매일 Amazon Redshift를 사용하여 클라우드에서 SQL 분석을 실행하고 비즈니스 인사이트를 위해 엑사바이트 규모의 데이터를 처리합니다. 증가하는 데이터가 운영 데이터 스토어, 데이터 레이크, 스트리밍 데이터 서비스 또는 타사 데이터 세트 중 어디에 저장되는지에 관계없이 Amazon Redshift를 통해 최소한의 이동 또는 복사만으로 안전하게 데이터에 액세스하고 데이터를 결합 및 공유할 수 있습니다. Amazon Redshift는 AWS 데이터베이스, 분석 및 기계 학습 서비스와 긴밀하게 통합되어 Zero-ETL 접근 방식을 사용하거나 실시간에 가까운 분석을 위해 작업할 준비가 된 데이터에 액세스하고, SQL로 기계 학습 모델을 구축하고, Redshift의 데이터를 사용한 Apache Spark 분석을 활성화할 수 있게 지원합니다. Amazon Redshift Serverless를 사용하면 엔지니어, 개발자, 데이터 사이언티스트 및 분석가가 관리가 필요 없는 환경에서 쉽게 시작하고 신속하게 분석을 확장할 수 있습니다. 효율적인 확장을 위해 컴퓨팅과 스토리지를 분리하는 MPP(Massively Parallel Processing) 엔진 및 아키텍처와 기계 학습 기반 성능 혁신(예: AutoMaterialized Views)을 통해 Amazon Redshift는 확장을 고려하여 구축되었으며 다른 클라우드 데이터 웨어하우스보다 최대 5배 더 뛰어난 가격 대비 성능을 제공합니다.\n",
            "\"category : Redshift, question : Amazon Redshift란 무엇입니까?, answer : 수만 명의 고객이 매일 Amazon Redshift를 사용하여 클라우드에서 SQL 분석을 실행하고 비즈니스 인사이트를 위해 엑사바이트 규모의 데이터를 처리합니다. 증가하는 데이터가 운영 데이터 스토어, 데이터 레이크, 스트리밍 데이터 서비스 또는 타사 데이터 세트 중 어디에 저장되는지에 관계없이 Amazon Redshift를 통해 최소한의 이동 또는 복사만으로 안전하게 데이터에 액세스하고 데이터를 결합 및 공유할 수 있습니다. Amazon Redshift는 AWS 데이터베이스, 분석 및 기계 학습 서비스와 긴밀하게 통합되어 Zero-ETL 접근 방식을 사용하거나 실시간에 가까운 분석을 위해 작업할 준비가 된 데이터에 액세스하고, SQL로 기계 학습 모델을 구축하고, Redshift의 데이터를 사용한 Apache Spark 분석을 활성화할 수 있게 지원합니다. Amazon Redshift Serverless를 사용하면 엔지니어, 개발자, 데이터 사이언티스트 및 분석가가 관리가 필요 없는 환경에서 쉽게 시작하고 신속하게 분석을 확장할 수 있습니다. 효율적인 확장을 위해 컴퓨팅과 스토리지를 분리하는 MPP(Massively Parallel Processing) 엔진 및 아키텍처와 기계 학습 기반 성능 혁신(예: AutoMaterialized Views)을 통해 Amazon Redshift는 확장을 고려하여 구축되었으며 다른 클라우드 데이터 웨어하우스보다 최대 5배 더 뛰어난 가격 대비 성능을 제공합니다.\"\n",
            "고객들이 Amazon Redshift를 선택하는 가장 중요한 이유는 무엇인가요?\n",
            "Amazon Redshift는 데이터베이스 및 기계 학습 서비스와 잘 통합되고, 사용하기 쉽게 간소화되어 있고, 모든 분석 요구 사항을 충족하는 중앙 서비스로 사용 가능한 강력한 분석 시스템이기 때문에 수천 명의 고객이 인사이트 획득 시간 단축을 위해 Amazon Redshift를 선택합니다. Amazon Redshift Serverless는 데이터 웨어하우스 용량을 자동으로 프로비저닝하고 확장하여 수요가 많고 예측할 수 없는 워크로드에 맞춰 높은 성능을 제공합니다. Amazon Redshift는 대시보드 작업, 애플리케이션 개발, 데이터 공유, 추출, 전환, 적재(ETL) 작업 등 다양한 분석 워크로드에 대해 최고의 가격 대비 성능을 제공합니다. 수만 명의 고객이 수 테라바이트에서 수 페타바이트에 이르는 대규모 데이터에 대한 분석을 실행함에 따라 Amazon Redshift는 플릿 성능 원격 측정을 기반으로 실제 고객 워크로드 성능을 최적화하고 비용을 낮게 유지하면서 워크로드에 맞춰 선형적으로 확장되는 성능을 제공합니다. 성능 혁신은 추가 비용 없이 고객에게 제공됩니다. Amazon Redshift를 사용하면 운영 데이터베이스, 데이터 레이크, 데이터 웨어하우스, 스트리밍 데이터 및 타사 데이터 세트에 존재하는 모든 데이터에 대해 실시간 예측 분석을 실행하여 인사이트를 얻을 수 있습니다. Amazon Redshift는 AWS Single Sign-On(SSO), 다중 인증, 열 수준 액세스 제어, 행 수준 보안, 역할 기반 액세스 제어, Amazon Virtual Private Cloud(VPC) 및 더 빠른 클러스터 크기 조정을 위한 기본 제공 ID 관리 및 페더레이션을 통해 업계 최고 수준의 보안을 지원합니다.\n",
            "\"category : Redshift, question : 고객들이 Amazon Redshift를 선택하는 가장 중요한 이유는 무엇인가요?, answer : Amazon Redshift는 데이터베이스 및 기계 학습 서비스와 잘 통합되고, 사용하기 쉽게 간소화되어 있고, 모든 분석 요구 사항을 충족하는 중앙 서비스로 사용 가능한 강력한 분석 시스템이기 때문에 수천 명의 고객이 인사이트 획득 시간 단축을 위해 Amazon Redshift를 선택합니다. Amazon Redshift Serverless는 데이터 웨어하우스 용량을 자동으로 프로비저닝하고 확장하여 수요가 많고 예측할 수 없는 워크로드에 맞춰 높은 성능을 제공합니다. Amazon Redshift는 대시보드 작업, 애플리케이션 개발, 데이터 공유, 추출, 전환, 적재(ETL) 작업 등 다양한 분석 워크로드에 대해 최고의 가격 대비 성능을 제공합니다. 수만 명의 고객이 수 테라바이트에서 수 페타바이트에 이르는 대규모 데이터에 대한 분석을 실행함에 따라 Amazon Redshift는 플릿 성능 원격 측정을 기반으로 실제 고객 워크로드 성능을 최적화하고 비용을 낮게 유지하면서 워크로드에 맞춰 선형적으로 확장되는 성능을 제공합니다. 성능 혁신은 추가 비용 없이 고객에게 제공됩니다. Amazon Redshift를 사용하면 운영 데이터베이스, 데이터 레이크, 데이터 웨어하우스, 스트리밍 데이터 및 타사 데이터 세트에 존재하는 모든 데이터에 대해 실시간 예측 분석을 실행하여 인사이트를 얻을 수 있습니다. Amazon Redshift는 AWS Single Sign-On(SSO), 다중 인증, 열 수준 액세스 제어, 행 수준 보안, 역할 기반 액세스 제어, Amazon Virtual Private Cloud(VPC) 및 더 빠른 클러스터 크기 조정을 위한 기본 제공 ID 관리 및 페더레이션을 통해 업계 최고 수준의 보안을 지원합니다.\"\n",
            "Amazon Redshift는 어떤 방식으로 데이터 웨어하우스 및 분석 관리를 단순화하나요?\n",
            "Amazon Redshift의 관리는 AWS에서 모두 이루어지기 때문에 사용자는 하드웨어 프로비저닝, 소프트웨어 패치 적용, 설정, 구성, 장애로부터 복구를 위한 노드 및 드라이브 모니터링, 백업 등의 데이터 웨어하우스 관리 작업에 대해 더 이상 걱정할 필요가 없습니다. AWS에서 고객을 대신해 데이터 웨어하우스를 설정하고 운영하고 확장하는 데 필요한 작업을 관리하므로 고객은 애플리케이션을 구축하는 데 집중할 수 있습니다. Amazon Redshift Serverless는 데이터 웨어하우스 용량을 자동으로 프로비저닝하고 확장하여 수요가 많고 예측할 수 없는 워크로드에 맞춰 높은 성능을 제공하며, 사용한 리소스에 대해서만 비용을 지불하면 됩니다. Amazon Redshift는 자동 튜닝 기능도 제공하며, Redshift Advisor에서 웨어하우스를 관리하기 위한 권장 사항도 표시합니다. Redshift Spectrum을 통해 Amazon Redshift는 Amazon S3에 저장된 데이터에 대한 쿼리와 관련된 모든 컴퓨팅 인프라, 로드 밸런싱, 계획, 일정 예약 및 실행을 관리합니다. Amazon Redshift는 Amazon Aurora Zero-ETL to Amazon Redshift와 같은 기능을 통한 데이터베이스 서비스와의 긴밀한 통합과 Amazon RDS, Amazon S3 데이터 레이크 등의 운영 데이터베이스에서 작업할 준비가 된 데이터에 액세스하기 위한 페더레이션 쿼리로 모든 데이터에 대한 분석을 지원합니다. Redshift는 스트리밍 데이터 또는 Amazon S3 파일을 자동으로 모으는 노코드 자동 데이터 파이프라인을 통해 간소화된 데이터 모으기를 지원합니다. 또한 Redshift는 AWS Data Exchange와 통합되어 사용자가 타사 데이터 세트를 찾고, 구독하고, 쿼리하고 포괄적인 인사이트 획득을 위해 사용자의 데이터와 결합할 수 있습니다. Amazon SageMaker로의 기본 통합을 통해 고객은 데이터 웨어하우스 내에서 SQL로 기계 학습 모델을 생성, 교육 및 구축할 수 있습니다. Amazon Redshift는 다른 클라우드 데이터 웨어하우스보다 최대 5배 뛰어난 가격 대비 성능으로 모든 SQL 분석 요구 사항을 충족합니다.\n",
            "\"category : Redshift, question : Amazon Redshift는 어떤 방식으로 데이터 웨어하우스 및 분석 관리를 단순화하나요?, answer : Amazon Redshift의 관리는 AWS에서 모두 이루어지기 때문에 사용자는 하드웨어 프로비저닝, 소프트웨어 패치 적용, 설정, 구성, 장애로부터 복구를 위한 노드 및 드라이브 모니터링, 백업 등의 데이터 웨어하우스 관리 작업에 대해 더 이상 걱정할 필요가 없습니다. AWS에서 고객을 대신해 데이터 웨어하우스를 설정하고 운영하고 확장하는 데 필요한 작업을 관리하므로 고객은 애플리케이션을 구축하는 데 집중할 수 있습니다. Amazon Redshift Serverless는 데이터 웨어하우스 용량을 자동으로 프로비저닝하고 확장하여 수요가 많고 예측할 수 없는 워크로드에 맞춰 높은 성능을 제공하며, 사용한 리소스에 대해서만 비용을 지불하면 됩니다. Amazon Redshift는 자동 튜닝 기능도 제공하며, Redshift Advisor에서 웨어하우스를 관리하기 위한 권장 사항도 표시합니다. Redshift Spectrum을 통해 Amazon Redshift는 Amazon S3에 저장된 데이터에 대한 쿼리와 관련된 모든 컴퓨팅 인프라, 로드 밸런싱, 계획, 일정 예약 및 실행을 관리합니다. Amazon Redshift는 Amazon Aurora Zero-ETL to Amazon Redshift와 같은 기능을 통한 데이터베이스 서비스와의 긴밀한 통합과 Amazon RDS, Amazon S3 데이터 레이크 등의 운영 데이터베이스에서 작업할 준비가 된 데이터에 액세스하기 위한 페더레이션 쿼리로 모든 데이터에 대한 분석을 지원합니다. Redshift는 스트리밍 데이터 또는 Amazon S3 파일을 자동으로 모으는 노코드 자동 데이터 파이프라인을 통해 간소화된 데이터 모으기를 지원합니다. 또한 Redshift는 AWS Data Exchange와 통합되어 사용자가 타사 데이터 세트를 찾고, 구독하고, 쿼리하고 포괄적인 인사이트 획득을 위해 사용자의 데이터와 결합할 수 있습니다. Amazon SageMaker로의 기본 통합을 통해 고객은 데이터 웨어하우스 내에서 SQL로 기계 학습 모델을 생성, 교육 및 구축할 수 있습니다. Amazon Redshift는 다른 클라우드 데이터 웨어하우스보다 최대 5배 뛰어난 가격 대비 성능으로 모든 SQL 분석 요구 사항을 충족합니다.\"\n",
            "Amazon Redshift의 배포 옵션은 무엇인가요?\n",
            "Amazon Redshift는 완전관리형 서비스로서 사전 프로비저닝 옵션과 서버리스 옵션을 모두 제공하므로 데이터 웨어하우스를 관리하지 않고도 더 효율적으로 분석을 실행하고 확장할 수 있습니다. 새로운 Amazon Redshift Serverless 엔드포인트를 가동하여 몇 초 만에 데이터 웨어하우스를 자동으로 프로비저닝하거나 예측 가능한 워크로드를 위해 프로비저닝된 옵션을 선택할 수 있습니다.\n",
            "\"category : Redshift, question : Amazon Redshift의 배포 옵션은 무엇인가요?, answer : Amazon Redshift는 완전관리형 서비스로서 사전 프로비저닝 옵션과 서버리스 옵션을 모두 제공하므로 데이터 웨어하우스를 관리하지 않고도 더 효율적으로 분석을 실행하고 확장할 수 있습니다. 새로운 Amazon Redshift Serverless 엔드포인트를 가동하여 몇 초 만에 데이터 웨어하우스를 자동으로 프로비저닝하거나 예측 가능한 워크로드를 위해 프로비저닝된 옵션을 선택할 수 있습니다.\"\n",
            "Amazon Redshift를 시작하려면 어떻게 해야 하나요?\n",
            "AWS Management Console에서 몇 단계만으로 데이터 쿼리를 시작할 수 있습니다. 사전 로드된 샘플 데이터 세트(벤치마크 데이터 세트 TPC-H, TPC-DS 등) 및 기타 샘플 쿼리를 활용해 분석을 즉시 시작할 수 있습니다. Amazon Redshift Serverless를 시작하려면 ‘Try Amazon Redshift Serverless(Amazon Redshift Serverless 사용해 보기)’를 선택하고 데이터 쿼리를 시작합니다. 여기에서 시작하세요.\n",
            "\"category : Redshift, question : Amazon Redshift를 시작하려면 어떻게 해야 하나요?, answer : AWS Management Console에서 몇 단계만으로 데이터 쿼리를 시작할 수 있습니다. 사전 로드된 샘플 데이터 세트(벤치마크 데이터 세트 TPC-H, TPC-DS 등) 및 기타 샘플 쿼리를 활용해 분석을 즉시 시작할 수 있습니다. Amazon Redshift Serverless를 시작하려면 ‘Try Amazon Redshift Serverless(Amazon Redshift Serverless 사용해 보기)’를 선택하고 데이터 쿼리를 시작합니다. 여기에서 시작하세요.\"\n",
            "다른 데이터 웨어하우스의 성능과 비교하여 Amazon Redshift의 성능은 어떤가요?\n",
            "TPC-DS 벤치마크 결과에 따르면 Amazon Redshift는 가장 우수한 가격 대비 성능을 제공합니다. 이는 비교적 작은 3TB의 데이터 세트에서도 마찬가지입니다. Amazon Redshift는 다른 클라우드 데이터 웨어하우스보다 최대 5배 더 뛰어난 가격 대비 성능을 제공합니다. 즉, 수동 튜닝 없이 Amazon Redshift가 제공하는 최고의 가격 대비 성능을 누릴 수 있습니다. 성능 플릿 원격 측정을 기반으로 대부분의 워크로드가 짧은 쿼리 워크로드(1초 이내에 실행되는 워크로드)라는 것도 알고 있습니다. 이러한 워크로드의 경우 최신 벤치마크를 통해 Amazon Redshift가 높은 동시성과 짧은 지연 시간 워크로드에서 다른 클라우드 데이터 웨어하우스보다 최대 7배 더 뛰어난 가격 성능을 제공한다는 것을 알 수 있습니다. 여기에서 자세히 알아보세요.\n",
            "\"category : Redshift, question : 다른 데이터 웨어하우스의 성능과 비교하여 Amazon Redshift의 성능은 어떤가요?, answer : TPC-DS 벤치마크 결과에 따르면 Amazon Redshift는 가장 우수한 가격 대비 성능을 제공합니다. 이는 비교적 작은 3TB의 데이터 세트에서도 마찬가지입니다. Amazon Redshift는 다른 클라우드 데이터 웨어하우스보다 최대 5배 더 뛰어난 가격 대비 성능을 제공합니다. 즉, 수동 튜닝 없이 Amazon Redshift가 제공하는 최고의 가격 대비 성능을 누릴 수 있습니다. 성능 플릿 원격 측정을 기반으로 대부분의 워크로드가 짧은 쿼리 워크로드(1초 이내에 실행되는 워크로드)라는 것도 알고 있습니다. 이러한 워크로드의 경우 최신 벤치마크를 통해 Amazon Redshift가 높은 동시성과 짧은 지연 시간 워크로드에서 다른 클라우드 데이터 웨어하우스보다 최대 7배 더 뛰어난 가격 성능을 제공한다는 것을 알 수 있습니다. 여기에서 자세히 알아보세요.\"\n",
            "Amazon Redshift에 대해 자세히 알아보고 온보딩하는 데 도움을 받을 수 있나요?\n",
            "예. Amazon Redshift 전문가로부터 질문에 대한 답변을 듣고 지원을 받을 수 있습니다. AWS에 문의하면 영업일 기준 1일 이내에 회신하여 AWS가 귀사에 어떤 도움이 될 수 있는지 설명해 드립니다.\n",
            "\"category : Redshift, question : Amazon Redshift에 대해 자세히 알아보고 온보딩하는 데 도움을 받을 수 있나요?, answer : 예. Amazon Redshift 전문가로부터 질문에 대한 답변을 듣고 지원을 받을 수 있습니다. AWS에 문의하면 영업일 기준 1일 이내에 회신하여 AWS가 귀사에 어떤 도움이 될 수 있는지 설명해 드립니다.\"\n",
            "Amazon Redshift 관리형 스토리지는 무엇인가요?\n",
            "Amazon Redshift 관리형 스토리지는 서버리스 및 RA3 노드 유형과 함께 사용할 수 있으며, 이를 사용하면 컴퓨팅과 스토리지를 개별적으로 크기 조정하고 비용을 지불할 수 있으므로 컴퓨팅 요구 사항만을 기준으로 클러스터의 크기를 조정할 수 있습니다. 이 스토리지는 고성능 SSD 기반 로컬 스토리지를 자동으로 Tier-1 캐시로 사용하고, 데이터 블록 온도, 데이터 블록 에이지 및 워크로드 패턴과 같은 최적화 기술을 활용하여 고성능을 제공하면서, 특별한 조치를 요하지 않고 필요할 때 자동으로 스토리지를 Amazon S3로 확장합니다.\n",
            "\"category : Redshift, question : Amazon Redshift 관리형 스토리지는 무엇인가요?, answer : Amazon Redshift 관리형 스토리지는 서버리스 및 RA3 노드 유형과 함께 사용할 수 있으며, 이를 사용하면 컴퓨팅과 스토리지를 개별적으로 크기 조정하고 비용을 지불할 수 있으므로 컴퓨팅 요구 사항만을 기준으로 클러스터의 크기를 조정할 수 있습니다. 이 스토리지는 고성능 SSD 기반 로컬 스토리지를 자동으로 Tier-1 캐시로 사용하고, 데이터 블록 온도, 데이터 블록 에이지 및 워크로드 패턴과 같은 최적화 기술을 활용하여 고성능을 제공하면서, 특별한 조치를 요하지 않고 필요할 때 자동으로 스토리지를 Amazon S3로 확장합니다.\"\n",
            "Amazon Redshift의 관리형 스토리지를 사용하려면 어떻게 해야 하나요?\n",
            "Amazon Redshift Dense Storage 또는 Dense Compute 노드를 이미 사용하고 있다면 탄력적 크기 조정을 사용해 기존 클러스터를 새로운 컴퓨팅 인스턴스인 RA3로 업그레이드할 수 있습니다. Amazon Redshift Serverless와 RA3 인스턴스 사용 클러스터는 자동으로 Redshift 관리형 스토리지를 사용해 데이터를 저장합니다. 이 기능을 사용하는 데 있어, Amazon Redshift Serverless 또는 RA3 인스턴스를 사용하는 것 외에 다른 작업은 필요하지 않습니다.\n",
            "\"category : Redshift, question : Amazon Redshift의 관리형 스토리지를 사용하려면 어떻게 해야 하나요?, answer : Amazon Redshift Dense Storage 또는 Dense Compute 노드를 이미 사용하고 있다면 탄력적 크기 조정을 사용해 기존 클러스터를 새로운 컴퓨팅 인스턴스인 RA3로 업그레이드할 수 있습니다. Amazon Redshift Serverless와 RA3 인스턴스 사용 클러스터는 자동으로 Redshift 관리형 스토리지를 사용해 데이터를 저장합니다. 이 기능을 사용하는 데 있어, Amazon Redshift Serverless 또는 RA3 인스턴스를 사용하는 것 외에 다른 작업은 필요하지 않습니다.\"\n",
            "AWS 데이터 레이크에 저장된 데이터에 대해 Redshift에서 쿼리를 실행하려면 어떻게 해야 하나요?\n",
            "Amazon Redshift Spectrum은 데이터 로딩이나 ETL 없이도 Amazon S3의 데이터 레이크에 대해 쿼리를 실행할 수 있게 하는 Amazon Redshift의 기능입니다. SQL 쿼리를 발행하면, 쿼리가 Amazon Redshift 엔드포인트로 전달되고 여기에서 쿼리 플랜을 생성하고 최적화합니다. Amazon Redshift는 로컬에 있는 데이터와 Amazon S3에 있는 데이터가 무엇인지 파악하고, 읽어와야 하는 S3 데이터 양을 최소화하기 위한 플랜을 생성하고, 공유 리소스 풀의 Amazon Redshift Spectrum 작업자에게 Amazon S3에서 데이터를 읽고 처리하도록 요청합니다.\n",
            "\"category : Redshift, question : AWS 데이터 레이크에 저장된 데이터에 대해 Redshift에서 쿼리를 실행하려면 어떻게 해야 하나요?, answer : Amazon Redshift Spectrum은 데이터 로딩이나 ETL 없이도 Amazon S3의 데이터 레이크에 대해 쿼리를 실행할 수 있게 하는 Amazon Redshift의 기능입니다. SQL 쿼리를 발행하면, 쿼리가 Amazon Redshift 엔드포인트로 전달되고 여기에서 쿼리 플랜을 생성하고 최적화합니다. Amazon Redshift는 로컬에 있는 데이터와 Amazon S3에 있는 데이터가 무엇인지 파악하고, 읽어와야 하는 S3 데이터 양을 최소화하기 위한 플랜을 생성하고, 공유 리소스 풀의 Amazon Redshift Spectrum 작업자에게 Amazon S3에서 데이터를 읽고 처리하도록 요청합니다.\"\n",
            "RA3 인스턴스 사용을 고려해야 하는 경우는 언제인가요?\n",
            "다음과 같은 경우에 RA3 노드 유형의 선택을 고려할 수 있습니다.\n",
            "\n",
            "스토리지와 별도로 컴퓨팅 크기를 조정하여 컴퓨팅 비용을 결제할 수 있는 유연성이 필요한 경우\n",
            "전체 데이터 중 일부를 쿼리하는 경우\n",
            "데이터 볼륨이 급격히 증가하거나 급격히 증가할 것으로 예상되는 경우\n",
            "성능 요구 사항만을 기준으로 클러스터의 크기를 결정할 수 있는 유연성을 원하는 경우\n",
            "\n",
            "데이터의 규모가 계속 증가해 페타바이트 수준에 이르는 과정에서 Amazon Redshift 데이터 웨어하우스에 모으는 데이터의 양도 증가합니다. 그러면 모든 데이터를 비용 효율적으로 분석할 수 있는 방법을 찾아야 할 수 있습니다.\n",
            "관리형 스토리지를 사용하는 새로운 Amazon Redshift RA3 인스턴스를 사용하면 성능 요구 사항에 따라 노드의 수를 선택할 수 있으며 사용한 관리형 스토리지의 비용만 지불하면 됩니다. 이는 스토리지 비용 증가 없이 날마다 처리하는 데이터 양에 따라 RA3 클러스터의 크기를 조정할 수 있는 유연성을 제공합니다. AWS Nitro System을 기반으로 관리형 스토리지가 포함된 RA3 인스턴스는 핫 데이터에 고성능 SSD를 사용하고 콜드 데이터에는 Amazon S3를 사용하여 스토리지의 경제성과 편의성을 높이고 쿼리 성능을 개선합니다.\n",
            "\"category : Redshift, question : RA3 인스턴스 사용을 고려해야 하는 경우는 언제인가요?, answer : 다음과 같은 경우에 RA3 노드 유형의 선택을 고려할 수 있습니다.\n",
            "\n",
            "스토리지와 별도로 컴퓨팅 크기를 조정하여 컴퓨팅 비용을 결제할 수 있는 유연성이 필요한 경우\n",
            "전체 데이터 중 일부를 쿼리하는 경우\n",
            "데이터 볼륨이 급격히 증가하거나 급격히 증가할 것으로 예상되는 경우\n",
            "성능 요구 사항만을 기준으로 클러스터의 크기를 결정할 수 있는 유연성을 원하는 경우\n",
            "\n",
            "데이터의 규모가 계속 증가해 페타바이트 수준에 이르는 과정에서 Amazon Redshift 데이터 웨어하우스에 모으는 데이터의 양도 증가합니다. 그러면 모든 데이터를 비용 효율적으로 분석할 수 있는 방법을 찾아야 할 수 있습니다.\n",
            "관리형 스토리지를 사용하는 새로운 Amazon Redshift RA3 인스턴스를 사용하면 성능 요구 사항에 따라 노드의 수를 선택할 수 있으며 사용한 관리형 스토리지의 비용만 지불하면 됩니다. 이는 스토리지 비용 증가 없이 날마다 처리하는 데이터 양에 따라 RA3 클러스터의 크기를 조정할 수 있는 유연성을 제공합니다. AWS Nitro System을 기반으로 관리형 스토리지가 포함된 RA3 인스턴스는 핫 데이터에 고성능 SSD를 사용하고 콜드 데이터에는 Amazon S3를 사용하여 스토리지의 경제성과 편의성을 높이고 쿼리 성능을 개선합니다.\"\n",
            "위치 기반 분석에 사용할 수 있는 기능은 무엇인가요?\n",
            "Amazon Redshift Spatial은 데이터에 대한 풍부한 인사이트를 위해 위치 기반 분석을 제공합니다. 이 기능은 의사 결정을 위한 분석을 제공하기 위해 공간 및 비즈니스 데이터를 매끄럽게 통합합니다. Amazon Redshift는 2019년 11월에 다형 데이터 형식인 GEOMETRY와 여러 가지 핵심적인 SQL 공간 함수를 포함하여 네이티브 공간 데이터 처리 지원 기능을 출시했습니다. AWS는 이제 GEOGRAPHY 데이터 형식을 지원하며, SQL 공간 함수 라이브러리는 80개로 증가했습니다. AWS는 형상 파일, GeoJSON, WKT, WKB, eWKT, eWKB를 비롯한 일반적인 공간 데이터 형식 및 표준을 지원합니다. 자세히 알아보려면 설명서 페이지나 Amazon Redshift Spatial 자습서 페이지를 방문하세요.\n",
            "\"category : Redshift, question : 위치 기반 분석에 사용할 수 있는 기능은 무엇인가요?, answer : Amazon Redshift Spatial은 데이터에 대한 풍부한 인사이트를 위해 위치 기반 분석을 제공합니다. 이 기능은 의사 결정을 위한 분석을 제공하기 위해 공간 및 비즈니스 데이터를 매끄럽게 통합합니다. Amazon Redshift는 2019년 11월에 다형 데이터 형식인 GEOMETRY와 여러 가지 핵심적인 SQL 공간 함수를 포함하여 네이티브 공간 데이터 처리 지원 기능을 출시했습니다. AWS는 이제 GEOGRAPHY 데이터 형식을 지원하며, SQL 공간 함수 라이브러리는 80개로 증가했습니다. AWS는 형상 파일, GeoJSON, WKT, WKB, eWKT, eWKB를 비롯한 일반적인 공간 데이터 형식 및 표준을 지원합니다. 자세히 알아보려면 설명서 페이지나 Amazon Redshift Spatial 자습서 페이지를 방문하세요.\"\n",
            "Redshift의 SQL 지원과 비교하여 Athena의 SQL 지원은 어떤가요? 그리고 두 서비스 중에서 어떻게 선택하나요?\n",
            "Amazon Athena와 Amazon Redshift Serverless는 모두 서버리스 서비스이고 SQL 사용자를 지원하지만 해결할 수 있는 요구 사항과 사용 사례가 서로 다릅니다.\n",
            "스토리지와 컴퓨팅 및 기계 학습 주도 자동 최적화 기능을 분리하는 MPP(Massively Parallel Processing) 아키텍처를 가진 Amazon Redshift(서버리스 또는 프로비저닝됨)와 같은 데이터 웨어하우스는 복잡한 BI 및 분석 워크로드를 위해 모든 규모에서 최고의 가격 대비 성능을 필요로 하는 고객에게 탁월한 선택입니다. 고객은 ZeroETL 및 노코드 방법을 사용한 고성능 분석을 위해 작업할 준비가 된 데이터에 액세스하거나 데이터를 웨어하우스로 쉽게 모으거나 이동하는 데 사용할 수 있는 심층 통합을 통해 Amazon Redshift를 데이터 아키텍처의 중앙 구성 요소로 사용할 수 있습니다. 고객은 AWS Data Exchange와의 통합을 통해 Amazon S3에 저장된 데이터, Aurora 및 Amazon RDS와 같은 운영 데이터베이스, 타사 데이터 웨어하우스에 액세스하고 분석을 위해 Amazon Redshift 데이터 웨어하우스에 저장된 데이터와 결합할 수 있습니다. 고객은 데이터 웨어하우징을 쉽게 시작하고 이 모든 데이터를 기반으로 기계 학습을 수행할 수 있습니다.\n",
            "Amazon Athena는 데이터 모으기나 처리에 대한 걱정 없이 확장 가능한 커넥터 프레임워크(애플리케이션 및 온프레미스 또는 기타 클라우드 분석 시스템을 위한 30개 이상의 기본 제공 커넥터 포함)를 통해 데이터 레이크 또는 모든 데이터 소스에 있는 데이터의 대화형 분석 및 데이터 탐색에 매우 적합합니다. Amazon Athena는 Spark, Presto 및 Apache Iceberg와 같은 오픈 소스 엔진 및 프레임워크를 기반으로 구축되어 고객에게 Python 또는 SQL을 사용하거나 개방형 데이터 형식으로 작업할 수 있는 유연성을 제공합니다. 고객이 오픈 소스 프레임워크와 데이터 형식을 사용하여 대화형 분석을 수행하려는 경우 Amazon Athena에서 시작하는 것이 좋습니다.\n",
            "\"category : Redshift, question : Redshift의 SQL 지원과 비교하여 Athena의 SQL 지원은 어떤가요? 그리고 두 서비스 중에서 어떻게 선택하나요?, answer : Amazon Athena와 Amazon Redshift Serverless는 모두 서버리스 서비스이고 SQL 사용자를 지원하지만 해결할 수 있는 요구 사항과 사용 사례가 서로 다릅니다.\n",
            "스토리지와 컴퓨팅 및 기계 학습 주도 자동 최적화 기능을 분리하는 MPP(Massively Parallel Processing) 아키텍처를 가진 Amazon Redshift(서버리스 또는 프로비저닝됨)와 같은 데이터 웨어하우스는 복잡한 BI 및 분석 워크로드를 위해 모든 규모에서 최고의 가격 대비 성능을 필요로 하는 고객에게 탁월한 선택입니다. 고객은 ZeroETL 및 노코드 방법을 사용한 고성능 분석을 위해 작업할 준비가 된 데이터에 액세스하거나 데이터를 웨어하우스로 쉽게 모으거나 이동하는 데 사용할 수 있는 심층 통합을 통해 Amazon Redshift를 데이터 아키텍처의 중앙 구성 요소로 사용할 수 있습니다. 고객은 AWS Data Exchange와의 통합을 통해 Amazon S3에 저장된 데이터, Aurora 및 Amazon RDS와 같은 운영 데이터베이스, 타사 데이터 웨어하우스에 액세스하고 분석을 위해 Amazon Redshift 데이터 웨어하우스에 저장된 데이터와 결합할 수 있습니다. 고객은 데이터 웨어하우징을 쉽게 시작하고 이 모든 데이터를 기반으로 기계 학습을 수행할 수 있습니다.\n",
            "Amazon Athena는 데이터 모으기나 처리에 대한 걱정 없이 확장 가능한 커넥터 프레임워크(애플리케이션 및 온프레미스 또는 기타 클라우드 분석 시스템을 위한 30개 이상의 기본 제공 커넥터 포함)를 통해 데이터 레이크 또는 모든 데이터 소스에 있는 데이터의 대화형 분석 및 데이터 탐색에 매우 적합합니다. Amazon Athena는 Spark, Presto 및 Apache Iceberg와 같은 오픈 소스 엔진 및 프레임워크를 기반으로 구축되어 고객에게 Python 또는 SQL을 사용하거나 개방형 데이터 형식으로 작업할 수 있는 유연성을 제공합니다. 고객이 오픈 소스 프레임워크와 데이터 형식을 사용하여 대화형 분석을 수행하려는 경우 Amazon Athena에서 시작하는 것이 좋습니다.\"\n",
            "Amazon Redshift Serverless는 무엇인가요?\n",
            "Amazon Redshift Serverless는 Amazon Redshift의 서버리스 옵션으로, 데이터 웨어하우스 인프라를 설정하고 관리할 필요 없이 몇 초 안에 분석을 더 효율적으로 실행하고 확장할 수 있습니다. Redshift Serverless를 사용하면 데이터 분석가, 개발자, 비즈니스 전문가, 데이터 사이언티스트를 비롯한 모든 사용자가 데이터 웨어하우스에서 데이터를 단순히 로드하고 쿼리하는 방법으로 데이터에서 인사이트를 얻을 수 있습니다.\n",
            "\"category : Redshift, question : Amazon Redshift Serverless는 무엇인가요?, answer : Amazon Redshift Serverless는 Amazon Redshift의 서버리스 옵션으로, 데이터 웨어하우스 인프라를 설정하고 관리할 필요 없이 몇 초 안에 분석을 더 효율적으로 실행하고 확장할 수 있습니다. Redshift Serverless를 사용하면 데이터 분석가, 개발자, 비즈니스 전문가, 데이터 사이언티스트를 비롯한 모든 사용자가 데이터 웨어하우스에서 데이터를 단순히 로드하고 쿼리하는 방법으로 데이터에서 인사이트를 얻을 수 있습니다.\"\n",
            "Amazon Redshift Serverless를 시작하려면 어떻게 해야 하나요?\n",
            "AWS Management Console에서 몇 단계만으로 ‘configure Amazon Redshift Serverless(Amazon Redshift Serverless 구성)’를 선택해 데이터 쿼리를 시작할 수 있습니다. 샘플 쿼리와 함께 데이터, 센서스(census) 데이터, 벤치마크 데이터 세트와 같은 사전 로드된 샘플 데이터 세트를 활용하면 분석을 바로 시작할 수 있습니다. 데이터베이스, 스키마, 테이블을 생성하고 Amazon S3로부터 데이터를 로드하거나, Amazon Redshift 데이터 공유를 통해 데이터에 액세스하거나, Redshift가 프로비저닝한 기존 클러스터 스냅샷으로부터 복원할 수 있습니다. 또한 Amazon S3 데이터 레이크에서 Parquet나 ORC와 같은 개방형 형식의 데이터를 직접 쿼리하거나 Amazon Aurora, Amazon RDS PostgreSQL, MySQL과 같은 운영 데이터베이스에서 데이터를 쿼리할 수도 있습니다. 시작 안내서를 참조하세요.\n",
            "\"category : Redshift, question : Amazon Redshift Serverless를 시작하려면 어떻게 해야 하나요?, answer : AWS Management Console에서 몇 단계만으로 ‘configure Amazon Redshift Serverless(Amazon Redshift Serverless 구성)’를 선택해 데이터 쿼리를 시작할 수 있습니다. 샘플 쿼리와 함께 데이터, 센서스(census) 데이터, 벤치마크 데이터 세트와 같은 사전 로드된 샘플 데이터 세트를 활용하면 분석을 바로 시작할 수 있습니다. 데이터베이스, 스키마, 테이블을 생성하고 Amazon S3로부터 데이터를 로드하거나, Amazon Redshift 데이터 공유를 통해 데이터에 액세스하거나, Redshift가 프로비저닝한 기존 클러스터 스냅샷으로부터 복원할 수 있습니다. 또한 Amazon S3 데이터 레이크에서 Parquet나 ORC와 같은 개방형 형식의 데이터를 직접 쿼리하거나 Amazon Aurora, Amazon RDS PostgreSQL, MySQL과 같은 운영 데이터베이스에서 데이터를 쿼리할 수도 있습니다. 시작 안내서를 참조하세요.\"\n",
            "Amazon Redshift Serverless 사용 시 어떤 이점이 있나요?\n",
            "데이터 웨어하우스 관리 경험이 없어도 클러스터를 설정, 구성, 관리하거나 웨어하우스를 튜닝하는 데 대해 걱정할 필요가 없습니다. 데이터에서 의미 있는 인사이트를 도출하거나 데이터를 통해 핵심적인 비즈니스 결과를 제공하는 데 집중할 수 있습니다. 비용을 관리 가능하게 유지하면서 사용한 부분에 대해서만 지불하면 됩니다. 사용자는 Amazon Redshift의 최고의 성능, 다양한 SQL 기능, 데이터 레이크 및 운영 데이터 웨어하우스와의 긴밀한 통합, 내장된 예측 분석 및 데이터 공유 기능에서 이점을 얻을 수 있습니다. 데이터 웨어하우스의 미세 제어가 필요하다면 Redshift 클러스터를 프로비저닝할 수 있습니다.\n",
            "\"category : Redshift, question : Amazon Redshift Serverless 사용 시 어떤 이점이 있나요?, answer : 데이터 웨어하우스 관리 경험이 없어도 클러스터를 설정, 구성, 관리하거나 웨어하우스를 튜닝하는 데 대해 걱정할 필요가 없습니다. 데이터에서 의미 있는 인사이트를 도출하거나 데이터를 통해 핵심적인 비즈니스 결과를 제공하는 데 집중할 수 있습니다. 비용을 관리 가능하게 유지하면서 사용한 부분에 대해서만 지불하면 됩니다. 사용자는 Amazon Redshift의 최고의 성능, 다양한 SQL 기능, 데이터 레이크 및 운영 데이터 웨어하우스와의 긴밀한 통합, 내장된 예측 분석 및 데이터 공유 기능에서 이점을 얻을 수 있습니다. 데이터 웨어하우스의 미세 제어가 필요하다면 Redshift 클러스터를 프로비저닝할 수 있습니다.\"\n",
            "Amazon Redshift Serverless은 다른 AWS 서비스와 어떻게 작동하나요?\n",
            "복잡한 조인, Amazon S3 데이터 레이크 및 운영 데이터베이스의 데이터에 대한 직접 쿼리, 구체화된 뷰, 저장 프로시저, 반정형 데이터 지원, ML과 같은 Amazon Redshift의 다양한 분석 기능과 전체적인 고성능을 계속해서 이용할 수 있습니다. Amazon Redshift와 통합된 모든 관련 서비스(Amazon Kinesis, AWS Lambda, Amazon QuickSight, Amazon SageMaker, Amazon EMR, AWS Lake formation, AWS Glue 등)는 Amazon Redshift Serverless에서도 계속 작동합니다.\n",
            "\"category : Redshift, question : Amazon Redshift Serverless은 다른 AWS 서비스와 어떻게 작동하나요?, answer : 복잡한 조인, Amazon S3 데이터 레이크 및 운영 데이터베이스의 데이터에 대한 직접 쿼리, 구체화된 뷰, 저장 프로시저, 반정형 데이터 지원, ML과 같은 Amazon Redshift의 다양한 분석 기능과 전체적인 고성능을 계속해서 이용할 수 있습니다. Amazon Redshift와 통합된 모든 관련 서비스(Amazon Kinesis, AWS Lambda, Amazon QuickSight, Amazon SageMaker, Amazon EMR, AWS Lake formation, AWS Glue 등)는 Amazon Redshift Serverless에서도 계속 작동합니다.\"\n",
            "Amazon Redshift Serverless로 처리할 수 있는 사용 사례에는 어떤 것이 있나요?\n",
            "모든 분석 사용 사례를 변함없이 실행할 수 있습니다. 간단한 시작 워크플로, 자동 확장, 사용량에 따른 비용 지불을 제공하는 Amazon Redshift Serverless는 계속 변화하고 예측할 수 없는 컴퓨팅 수요를 가진 임시적 비즈니스 분석 워크로드와 간헐적이거나 산발적인 워크로드를 빠르게 시작할 필요가 있는 개발 및 테스트 환경을 실행하는 데 있어 이전보다 더욱 효율적이고 저렴해졌습니다.\n",
            "\"category : Redshift, question : Amazon Redshift Serverless로 처리할 수 있는 사용 사례에는 어떤 것이 있나요?, answer : 모든 분석 사용 사례를 변함없이 실행할 수 있습니다. 간단한 시작 워크플로, 자동 확장, 사용량에 따른 비용 지불을 제공하는 Amazon Redshift Serverless는 계속 변화하고 예측할 수 없는 컴퓨팅 수요를 가진 임시적 비즈니스 분석 워크로드와 간헐적이거나 산발적인 워크로드를 빠르게 시작할 필요가 있는 개발 및 테스트 환경을 실행하는 데 있어 이전보다 더욱 효율적이고 저렴해졌습니다.\"\n",
            "내 Amazon Redshift 데이터 웨어하우스로 데이터를 로드하려면 어떻게 해야 하나요?\n",
            "Amazon S3, Amazon RDS, Amazon DynamoDB, Amazon EMR, AWS Glue, AWS Data Pipeline 및 Amazon EC2 또는 온프레미스의 모든 SSH 지원 호스트를 비롯하여 다양한 데이터 소스에서 Amazon Redshift로 데이터를 로드할 수 있습니다. Amazon Redshift는 데이터 웨어하우스 클러스터에 데이터를 모을 수 있는 속도를 최대화하기 위해 각 컴퓨팅 노드에 데이터를 병렬로 로드합니다. 클라이언트는 ODBC 또는 JDBC를 사용하여 Amazon Redshift에 연결한 다음 'Insert' SQL 문을 사용하여 데이터를 삽입할 수 있습니다. 그러한 방법은 SQL Insert 문이 단일 리더 노드를 통해 로드하는 반면 각 컴퓨팅 노드에 데이터를 병렬로 로드하기 때문에 S3 또는 DynamoDB를 사용하는 것보다 더 느리게 처리될 수 있습니다. Amazon Redshift에 데이터를 로드하는 방법에 대한 자세한 내용은 시작 안내서를 참조하세요.\n",
            "\"category : Redshift, question : 내 Amazon Redshift 데이터 웨어하우스로 데이터를 로드하려면 어떻게 해야 하나요?, answer : Amazon S3, Amazon RDS, Amazon DynamoDB, Amazon EMR, AWS Glue, AWS Data Pipeline 및 Amazon EC2 또는 온프레미스의 모든 SSH 지원 호스트를 비롯하여 다양한 데이터 소스에서 Amazon Redshift로 데이터를 로드할 수 있습니다. Amazon Redshift는 데이터 웨어하우스 클러스터에 데이터를 모을 수 있는 속도를 최대화하기 위해 각 컴퓨팅 노드에 데이터를 병렬로 로드합니다. 클라이언트는 ODBC 또는 JDBC를 사용하여 Amazon Redshift에 연결한 다음 'Insert' SQL 문을 사용하여 데이터를 삽입할 수 있습니다. 그러한 방법은 SQL Insert 문이 단일 리더 노드를 통해 로드하는 반면 각 컴퓨팅 노드에 데이터를 병렬로 로드하기 때문에 S3 또는 DynamoDB를 사용하는 것보다 더 느리게 처리될 수 있습니다. Amazon Redshift에 데이터를 로드하는 방법에 대한 자세한 내용은 시작 안내서를 참조하세요.\"\n",
            "Redshift 자동 복사는 복사 명령과 어떻게 다른가요?\n",
            "Redshift 자동 복사는 고객 개입 없이 Amazon S3 폴더를 추적하고 새 파일을 모아서 복사 문을 자동화하는 기능을 제공합니다. 자동 복사를 사용하지 않으면 복사 문이 기존 파일에 대한 파일 모으기 프로세스를 즉시 시작합니다. 자동 복사는 기존 복사 명령을 확장하며 1/ 새 파일에 대해 지정된 Amazon S3 경로를 모니터링하여 파일 모으기 프로세스를 자동화하고, 2/ 복사 구성을 재사용하여 반복적인 모으기 작업을 위해 새 복사 문을 만들고 실행할 필요성을 줄이고, 3/ 데이터 중복을 피하기 위해 로드된 파일을 추적하는 기능을 제공합니다.\n",
            "\"category : Redshift, question : Redshift 자동 복사는 복사 명령과 어떻게 다른가요?, answer : Redshift 자동 복사는 고객 개입 없이 Amazon S3 폴더를 추적하고 새 파일을 모아서 복사 문을 자동화하는 기능을 제공합니다. 자동 복사를 사용하지 않으면 복사 문이 기존 파일에 대한 파일 모으기 프로세스를 즉시 시작합니다. 자동 복사는 기존 복사 명령을 확장하며 1/ 새 파일에 대해 지정된 Amazon S3 경로를 모니터링하여 파일 모으기 프로세스를 자동화하고, 2/ 복사 구성을 재사용하여 반복적인 모으기 작업을 위해 새 복사 문을 만들고 실행할 필요성을 줄이고, 3/ 데이터 중복을 피하기 위해 로드된 파일을 추적하는 기능을 제공합니다.\"\n",
            "Redshift 자동 복사를 시작하려면 어떻게 해야 하나요?\n",
            "시작하려면 고객은 연결된 IAM 역할을 사용하여 Redshift 클러스터/서버리스 엔드포인트에서 액세스할 수 있는 Amazon S3 폴더를 갖고 있고 대상으로 사용할 Redshift 테이블을 생성해야 합니다. Amazon S3 경로와 Redshift 테이블이 준비되면 고객은 복사 명령을 사용하여 복사 작업을 생성할 수 있습니다. 복사 작업이 생성되면 Redshift는 백그라운드에서 지정된 Amazon S3 경로를 추적하기 시작하고 사용자 정의 복사 문을 시작하여 새 파일을 대상 테이블에 자동으로 복사합니다.\n",
            "\"category : Redshift, question : Redshift 자동 복사를 시작하려면 어떻게 해야 하나요?, answer : 시작하려면 고객은 연결된 IAM 역할을 사용하여 Redshift 클러스터/서버리스 엔드포인트에서 액세스할 수 있는 Amazon S3 폴더를 갖고 있고 대상으로 사용할 Redshift 테이블을 생성해야 합니다. Amazon S3 경로와 Redshift 테이블이 준비되면 고객은 복사 명령을 사용하여 복사 작업을 생성할 수 있습니다. 복사 작업이 생성되면 Redshift는 백그라운드에서 지정된 Amazon S3 경로를 추적하기 시작하고 사용자 정의 복사 문을 시작하여 새 파일을 대상 테이블에 자동으로 복사합니다.\"\n",
            "Amazon Redshift Integration for Apache Spark의 사용 사례는 무엇인가요?\n",
            "주요 사용 사례로 1/ Amazon EMR 및 AWS Glue를 사용하여 데이터 모으기 및 변환 파이프라인(배치 및 스트리밍)의 일부로 Amazon Redshift에 데이터를 액세스하고 로드하는 Apache Spark 작업을 실행하는 고객, 2/ Amazon SageMaker를 사용하여 Apache Spark로 기계 학습을 수행하고 특성 추출 및 변환을 위해 Amazon Redshift에 저장된 데이터에 액세스해야 하는 고객, 3/ Apache Spark를 사용하여 Amazon Redshift의 데이터에 대한 대화형 분석을 수행하는 Amazon Athena 고객이 있습니다.\n",
            "\"category : Redshift, question : Amazon Redshift Integration for Apache Spark의 사용 사례는 무엇인가요?, answer : 주요 사용 사례로 1/ Amazon EMR 및 AWS Glue를 사용하여 데이터 모으기 및 변환 파이프라인(배치 및 스트리밍)의 일부로 Amazon Redshift에 데이터를 액세스하고 로드하는 Apache Spark 작업을 실행하는 고객, 2/ Amazon SageMaker를 사용하여 Apache Spark로 기계 학습을 수행하고 특성 추출 및 변환을 위해 Amazon Redshift에 저장된 데이터에 액세스해야 하는 고객, 3/ Apache Spark를 사용하여 Amazon Redshift의 데이터에 대한 대화형 분석을 수행하는 Amazon Athena 고객이 있습니다.\"\n",
            "Amazon Redshift Integration for Apache Spark의 이점은 무엇인가요?\n",
            "Baikal은 다음과 같은 이점을 제공합니다.\n",
            "\n",
            "인증되지 않은 Apache Spark 버전을 설정하고 유지 관리하는 수작업 단계에 대해 걱정할 필요 없이 Amazon Redshift의 데이터에 대해 Spark 애플리케이션을 시작하고 실행할 수 있어 사용이 간편합니다.\n",
            "Amazon EMR, AWS Glue, Amazon Athena, Amazon SageMaker와 같은 다양한 AWS 서비스에서 Amazon Redshift와 함께 Apache Spark를 최소한의 구성으로 편리하게 사용할 수 있습니다.\n",
            "Amazon Redshift에서 Apache Spark 애플리케이션을 실행하는 동안 향상된 성능 제공합니다.\n",
            "\"category : Redshift, question : Amazon Redshift Integration for Apache Spark의 이점은 무엇인가요?, answer : Baikal은 다음과 같은 이점을 제공합니다.\n",
            "\n",
            "인증되지 않은 Apache Spark 버전을 설정하고 유지 관리하는 수작업 단계에 대해 걱정할 필요 없이 Amazon Redshift의 데이터에 대해 Spark 애플리케이션을 시작하고 실행할 수 있어 사용이 간편합니다.\n",
            "Amazon EMR, AWS Glue, Amazon Athena, Amazon SageMaker와 같은 다양한 AWS 서비스에서 Amazon Redshift와 함께 Apache Spark를 최소한의 구성으로 편리하게 사용할 수 있습니다.\n",
            "Amazon Redshift에서 Apache Spark 애플리케이션을 실행하는 동안 향상된 성능 제공합니다.\"\n",
            "언제 Federated Querying 대신 Amazon Aurora Zero-ETL to Amazon Redshift를 사용해야 하나요?\n",
            "Amazon Aurora Zero-ETL to Amazon Redshift는 작성 후 몇 초 만에 Amazon Aurora의 트랜잭션 데이터를 Amazon Redshift에서 사용할 수 있게 완전관리형 솔루션을 제공하므로 Amazon Aurora 및 Amazon Redshift 고객은 페타바이트 규모의 트랜잭션 데이터에 대해 실시간에 가까운 분석 및 기계 학습을 실행할 수 있습니다. Amazon Aurora Zero-ETL to Amazon Redshift를 사용하면 고객은 Amazon Redshift로 분석하려는 데이터가 포함된 Amazon Aurora 테이블을 선택하기만 하면 됩니다. 이 기능은 스키마와 데이터를 Amazon Redshift로 원활하게 복제합니다. 이를 통해 복잡한 데이터 파이프라인을 구축하고 관리할 필요가 줄어들기 때문에 고객이 애플리케이션 개선에 집중할 수 있습니다. Amazon Aurora Zero-ETL to Amazon Redshift를 통해 고객은 여러 Amazon Aurora 데이터베이스 클러스터의 데이터를 동일한 Amazon Redshift 인스턴스로 복제하여 여러 애플리케이션에서 포괄적인 인사이트를 획득하는 동시에 핵심 분석 자산을 통합하여 상당한 비용 절감 효과를 얻고 운영 효율성을 높일 수 있습니다. Amazon Aurora Zero-ETL to Amazon Redshift를 통해 고객은 구체화된 뷰, 데이터 공유, 여러 데이터 스토어 및 데이터 레이크에 대한 연동 액세스와 같은 Amazon Redshift의 핵심 분석 및 기계 학습 기능에 액세스할 수도 있습니다. 이를 통해 고객은 실시간에 가까운 분석과 핵심 분석을 결합하여 비즈니스 의사 결정을 알리는 시간에 민감한 인사이트를 효과적으로 도출할 수 있습니다. 또한 고객은 트랜잭션에 Amazon Aurora를 사용하고 분석에 Amazon Redshift를 사용하여 공유 컴퓨팅 리소스가 없으므로 성능이 뛰어나고 운영상 안정적인 솔루션을 제공합니다.\n",
            "\"category : Redshift, question : 언제 Federated Querying 대신 Amazon Aurora Zero-ETL to Amazon Redshift를 사용해야 하나요?, answer : Amazon Aurora Zero-ETL to Amazon Redshift는 작성 후 몇 초 만에 Amazon Aurora의 트랜잭션 데이터를 Amazon Redshift에서 사용할 수 있게 완전관리형 솔루션을 제공하므로 Amazon Aurora 및 Amazon Redshift 고객은 페타바이트 규모의 트랜잭션 데이터에 대해 실시간에 가까운 분석 및 기계 학습을 실행할 수 있습니다. Amazon Aurora Zero-ETL to Amazon Redshift를 사용하면 고객은 Amazon Redshift로 분석하려는 데이터가 포함된 Amazon Aurora 테이블을 선택하기만 하면 됩니다. 이 기능은 스키마와 데이터를 Amazon Redshift로 원활하게 복제합니다. 이를 통해 복잡한 데이터 파이프라인을 구축하고 관리할 필요가 줄어들기 때문에 고객이 애플리케이션 개선에 집중할 수 있습니다. Amazon Aurora Zero-ETL to Amazon Redshift를 통해 고객은 여러 Amazon Aurora 데이터베이스 클러스터의 데이터를 동일한 Amazon Redshift 인스턴스로 복제하여 여러 애플리케이션에서 포괄적인 인사이트를 획득하는 동시에 핵심 분석 자산을 통합하여 상당한 비용 절감 효과를 얻고 운영 효율성을 높일 수 있습니다. Amazon Aurora Zero-ETL to Amazon Redshift를 통해 고객은 구체화된 뷰, 데이터 공유, 여러 데이터 스토어 및 데이터 레이크에 대한 연동 액세스와 같은 Amazon Redshift의 핵심 분석 및 기계 학습 기능에 액세스할 수도 있습니다. 이를 통해 고객은 실시간에 가까운 분석과 핵심 분석을 결합하여 비즈니스 의사 결정을 알리는 시간에 민감한 인사이트를 효과적으로 도출할 수 있습니다. 또한 고객은 트랜잭션에 Amazon Aurora를 사용하고 분석에 Amazon Redshift를 사용하여 공유 컴퓨팅 리소스가 없으므로 성능이 뛰어나고 운영상 안정적인 솔루션을 제공합니다.\"\n",
            "Amazon Aurora Zero-ETL to Amazon Redshift는 다른 AWS 서비스와 어떤 관련이 있나요?\n",
            "Amazon Redshift와 Amazon Aurora 제로 ETL 통합은 트랜잭션 분석을 위한 두 서비스 간의 원활한 통합을 제공합니다.\n",
            "\"category : Redshift, question : Amazon Aurora Zero-ETL to Amazon Redshift는 다른 AWS 서비스와 어떤 관련이 있나요?, answer : Amazon Redshift와 Amazon Aurora 제로 ETL 통합은 트랜잭션 분석을 위한 두 서비스 간의 원활한 통합을 제공합니다.\"\n",
            "스트리밍 수집은 어떻게 작동하나요?\n",
            "스트리밍 데이터는 스트림을 쿼리할 때 시간 가변성 관계의 발전을 캡처한다는 점에서 기존 데이터베이스 테이블과 다릅니다. 반면에 테이블은 이 시간 가변성 관계의 특정 시점 스냅샷을 캡처합니다. Amazon Redshift의 고객은 일반 테이블에서 작업하고 'ELT'와 같은 기존 배치 모델을 사용하여 데이터의 다운스트림 처리(즉, 변환)를 수행하는 데 익숙합니다. 고객이 ELT 워크플로를 지원하기 위해 쿼리된 시간까지 누적된 스트림의 특정 시점 뷰를 가능한 한 빨리 구체화할 수 있도록 AWS는 Redshift 구체화된 뷰(MV)를 사용하는 방법을 제공합니다.\n",
            "\"category : Redshift, question : 스트리밍 수집은 어떻게 작동하나요?, answer : 스트리밍 데이터는 스트림을 쿼리할 때 시간 가변성 관계의 발전을 캡처한다는 점에서 기존 데이터베이스 테이블과 다릅니다. 반면에 테이블은 이 시간 가변성 관계의 특정 시점 스냅샷을 캡처합니다. Amazon Redshift의 고객은 일반 테이블에서 작업하고 'ELT'와 같은 기존 배치 모델을 사용하여 데이터의 다운스트림 처리(즉, 변환)를 수행하는 데 익숙합니다. 고객이 ELT 워크플로를 지원하기 위해 쿼리된 시간까지 누적된 스트림의 특정 시점 뷰를 가능한 한 빨리 구체화할 수 있도록 AWS는 Redshift 구체화된 뷰(MV)를 사용하는 방법을 제공합니다.\"\n",
            "데이터 공유에 대한 사용 사례에는 어떤 것이 있나요?\n",
            "주요한 사용 사례는 다음과 같습니다.\n",
            "\n",
            "읽기 워크로드 격리를 제공하고 요금을 최적화하기 위해 중앙 ETL 클러스터가 데이터를 다수의 BI/분석 클러스터와 공유합니다.\n",
            "데이터 공급자가 데이터를 외부 소비자와 공유합니다.\n",
            "고객, 제품과 같은 일반적인 데이터 집합을 다른 비즈니스 그룹 간에 공유하고 광범위한 분석 및 데이터 과학을 위해 협업합니다.\n",
            "관리를 간소화하기 위해 데이터 웨어하우스를 분산합니다.\n",
            "개발, 테스트, 프로덕션 환경 간에 데이터를 공유합니다.\n",
            "다른 AWS 분석 서비스의 Redshift 데이터에 액세스합니다.\n",
            "\"category : Redshift, question : 데이터 공유에 대한 사용 사례에는 어떤 것이 있나요?, answer : 주요한 사용 사례는 다음과 같습니다.\n",
            "\n",
            "읽기 워크로드 격리를 제공하고 요금을 최적화하기 위해 중앙 ETL 클러스터가 데이터를 다수의 BI/분석 클러스터와 공유합니다.\n",
            "데이터 공급자가 데이터를 외부 소비자와 공유합니다.\n",
            "고객, 제품과 같은 일반적인 데이터 집합을 다른 비즈니스 그룹 간에 공유하고 광범위한 분석 및 데이터 과학을 위해 협업합니다.\n",
            "관리를 간소화하기 위해 데이터 웨어하우스를 분산합니다.\n",
            "개발, 테스트, 프로덕션 환경 간에 데이터를 공유합니다.\n",
            "다른 AWS 분석 서비스의 Redshift 데이터에 액세스합니다.\"\n",
            "Amazon Redshift에서 교차 데이터베이스 쿼리는 무엇인가요?\n",
            "교차 데이터베이스 쿼리를 사용하면 연결된 데이터베이스에 관계 없이 액세스 권한이 있는 모든 Redshift 데이터베이스에서 데이터를 원활하게 쿼리하고 조인할 수 있습니다. 여기에는 클러스터의 로컬 데이터베이스와 함께 원격 클러스터에서 사용 가능한 공유 데이터 세트가 포함될 수 있습니다. 교차 데이터베이스 쿼리는 멀티 테넌트 구성을 지원하기 위해 별도의 데이터베이스로 데이터를 구성하는 유연성을 제공합니다.\n",
            "\"category : Redshift, question : Amazon Redshift에서 교차 데이터베이스 쿼리는 무엇인가요?, answer : 교차 데이터베이스 쿼리를 사용하면 연결된 데이터베이스에 관계 없이 액세스 권한이 있는 모든 Redshift 데이터베이스에서 데이터를 원활하게 쿼리하고 조인할 수 있습니다. 여기에는 클러스터의 로컬 데이터베이스와 함께 원격 클러스터에서 사용 가능한 공유 데이터 세트가 포함될 수 있습니다. 교차 데이터베이스 쿼리는 멀티 테넌트 구성을 지원하기 위해 별도의 데이터베이스로 데이터를 구성하는 유연성을 제공합니다.\"\n",
            "AWS Data Exchange의 주된 사용자는 누구인가요?\n",
            "AWS Data Exchange를 이용하면 AWS 고객이 AWS에서 서드 파티 데이터를 안전하고 더 효율적으로 교환하고 사용할 수 있습니다. 거의 모든 산업에서 데이터 분석가, 제품 관리자, 포트폴리오 관리자, 데이터 사이언티스트, 금융시장 분석가, 임상시험 기술자, 개발자는 분석을 실시하고, ML을 훈련하고, 데이터에 기반한 의사결정을 내리기 위해 더 많은 데이터에 접근하길 원합니다. 그러나 다수의 공급자로부터 데이터를 검색할 수 있는 하나의 장소가 없고 공급자가 데이터를 제공하는 방식에 일관성이 없기 때문에 이들은 배송된물리적인 미디어, FTP 자격 증명, 맞춤형 API 호출 등을 이용하는 현실입니다. 반대로 많은 조직은 자신의 데이터를 연구나 상업적 목적으로 제공하려 하지만 데이터 전달, 자격 부여, 결제 기술을 구축하고 유지하기가 어렵고 비용이 많이 듭니다. 이러한 점은 가치 있는 데이터의 공급을 더욱 저해하고 있습니다.\n",
            "\"category : Redshift, question : AWS Data Exchange의 주된 사용자는 누구인가요?, answer : AWS Data Exchange를 이용하면 AWS 고객이 AWS에서 서드 파티 데이터를 안전하고 더 효율적으로 교환하고 사용할 수 있습니다. 거의 모든 산업에서 데이터 분석가, 제품 관리자, 포트폴리오 관리자, 데이터 사이언티스트, 금융시장 분석가, 임상시험 기술자, 개발자는 분석을 실시하고, ML을 훈련하고, 데이터에 기반한 의사결정을 내리기 위해 더 많은 데이터에 접근하길 원합니다. 그러나 다수의 공급자로부터 데이터를 검색할 수 있는 하나의 장소가 없고 공급자가 데이터를 제공하는 방식에 일관성이 없기 때문에 이들은 배송된물리적인 미디어, FTP 자격 증명, 맞춤형 API 호출 등을 이용하는 현실입니다. 반대로 많은 조직은 자신의 데이터를 연구나 상업적 목적으로 제공하려 하지만 데이터 전달, 자격 부여, 결제 기술을 구축하고 유지하기가 어렵고 비용이 많이 듭니다. 이러한 점은 가치 있는 데이터의 공급을 더욱 저해하고 있습니다.\"\n",
            "Amazon Redshift 데이터 웨어하우스 클러스터의 크기와 성능을 조정하려면 어떻게 해야 하나요?\n",
            "Amazon Redshift Serverless는 데이터 웨어하우스 용량을 자동으로 프로비저닝하고 기본 리소스를 지능적으로 확장합니다. Amazon Redshift Serverless는 몇 초 만에 용량을 조정하여 가장 까다롭고 변동성이 큰 워크로드에 대해서도 일관된 고성능과 단순화된 운영을 제공합니다. 동시성 확장 기능을 사용하면 일관성 있게 빠른 쿼리 성능을 유지하면서 무제한으로 동시 사용자 및 동시 쿼리를 지원할 수 있습니다. 동시성 확장 기능을 활성화하면 Amazon Redshift는 클러스터의 쿼리 대기열이 증가할 경우 클러스터 용량을 자동으로 추가합니다.\n",
            "수동 확장을 위해 쿼리 성능을 향상하거나 CPU, 메모리 또는 I/O 초과 사용률에 대응하려는 경우 AWS Management Console이나 ModifyCluster API를 통해 탄력적 확장을 사용하여 데이터 웨어하우스 클러스터의 노드 수를 늘릴 수 있습니다. 데이터 웨어하우스 클러스터를 수정하는 경우 요청한 변경 사항이 즉시 적용됩니다. Redshift 데이터 웨어하우스 클러스터의 컴퓨팅 사용률, 스토리지 사용률 및 읽기/쓰기 트래픽에 대한 지표는 AWS Management Console 또는 Amazon CloudWatch API를 통해 무료로 확인할 수 있습니다. 또한 Amazon CloudWatch의 사용자 지정 지표 기능을 사용하여 사용자 정의 지표를 추가할 수 있습니다.\n",
            "Amazon Redshift Spectrum에서는 여러 Redshift 클러스터를 실행하여 Amazon S3에 있는 같은 데이터를 액세스할 수 있습니다. 서로 다른 클러스터를 다양한 사용 사례에 사용할 수 있습니다. 예를 들어 한 클러스터를 표준 보고에 사용하고 다른 클러스터를 데이터 과학 쿼리에 사용할 수 있습니다. 마케팅 팀에서는 운영 팀과 별개로 자체 클러스터를 사용할 수 있습니다. Redshift Spectrum은 쿼리 실행을 공유 리소스 풀의 여러 Redshift Spectrum 작업자에게 자동으로 분산하여 Amazon S3의 데이터를 읽고 처리하며, 결과를 Redshift 클러스터로 가져와서 나머지 작업을 처리합니다.\n",
            "\"category : Redshift, question : Amazon Redshift 데이터 웨어하우스 클러스터의 크기와 성능을 조정하려면 어떻게 해야 하나요?, answer : Amazon Redshift Serverless는 데이터 웨어하우스 용량을 자동으로 프로비저닝하고 기본 리소스를 지능적으로 확장합니다. Amazon Redshift Serverless는 몇 초 만에 용량을 조정하여 가장 까다롭고 변동성이 큰 워크로드에 대해서도 일관된 고성능과 단순화된 운영을 제공합니다. 동시성 확장 기능을 사용하면 일관성 있게 빠른 쿼리 성능을 유지하면서 무제한으로 동시 사용자 및 동시 쿼리를 지원할 수 있습니다. 동시성 확장 기능을 활성화하면 Amazon Redshift는 클러스터의 쿼리 대기열이 증가할 경우 클러스터 용량을 자동으로 추가합니다.\n",
            "수동 확장을 위해 쿼리 성능을 향상하거나 CPU, 메모리 또는 I/O 초과 사용률에 대응하려는 경우 AWS Management Console이나 ModifyCluster API를 통해 탄력적 확장을 사용하여 데이터 웨어하우스 클러스터의 노드 수를 늘릴 수 있습니다. 데이터 웨어하우스 클러스터를 수정하는 경우 요청한 변경 사항이 즉시 적용됩니다. Redshift 데이터 웨어하우스 클러스터의 컴퓨팅 사용률, 스토리지 사용률 및 읽기/쓰기 트래픽에 대한 지표는 AWS Management Console 또는 Amazon CloudWatch API를 통해 무료로 확인할 수 있습니다. 또한 Amazon CloudWatch의 사용자 지정 지표 기능을 사용하여 사용자 정의 지표를 추가할 수 있습니다.\n",
            "Amazon Redshift Spectrum에서는 여러 Redshift 클러스터를 실행하여 Amazon S3에 있는 같은 데이터를 액세스할 수 있습니다. 서로 다른 클러스터를 다양한 사용 사례에 사용할 수 있습니다. 예를 들어 한 클러스터를 표준 보고에 사용하고 다른 클러스터를 데이터 과학 쿼리에 사용할 수 있습니다. 마케팅 팀에서는 운영 팀과 별개로 자체 클러스터를 사용할 수 있습니다. Redshift Spectrum은 쿼리 실행을 공유 리소스 풀의 여러 Redshift Spectrum 작업자에게 자동으로 분산하여 Amazon S3의 데이터를 읽고 처리하며, 결과를 Redshift 클러스터로 가져와서 나머지 작업을 처리합니다.\"\n",
            "규모를 조정하는 동안 데이터 웨어하우스 클러스터를 사용할 수 있나요?\n",
            "경우에 따라 다릅니다. 동시성 확장 기능을 사용하면 동시성 확장 시 클러스터에서 읽기 및 쓰기를 완전히 사용할 수 있습니다. 탄력적 크기 조정을 사용할 경우에는 크기 조정 기간에 4~8분 동안 클러스터를 사용할 수 없습니다. 관리형 스토리지에서 Redshift RA3 스토리지 탄력성 기능을 통해 클러스터를 완전히 사용할 수 있으며, 관리형 스토리지와 컴퓨팅 노드 간에 데이터가 자동으로 이동합니다.\n",
            "\"category : Redshift, question : 규모를 조정하는 동안 데이터 웨어하우스 클러스터를 사용할 수 있나요?, answer : 경우에 따라 다릅니다. 동시성 확장 기능을 사용하면 동시성 확장 시 클러스터에서 읽기 및 쓰기를 완전히 사용할 수 있습니다. 탄력적 크기 조정을 사용할 경우에는 크기 조정 기간에 4~8분 동안 클러스터를 사용할 수 없습니다. 관리형 스토리지에서 Redshift RA3 스토리지 탄력성 기능을 통해 클러스터를 완전히 사용할 수 있으며, 관리형 스토리지와 컴퓨팅 노드 간에 데이터가 자동으로 이동합니다.\"\n",
            "탄력적 크기 조정이란 무엇이며, 동시성 확장과는 어떻게 다른가요?\n",
            "탄력적 크기 조정은 쿼리 처리량(throughput)을 관리하기 위해 단일 Redshift 클러스터에서 노드를 몇 분 내에 추가하거나 제거합니다. 예를 들어 하루의 특정 시간대 또는 월말 보고 업무를 위한 ETL 워크로드를 정시에 완료하려면 Amazon Redshift 리소스가 추가로 필요할 수 있습니다. 동시성 확장은 클러스터 리소스를 추가하여 전반적인 쿼리 동시성을 높입니다.\n",
            "\"category : Redshift, question : 탄력적 크기 조정이란 무엇이며, 동시성 확장과는 어떻게 다른가요?, answer : 탄력적 크기 조정은 쿼리 처리량(throughput)을 관리하기 위해 단일 Redshift 클러스터에서 노드를 몇 분 내에 추가하거나 제거합니다. 예를 들어 하루의 특정 시간대 또는 월말 보고 업무를 위한 ETL 워크로드를 정시에 완료하려면 Amazon Redshift 리소스가 추가로 필요할 수 있습니다. 동시성 확장은 클러스터 리소스를 추가하여 전반적인 쿼리 동시성을 높입니다.\"\n",
            "동시성 확장 클러스터에 직접 액세스할 수 있나요?\n",
            "아니요. 동시성 확장은 대규모로 확장 가능한 Amazon Redshift 리소스 풀로서, 고객은 직접 액세스할 수 없습니다.\n",
            "\"category : Redshift, question : 동시성 확장 클러스터에 직접 액세스할 수 있나요?, answer : 아니요. 동시성 확장은 대규모로 확장 가능한 Amazon Redshift 리소스 풀로서, 고객은 직접 액세스할 수 없습니다.\"\n",
            "Amazon Redshift는 어떻게 내 데이터를 안전하게 유지하나요?\n",
            "Amazon Redshift는 AWS Single Sign-On(SSO), 다중 인증, 열 수준 액세스 제어, 행 수준 보안, 역할 기반 액세스 제어 및 Amazon Virtual Private Cloud(VPC)를 위한 기본 제공 ID 관리 및 페더레이션을 통해 업계 최고 수준의 보안을 지원합니다. Amazon Redshift를 사용하면 전송 중과 저장 시에 데이터가 암호화됩니다. 모든 Amazon Redshift 보안 기능은 추가 비용 없이 즉시 사용 가능한 상태로 제공되며, 가장 까다로운 보안, 개인 정보 보호 및 규정 준수 요구 사항을 지원할 수 있습니다. AWS는 ISO 27001, SOC, HIPAA/HITECH, FedRAMP 등 어떤 제공업체보다 많은 보안 표준 및 규정 준수 인증을 지원하는 이점을 제공합니다.\n",
            "\"category : Redshift, question : Amazon Redshift는 어떻게 내 데이터를 안전하게 유지하나요?, answer : Amazon Redshift는 AWS Single Sign-On(SSO), 다중 인증, 열 수준 액세스 제어, 행 수준 보안, 역할 기반 액세스 제어 및 Amazon Virtual Private Cloud(VPC)를 위한 기본 제공 ID 관리 및 페더레이션을 통해 업계 최고 수준의 보안을 지원합니다. Amazon Redshift를 사용하면 전송 중과 저장 시에 데이터가 암호화됩니다. 모든 Amazon Redshift 보안 기능은 추가 비용 없이 즉시 사용 가능한 상태로 제공되며, 가장 까다로운 보안, 개인 정보 보호 및 규정 준수 요구 사항을 지원할 수 있습니다. AWS는 ISO 27001, SOC, HIPAA/HITECH, FedRAMP 등 어떤 제공업체보다 많은 보안 표준 및 규정 준수 인증을 지원하는 이점을 제공합니다.\"\n",
            "Redshift는 세분화된 액세스 제어를 지원하나요?\n",
            "예. Amazon Redshift는 역할 기반 액세스 제어를 지원합니다. 행 수준 액세스 제어를 통해 하나 이상의 역할을 사용자에게 할당하고 역할별로 시스템 및 객체 권한을 할당할 수 있습니다. 즉시 사용 가능한 시스템 역할(루트 사용자, dba, 운영자 및 보안 관리자)을 사용하거나 역할을 직접 만들 수 있습니다.\n",
            "\"category : Redshift, question : Redshift는 세분화된 액세스 제어를 지원하나요?, answer : 예. Amazon Redshift는 역할 기반 액세스 제어를 지원합니다. 행 수준 액세스 제어를 통해 하나 이상의 역할을 사용자에게 할당하고 역할별로 시스템 및 객체 권한을 할당할 수 있습니다. 즉시 사용 가능한 시스템 역할(루트 사용자, dba, 운영자 및 보안 관리자)을 사용하거나 역할을 직접 만들 수 있습니다.\"\n",
            "Amazon Redshift에서 데이터 마스킹 또는 데이터 토큰화를 지원하나요?\n",
            "AWS Lambda 사용자 정의 함수(UDF)를 통해 Amazon Redshift에서 AWS Lambda 함수를 UDF로 사용하고 Redshift SQL 쿼리에서 호출할 수 있습니다. 이 기능을 사용하면 SQL 쿼리용 사용자 지정 확장을 작성하여 다른 서비스 또는 타사 제품과 더 긴밀하게 통합할 수 있습니다. Protegrity와 같은 공급업체와 통합하여 외부 토큰화, 데이터 마스킹, 데이터 식별 또는 식별 취소를 수행하고, 쿼리 시간에 사용자의 권한 및 그룹에 따라 민감한 데이터를 보호하거나 보호 해제하는 Lambda UDF를 작성할 수 있습니다.\n",
            "동적 데이터 마스킹 지원을 통해 고객은 데이터 마스킹 정책을 관리하여 민감한 데이터를 쉽게 보호하고 세분화된 액세스를 제어할 수 있습니다. 모든 사용자에게 노출할 수 없는 민감한 데이터를 갖고 있는 여러 사용자와 객체가 포함된 애플리케이션이 있다고 가정합니다. 다양한 사용자 그룹에 부여할 다양한 세분화된 보안 수준을 제공해야 합니다. Redshift Dynamic Data Masking은 고객이 일관되고, 형식을 보존하며, 되돌릴 수 없는 마스킹된 데이터 값을 정의할 수 있도록 구성할 수 있습니다. 기능이 정식 출시되면 즉시 사용을 시작합니다. 보안 관리자는 몇 가지 명령만으로 정책을 생성하고 적용할 수 있습니다.\n",
            "\"category : Redshift, question : Amazon Redshift에서 데이터 마스킹 또는 데이터 토큰화를 지원하나요?, answer : AWS Lambda 사용자 정의 함수(UDF)를 통해 Amazon Redshift에서 AWS Lambda 함수를 UDF로 사용하고 Redshift SQL 쿼리에서 호출할 수 있습니다. 이 기능을 사용하면 SQL 쿼리용 사용자 지정 확장을 작성하여 다른 서비스 또는 타사 제품과 더 긴밀하게 통합할 수 있습니다. Protegrity와 같은 공급업체와 통합하여 외부 토큰화, 데이터 마스킹, 데이터 식별 또는 식별 취소를 수행하고, 쿼리 시간에 사용자의 권한 및 그룹에 따라 민감한 데이터를 보호하거나 보호 해제하는 Lambda UDF를 작성할 수 있습니다.\n",
            "동적 데이터 마스킹 지원을 통해 고객은 데이터 마스킹 정책을 관리하여 민감한 데이터를 쉽게 보호하고 세분화된 액세스를 제어할 수 있습니다. 모든 사용자에게 노출할 수 없는 민감한 데이터를 갖고 있는 여러 사용자와 객체가 포함된 애플리케이션이 있다고 가정합니다. 다양한 사용자 그룹에 부여할 다양한 세분화된 보안 수준을 제공해야 합니다. Redshift Dynamic Data Masking은 고객이 일관되고, 형식을 보존하며, 되돌릴 수 없는 마스킹된 데이터 값을 정의할 수 있도록 구성할 수 있습니다. 기능이 정식 출시되면 즉시 사용을 시작합니다. 보안 관리자는 몇 가지 명령만으로 정책을 생성하고 적용할 수 있습니다.\"\n",
            "Amazon Redshift는 Single Sign-On을 지원하나요?\n",
            "예. Microsoft Azure Active Directory, Active Directory Federation Services, Okta, Ping Federate 또는 기타 SAML 호환 ID 제공업체와 같은 회사 ID 제공업체를 사용하려는 고객은 Single Sign-On을 제공하도록 Amazon Redshift를 구성할 수 있습니다. Microsoft Azure Active Directory(AD) ID를 사용하여 Amazon Redshift 클러스터에 사인온할 수 있습니다. 따라서 Redshift에서 Azure Active Directory ID를 복제하지 않고도 Redshift에 사인온할 수 있습니다.\n",
            "\"category : Redshift, question : Amazon Redshift는 Single Sign-On을 지원하나요?, answer : 예. Microsoft Azure Active Directory, Active Directory Federation Services, Okta, Ping Federate 또는 기타 SAML 호환 ID 제공업체와 같은 회사 ID 제공업체를 사용하려는 고객은 Single Sign-On을 제공하도록 Amazon Redshift를 구성할 수 있습니다. Microsoft Azure Active Directory(AD) ID를 사용하여 Amazon Redshift 클러스터에 사인온할 수 있습니다. 따라서 Redshift에서 Azure Active Directory ID를 복제하지 않고도 Redshift에 사인온할 수 있습니다.\"\n",
            "Amazon Redshift는 다중 인증(MFA)을 지원하나요?\n",
            "예. Amazon Redshift 클러스터에 인증할 때 추가적인 보안을 위해 다중 인증(MFA)을 사용할 수 있습니다.\n",
            "\"category : Redshift, question : Amazon Redshift는 다중 인증(MFA)을 지원하나요?, answer : 예. Amazon Redshift 클러스터에 인증할 때 추가적인 보안을 위해 다중 인증(MFA)을 사용할 수 있습니다.\"\n",
            "개별 노드 장애 발생 시 데이터 웨어하우스 클러스터 가용성 및 데이터 내구성에 어떤 일이 발생하나요?\n",
            "Amazon Redshift는 자동으로 데이터 웨어하우스 클러스터에서 장애가 발생한 노드를 검색하고 해당 노드를 교체합니다. Dense Compute(DC) 및 Dense Storage(DS2) 클러스터에서 데이터는 높은 데이터 내구성을 보장하기 위해 컴퓨팅 노드에 저장됩니다. 노드가 교체되면 데이터가 다른 노드의 미러 복사본으로 새로 고쳐집니다. Amazon S3에 데이터가 저장되고 로컬 드라이브는 데이터 캐시로만 사용되기 때문에 RA3 클러스터와 Redshift 서버리스는 동일한 방식으로 영향을 받지 않습니다. 데이터 웨어하우스 클러스터는 교체 노드가 프로비저닝되고 DB에 추가될 때까지 쿼리 및 업데이트에 사용할 수 없습니다. Amazon Redshift는 교체 노드를 즉각적으로 사용할 수 있게 만들고, 먼저 Amazon S3에서 가장 빈번하게 액세스되는 데이터를 로드하여 가능한 한 빨리 데이터 쿼리 작업을 재개할 수 있게 해 줍니다. 단일 노드 클러스터는 데이터 복제를 지원하지 않습니다. 드라이브 장애가 발생하는 경우 S3의 스냅샷에서 클러스터를 복원해야 합니다. 프로덕션에 최소 두 개의 노드를 사용하는 것이 좋습니다.\n",
            "\"category : Redshift, question : 개별 노드 장애 발생 시 데이터 웨어하우스 클러스터 가용성 및 데이터 내구성에 어떤 일이 발생하나요?, answer : Amazon Redshift는 자동으로 데이터 웨어하우스 클러스터에서 장애가 발생한 노드를 검색하고 해당 노드를 교체합니다. Dense Compute(DC) 및 Dense Storage(DS2) 클러스터에서 데이터는 높은 데이터 내구성을 보장하기 위해 컴퓨팅 노드에 저장됩니다. 노드가 교체되면 데이터가 다른 노드의 미러 복사본으로 새로 고쳐집니다. Amazon S3에 데이터가 저장되고 로컬 드라이브는 데이터 캐시로만 사용되기 때문에 RA3 클러스터와 Redshift 서버리스는 동일한 방식으로 영향을 받지 않습니다. 데이터 웨어하우스 클러스터는 교체 노드가 프로비저닝되고 DB에 추가될 때까지 쿼리 및 업데이트에 사용할 수 없습니다. Amazon Redshift는 교체 노드를 즉각적으로 사용할 수 있게 만들고, 먼저 Amazon S3에서 가장 빈번하게 액세스되는 데이터를 로드하여 가능한 한 빨리 데이터 쿼리 작업을 재개할 수 있게 해 줍니다. 단일 노드 클러스터는 데이터 복제를 지원하지 않습니다. 드라이브 장애가 발생하는 경우 S3의 스냅샷에서 클러스터를 복원해야 합니다. 프로덕션에 최소 두 개의 노드를 사용하는 것이 좋습니다.\"\n",
            "데이터 웨어하우스 클러스터의 가용 영역(AZ)이 중단되면 데이터 웨어하우스 클러스터 가용성 및 데이터 내구성에 어떤 일이 발생하나요?\n",
            "Amazon Redshift 데이터 웨어하우스가 단일 AZ 배포이고 클러스터의 가용 영역이 사용 불가능해지면 Amazon Redshift는 데이터 손실 또는 애플리케이션 변경 없이도 클러스터를 다른 AWS 가용 영역(AZ)으로 자동으로 이동합니다. 이를 활성화하려면 클러스터 구성 설정에서 재배치 기능을 활성화해야 합니다.\n",
            "\"category : Redshift, question : 데이터 웨어하우스 클러스터의 가용 영역(AZ)이 중단되면 데이터 웨어하우스 클러스터 가용성 및 데이터 내구성에 어떤 일이 발생하나요?, answer : Amazon Redshift 데이터 웨어하우스가 단일 AZ 배포이고 클러스터의 가용 영역이 사용 불가능해지면 Amazon Redshift는 데이터 손실 또는 애플리케이션 변경 없이도 클러스터를 다른 AWS 가용 영역(AZ)으로 자동으로 이동합니다. 이를 활성화하려면 클러스터 구성 설정에서 재배치 기능을 활성화해야 합니다.\"\n",
            "왜 Redshift 다중 AZ 배포를 사용해야 하나요?\n",
            "단일 AZ 배포와 달리 이제 고객은 다중 AZ 배포에서 데이터 웨어하우스를 실행하여 Redshift의 가용성을 개선할 수 있습니다. 다중 AZ 배포를 통해 여러 AWS 가용 영역(AZ)에서 데이터 웨어하우스를 동시에 실행하고 예기치 않은 장애 시나리오에서도 계속 운영할 수 있습니다. 다중 AZ 배포가 하나의 엔드포인트가 있는 단일 데이터 웨어하우스로 관리되기 때문에 비즈니스 연속성을 유지하기 위해 애플리케이션을 변경할 필요가 없습니다. 다중 AZ 배포는 자동 복구 용량을 보장하여 복구 시간을 단축하며 AZ 장애에 대한 최고 수준의 가용성과 복원력이 필요한 비즈니스 크리티컬 분석 애플리케이션을 사용하는 고객을 대상으로 합니다. 또한 이를 통해 고객은 AWS Well-Architected Framework의 신뢰성 원칙 권장 사항을 더 잘 준수하는 솔루션을 구현할 수 있습니다. Amazon Redshift 다중 AZ에 대한 자세한 내용을 알아보려면 여기를 참조하세요.\n",
            "\"category : Redshift, question : 왜 Redshift 다중 AZ 배포를 사용해야 하나요?, answer : 단일 AZ 배포와 달리 이제 고객은 다중 AZ 배포에서 데이터 웨어하우스를 실행하여 Redshift의 가용성을 개선할 수 있습니다. 다중 AZ 배포를 통해 여러 AWS 가용 영역(AZ)에서 데이터 웨어하우스를 동시에 실행하고 예기치 않은 장애 시나리오에서도 계속 운영할 수 있습니다. 다중 AZ 배포가 하나의 엔드포인트가 있는 단일 데이터 웨어하우스로 관리되기 때문에 비즈니스 연속성을 유지하기 위해 애플리케이션을 변경할 필요가 없습니다. 다중 AZ 배포는 자동 복구 용량을 보장하여 복구 시간을 단축하며 AZ 장애에 대한 최고 수준의 가용성과 복원력이 필요한 비즈니스 크리티컬 분석 애플리케이션을 사용하는 고객을 대상으로 합니다. 또한 이를 통해 고객은 AWS Well-Architected Framework의 신뢰성 원칙 권장 사항을 더 잘 준수하는 솔루션을 구현할 수 있습니다. Amazon Redshift 다중 AZ에 대한 자세한 내용을 알아보려면 여기를 참조하세요.\"\n",
            "RPO와 RTO는 무엇인가요? 다중 AZ 배포에서 지원되는 RPO 및 RTO는 무엇인가요?\n",
            "RPO는 Recovery Point Objective의 약어로 장애 발생 시 데이터 최신성 보장을 설명하는 용어입니다. RPO는 마지막 데이터 복구 지점 이후 허용 가능한 최대 시간입니다. 이는 마지막 복구 지점과 서비스 중단 사이에 허용 가능한 데이터 손실로 간주되는 것을 결정합니다. Redshift 다중 AZ는 RPO = 0을 지원합니다. 즉, 장애 발생 시 데이터가 최신 상태로 유지됩니다. 출시 전 테스트에서 Amazon Redshift 다중 AZ 배포를 통한 RTO는 드물지만 AZ 장애가 발생할 경우 60초 미만인 것으로 나타났습니다.\n",
            "\"category : Redshift, question : RPO와 RTO는 무엇인가요? 다중 AZ 배포에서 지원되는 RPO 및 RTO는 무엇인가요?, answer : RPO는 Recovery Point Objective의 약어로 장애 발생 시 데이터 최신성 보장을 설명하는 용어입니다. RPO는 마지막 데이터 복구 지점 이후 허용 가능한 최대 시간입니다. 이는 마지막 복구 지점과 서비스 중단 사이에 허용 가능한 데이터 손실로 간주되는 것을 결정합니다. Redshift 다중 AZ는 RPO = 0을 지원합니다. 즉, 장애 발생 시 데이터가 최신 상태로 유지됩니다. 출시 전 테스트에서 Amazon Redshift 다중 AZ 배포를 통한 RTO는 드물지만 AZ 장애가 발생할 경우 60초 미만인 것으로 나타났습니다.\"\n",
            "기존 Redshift 재배치 기능과 비교하여 Redshift 다중 AZ는 어떤가요?\n",
            "Redshift 재배치는 모든 새 RA3 클러스터 및 서버리스 엔드포인트에서 기본적으로 사용되며, 이를 통해 대규모 운영 중단 시 데이터 손실이나 추가 비용 없이 다른 AZ에서 데이터 웨어하우스를 다시 시작할 수 있습니다. 재배치 사용은 무료이지만 이는 복구 중인 AZ의 리소스 가용성에 따른 최선의 방법이며 Recovery Time Objective(RTO)가 새 클러스터 시작과 관련된 다른 문제의 영향을 받을 수 있다는 제한 사항이 있습니다. 이로 인해 복구 시간이 10분에서 60분 사이가 될 수 있습니다. Redshift 다중 AZ는 수십 초 단위로 측정된 RTO를 제공하여 고가용성 요구 사항을 지원하며 용량 제한이나 새 클러스터 생성 시 발생할 수 있는 기타 잠재적인 문제의 영향을 받지 않기 때문에 지속적인 운영을 보장합니다.\n",
            "\"category : Redshift, question : 기존 Redshift 재배치 기능과 비교하여 Redshift 다중 AZ는 어떤가요?, answer : Redshift 재배치는 모든 새 RA3 클러스터 및 서버리스 엔드포인트에서 기본적으로 사용되며, 이를 통해 대규모 운영 중단 시 데이터 손실이나 추가 비용 없이 다른 AZ에서 데이터 웨어하우스를 다시 시작할 수 있습니다. 재배치 사용은 무료이지만 이는 복구 중인 AZ의 리소스 가용성에 따른 최선의 방법이며 Recovery Time Objective(RTO)가 새 클러스터 시작과 관련된 다른 문제의 영향을 받을 수 있다는 제한 사항이 있습니다. 이로 인해 복구 시간이 10분에서 60분 사이가 될 수 있습니다. Redshift 다중 AZ는 수십 초 단위로 측정된 RTO를 제공하여 고가용성 요구 사항을 지원하며 용량 제한이나 새 클러스터 생성 시 발생할 수 있는 기타 잠재적인 문제의 영향을 받지 않기 때문에 지속적인 운영을 보장합니다.\"\n",
            "Amazon Redshift와 Redshift Spectrum은 기본 비즈니스 인텔리전스 소프트웨어 패키지 및 ETL 도구와 호환되나요?\n",
            "예. Amazon Redshift는 업계 표준 SQL을 사용하며 표준 JDBC 및 ODBC 드라이버를 사용하여 액세스할 수 있습니다. Amazon Redshift 사용자 지정 JDBC 및 ODBC 드라이버는 Redshift Console의 [클라이언트 연결(Connect Client)] 탭에서 다운로드할 수 있습니다. AWS에서는 주요 BI 및 ETL 공급업체와의 통합을 검증했으며, 이 중 상당수가 데이터 로드 및 분석을 시작하는 데 도움이 되도록 무료 평가판을 제공합니다. 또한, AWS Marketplace로 이동하여 Amazon Redshift와 연동되도록 설계된 솔루션을 몇 분 만에 배포 및 구성할 수 있습니다.\n",
            "Amazon Redshift Spectrum은 모든 Amazon Redshift 클라이언트 도구를 지원합니다. 이러한 클라이언트 도구는 ODBC 또는 JDBC 연결을 사용하여 계속해서 Amazon Redshift 클러스터 엔드포인트에 연결할 수 있으므로, 변경할 필요가 없습니다.\n",
            "Redshift 클러스터의 로컬 스토리지에 있는 테이블에 사용하는 것과 정확히 동일한 쿼리 구문 및 동일한 쿼리 기능을 사용하여 Redshift Spectrum에 있는 테이블에 액세스합니다. 외부 테이블은 이를 등록할 때 사용한 CREATE EXTERNAL SCHEMA 명령에 정의된 스키마 이름을 사용하여 참조됩니다.\n",
            "\"category : Redshift, question : Amazon Redshift와 Redshift Spectrum은 기본 비즈니스 인텔리전스 소프트웨어 패키지 및 ETL 도구와 호환되나요?, answer : 예. Amazon Redshift는 업계 표준 SQL을 사용하며 표준 JDBC 및 ODBC 드라이버를 사용하여 액세스할 수 있습니다. Amazon Redshift 사용자 지정 JDBC 및 ODBC 드라이버는 Redshift Console의 [클라이언트 연결(Connect Client)] 탭에서 다운로드할 수 있습니다. AWS에서는 주요 BI 및 ETL 공급업체와의 통합을 검증했으며, 이 중 상당수가 데이터 로드 및 분석을 시작하는 데 도움이 되도록 무료 평가판을 제공합니다. 또한, AWS Marketplace로 이동하여 Amazon Redshift와 연동되도록 설계된 솔루션을 몇 분 만에 배포 및 구성할 수 있습니다.\n",
            "Amazon Redshift Spectrum은 모든 Amazon Redshift 클라이언트 도구를 지원합니다. 이러한 클라이언트 도구는 ODBC 또는 JDBC 연결을 사용하여 계속해서 Amazon Redshift 클러스터 엔드포인트에 연결할 수 있으므로, 변경할 필요가 없습니다.\n",
            "Redshift 클러스터의 로컬 스토리지에 있는 테이블에 사용하는 것과 정확히 동일한 쿼리 구문 및 동일한 쿼리 기능을 사용하여 Redshift Spectrum에 있는 테이블에 액세스합니다. 외부 테이블은 이를 등록할 때 사용한 CREATE EXTERNAL SCHEMA 명령에 정의된 스키마 이름을 사용하여 참조됩니다.\"\n",
            "Amazon Redshift Spectrum은 어떤 데이터 형식 및 압축 형식을 지원하나요?\n",
            "Amazon Redshift Spectrum은 현재 Avro, CSV, Grok, Amazon Ion, JSON, ORC, Parquet, RCFile, RegexSerDe, Sequence, Text 및 TSV를 비롯한 수많은 오픈 소스 데이터 형식을 지원합니다.<br>Amazon Redshift Spectrum에서는 현재 Gzip 및 Snappy 압축을 지원합니다.\n",
            "\"category : Redshift, question : Amazon Redshift Spectrum은 어떤 데이터 형식 및 압축 형식을 지원하나요?, answer : Amazon Redshift Spectrum은 현재 Avro, CSV, Grok, Amazon Ion, JSON, ORC, Parquet, RCFile, RegexSerDe, Sequence, Text 및 TSV를 비롯한 수많은 오픈 소스 데이터 형식을 지원합니다.<br>Amazon Redshift Spectrum에서는 현재 Gzip 및 Snappy 압축을 지원합니다.\"\n",
            "내 로컬 스토리지에 있는 테이블 이름과 외부 테이블 이름이 같으면 어떻게 되나요?\n",
            "로컬 테이블에서와 마찬가지로 스키마 이름을 사용하여 원하는 테이블을 정확히 선택할 수 있습니다. 쿼리에 schema_name.table_name을 사용하면 됩니다.\n",
            "\"category : Redshift, question : 내 로컬 스토리지에 있는 테이블 이름과 외부 테이블 이름이 같으면 어떻게 되나요?, answer : 로컬 테이블에서와 마찬가지로 스키마 이름을 사용하여 원하는 테이블을 정확히 선택할 수 있습니다. 쿼리에 schema_name.table_name을 사용하면 됩니다.\"\n",
            "Hive Metastore를 사용하여 내 S3 데이터 레이크에 대한 메타데이터를 저장하고 있습니다. Redshift Spectrum을 사용할 수 있나요?\n",
            "예. CREATE EXTERNAL SCHEMA 명령은 Hive Metastore를 지원합니다. 현재 Hive Metastore에 대한 DDL은 지원하지 않습니다.\n",
            "\"category : Redshift, question : Hive Metastore를 사용하여 내 S3 데이터 레이크에 대한 메타데이터를 저장하고 있습니다. Redshift Spectrum을 사용할 수 있나요?, answer : 예. CREATE EXTERNAL SCHEMA 명령은 Hive Metastore를 지원합니다. 현재 Hive Metastore에 대한 DDL은 지원하지 않습니다.\"\n",
            "내 클러스터에 생성된 모든 외부 데이터베이스 테이블 목록을 받으려면 어떻게 해야 하나요?\n",
            "시스템 테이블 SVV_EXTERNAL_TABLES를 쿼리하여 해당 정보를 받을 수 있습니다.\n",
            "\"category : Redshift, question : 내 클러스터에 생성된 모든 외부 데이터베이스 테이블 목록을 받으려면 어떻게 해야 하나요?, answer : 시스템 테이블 SVV_EXTERNAL_TABLES를 쿼리하여 해당 정보를 받을 수 있습니다.\"\n",
            "Redshift는 SQL에서 기계 학습을 사용하는 기능을 지원하나요?\n",
            "예. Amazon Redshift ML 기능을 통해 SQL 사용자는 친숙한 SQL 명령을 사용하여 기계 학습(ML) 모델을 생성하고 훈련하며 배포할 수 있습니다. Amazon Redshift ML을 사용하면 완전관리형 ML 서비스인 Amazon SageMaker에서 Amazon Redshift의 데이터를 활용할 수 있습니다. Amazon Redshift는 비지도 학습(K-Means) 및 지도 학습(Autopilot, XGBoost, MLP 알고리즘)을 모두 지원합니다. 또한 AWS Language AI 서비스를 사용하여 사전 구축된 Lambda UDF 함수로 SQL 쿼리의 텍스트 필드를 변환, 수정 및 분석할 수 있습니다. 블로그 게시물을 참조하세요.\n",
            "\"category : Redshift, question : Redshift는 SQL에서 기계 학습을 사용하는 기능을 지원하나요?, answer : 예. Amazon Redshift ML 기능을 통해 SQL 사용자는 친숙한 SQL 명령을 사용하여 기계 학습(ML) 모델을 생성하고 훈련하며 배포할 수 있습니다. Amazon Redshift ML을 사용하면 완전관리형 ML 서비스인 Amazon SageMaker에서 Amazon Redshift의 데이터를 활용할 수 있습니다. Amazon Redshift는 비지도 학습(K-Means) 및 지도 학습(Autopilot, XGBoost, MLP 알고리즘)을 모두 지원합니다. 또한 AWS Language AI 서비스를 사용하여 사전 구축된 Lambda UDF 함수로 SQL 쿼리의 텍스트 필드를 변환, 수정 및 분석할 수 있습니다. 블로그 게시물을 참조하세요.\"\n",
            "Amazon Redshift는 데이터를 쿼리하는 API를 제공하나요?\n",
            "Amazon Redshift가 제공하는 데이터 API를 사용하여 모든 유형의 클라우드 네이티브 및 컨테이너화된 기존 서버리스 웹 서비스 기반 애플리케이션 및 이벤트 기반 애플리케이션에서 Amazon Redshift의 데이터에 원활하게 액세스할 수 있습니다. 데이터 API를 사용하면 드라이버를 구성하고 데이터베이스 연결을 관리할 필요가 없기 때문에 Amazon Redshift에 대한 액세스가 간소화됩니다. 대신, 데이터 API에서 제공하는 보안 API 엔드포인트를 호출하기만 하면 Amazon Redshift 클러스터에 대해 SQL 명령을 실행할 수 있습니다. 데이터베이스 연결 관리와 데이터 버퍼링은 데이터 API에서 처리합니다. 데이터 API는 비동기식이므로, 나중에 결과를 검색할 수 있습니다. 쿼리 결과는 24시간 동안 저장됩니다.\n",
            "\"category : Redshift, question : Amazon Redshift는 데이터를 쿼리하는 API를 제공하나요?, answer : Amazon Redshift가 제공하는 데이터 API를 사용하여 모든 유형의 클라우드 네이티브 및 컨테이너화된 기존 서버리스 웹 서비스 기반 애플리케이션 및 이벤트 기반 애플리케이션에서 Amazon Redshift의 데이터에 원활하게 액세스할 수 있습니다. 데이터 API를 사용하면 드라이버를 구성하고 데이터베이스 연결을 관리할 필요가 없기 때문에 Amazon Redshift에 대한 액세스가 간소화됩니다. 대신, 데이터 API에서 제공하는 보안 API 엔드포인트를 호출하기만 하면 Amazon Redshift 클러스터에 대해 SQL 명령을 실행할 수 있습니다. 데이터베이스 연결 관리와 데이터 버퍼링은 데이터 API에서 처리합니다. 데이터 API는 비동기식이므로, 나중에 결과를 검색할 수 있습니다. 쿼리 결과는 24시간 동안 저장됩니다.\"\n",
            "Amazon Redshift 데이터 API에서 어떤 유형의 보안 인증 정보를 사용할 수 있나요?\n",
            "데이터 API는 IAM 보안 인증 정보 및 AWS Secrets Manager의 보안 암호 키 사용 모두를 지원합니다. 데이터 API는 AWS Identity and Access Management(IAM) 보안 인증 정보를 연동하므로, API 호출에서 데이터베이스 보안 인증 정보를 전달하지 않고도 Okta 또는 Azure Active Directory와 같은 ID 제공업체나 Secrets Manager에 저장된 데이터베이스 보안 인증 정보를 사용할 수 있습니다.\n",
            "\"category : Redshift, question : Amazon Redshift 데이터 API에서 어떤 유형의 보안 인증 정보를 사용할 수 있나요?, answer : 데이터 API는 IAM 보안 인증 정보 및 AWS Secrets Manager의 보안 암호 키 사용 모두를 지원합니다. 데이터 API는 AWS Identity and Access Management(IAM) 보안 인증 정보를 연동하므로, API 호출에서 데이터베이스 보안 인증 정보를 전달하지 않고도 Okta 또는 Azure Active Directory와 같은 ID 제공업체나 Secrets Manager에 저장된 데이터베이스 보안 인증 정보를 사용할 수 있습니다.\"\n",
            "AWS CLI에서 Amazon Redshift 데이터 API를 사용할 수 있나요?\n",
            "예. AWS CLI에서 aws redshift-data 명령줄 옵션을 사용하여 데이터 API를 사용할 수 있습니다.\n",
            "\"category : Redshift, question : AWS CLI에서 Amazon Redshift 데이터 API를 사용할 수 있나요?, answer : 예. AWS CLI에서 aws redshift-data 명령줄 옵션을 사용하여 데이터 API를 사용할 수 있습니다.\"\n",
            "Redshift 데이터 API는 다른 AWS 서비스와 통합되나요?\n",
            "AWS Lambda, AWS Cloud9, AWS AppSync, Amazon EventBridge 등의 다른 서비스에서도 데이터 API를 사용할 수 있습니다.\n",
            "\"category : Redshift, question : Redshift 데이터 API는 다른 AWS 서비스와 통합되나요?, answer : AWS Lambda, AWS Cloud9, AWS AppSync, Amazon EventBridge 등의 다른 서비스에서도 데이터 API를 사용할 수 있습니다.\"\n",
            "Amazon Redshift 데이터 API 사용에 대한 요금을 별도로 내야 하나요?\n",
            "아니요. 데이터 API 사용에 대한 별도의 요금은 없습니다.\n",
            "\"category : Redshift, question : Amazon Redshift 데이터 API 사용에 대한 요금을 별도로 내야 하나요?, answer : 아니요. 데이터 API 사용에 대한 별도의 요금은 없습니다.\"\n",
            "Amazon Aurora와 Amazon Redshift의 제로 ETL 통합은 언제 사용해야 하나요?\n",
            "트랜잭션 데이터에 거의 실시간으로 액세스해야 하는 경우 Amazon Redshift와 Aurora 제로 ETL 통합을 사용해야 합니다. 이 통합을 사용하면 간단한 SQL 명령으로 Amazon Redshift ML을 활용할 수 있습니다.\n",
            "\"category : Redshift, question : Amazon Aurora와 Amazon Redshift의 제로 ETL 통합은 언제 사용해야 하나요?, answer : 트랜잭션 데이터에 거의 실시간으로 액세스해야 하는 경우 Amazon Redshift와 Aurora 제로 ETL 통합을 사용해야 합니다. 이 통합을 사용하면 간단한 SQL 명령으로 Amazon Redshift ML을 활용할 수 있습니다.\"\n",
            "제로 ETL 통합을 지원하는 Amazon Aurora의 엔진 및 버전은 무엇인가요?\n",
            "미국 동부(오하이오), 미국 동부(버지니아 북부), 미국 서부(오레곤), 아시아 태평양(싱가포르), 아시아 태평양(시드니), 아시아 태평양(도쿄), 유럽(프랑크푸르트), 유럽(아일랜드) 및 유럽(스톡홀름) 리전에서 Aurora MySQL-Compatible Edition for Aurora MySQL 3.05 버전(MySQL 8.0.32 호환) 이상에 대해 Amazon Redshift와 Aurora 제로 ETL 통합을 사용할 수 있습니다. Amazon Redshift와 Aurora의 제로 ETL 통합은 미국 동부(오하이오) 리전의 Aurora PostgreSQL 15.4용 Aurora PostgreSQL 호환 에디션에서 사용할 수 있습니다.\n",
            "\"category : Redshift, question : 제로 ETL 통합을 지원하는 Amazon Aurora의 엔진 및 버전은 무엇인가요?, answer : 미국 동부(오하이오), 미국 동부(버지니아 북부), 미국 서부(오레곤), 아시아 태평양(싱가포르), 아시아 태평양(시드니), 아시아 태평양(도쿄), 유럽(프랑크푸르트), 유럽(아일랜드) 및 유럽(스톡홀름) 리전에서 Aurora MySQL-Compatible Edition for Aurora MySQL 3.05 버전(MySQL 8.0.32 호환) 이상에 대해 Amazon Redshift와 Aurora 제로 ETL 통합을 사용할 수 있습니다. Amazon Redshift와 Aurora의 제로 ETL 통합은 미국 동부(오하이오) 리전의 Aurora PostgreSQL 15.4용 Aurora PostgreSQL 호환 에디션에서 사용할 수 있습니다.\"\n",
            "제로 ETL 통합의 이점은 무엇인가요?\n",
            "Amazon Redshift와 Aurora의 제로 ETL 통합은 복잡한 데이터 파이프라인을 구축하고 유지 관리해야 하는 필요성을 없애줍니다. 단일 또는 여러 Aurora 데이터베이스 클러스터의 데이터를 단일 Amazon Redshift 데이터베이스 클러스터로 통합하고, Amazon Redshift를 사용하여 Amazon Aurora의 페타바이트급 트랜잭션 데이터에 대해 거의 실시간 분석 및 ML을 실행할 수 있습니다.\n",
            "\"category : Redshift, question : 제로 ETL 통합의 이점은 무엇인가요?, answer : Amazon Redshift와 Aurora의 제로 ETL 통합은 복잡한 데이터 파이프라인을 구축하고 유지 관리해야 하는 필요성을 없애줍니다. 단일 또는 여러 Aurora 데이터베이스 클러스터의 데이터를 단일 Amazon Redshift 데이터베이스 클러스터로 통합하고, Amazon Redshift를 사용하여 Amazon Aurora의 페타바이트급 트랜잭션 데이터에 대해 거의 실시간 분석 및 ML을 실행할 수 있습니다.\"\n",
            "제로 ETL 통합은 Amazon Redshift Serverless와 호환되나요?\n",
            "Aurora와 Amazon Redshift의 제로 ETL 통합은 Amazon Redshift Serverless 및 Amazon Aurora Serverless v2와 호환됩니다. Aurora Serverless v2와 Amazon Redshift Serverless를 모두 사용하는 경우 데이터 파이프라인용 인프라를 관리할 필요 없이 트랜잭션 데이터에 대해 거의 실시간 분석을 생성할 수 있습니다.\n",
            "\"category : Redshift, question : 제로 ETL 통합은 Amazon Redshift Serverless와 호환되나요?, answer : Aurora와 Amazon Redshift의 제로 ETL 통합은 Amazon Redshift Serverless 및 Amazon Aurora Serverless v2와 호환됩니다. Aurora Serverless v2와 Amazon Redshift Serverless를 모두 사용하는 경우 데이터 파이프라인용 인프라를 관리할 필요 없이 트랜잭션 데이터에 대해 거의 실시간 분석을 생성할 수 있습니다.\"\n",
            "제로 ETL 통합을 시작하려면 어떻게 해야 하나요?\n",
            "Amazon RDS 콘솔에서 Aurora 소스와 Amazon Redshift 대상을 지정하여 제로 ETL 통합을 생성하고 시작할 수 있습니다. 통합이 생성되면 Aurora 데이터베이스가 Amazon Redshift에 복제되고, 초기 시드가 완료되면 데이터 쿼리를 시작할 수 있습니다. 자세한 내용은 시작 안내서에서 Aurora와 Amazon Redshift의 제로 ETL 통합에 대해 읽어보세요.\n",
            "\"category : Redshift, question : 제로 ETL 통합을 시작하려면 어떻게 해야 하나요?, answer : Amazon RDS 콘솔에서 Aurora 소스와 Amazon Redshift 대상을 지정하여 제로 ETL 통합을 생성하고 시작할 수 있습니다. 통합이 생성되면 Aurora 데이터베이스가 Amazon Redshift에 복제되고, 초기 시드가 완료되면 데이터 쿼리를 시작할 수 있습니다. 자세한 내용은 시작 안내서에서 Aurora와 Amazon Redshift의 제로 ETL 통합에 대해 읽어보세요.\"\n",
            "제로 ETL 통합에서 트랜잭션은 어떻게 처리되나요? 복제할 때 자동으로 커밋되나요?\n",
            "Aurora에서 Amazon Redshift로의 제로 ETL 통합은 트랜잭션을 원자적으로 복제하여 소스 Aurora 데이터베이스와 대상 Amazon Redshift 클러스터 간의 데이터 일관성을 보장합니다. 이 통합에서 트랜잭션의 원자성에 대한 몇 가지 요점은 다음과 같습니다.\n",
            "\n",
            "Aurora에서 커밋된 트랜잭션만 Amazon Redshift에 복제됩니다. 커밋되지 않았거나 롤백된 트랜잭션은 해당되지 않습니다.\n",
            "통합에서는 2단계 커밋 프로세스를 사용하여 각 트랜잭션을 Amazon Redshift에 원자적으로 적용합니다. 트랜잭션의 모든 데이터 변경 사항이 적용되거나, 오류가 발생하는 경우 아무 것도 적용되지 않습니다.\n",
            "소스와 대상 간에 트랜잭션 일관성이 유지됩니다. 복제 후에는 특정 트랜잭션에 대한 데이터가 Aurora와 Amazon Redshift에서 모두 일관되게 유지됩니다.\n",
            "DDL 또는 DML을 통한 스키마 변경도 무결성을 유지하기 위해 원자적으로 적용됩니다.\n",
            "트랜잭션의 원자적 적용은 데이터베이스 간에 부분 트랜잭션이나 일관되지 않은 데이터 상태가 발생하지 않도록 합니다.\n",
            "\"category : Redshift, question : 제로 ETL 통합에서 트랜잭션은 어떻게 처리되나요? 복제할 때 자동으로 커밋되나요?, answer : Aurora에서 Amazon Redshift로의 제로 ETL 통합은 트랜잭션을 원자적으로 복제하여 소스 Aurora 데이터베이스와 대상 Amazon Redshift 클러스터 간의 데이터 일관성을 보장합니다. 이 통합에서 트랜잭션의 원자성에 대한 몇 가지 요점은 다음과 같습니다.\n",
            "\n",
            "Aurora에서 커밋된 트랜잭션만 Amazon Redshift에 복제됩니다. 커밋되지 않았거나 롤백된 트랜잭션은 해당되지 않습니다.\n",
            "통합에서는 2단계 커밋 프로세스를 사용하여 각 트랜잭션을 Amazon Redshift에 원자적으로 적용합니다. 트랜잭션의 모든 데이터 변경 사항이 적용되거나, 오류가 발생하는 경우 아무 것도 적용되지 않습니다.\n",
            "소스와 대상 간에 트랜잭션 일관성이 유지됩니다. 복제 후에는 특정 트랜잭션에 대한 데이터가 Aurora와 Amazon Redshift에서 모두 일관되게 유지됩니다.\n",
            "DDL 또는 DML을 통한 스키마 변경도 무결성을 유지하기 위해 원자적으로 적용됩니다.\n",
            "트랜잭션의 원자적 적용은 데이터베이스 간에 부분 트랜잭션이나 일관되지 않은 데이터 상태가 발생하지 않도록 합니다.\"\n",
            "Aurora에서 변경한 내용은 어떤 순서로 Amazon Redshift에 복제되나요?\n",
            "Aurora와 Amazon Redshift의 제로 ETL 통합에서는 소스 Aurora 데이터베이스와 대상 Amazon Redshift 클러스터 간의 완전한 트랜잭션 일관성이 유지됩니다.\n",
            "\"category : Redshift, question : Aurora에서 변경한 내용은 어떤 순서로 Amazon Redshift에 복제되나요?, answer : Aurora와 Amazon Redshift의 제로 ETL 통합에서는 소스 Aurora 데이터베이스와 대상 Amazon Redshift 클러스터 간의 완전한 트랜잭션 일관성이 유지됩니다.\"\n",
            "제로 ETL 통합에서 스키마 변경은 어떻게 처리되나요?\n",
            "다음은 스키마 변경이 처리되는 방법에 대한 몇 가지 요점입니다.\n",
            "\n",
            "CREATE TABLE, ALTER TABLE, DROP TABLE과 같은 DDL 문은 Aurora에서 Amazon Redshift로 자동으로 복제됩니다.\n",
            "통합 중에 복제된 스키마 변경에 맞게 Amazon Redshift 테이블이 확인되고 조정됩니다. 예를 들어 Aurora에 열을 추가하면 Amazon Redshift에 해당 열이 추가됩니다.\n",
            "복제 및 스키마 동기화는 소스 데이터베이스와 대상 데이터베이스 간의 지연을 최소화하면서 실시간으로 자동으로 수행됩니다.\n",
            "DML 변경이 DDL 변경과 동시에 발생하는 경우에도 스키마 일관성이 유지됩니다.\n",
            "\"category : Redshift, question : 제로 ETL 통합에서 스키마 변경은 어떻게 처리되나요?, answer : 다음은 스키마 변경이 처리되는 방법에 대한 몇 가지 요점입니다.\n",
            "\n",
            "CREATE TABLE, ALTER TABLE, DROP TABLE과 같은 DDL 문은 Aurora에서 Amazon Redshift로 자동으로 복제됩니다.\n",
            "통합 중에 복제된 스키마 변경에 맞게 Amazon Redshift 테이블이 확인되고 조정됩니다. 예를 들어 Aurora에 열을 추가하면 Amazon Redshift에 해당 열이 추가됩니다.\n",
            "복제 및 스키마 동기화는 소스 데이터베이스와 대상 데이터베이스 간의 지연을 최소화하면서 실시간으로 자동으로 수행됩니다.\n",
            "DML 변경이 DDL 변경과 동시에 발생하는 경우에도 스키마 일관성이 유지됩니다.\"\n",
            "제로 ETL을 사용하여 데이터에서 변환을 실행하려면 어떻게 해야 하나요?\n",
            "로컬 Amazon Redshift 데이터베이스에서 구체화된 뷰를 생성하여 제로 ETL 통합을 통해 복제된 데이터를 변환할 수 있습니다. 로컬 데이터베이스에 연결하고 데이터베이스 간 쿼리를 사용하여 대상 데이터베이스에 액세스합니다. 세 부분으로 구성된 표기법(destination-database-name.schema-name.table-name)을 사용하여 정규화된 객체 이름을 사용하거나, 대상 데이터베이스 및 스키마 페어를 참조하는 외부 스키마를 생성하고 두 부분으로 구성된 표기법(external-schema-name.table-name)을 사용할 수 있습니다.\n",
            "\"category : Redshift, question : 제로 ETL을 사용하여 데이터에서 변환을 실행하려면 어떻게 해야 하나요?, answer : 로컬 Amazon Redshift 데이터베이스에서 구체화된 뷰를 생성하여 제로 ETL 통합을 통해 복제된 데이터를 변환할 수 있습니다. 로컬 데이터베이스에 연결하고 데이터베이스 간 쿼리를 사용하여 대상 데이터베이스에 액세스합니다. 세 부분으로 구성된 표기법(destination-database-name.schema-name.table-name)을 사용하여 정규화된 객체 이름을 사용하거나, 대상 데이터베이스 및 스키마 페어를 참조하는 외부 스키마를 생성하고 두 부분으로 구성된 표기법(external-schema-name.table-name)을 사용할 수 있습니다.\"\n",
            "제로 ETL 통합 비용은 얼마인가요?\n",
            "제로 ETL 및 지속적인 데이터 변경 처리는 추가 비용 없이 제공됩니다. 제로 ETL 통합을 생성하고 그 일부로 생성되는 변경 데이터를 처리하는 데 사용한 기존 Amazon RDS 및 Amazon Redshift 리소스에 대한 요금이 부과됩니다. 이러한 리소스에는 다음이 포함될 수 있습니다.\n",
            "\n",
            "향상된 binlog를 활성화하여 사용된 추가 I/O 및 스토리지\n",
            "Amazon Redshift 데이터베이스를 시드하기 위한 초기 데이터 내보내기에 드는 스냅샷 내보내기 비용\n",
            "복제된 데이터를 저장하기 위한 추가 Amazon Redshift 스토리지\n",
            "소스에서 대상으로 데이터를 이동하는 데 드는 AZ 사이의 데이터 전송 비용\n",
            "\n",
            "자세한 내용은 Aurora 요금 페이지를 참조하세요.\n",
            "\"category : Redshift, question : 제로 ETL 통합 비용은 얼마인가요?, answer : 제로 ETL 및 지속적인 데이터 변경 처리는 추가 비용 없이 제공됩니다. 제로 ETL 통합을 생성하고 그 일부로 생성되는 변경 데이터를 처리하는 데 사용한 기존 Amazon RDS 및 Amazon Redshift 리소스에 대한 요금이 부과됩니다. 이러한 리소스에는 다음이 포함될 수 있습니다.\n",
            "\n",
            "향상된 binlog를 활성화하여 사용된 추가 I/O 및 스토리지\n",
            "Amazon Redshift 데이터베이스를 시드하기 위한 초기 데이터 내보내기에 드는 스냅샷 내보내기 비용\n",
            "복제된 데이터를 저장하기 위한 추가 Amazon Redshift 스토리지\n",
            "소스에서 대상으로 데이터를 이동하는 데 드는 AZ 사이의 데이터 전송 비용\n",
            "\n",
            "자세한 내용은 Aurora 요금 페이지를 참조하세요.\"\n",
            "Amazon Redshift는 내 데이터를 어떻게 백업하나요? 백업에서 클러스터를 복원하려면 어떻게 해야 합니까?\n",
            "Amazon Redshift RA3 클러스터와 Amazon Redshift Serverless는 Redshift 관리형 스토리지를 사용하여 가용한 데이터의 최신 복사본을 항시 유지합니다. DS2 및 DC2 클러스터는 장애 발생 시 최신 복사본을 이용할 수 있도록 클러스터의 데이터를 미러링합니다. 백업은 모든 Redshift 클러스터 유형에서 자동으로 생성되어 24시간 유지되며, 지난 24시간에 대한 서버리스 복구 지점이 제공됩니다.\n",
            "무기한 보관할 수 있는 자체 백업을 생성할 수도 있습니다. 이러한 백업은 언제든 생성할 수 있으며, 장기 보관을 위해 Amazon Redshift 자동 백업 또는 Amazon Redshift Serverless 복구 지점을 사용자 백업으로 변환할 수 있습니다.\n",
            "또한, Amazon Redshift는 재해 복구를 위해 스냅샷 또는 복구 지점을 다른 리전의 Amazon S3에 비동기적으로 복제할 수 있습니다.\n",
            "DS2 또는 DC2 클러스터에서 무료 백업 스토리지는 데이터 웨어하우스 클러스터의 노드 전체 스토리지 크기로 제한되며 활성 데이터 웨어하우스 클러스터에만 적용됩니다.\n",
            "예를 들어, 총 8TB의 데이터 웨어하우스 스토리지가 있는 경우 추가 요금 없이 최대 8TB의 백업 스토리지가 제공됩니다. 백업 보존 기간을 1일보다 길게 설정하려는 경우 AWS Management Console 또는 Amazon Redshift API를 사용하여 해당 작업을 수행할 수 있습니다. 자동 스냅샷에 대한 자세한 내용은 Amazon Redshift 관리 안내서를 참조하세요.\n",
            "Amazon Redshift는 변경된 데이터만 백업하므로 대부분 스냅샷은 적은 양의 무료 백업 스토리지만 사용합니다. 백업을 복원해야 하는 경우, 백업 보존 기간 내 모든 자동 백업에 액세스할 수 있습니다. 복원할 백업을 선택하면 새로운 데이터 웨어하우스 클러스터가 프로비저닝되고 데이터가 해당 데이터 웨어하우스 클러스터로 복구됩니다.\n",
            "\"category : Redshift, question : Amazon Redshift는 내 데이터를 어떻게 백업하나요? 백업에서 클러스터를 복원하려면 어떻게 해야 합니까?, answer : Amazon Redshift RA3 클러스터와 Amazon Redshift Serverless는 Redshift 관리형 스토리지를 사용하여 가용한 데이터의 최신 복사본을 항시 유지합니다. DS2 및 DC2 클러스터는 장애 발생 시 최신 복사본을 이용할 수 있도록 클러스터의 데이터를 미러링합니다. 백업은 모든 Redshift 클러스터 유형에서 자동으로 생성되어 24시간 유지되며, 지난 24시간에 대한 서버리스 복구 지점이 제공됩니다.\n",
            "무기한 보관할 수 있는 자체 백업을 생성할 수도 있습니다. 이러한 백업은 언제든 생성할 수 있으며, 장기 보관을 위해 Amazon Redshift 자동 백업 또는 Amazon Redshift Serverless 복구 지점을 사용자 백업으로 변환할 수 있습니다.\n",
            "또한, Amazon Redshift는 재해 복구를 위해 스냅샷 또는 복구 지점을 다른 리전의 Amazon S3에 비동기적으로 복제할 수 있습니다.\n",
            "DS2 또는 DC2 클러스터에서 무료 백업 스토리지는 데이터 웨어하우스 클러스터의 노드 전체 스토리지 크기로 제한되며 활성 데이터 웨어하우스 클러스터에만 적용됩니다.\n",
            "예를 들어, 총 8TB의 데이터 웨어하우스 스토리지가 있는 경우 추가 요금 없이 최대 8TB의 백업 스토리지가 제공됩니다. 백업 보존 기간을 1일보다 길게 설정하려는 경우 AWS Management Console 또는 Amazon Redshift API를 사용하여 해당 작업을 수행할 수 있습니다. 자동 스냅샷에 대한 자세한 내용은 Amazon Redshift 관리 안내서를 참조하세요.\n",
            "Amazon Redshift는 변경된 데이터만 백업하므로 대부분 스냅샷은 적은 양의 무료 백업 스토리지만 사용합니다. 백업을 복원해야 하는 경우, 백업 보존 기간 내 모든 자동 백업에 액세스할 수 있습니다. 복원할 백업을 선택하면 새로운 데이터 웨어하우스 클러스터가 프로비저닝되고 데이터가 해당 데이터 웨어하우스 클러스터로 복구됩니다.\"\n",
            "자동 백업 및 스냅샷의 보존 기간을 관리하려면 어떻게 해야 하나요?\n",
            "AWS Management Console 또는 ModifyCluster API를 사용하여 RetentionPeriod 파라미터를 수정하면 자동 백업 보존 기간을 관리할 수 있습니다. 자동 백업을 완전히 비활성화하려는 경우 보존 기간을 0으로 설정하면 됩니다(권장하지 않음).\n",
            "\"category : Redshift, question : 자동 백업 및 스냅샷의 보존 기간을 관리하려면 어떻게 해야 하나요?, answer : AWS Management Console 또는 ModifyCluster API를 사용하여 RetentionPeriod 파라미터를 수정하면 자동 백업 보존 기간을 관리할 수 있습니다. 자동 백업을 완전히 비활성화하려는 경우 보존 기간을 0으로 설정하면 됩니다(권장하지 않음).\"\n",
            "데이터 웨어하우스 클러스터를 삭제하는 경우 백업은 어떻게 되나요?\n",
            "데이터 웨어하우스 클러스터를 삭제할 때 삭제 시 최종 스냅샷을 생성할지 여부를 지정할 수 있습니다. 이렇게 하면 나중에 삭제된 데이터 웨어하우스 클러스터를 복원할 수 있습니다. 이전에 생성한 데이터 웨어하우스 클러스터의 모든 수동 스냅샷은 삭제하도록 선택하지 않는 한 유지되며 표준 Amazon S3 요금이 청구됩니다.\n",
            "\"category : Redshift, question : 데이터 웨어하우스 클러스터를 삭제하는 경우 백업은 어떻게 되나요?, answer : 데이터 웨어하우스 클러스터를 삭제할 때 삭제 시 최종 스냅샷을 생성할지 여부를 지정할 수 있습니다. 이렇게 하면 나중에 삭제된 데이터 웨어하우스 클러스터를 복원할 수 있습니다. 이전에 생성한 데이터 웨어하우스 클러스터의 모든 수동 스냅샷은 삭제하도록 선택하지 않는 한 유지되며 표준 Amazon S3 요금이 청구됩니다.\"\n",
            "Amazon Redshift 데이터 웨어하우스 클러스터의 성능을 모니터링하려면 어떻게 해야 하나요?\n",
            "Amazon Redshift 데이터 웨어하우스 클러스터의 컴퓨팅 사용률, 스토리지 사용률 및 읽기/쓰기 트래픽에 대한 지표는 AWS Management Console 또는 Amazon CloudWatch API를 통해 무료로 확인할 수 있습니다. 또한, Amazon CloudWatch의 사용자 지정 지표 기능을 사용하여 추가적인 사용자 정의 지표를 추가할 수 있습니다. AWS Management Console에서는 모든 클러스터의 상태와 성능을 모니터링할 수 있는 모니터링 대시보드를 제공합니다. Amazon Redshift는 AWS Management Console을 통해 쿼리 및 클러스터 성능 정보도 제공합니다. 이 정보를 사용하면 가장 많은 시스템 리소스를 소비하고 있는 사용자 및 쿼리를 확인할 수 있고 쿼리 계획 및 실행 통계를 확인하여 성능 문제를 진단할 수 있습니다. 또한 각 컴퓨팅 노드의 리소스 사용량을 확인하여 모든 노드에서 데이터 및 쿼리의 로드가 균등하게 분배되고 있는지 확인할 수 있습니다.\n",
            "\"category : Redshift, question : Amazon Redshift 데이터 웨어하우스 클러스터의 성능을 모니터링하려면 어떻게 해야 하나요?, answer : Amazon Redshift 데이터 웨어하우스 클러스터의 컴퓨팅 사용률, 스토리지 사용률 및 읽기/쓰기 트래픽에 대한 지표는 AWS Management Console 또는 Amazon CloudWatch API를 통해 무료로 확인할 수 있습니다. 또한, Amazon CloudWatch의 사용자 지정 지표 기능을 사용하여 추가적인 사용자 정의 지표를 추가할 수 있습니다. AWS Management Console에서는 모든 클러스터의 상태와 성능을 모니터링할 수 있는 모니터링 대시보드를 제공합니다. Amazon Redshift는 AWS Management Console을 통해 쿼리 및 클러스터 성능 정보도 제공합니다. 이 정보를 사용하면 가장 많은 시스템 리소스를 소비하고 있는 사용자 및 쿼리를 확인할 수 있고 쿼리 계획 및 실행 통계를 확인하여 성능 문제를 진단할 수 있습니다. 또한 각 컴퓨팅 노드의 리소스 사용량을 확인하여 모든 노드에서 데이터 및 쿼리의 로드가 균등하게 분배되고 있는지 확인할 수 있습니다.\"\n",
            "유지 관리 기간이란 무엇입니까? 소프트웨어를 유지 관리하는 동안 데이터 웨어하우스 클러스터를 사용할 수 있나요?\n",
            "Amazon Redshift에서는 수정 사항, 개선 사항 및 새로운 기능을 클러스터에 적용하기 위해 정기적으로 유지 관리를 수행합니다. 프로그래밍 방식으로 또는 Redshift 콘솔을 사용하여 클러스터를 수정하여 예약된 유지 관리 기간을 변경할 수 있습니다. 이러한 유지 관리 기간에는 Amazon Redshift 클러스터가 일반 작업에 사용되지 않습니다. 리전별 유지 관리 기간과 일정에 대한 자세한 내용은 Amazon Redshift 관리 안내서의 유지 관리 기간을 참조하세요.\n",
            "\"category : Redshift, question : 유지 관리 기간이란 무엇입니까? 소프트웨어를 유지 관리하는 동안 데이터 웨어하우스 클러스터를 사용할 수 있나요?, answer : Amazon Redshift에서는 수정 사항, 개선 사항 및 새로운 기능을 클러스터에 적용하기 위해 정기적으로 유지 관리를 수행합니다. 프로그래밍 방식으로 또는 Redshift 콘솔을 사용하여 클러스터를 수정하여 예약된 유지 관리 기간을 변경할 수 있습니다. 이러한 유지 관리 기간에는 Amazon Redshift 클러스터가 일반 작업에 사용되지 않습니다. 리전별 유지 관리 기간과 일정에 대한 자세한 내용은 Amazon Redshift 관리 안내서의 유지 관리 기간을 참조하세요.\"\n",
            "========== DocumentDB  :  https://aws.amazon.com/ko/documentdb/faqs/ 사이트 크롤링 진행중 ==========\n",
            "76\n",
            "Amazon DocumentDB(MongoDB 호환)란 어떤 서비스인가요?\n",
            "Amazon DocumentDB(MongoDB 호환)는 네이티브 JSON 워크로드를 지원하는 빠르고 확장 가능하며, 가용성이 뛰어난 완전관리형 엔터프라이즈 도큐먼트 데이터베이스 서비스입니다. Amazon DocumentDB 도큐먼트 데이터베이스를 사용하면 JSON 데이터를 손쉽게 저장, 쿼리 및 인덱싱할 수 있습니다. 개발자는 Amazon DocumentDB에서 워크로드를 실행, 관리 및 크기 조정할 때 사용하는 것과 동일한 MongoDB 애플리케이션 코드, 드라이버 및 도구를 사용할 수 있습니다. 기본 인프라 관리에 대한 우려 없이 개선된 성능, 확장성 및 가용성을 활용하세요.\n",
            "고객은 AWS Database Migration Service(DMS)를 사용하여 사실상 가동 중단 없이 온프레미스 또는 Amazon Elastic Compute Cloud(EC2) MongoDB 비관계형 데이터베이스를 Amazon DocumentDB로 손쉽게 마이그레이션할 수 있습니다. Amazon DocumentDB는 선불 투자 없이 사용할 수 있으며 사용한 용량에 대해서만 요금을 지불하면 됩니다.\n",
            "\"category : DocumentDB, question : Amazon DocumentDB(MongoDB 호환)란 어떤 서비스인가요?, answer : Amazon DocumentDB(MongoDB 호환)는 네이티브 JSON 워크로드를 지원하는 빠르고 확장 가능하며, 가용성이 뛰어난 완전관리형 엔터프라이즈 도큐먼트 데이터베이스 서비스입니다. Amazon DocumentDB 도큐먼트 데이터베이스를 사용하면 JSON 데이터를 손쉽게 저장, 쿼리 및 인덱싱할 수 있습니다. 개발자는 Amazon DocumentDB에서 워크로드를 실행, 관리 및 크기 조정할 때 사용하는 것과 동일한 MongoDB 애플리케이션 코드, 드라이버 및 도구를 사용할 수 있습니다. 기본 인프라 관리에 대한 우려 없이 개선된 성능, 확장성 및 가용성을 활용하세요.\n",
            "고객은 AWS Database Migration Service(DMS)를 사용하여 사실상 가동 중단 없이 온프레미스 또는 Amazon Elastic Compute Cloud(EC2) MongoDB 비관계형 데이터베이스를 Amazon DocumentDB로 손쉽게 마이그레이션할 수 있습니다. Amazon DocumentDB는 선불 투자 없이 사용할 수 있으며 사용한 용량에 대해서만 요금을 지불하면 됩니다.\"\n",
            "Amazon DocumentDB와 같은 도큐먼트 데이터베이스에는 어떤 사용 사례가 적합하나요?\n",
            "문서 지향 데이터베이스는 가장 빠르게 성장하고 있는  NoSQL  데이터베이스 범주 중 하나입니다. 주된 이유는 도큐먼트 데이터베이스가 유연한 스키마와 광범위한 쿼리 기능을 모두 제공하기 때문입니다. 문서 모델은 임시 쿼리, 인덱싱 및 집계가 필요한 동적 데이터 세트가 포함된 사용 사례에 탁월한 선택입니다. Amazon DocumentDB는 이 서비스가 제공하는 규모 덕에 다양한 고객의 콘텐츠 관리, 개인화, 카탈로그, 모바일 및 웹 애플리케이션, IoT 및 프로필 관리와 같은 사용 사례에 사용됩니다.\n",
            "\"category : DocumentDB, question : Amazon DocumentDB와 같은 도큐먼트 데이터베이스에는 어떤 사용 사례가 적합하나요?, answer : 문서 지향 데이터베이스는 가장 빠르게 성장하고 있는  NoSQL  데이터베이스 범주 중 하나입니다. 주된 이유는 도큐먼트 데이터베이스가 유연한 스키마와 광범위한 쿼리 기능을 모두 제공하기 때문입니다. 문서 모델은 임시 쿼리, 인덱싱 및 집계가 필요한 동적 데이터 세트가 포함된 사용 사례에 탁월한 선택입니다. Amazon DocumentDB는 이 서비스가 제공하는 규모 덕에 다양한 고객의 콘텐츠 관리, 개인화, 카탈로그, 모바일 및 웹 애플리케이션, IoT 및 프로필 관리와 같은 사용 사례에 사용됩니다.\"\n",
            "‘MongoDB 호환’은 무엇을 의미하나요?\n",
            "‘MongoDB 호환’이란 Amazon DocumentDB가 Apache 2.0 오픈 소스 MongoDB 3.6, 4.0 및 5.0 API와 상호 작용을 주고받는다는 뜻입니다. 따라서 Amazon DocumentDB에서 동일한 MongoDB 드라이버, 애플리케이션 및 도구를 거의 또는 전혀 변경하지 않고 사용할 수 있습니다. Amazon DocumentDB는 고객이 실제로 사용하는 MongoDB API 중 대다수를 지원하지만, MongoDB API라면 모두 지원하는 것은 아닙니다. 그간 AWS에서는 고객이 실제로 사용하고 필요로 하는 기능을 제공하는 데 주안점을 두었습니다.\n",
            "출시 이후, 종점인 고객을 출발점으로 삼아 거꾸로 작업하여 추가로 80여 가지 기능을 제공해왔으며, MongoDB 4.0 및 5.0 호환, 트랜잭션 및 샤딩이 좋은 예입니다. 지원되는 MongoDB API에 관한 자세한 내용은 호환 설명서를 참조하세요. 최근 출시된 Amazon DocumentDB 제품에 관한 자세한 내용은 Amazon DocumentDB 리소스 페이지에 있는 ‘Amazon DocumentDB 공지 사항’에서 확인하실 수 있습니다.\n",
            "\"category : DocumentDB, question : ‘MongoDB 호환’은 무엇을 의미하나요?, answer : ‘MongoDB 호환’이란 Amazon DocumentDB가 Apache 2.0 오픈 소스 MongoDB 3.6, 4.0 및 5.0 API와 상호 작용을 주고받는다는 뜻입니다. 따라서 Amazon DocumentDB에서 동일한 MongoDB 드라이버, 애플리케이션 및 도구를 거의 또는 전혀 변경하지 않고 사용할 수 있습니다. Amazon DocumentDB는 고객이 실제로 사용하는 MongoDB API 중 대다수를 지원하지만, MongoDB API라면 모두 지원하는 것은 아닙니다. 그간 AWS에서는 고객이 실제로 사용하고 필요로 하는 기능을 제공하는 데 주안점을 두었습니다.\n",
            "출시 이후, 종점인 고객을 출발점으로 삼아 거꾸로 작업하여 추가로 80여 가지 기능을 제공해왔으며, MongoDB 4.0 및 5.0 호환, 트랜잭션 및 샤딩이 좋은 예입니다. 지원되는 MongoDB API에 관한 자세한 내용은 호환 설명서를 참조하세요. 최근 출시된 Amazon DocumentDB 제품에 관한 자세한 내용은 Amazon DocumentDB 리소스 페이지에 있는 ‘Amazon DocumentDB 공지 사항’에서 확인하실 수 있습니다.\"\n",
            "Amazon DocumentDB는 MongoDB SSPL 라이선스로 제한되나요?\n",
            "아니요. Amazon DocumentDB는 MongoDB SSPL 코드를 활용하지 않으므로 이 라이선스로 인한 제한을 받지 않습니다. 대신, Amazon DocumentDB는 Apache 2.0 오픈 소스 MongoDB 3.6, 4.0 및 5.0 API와 상호 작용을 주고받습니다. AWS에서는 앞으로도 고객의 의견에 귀를 기울이고 고객에게서 역방향으로 작업하여 고객에게 필요한 기능을 제공하고자 합니다. 지원되는 MongoDB API에 관한 자세한 내용은 호환 설명서를 참조하세요. 최근 출시된 Amazon DocumentDB 제품에 관한 자세한 내용은 Amazon DocumentDB 리소스 페이지에 있는 ‘Amazon DocumentDB 공지 사항’에서 확인하실 수 있습니다.\n",
            "\"category : DocumentDB, question : Amazon DocumentDB는 MongoDB SSPL 라이선스로 제한되나요?, answer : 아니요. Amazon DocumentDB는 MongoDB SSPL 코드를 활용하지 않으므로 이 라이선스로 인한 제한을 받지 않습니다. 대신, Amazon DocumentDB는 Apache 2.0 오픈 소스 MongoDB 3.6, 4.0 및 5.0 API와 상호 작용을 주고받습니다. AWS에서는 앞으로도 고객의 의견에 귀를 기울이고 고객에게서 역방향으로 작업하여 고객에게 필요한 기능을 제공하고자 합니다. 지원되는 MongoDB API에 관한 자세한 내용은 호환 설명서를 참조하세요. 최근 출시된 Amazon DocumentDB 제품에 관한 자세한 내용은 Amazon DocumentDB 리소스 페이지에 있는 ‘Amazon DocumentDB 공지 사항’에서 확인하실 수 있습니다.\"\n",
            "기존 MongoDB 데이터베이스에서 Amazon DocumentDB로 데이터를 마이그레이션하려면 어떻게 해야 하나요?\n",
            "AWS Database Migration Service(DMS)를 사용하면 사실상 가동 중단 없이 온프레미스 또는 Amazon Elastic Compute Cloud(EC2) MongoDB 데이터베이스를 Amazon DocumentDB로 손쉽게 마이그레이션할 수 있습니다. DMS를 사용하면 MongoDB 복제본 세트 또는 샤딩된 클러스터에서 Amazon DocumentDB로 마이그레이션할 수 있습니다. 또한 mongodump/mongorestore, mongoexport/mongoimport, 그리고 oplog를 통해 변경 데이터 캡처(CDC)를 지원하는 서드 파티 도구를 비롯해 대부분의 기존 도구를 사용하여 MongoDB 데이터베이스에서 Amazon DocumentDB로 데이터를 마이그레이션할 수 있습니다. 자세한 내용은 Amazon DocumentDB로 마이그레이션을 참조하세요.\n",
            "\"category : DocumentDB, question : 기존 MongoDB 데이터베이스에서 Amazon DocumentDB로 데이터를 마이그레이션하려면 어떻게 해야 하나요?, answer : AWS Database Migration Service(DMS)를 사용하면 사실상 가동 중단 없이 온프레미스 또는 Amazon Elastic Compute Cloud(EC2) MongoDB 데이터베이스를 Amazon DocumentDB로 손쉽게 마이그레이션할 수 있습니다. DMS를 사용하면 MongoDB 복제본 세트 또는 샤딩된 클러스터에서 Amazon DocumentDB로 마이그레이션할 수 있습니다. 또한 mongodump/mongorestore, mongoexport/mongoimport, 그리고 oplog를 통해 변경 데이터 캡처(CDC)를 지원하는 서드 파티 도구를 비롯해 대부분의 기존 도구를 사용하여 MongoDB 데이터베이스에서 Amazon DocumentDB로 데이터를 마이그레이션할 수 있습니다. 자세한 내용은 Amazon DocumentDB로 마이그레이션을 참조하세요.\"\n",
            "Amazon DocumentDB를 사용하려면 클라이언트 드라이버를 변경해야 하나요?\n",
            "아니요. Amazon DocumentDB는 MongoDB 3.4 이상과 호환되는 대부분의 MongoDB 드라이버와 연동됩니다.\n",
            "\"category : DocumentDB, question : Amazon DocumentDB를 사용하려면 클라이언트 드라이버를 변경해야 하나요?, answer : 아니요. Amazon DocumentDB는 MongoDB 3.4 이상과 호환되는 대부분의 MongoDB 드라이버와 연동됩니다.\"\n",
            "Amazon DocumentDB는 ACID 트랜잭션을 지원하나요?\n",
            "예. 이제 MongoDB 4.0 호환성 지원이 출시되면서 Amazon DocumentDB는 여러 문서, 명령문, 컬렉션 및 데이터베이스에서 원자성, 일관성, 격리, 내구성(ACID) 트랜잭션을 수행하는 있는 기능을 지원합니다.\n",
            "\"category : DocumentDB, question : Amazon DocumentDB는 ACID 트랜잭션을 지원하나요?, answer : 예. 이제 MongoDB 4.0 호환성 지원이 출시되면서 Amazon DocumentDB는 여러 문서, 명령문, 컬렉션 및 데이터베이스에서 원자성, 일관성, 격리, 내구성(ACID) 트랜잭션을 수행하는 있는 기능을 지원합니다.\"\n",
            "Amazon DocumentDB는 MongoDB의 지원 종료(EOL) 일정을 따르나요?\n",
            "아니요. Amazon DocumentDB는 MongoDB의 동일한 지원 수명 주기를 따르지 않으며 MongoDB의 EOL 일정은 Amazon DocumentDB에 적용되지 않습니다.\n",
            "\"category : DocumentDB, question : Amazon DocumentDB는 MongoDB의 지원 종료(EOL) 일정을 따르나요?, answer : 아니요. Amazon DocumentDB는 MongoDB의 동일한 지원 수명 주기를 따르지 않으며 MongoDB의 EOL 일정은 Amazon DocumentDB에 적용되지 않습니다.\"\n",
            "내 Amazon DocumentDB 클러스터에 액세스하려면 어떻게 해야 합니까?\n",
            "Amazon DocumentDB 클러스터는 고객의 Amazon VPC(VPC) 내에 배포되며, 동일한 VPC 내에 배포된 Amazon Elastic Compute Cloud(EC2) 인스턴스 또는 다른 AWS 서비스에서 직접 액세스할 수 있습니다. 또한, Amazon DocumentDB는 동일한 리전이나 기타 리전의 서로 다른 VPC에 있는 Amazon EC2 인스턴스 또는 기타 AWS 서비스에서 VPC 피어링을 통해서도 액세스할 수 있습니다. Amazon DocumentDB 클러스터 대한 액세스는 mongo shell 또는 MongoDB 드라이버를 통해 수행되어야 합니다. Amazon DocumentDB를 사용하려면 클러스터에 연결할 때 인증 과정을 거쳐야 합니다. 추가 옵션은 Amazon VPC 외부에서 Amazon DocumentDB 클러스터에 연결을 참조하세요.\n",
            "\"category : DocumentDB, question : 내 Amazon DocumentDB 클러스터에 액세스하려면 어떻게 해야 합니까?, answer : Amazon DocumentDB 클러스터는 고객의 Amazon VPC(VPC) 내에 배포되며, 동일한 VPC 내에 배포된 Amazon Elastic Compute Cloud(EC2) 인스턴스 또는 다른 AWS 서비스에서 직접 액세스할 수 있습니다. 또한, Amazon DocumentDB는 동일한 리전이나 기타 리전의 서로 다른 VPC에 있는 Amazon EC2 인스턴스 또는 기타 AWS 서비스에서 VPC 피어링을 통해서도 액세스할 수 있습니다. Amazon DocumentDB 클러스터 대한 액세스는 mongo shell 또는 MongoDB 드라이버를 통해 수행되어야 합니다. Amazon DocumentDB를 사용하려면 클러스터에 연결할 때 인증 과정을 거쳐야 합니다. 추가 옵션은 Amazon VPC 외부에서 Amazon DocumentDB 클러스터에 연결을 참조하세요.\"\n",
            "Amazon DocumentDB를 사용하기 위해 Amazon RDS 권한 및 리소스가 필요한 이유는 무엇입니까?\n",
            "인스턴스 수명 주기 관리, Amazon Key Management Service(KMS)를 사용한 저장 중 암호화, 보안 그룹 관리와 같은 특정 관리 기능을 위해 Amazon DocumentDB는 Amazon Relational Database Service(RDS) 및 Amazon Neptune과 공유되는 운영 기술을 활용합니다. describe-db-instances 및 describe-db-clusters AWS CLI API를 사용하는 경우 \"--filter Name=engine,Values=docdb\" 파라미터를 사용하여 Amazon DocumentDB 리소스를 필터링하는 것이 좋습니다.\n",
            "\"category : DocumentDB, question : Amazon DocumentDB를 사용하기 위해 Amazon RDS 권한 및 리소스가 필요한 이유는 무엇입니까?, answer : 인스턴스 수명 주기 관리, Amazon Key Management Service(KMS)를 사용한 저장 중 암호화, 보안 그룹 관리와 같은 특정 관리 기능을 위해 Amazon DocumentDB는 Amazon Relational Database Service(RDS) 및 Amazon Neptune과 공유되는 운영 기술을 활용합니다. describe-db-instances 및 describe-db-clusters AWS CLI API를 사용하는 경우 \"--filter Name=engine,Values=docdb\" 파라미터를 사용하여 Amazon DocumentDB 리소스를 필터링하는 것이 좋습니다.\"\n",
            "Amazon DocumentDB에서는 어떤 인스턴스 유형을 제공하나요?\n",
            "리전별로 사용 가능한 인스턴스 유형에 대한 최신 정보는 Amazon DocumentDB 요금 페이지를 참조하세요.\n",
            "\"category : DocumentDB, question : Amazon DocumentDB에서는 어떤 인스턴스 유형을 제공하나요?, answer : 리전별로 사용 가능한 인스턴스 유형에 대한 최신 정보는 Amazon DocumentDB 요금 페이지를 참조하세요.\"\n",
            "Amazon DocumentDB를 사용해 보려면 어떻게 해야 하나요?\n",
            "Amazon DocumentDB를 사용해 보려면 시작 가이드를 참조하세요.\n",
            "\"category : DocumentDB, question : Amazon DocumentDB를 사용해 보려면 어떻게 해야 하나요?, answer : Amazon DocumentDB를 사용해 보려면 시작 가이드를 참조하세요.\"\n",
            "Amazon DocumentDB에는 SLA가 있습니까?\n",
            "예. 자세한 내용은 Amazon DocumentDB(MongoDB 호환) 서비스 수준 계약을 참조하세요.\n",
            "\"category : DocumentDB, question : Amazon DocumentDB에는 SLA가 있습니까?, answer : 예. 자세한 내용은 Amazon DocumentDB(MongoDB 호환) 서비스 수준 계약을 참조하세요.\"\n",
            "Amazon DocumentDB에서 어떤 유형의 성능을 기대할 수 있습니까?\n",
            "스토리지에 작성할 때 Amazon DocumentDB는 미리 쓰기 로그만 유지하며, 전체 버퍼 페이지 동기화를 작성할 필요가 없습니다. 안정성을 저하하지 않는 이러한 최적화 덕분에 Amazon DocumentDB 쓰기 속도는 일반적으로 기존 데이터베이스보다 더 빠릅니다. Amazon DocumentDB 클러스터는 최대 15개의 읽기 전용 복제본을 통해 초당 수백만 건의 읽기를 처리할 수 있는 규모로 스케일 아웃할 수 있습니다.\n",
            "\"category : DocumentDB, question : Amazon DocumentDB에서 어떤 유형의 성능을 기대할 수 있습니까?, answer : 스토리지에 작성할 때 Amazon DocumentDB는 미리 쓰기 로그만 유지하며, 전체 버퍼 페이지 동기화를 작성할 필요가 없습니다. 안정성을 저하하지 않는 이러한 최적화 덕분에 Amazon DocumentDB 쓰기 속도는 일반적으로 기존 데이터베이스보다 더 빠릅니다. Amazon DocumentDB 클러스터는 최대 15개의 읽기 전용 복제본을 통해 초당 수백만 건의 읽기를 처리할 수 있는 규모로 스케일 아웃할 수 있습니다.\"\n",
            "Amazon DocumentDB 사용 요금은 얼마이고 Amazon DocumentDB를 사용할 수 있는 AWS 리전은 어디인가요?\n",
            "리전과 요금에 대한 최신 정보는 Amazon DocumentDB 요금 페이지를 참조하세요.\n",
            "\"category : DocumentDB, question : Amazon DocumentDB 사용 요금은 얼마이고 Amazon DocumentDB를 사용할 수 있는 AWS 리전은 어디인가요?, answer : 리전과 요금에 대한 최신 정보는 Amazon DocumentDB 요금 페이지를 참조하세요.\"\n",
            "Amazon DocumentDB에 무료로 시작할 수 있는 프리 티어가 있나요?\n",
            "예. 1개월 무료 평가판을 사용하여 Amazon DocumentDB를 무료로 사용해 볼 수 있습니다. 이전에 Amazon DocumentDB를 사용한 적이 없다면 1개월 무료 평가판을 사용할 수 있습니다. 월별 750시간의 t3.medium 인스턴스 사용량, 3천만 단위의 IO, 5GB의 스토리지, 5GB의 백업 스토리지를 30일 동안 무료로 이용할 수 있습니다. 1개월의 무료 평가판이 만료되거나 사용량이 무료 허용량을 초과하면 요금이 발생하지 않도록 클러스터를 종료하거나 표준 온디맨드 요금으로 계속 실행할 수 있습니다. 자세히 알아보려면 DocumentDB 무료 평가판 페이지를 참조하세요.\n",
            "\"category : DocumentDB, question : Amazon DocumentDB에 무료로 시작할 수 있는 프리 티어가 있나요?, answer : 예. 1개월 무료 평가판을 사용하여 Amazon DocumentDB를 무료로 사용해 볼 수 있습니다. 이전에 Amazon DocumentDB를 사용한 적이 없다면 1개월 무료 평가판을 사용할 수 있습니다. 월별 750시간의 t3.medium 인스턴스 사용량, 3천만 단위의 IO, 5GB의 스토리지, 5GB의 백업 스토리지를 30일 동안 무료로 이용할 수 있습니다. 1개월의 무료 평가판이 만료되거나 사용량이 무료 허용량을 초과하면 요금이 발생하지 않도록 클러스터를 종료하거나 표준 온디맨드 요금으로 계속 실행할 수 있습니다. 자세히 알아보려면 DocumentDB 무료 평가판 페이지를 참조하세요.\"\n",
            "Amazon DocumentDB I/O-Optimized를 사용해야 하는 이유는 무엇인가요?\n",
            "Amazon DocumentDB I/O-Optimized는 비용을 예측할 수 있어야 하거나 I/O 집약적 애플리케이션이 있는 경우에 적합한 선택입니다. I/O 비용이 전체 Amazon DocumentDB 데이터베이스 비용의 25%를 초과할 것으로 예상되는 경우 이 옵션을 선택하면 가격 대비 성능이 향상됩니다. 시작 방법을 비롯한 자세한 내용은 Amazon DocumentDB I/O-Optimized 설명서를 참조하세요.\n",
            "\"category : DocumentDB, question : Amazon DocumentDB I/O-Optimized를 사용해야 하는 이유는 무엇인가요?, answer : Amazon DocumentDB I/O-Optimized는 비용을 예측할 수 있어야 하거나 I/O 집약적 애플리케이션이 있는 경우에 적합한 선택입니다. I/O 비용이 전체 Amazon DocumentDB 데이터베이스 비용의 25%를 초과할 것으로 예상되는 경우 이 옵션을 선택하면 가격 대비 성능이 향상됩니다. 시작 방법을 비롯한 자세한 내용은 Amazon DocumentDB I/O-Optimized 설명서를 참조하세요.\"\n",
            "I/O-Optimized 스토리지 구성과 Standard 스토리지 구성 간에 전환이 가능한가요?\n",
            "기존 데이터베이스 클러스터를 30일 간격으로 Amazon DocumentDB I/O-Optimized로 전환할 수 있습니다. 언제든지 Amazon DocumentDB Standard 스토리지 구성으로 다시 전환할 수 있습니다.\n",
            "\"category : DocumentDB, question : I/O-Optimized 스토리지 구성과 Standard 스토리지 구성 간에 전환이 가능한가요?, answer : 기존 데이터베이스 클러스터를 30일 간격으로 Amazon DocumentDB I/O-Optimized로 전환할 수 있습니다. 언제든지 Amazon DocumentDB Standard 스토리지 구성으로 다시 전환할 수 있습니다.\"\n",
            "Amazon DocumentDB I/O-Optimized를 사용할 때 글로벌 클러스터를 사용하는 리전 간에 데이터를 복제하는 데 필요한 I/O에 대한 요금이 계속 부과되나요?\n",
            "예. 리전 간에 데이터를 복제하는 데 필요한 I/O 작업에 대한 요금은 계속 적용됩니다. Amazon DocumentDB I/O-Optimized는 데이터 복제와 달리 읽기 및 쓰기 I/O 작업에 대한 요금을 부과하지 않습니다. 자세한 내용은 Amazon DocumentDB I/O-Optimized 설명서를 참조하세요.\n",
            "\"category : DocumentDB, question : Amazon DocumentDB I/O-Optimized를 사용할 때 글로벌 클러스터를 사용하는 리전 간에 데이터를 복제하는 데 필요한 I/O에 대한 요금이 계속 부과되나요?, answer : 예. 리전 간에 데이터를 복제하는 데 필요한 I/O 작업에 대한 요금은 계속 적용됩니다. Amazon DocumentDB I/O-Optimized는 데이터 복제와 달리 읽기 및 쓰기 I/O 작업에 대한 요금을 부과하지 않습니다. 자세한 내용은 Amazon DocumentDB I/O-Optimized 설명서를 참조하세요.\"\n",
            "Amazon DocumentDB Elastic Clusters란 무엇인가요?\n",
            "Amazon DocumentDB Elastic Clusters는 페타바이트 용량의 스토리지에서 초당 수백만 개의 쓰기 및 읽기 요청을 처리할 수 있도록 도큐먼트 데이터베이스의 크기를 탄력적으로 조정할 수 있습니다. Elastic Clusters는 기본 인프라를 자동으로 관리하고 인스턴스 생성, 제거, 업그레이드, 크기 조정 필요성을 제거하여 고객과 Amazon DocumentDB의 상호 작용을 간소화합니다.\n",
            "\"category : DocumentDB, question : Amazon DocumentDB Elastic Clusters란 무엇인가요?, answer : Amazon DocumentDB Elastic Clusters는 페타바이트 용량의 스토리지에서 초당 수백만 개의 쓰기 및 읽기 요청을 처리할 수 있도록 도큐먼트 데이터베이스의 크기를 탄력적으로 조정할 수 있습니다. Elastic Clusters는 기본 인프라를 자동으로 관리하고 인스턴스 생성, 제거, 업그레이드, 크기 조정 필요성을 제거하여 고객과 Amazon DocumentDB의 상호 작용을 간소화합니다.\"\n",
            "Elastic Clusters를 시작하려면 어떻게 해야 하나요?\n",
            "Amazon DocumentDB API, SDK, CLI, CloudFormation(CFN), AWS Console을 사용하여 Elastic Clusters를 생성할 수 있습니다. 클러스터를 프로비저닝할 때 사용자의 워크로드에 필요한 샤드와 샤드당 컴퓨팅 수를 지정할 수 있습니다. 클러스터를 생성하고 나면 Elastic Clusters의 탄력적 확장성을 활용할 준비가 된 것입니다. 이제 Elastic Clusters 클러스터와 사용자 애플리케이션의 읽기 또는 쓰기 데이터를 연결할 수 있습니다. Elastic Clusters는 탄력적입니다. 워크로드의 요구 사항에 따라 AWS Console, API, CLI, SDK를 이용해서 샤드 수 또는 샤드당 컴퓨팅을 수정하여 컴퓨팅 용량을 늘리거나 줄일 수 있습니다. Elastic Clusters는 기본 인프라를 자동으로 프로비저닝하거나 프로비저닝 해제하고 데이터를 리밸런싱합니다.\n",
            "\"category : DocumentDB, question : Elastic Clusters를 시작하려면 어떻게 해야 하나요?, answer : Amazon DocumentDB API, SDK, CLI, CloudFormation(CFN), AWS Console을 사용하여 Elastic Clusters를 생성할 수 있습니다. 클러스터를 프로비저닝할 때 사용자의 워크로드에 필요한 샤드와 샤드당 컴퓨팅 수를 지정할 수 있습니다. 클러스터를 생성하고 나면 Elastic Clusters의 탄력적 확장성을 활용할 준비가 된 것입니다. 이제 Elastic Clusters 클러스터와 사용자 애플리케이션의 읽기 또는 쓰기 데이터를 연결할 수 있습니다. Elastic Clusters는 탄력적입니다. 워크로드의 요구 사항에 따라 AWS Console, API, CLI, SDK를 이용해서 샤드 수 또는 샤드당 컴퓨팅을 수정하여 컴퓨팅 용량을 늘리거나 줄일 수 있습니다. Elastic Clusters는 기본 인프라를 자동으로 프로비저닝하거나 프로비저닝 해제하고 데이터를 리밸런싱합니다.\"\n",
            "Elastic Clusters는 어떻게 작동하나요?\n",
            "Elastic Clusters는 샤딩을 사용하여 Amazon DocumentDB의 분산 스토리지 시스템 간의 데이터를 파티셔닝합니다. 샤딩(파티셔닝이라고도 부름)은 다수의 노드에 걸친 대형 데이터 세트를 작은 데이터 세트로 분할하여 고객이 단일 데이터베이스의 수직 확장 제한을 넘어 데이터베이스를 스케일 아웃할 수 있도록 합니다. Elastic Clusters는 Amazon DocumentDB의 컴퓨팅 및 스토리지 분리를 활용합니다. Elastic Clusters는 컴퓨팅 노드 간의 작은 데이터 청크를 이동하여 컬렉션을 재파티셔닝하는 대신 분산 스토리지 시스템 내에서 데이터를 효율적으로 복사할 수 있습니다.\n",
            "\"category : DocumentDB, question : Elastic Clusters는 어떻게 작동하나요?, answer : Elastic Clusters는 샤딩을 사용하여 Amazon DocumentDB의 분산 스토리지 시스템 간의 데이터를 파티셔닝합니다. 샤딩(파티셔닝이라고도 부름)은 다수의 노드에 걸친 대형 데이터 세트를 작은 데이터 세트로 분할하여 고객이 단일 데이터베이스의 수직 확장 제한을 넘어 데이터베이스를 스케일 아웃할 수 있도록 합니다. Elastic Clusters는 Amazon DocumentDB의 컴퓨팅 및 스토리지 분리를 활용합니다. Elastic Clusters는 컴퓨팅 노드 간의 작은 데이터 청크를 이동하여 컬렉션을 재파티셔닝하는 대신 분산 스토리지 시스템 내에서 데이터를 효율적으로 복사할 수 있습니다.\"\n",
            "Elastic Clusters는 어떤 유형의 샤딩을 지원하나요?\n",
            "Elastic Clusters는 해시 기반 파티셔닝을 지원합니다.\n",
            "\"category : DocumentDB, question : Elastic Clusters는 어떤 유형의 샤딩을 지원하나요?, answer : Elastic Clusters는 해시 기반 파티셔닝을 지원합니다.\"\n",
            "Elastic Clusters와 MongoDB 샤딩의 차이점은 무엇인가요?\n",
            "Elastic Clusters를 사용하면 데이터 크기를 불문하고 애플리케이션 가동 중단이나 성능에 영향을 주는 일이 거의 없이 Amazon DocumentDB상의 워크로드를 쉽게 스케일 아웃 또는 스케일 인할 수 있습니다. MongoDB에서의 비슷한 작업은 애플리케이션 성능에 영향을 주고 몇 시간, 어떤 경우에는 며칠이 걸립니다. Elastic Clusters는 또한 백업에 영향 없이 빠른 지정 시간 복원과 같은 차별화된 관리 기능을 제공하여 고객이 데이터베이스 관리보다는 애플리케이션에 더 오래 집중할 수 있도록 합니다.\n",
            "\"category : DocumentDB, question : Elastic Clusters와 MongoDB 샤딩의 차이점은 무엇인가요?, answer : Elastic Clusters를 사용하면 데이터 크기를 불문하고 애플리케이션 가동 중단이나 성능에 영향을 주는 일이 거의 없이 Amazon DocumentDB상의 워크로드를 쉽게 스케일 아웃 또는 스케일 인할 수 있습니다. MongoDB에서의 비슷한 작업은 애플리케이션 성능에 영향을 주고 몇 시간, 어떤 경우에는 며칠이 걸립니다. Elastic Clusters는 또한 백업에 영향 없이 빠른 지정 시간 복원과 같은 차별화된 관리 기능을 제공하여 고객이 데이터베이스 관리보다는 애플리케이션에 더 오래 집중할 수 있도록 합니다.\"\n",
            "Elastic Clusters를 사용하려면 기존 애플리케이션을 변경해야 하나요?\n",
            "아니요. Elastic Clusters를 사용하기 위해 애플리케이션을 변경할 필요는 없습니다.\n",
            "\"category : DocumentDB, question : Elastic Clusters를 사용하려면 기존 애플리케이션을 변경해야 하나요?, answer : 아니요. Elastic Clusters를 사용하기 위해 애플리케이션을 변경할 필요는 없습니다.\"\n",
            "기존 Amazon DocumentDB 클러스터를 Elastic Clusters 클러스터로 변환할 수 있나요?\n",
            "아니요. 가까운 시일 내에 AWS Database Migration Service(DMS)를 활용하여 기존 Amazon DocumentDB 클러스터의 데이터를 Elastic Clusters 클러스터로 마이그레이션할 수 있습니다.\n",
            "\"category : DocumentDB, question : 기존 Amazon DocumentDB 클러스터를 Elastic Clusters 클러스터로 변환할 수 있나요?, answer : 아니요. 가까운 시일 내에 AWS Database Migration Service(DMS)를 활용하여 기존 Amazon DocumentDB 클러스터의 데이터를 Elastic Clusters 클러스터로 마이그레이션할 수 있습니다.\"\n",
            "샤드 키를 어떻게 정의하나요?\n",
            "Elastic Clusters용으로 최적의 샤드 키를 선택하는 것은 다른 데이터베이스와 다를 바가 없습니다. 훌륭한 샤드 키는 높은 빈도와 카디널리티라는 두 가지 특성을 가지고 있습니다. 예를 들어 애플리케이션이 user_orders를 DocumentDB에 저장한다면, 일반적으로 데이터를 사용자별로 검색해야 합니다. 따라서 주어진 사용자와 관련된 모든 명령이 한 샤드에 있도록 하는 것이 좋습니다. 이런 경우 user_id는 훌륭한 샤드 키가 됩니다. 자세한 정보를 읽어 보세요.\n",
            "\"category : DocumentDB, question : 샤드 키를 어떻게 정의하나요?, answer : Elastic Clusters용으로 최적의 샤드 키를 선택하는 것은 다른 데이터베이스와 다를 바가 없습니다. 훌륭한 샤드 키는 높은 빈도와 카디널리티라는 두 가지 특성을 가지고 있습니다. 예를 들어 애플리케이션이 user_orders를 DocumentDB에 저장한다면, 일반적으로 데이터를 사용자별로 검색해야 합니다. 따라서 주어진 사용자와 관련된 모든 명령이 한 샤드에 있도록 하는 것이 좋습니다. 이런 경우 user_id는 훌륭한 샤드 키가 됩니다. 자세한 정보를 읽어 보세요.\"\n",
            "Elastic Clusters와 연관된 개념에는 어떤 것이 있나요?\n",
            "Elastic Clusters: 사용자 워크로드의 처리량(throughput)을 초당 수백만 개의 읽기/쓰기로 확장하고 스토리지를 페타바이트로 확장할 수 있는 Amazon DocumentDB 클러스터입니다. Elastic Cluster 클러스터는 컴퓨팅 및 스토리지 볼륨을 위한 하나 이상의 샤드로 구성되며 기본적으로 여러 가용 영역에 걸쳐 고가용성을 유지합니다.\n",
            "샤드: 샤드는 Elastic Clusters 클러스터에 컴퓨팅 리소스를 제공합니다. 기본적으로 샤드는 1개의 라이터 노드와 2개의 리더 노드와 같이 총 3개의 노드를 가집니다. 최대 32개의 샤드를 보유할 수 있으며, 각 샤드에는 최대 64개의 vCPU가 지원됩니다.\n",
            "샤드 키: 샤드 키는 Elastic Clusters가 일치하는 샤드로 읽기 및 쓰기 트래픽을 배포하기 위해 사용하는 JSON 문서 내 선택적 필드입니다. 고유한 값이 많이 포함된 키를 선택하는 것이 좋습니다. 훌륭한 샤드 키는 기본 샤드 간의 데이터를 균일하게 파티셔닝하여 워크로드에 최고의 처리량과 성능을 제공합니다. \n",
            "샤딩된 컬렉션: Elastic Clusters 클러스터에 분산된 데이터의 컬렉션입니다.\n",
            "\"category : DocumentDB, question : Elastic Clusters와 연관된 개념에는 어떤 것이 있나요?, answer : Elastic Clusters: 사용자 워크로드의 처리량(throughput)을 초당 수백만 개의 읽기/쓰기로 확장하고 스토리지를 페타바이트로 확장할 수 있는 Amazon DocumentDB 클러스터입니다. Elastic Cluster 클러스터는 컴퓨팅 및 스토리지 볼륨을 위한 하나 이상의 샤드로 구성되며 기본적으로 여러 가용 영역에 걸쳐 고가용성을 유지합니다.\n",
            "샤드: 샤드는 Elastic Clusters 클러스터에 컴퓨팅 리소스를 제공합니다. 기본적으로 샤드는 1개의 라이터 노드와 2개의 리더 노드와 같이 총 3개의 노드를 가집니다. 최대 32개의 샤드를 보유할 수 있으며, 각 샤드에는 최대 64개의 vCPU가 지원됩니다.\n",
            "샤드 키: 샤드 키는 Elastic Clusters가 일치하는 샤드로 읽기 및 쓰기 트래픽을 배포하기 위해 사용하는 JSON 문서 내 선택적 필드입니다. 고유한 값이 많이 포함된 키를 선택하는 것이 좋습니다. 훌륭한 샤드 키는 기본 샤드 간의 데이터를 균일하게 파티셔닝하여 워크로드에 최고의 처리량과 성능을 제공합니다. \n",
            "샤딩된 컬렉션: Elastic Clusters 클러스터에 분산된 데이터의 컬렉션입니다.\"\n",
            "Elastic Clusters와 다른 AWS 서비스 사이에는 어떤 관계가 있나요?\n",
            "Elastic Clusters는 현재 DocumentDB와 동일한 방식으로 다른 AWS 서비스와 통합됩니다. 먼저, AWS Database Migration Service(DMS)를 사용하여 MongoDB 및 기타 관계형 데이터베이스를 Elastic Clusters로 마이그레이션할 수 있습니다. 두 번째로, Amazon CloudWatch를 사용하여 Elastic Clusters 클러스터의 상태와 성능을 모니터링할 수 있습니다. 세 번째로, AWS IAM 사용자 및 역할을 통해 인증 및 권한 부여를 설정하고 안전한 VPC 전용 연결을 위해 AWS VPC를 사용할 수 있습니다. 마지막으로, AWS Glue를 사용하여 S3, Redshift 및 OpenSearch와 같은 다른 AWS 서비스의 데이터를 가져오거나 내보낼 수 있습니다.\n",
            "\"category : DocumentDB, question : Elastic Clusters와 다른 AWS 서비스 사이에는 어떤 관계가 있나요?, answer : Elastic Clusters는 현재 DocumentDB와 동일한 방식으로 다른 AWS 서비스와 통합됩니다. 먼저, AWS Database Migration Service(DMS)를 사용하여 MongoDB 및 기타 관계형 데이터베이스를 Elastic Clusters로 마이그레이션할 수 있습니다. 두 번째로, Amazon CloudWatch를 사용하여 Elastic Clusters 클러스터의 상태와 성능을 모니터링할 수 있습니다. 세 번째로, AWS IAM 사용자 및 역할을 통해 인증 및 권한 부여를 설정하고 안전한 VPC 전용 연결을 위해 AWS VPC를 사용할 수 있습니다. 마지막으로, AWS Glue를 사용하여 S3, Redshift 및 OpenSearch와 같은 다른 AWS 서비스의 데이터를 가져오거나 내보낼 수 있습니다.\"\n",
            "기존 MongoDB 샤딩된 워크로드를 Elastic Clusters로 마이그레이션할 수 있나요?\n",
            "예. 기존 MongoDB 샤딩된 워크로드를 Elastic Clusters로 마이그레이션할 수 있습니다. AWS Database Migration Service 또는 mongodump 및 mongorestore와 같은 네이티브 MongoDB 도구를 사용하여 MongoDB 워크로드를 Elastic Clusters로 마이그레이션할 수 있습니다. Elastic Clusters는 또한 shardCollection()과 같이 MongoDB에서 일반적으로 사용되는 API를 지원하여 기존 도구 및 스크립트를 Amazon DocumentDB에서 재사용할 수 있는 유연성을 제공합니다.\n",
            "\"category : DocumentDB, question : 기존 MongoDB 샤딩된 워크로드를 Elastic Clusters로 마이그레이션할 수 있나요?, answer : 예. 기존 MongoDB 샤딩된 워크로드를 Elastic Clusters로 마이그레이션할 수 있습니다. AWS Database Migration Service 또는 mongodump 및 mongorestore와 같은 네이티브 MongoDB 도구를 사용하여 MongoDB 워크로드를 Elastic Clusters로 마이그레이션할 수 있습니다. Elastic Clusters는 또한 shardCollection()과 같이 MongoDB에서 일반적으로 사용되는 API를 지원하여 기존 도구 및 스크립트를 Amazon DocumentDB에서 재사용할 수 있는 유연성을 제공합니다.\"\n",
            "Amazon DocumentDB 클러스터의 최소 및 최대 스토리지 한도는 어떻게 되나요?\n",
            "최소 스토리지는 10GB입니다. Amazon DocumentDB 스토리지는 클러스터 사용량을 기준으로, 성능에 영향을 미치지 않고 최대 128TiB까지 10GB 단위로 자동으로 증가합니다. Amazon DocumentDB Elastic Clusters를 사용하면 자동으로 스토리지가 최대 4PB까지 10GB 단위로 증가합니다. 두 경우 모두 스토리지를 미리 프로비저닝할 필요가 없습니다.\n",
            "\"category : DocumentDB, question : Amazon DocumentDB 클러스터의 최소 및 최대 스토리지 한도는 어떻게 되나요?, answer : 최소 스토리지는 10GB입니다. Amazon DocumentDB 스토리지는 클러스터 사용량을 기준으로, 성능에 영향을 미치지 않고 최대 128TiB까지 10GB 단위로 자동으로 증가합니다. Amazon DocumentDB Elastic Clusters를 사용하면 자동으로 스토리지가 최대 4PB까지 10GB 단위로 증가합니다. 두 경우 모두 스토리지를 미리 프로비저닝할 필요가 없습니다.\"\n",
            "Amazon DocumentDB는 어떻게 조정되나요?\n",
            "Amazon DocumentDB는 스토리지와 컴퓨팅이라는 두 가지 차원에서 스케일 인됩니다. Amazon DocumentDB의 스토리지는 인스턴스 기반 클러스터에서 10GB에서 128TiB까지, Amazon DocumentDB Elastic Clusters의 경우 최대 4PB까지 자동으로 조정됩니다. Amazon DocumentDB의 컴퓨팅 용량은 더 큰 인스턴스를 생성하여 확장하거나, 읽기 처리량을 늘리도록 클러스터에 복제본 인스턴스를 더 추가하여 스케일 업할 수 있습니다.\n",
            "\"category : DocumentDB, question : Amazon DocumentDB는 어떻게 조정되나요?, answer : Amazon DocumentDB는 스토리지와 컴퓨팅이라는 두 가지 차원에서 스케일 인됩니다. Amazon DocumentDB의 스토리지는 인스턴스 기반 클러스터에서 10GB에서 128TiB까지, Amazon DocumentDB Elastic Clusters의 경우 최대 4PB까지 자동으로 조정됩니다. Amazon DocumentDB의 컴퓨팅 용량은 더 큰 인스턴스를 생성하여 확장하거나, 읽기 처리량을 늘리도록 클러스터에 복제본 인스턴스를 더 추가하여 스케일 업할 수 있습니다.\"\n",
            "Amazon DocumentDB 클러스터와 관련된 컴퓨팅 리소스를 조정하려면 어떻게 해야 하나요?\n",
            "AWS Management Console에서 원하는 인스턴스를 선택하고 Modify(수정) 버튼을 클릭하면 해당 인스턴스에 할당된 컴퓨팅 리소스를 조정할 수 있습니다. 메모리와 CPU 리소스를 수정하려면 인스턴스 클래스를 변경합니다.\n",
            "인스턴스 클래스를 수정하는 경우 요청한 변경 사항은 지정된 유지 관리 기간에 적용됩니다. 또는 'Apply Immediately' 플래그를 사용하면 규모 조정 요청을 즉시 적용할 수 있습니다. 이 두 옵션을 사용하면 조정 작업이 수행되는 몇 분 동안 가용성에 영향을 미칩니다. 처리되지 않은 다른 시스템 변경 내용도 함께 적용됩니다.\n",
            "\"category : DocumentDB, question : Amazon DocumentDB 클러스터와 관련된 컴퓨팅 리소스를 조정하려면 어떻게 해야 하나요?, answer : AWS Management Console에서 원하는 인스턴스를 선택하고 Modify(수정) 버튼을 클릭하면 해당 인스턴스에 할당된 컴퓨팅 리소스를 조정할 수 있습니다. 메모리와 CPU 리소스를 수정하려면 인스턴스 클래스를 변경합니다.\n",
            "인스턴스 클래스를 수정하는 경우 요청한 변경 사항은 지정된 유지 관리 기간에 적용됩니다. 또는 'Apply Immediately' 플래그를 사용하면 규모 조정 요청을 즉시 적용할 수 있습니다. 이 두 옵션을 사용하면 조정 작업이 수행되는 몇 분 동안 가용성에 영향을 미칩니다. 처리되지 않은 다른 시스템 변경 내용도 함께 적용됩니다.\"\n",
            "내 클러스터에 백업을 사용하려면 어떻게 해야 하나요?\n",
            "Amazon DocumentDB 클러스터에는 자동 백업이 항상 사용됩니다. Amazon DocumentDB의 단순한 데이터베이스 백업 기능을 사용하여 클러스터를 특정 시점으로 복구할 수 있습니다. 특정 시점 복원을 위해 백업 기간을 최대 35일까지 늘릴 수 있습니다. 백업은 데이터베이스 성능에 영향을 미치지 않습니다.\n",
            "\"category : DocumentDB, question : 내 클러스터에 백업을 사용하려면 어떻게 해야 하나요?, answer : Amazon DocumentDB 클러스터에는 자동 백업이 항상 사용됩니다. Amazon DocumentDB의 단순한 데이터베이스 백업 기능을 사용하여 클러스터를 특정 시점으로 복구할 수 있습니다. 특정 시점 복원을 위해 백업 기간을 최대 35일까지 늘릴 수 있습니다. 백업은 데이터베이스 성능에 영향을 미치지 않습니다.\"\n",
            "클러스터 스냅샷을 생성하고 원하는 만큼 오래 유지할 수 있나요?\n",
            "예. 수동 스냅샷은 백업 기간을 초과하여 보존될 수 있으며 스냅샷 생성 시 성능에 영향을 미치지 않습니다. 클러스터 스냅샷에서 데이터를 복원하려면 새 클러스터를 생성해야 합니다.\n",
            "\"category : DocumentDB, question : 클러스터 스냅샷을 생성하고 원하는 만큼 오래 유지할 수 있나요?, answer : 예. 수동 스냅샷은 백업 기간을 초과하여 보존될 수 있으며 스냅샷 생성 시 성능에 영향을 미치지 않습니다. 클러스터 스냅샷에서 데이터를 복원하려면 새 클러스터를 생성해야 합니다.\"\n",
            "인스턴스에 장애가 발생하는 경우 복구 경로는 어떻게 되나요?\n",
            "Amazon DocumentDB는 리전 내 3개의 가용 영역(AZ)에 데이터를 안정적으로 유지하며, 데이터가 손실되지 않은 정상 AZ의 인스턴스를 자동으로 복구합니다. 드물지만 Amazon DocumentDB 스토리지 내에서 데이터를 사용할 수 없는 경우 클러스터 스냅샷에서 복원하거나 새 클러스터로 특정 시점 복원 작업을 수행할 수 있습니다. 특정 시점으로 복원 작업의 경우 최대 5분 전에 수행된 작업까지만 복원할 수 있습니다.\n",
            "\"category : DocumentDB, question : 인스턴스에 장애가 발생하는 경우 복구 경로는 어떻게 되나요?, answer : Amazon DocumentDB는 리전 내 3개의 가용 영역(AZ)에 데이터를 안정적으로 유지하며, 데이터가 손실되지 않은 정상 AZ의 인스턴스를 자동으로 복구합니다. 드물지만 Amazon DocumentDB 스토리지 내에서 데이터를 사용할 수 없는 경우 클러스터 스냅샷에서 복원하거나 새 클러스터로 특정 시점 복원 작업을 수행할 수 있습니다. 특정 시점으로 복원 작업의 경우 최대 5분 전에 수행된 작업까지만 복원할 수 있습니다.\"\n",
            "클러스터를 삭제하면 자동 백업과 클러스터 스냅샷은 어떻게 되나요?\n",
            "인스턴스를 삭제할 때 최종 스냅샷을 생성하도록 선택할 수 있습니다. 스냅샷을 생성하는 경우 나중에 이 스냅샷을 사용하여 삭제된 인스턴스를 복원할 수 있습니다. Amazon DocumentDB는 인스턴스가 삭제된 후에도 사용자가 생성한 이 최종 스냅샷을 수동으로 생성한 모든 다른 스냅샷과 함께 보관합니다. 인스턴스가 삭제된 후에는 스냅샷만 유지됩니다. 즉, 특정 시점 복원을 위해 생성한 자동 백업은 유지되지 않습니다.\n",
            "\"category : DocumentDB, question : 클러스터를 삭제하면 자동 백업과 클러스터 스냅샷은 어떻게 되나요?, answer : 인스턴스를 삭제할 때 최종 스냅샷을 생성하도록 선택할 수 있습니다. 스냅샷을 생성하는 경우 나중에 이 스냅샷을 사용하여 삭제된 인스턴스를 복원할 수 있습니다. Amazon DocumentDB는 인스턴스가 삭제된 후에도 사용자가 생성한 이 최종 스냅샷을 수동으로 생성한 모든 다른 스냅샷과 함께 보관합니다. 인스턴스가 삭제된 후에는 스냅샷만 유지됩니다. 즉, 특정 시점 복원을 위해 생성한 자동 백업은 유지되지 않습니다.\"\n",
            "계정을 삭제하면 자동 백업과 클러스터 스냅샷은 어떻게 되나요?\n",
            "AWS 계정을 삭제하면 계정에 포함된 모든 자동 백업과 스냅샷 백업이 삭제됩니다.\n",
            "\"category : DocumentDB, question : 계정을 삭제하면 자동 백업과 클러스터 스냅샷은 어떻게 되나요?, answer : AWS 계정을 삭제하면 계정에 포함된 모든 자동 백업과 스냅샷 백업이 삭제됩니다.\"\n",
            "내 스냅샷을 다른 AWS 계정과 공유할 수 있나요?\n",
            "예. Amazon DocumentDB는 클러스터 스냅샷을 생성하는 기능을 제공하며, 이 스냅샷은 나중에 클러스터를 복원하는 데 사용할 수 있습니다. 다른 AWS 계정과 스냅샷을 공유할 수 있으며, 수신 계정의 소유자는 사용자의 스냅샷을 사용하여 사용자의 데이터가 포함된 클러스터를 복원할 수 있습니다. 스냅샷을 퍼블릭으로 설정할 수도 있습니다. 즉, 누구나 사용자의 데이터(퍼블릭)가 포함된 클러스터를 복원할 수 있습니다. 이 기능을 사용하면 AWS 계정이 서로 다른 다양한 환경(프로덕션, 개발/테스트, 스테이징 등) 간에 데이터를 공유할 수 있고, 기본 AWS 계정이 손상될 경우에 대비하여 별도의 계정에 모든 데이터 백업을 안전하게 유지할 수 있습니다.\n",
            "\"category : DocumentDB, question : 내 스냅샷을 다른 AWS 계정과 공유할 수 있나요?, answer : 예. Amazon DocumentDB는 클러스터 스냅샷을 생성하는 기능을 제공하며, 이 스냅샷은 나중에 클러스터를 복원하는 데 사용할 수 있습니다. 다른 AWS 계정과 스냅샷을 공유할 수 있으며, 수신 계정의 소유자는 사용자의 스냅샷을 사용하여 사용자의 데이터가 포함된 클러스터를 복원할 수 있습니다. 스냅샷을 퍼블릭으로 설정할 수도 있습니다. 즉, 누구나 사용자의 데이터(퍼블릭)가 포함된 클러스터를 복원할 수 있습니다. 이 기능을 사용하면 AWS 계정이 서로 다른 다양한 환경(프로덕션, 개발/테스트, 스테이징 등) 간에 데이터를 공유할 수 있고, 기본 AWS 계정이 손상될 경우에 대비하여 별도의 계정에 모든 데이터 백업을 안전하게 유지할 수 있습니다.\"\n",
            "공유된 스냅샷에도 요금이 부과되나요?\n",
            "계정 간에 스냅샷을 공유하는 데는 비용이 부과되지 않습니다. 하지만 스냅샷 자체와 공유된 스냅샷에서 복원하는 클러스터에는 비용이 부과될 수 있습니다.\n",
            "\"category : DocumentDB, question : 공유된 스냅샷에도 요금이 부과되나요?, answer : 계정 간에 스냅샷을 공유하는 데는 비용이 부과되지 않습니다. 하지만 스냅샷 자체와 공유된 스냅샷에서 복원하는 클러스터에는 비용이 부과될 수 있습니다.\"\n",
            "스냅샷을 자동으로 공유할 수 있나요?\n",
            "자동 클러스터 스냅샷 공유 기능은 지원하지 않습니다. 자동 스냅샷을 공유하려면 수동으로 스냅샷 복사본을 생성한 다음, 해당 복사본을 공유해야 합니다.\n",
            "\"category : DocumentDB, question : 스냅샷을 자동으로 공유할 수 있나요?, answer : 자동 클러스터 스냅샷 공유 기능은 지원하지 않습니다. 자동 스냅샷을 공유하려면 수동으로 스냅샷 복사본을 생성한 다음, 해당 복사본을 공유해야 합니다.\"\n",
            "Amazon DocumentDB 스냅샷을 다른 리전과 공유할 수 있습니까?\n",
            "아니요. 공유된 Amazon DocumentDB 스냅샷은 이를 공유하는 계정과 같은 리전에 있는 계정에서만 액세스할 수 있습니다.\n",
            "\"category : DocumentDB, question : Amazon DocumentDB 스냅샷을 다른 리전과 공유할 수 있습니까?, answer : 아니요. 공유된 Amazon DocumentDB 스냅샷은 이를 공유하는 계정과 같은 리전에 있는 계정에서만 액세스할 수 있습니다.\"\n",
            "암호화된 Amazon DocumentDB 스냅샷을 공유할 수 있습니까?\n",
            "예. 암호화된 Amazon DocumentDB 스냅샷은 공유할 수 있습니다. 공유된 스냅샷의 수신자는 해당 스냅샷을 암호화하는 데 사용된 KMS 키에 대한 액세스 권한이 있어야 합니다.\n",
            "\"category : DocumentDB, question : 암호화된 Amazon DocumentDB 스냅샷을 공유할 수 있습니까?, answer : 예. 암호화된 Amazon DocumentDB 스냅샷은 공유할 수 있습니다. 공유된 스냅샷의 수신자는 해당 스냅샷을 암호화하는 데 사용된 KMS 키에 대한 액세스 권한이 있어야 합니다.\"\n",
            "Amazon DocumentDB 서비스 외부에서 Amazon DocumentDB 스냅샷을 사용할 수 있나요?\n",
            "아니요. Amazon DocumentDB 스냅샷은 서비스 내부에서만 사용할 수 있습니다.\n",
            "\"category : DocumentDB, question : Amazon DocumentDB 서비스 외부에서 Amazon DocumentDB 스냅샷을 사용할 수 있나요?, answer : 아니요. Amazon DocumentDB 스냅샷은 서비스 내부에서만 사용할 수 있습니다.\"\n",
            "클러스터를 삭제하는 경우 백업은 어떻게 되나요?\n",
            "클러스터를 삭제할 때 최종 스냅샷을 생성하도록 선택할 수 있습니다. 스냅샷을 만들면, 나중에 이를 사용하여 삭제된 클러스터를 복원할 수 있습니다. Amazon DocumentDB는 클러스터가 삭제된 후에도 사용자가 생성한 이 최종 스냅샷을 수동으로 생성한 모든 다른 스냅샷과 함께 보관합니다.\n",
            "\"category : DocumentDB, question : 클러스터를 삭제하는 경우 백업은 어떻게 되나요?, answer : 클러스터를 삭제할 때 최종 스냅샷을 생성하도록 선택할 수 있습니다. 스냅샷을 만들면, 나중에 이를 사용하여 삭제된 클러스터를 복원할 수 있습니다. Amazon DocumentDB는 클러스터가 삭제된 후에도 사용자가 생성한 이 최종 스냅샷을 수동으로 생성한 모든 다른 스냅샷과 함께 보관합니다.\"\n",
            "Amazon DocumentDB는 디스크 장애에 대한 클러스터 내결함성을 어떻게 개선하나요?\n",
            "Amazon DocumentDB는 스토리지 볼륨을 자동으로 10GB 세그먼트로 나누어 여러 디스크에 분산합니다. 스토리지 볼륨의 각 10GB 청크가 3개의 가용 영역(AZ)에 걸쳐 6가지 방법으로 복제됩니다. Amazon DocumentDB는 쓰기 가용성에 영향을 주지 않고 최대 2개의 데이터 사본 손실을 처리하고 읽기 가용성에 영향을 주지 않고 최대 3개의 사본 손실을 투명하게 처리하도록 설계되었습니다. Amazon DocumentDB의 스토리지 볼륨 또한 자가 복구형입니다. 데이터 블록과 디스크에 오류가 있는지 계속 스캔하고 오류가 있는 경우 자동으로 복구됩니다.\n",
            "\"category : DocumentDB, question : Amazon DocumentDB는 디스크 장애에 대한 클러스터 내결함성을 어떻게 개선하나요?, answer : Amazon DocumentDB는 스토리지 볼륨을 자동으로 10GB 세그먼트로 나누어 여러 디스크에 분산합니다. 스토리지 볼륨의 각 10GB 청크가 3개의 가용 영역(AZ)에 걸쳐 6가지 방법으로 복제됩니다. Amazon DocumentDB는 쓰기 가용성에 영향을 주지 않고 최대 2개의 데이터 사본 손실을 처리하고 읽기 가용성에 영향을 주지 않고 최대 3개의 사본 손실을 투명하게 처리하도록 설계되었습니다. Amazon DocumentDB의 스토리지 볼륨 또한 자가 복구형입니다. 데이터 블록과 디스크에 오류가 있는지 계속 스캔하고 오류가 있는 경우 자동으로 복구됩니다.\"\n",
            "Amazon DocumentDB에서 데이터베이스 충돌 발생 후 복구 시간을 개선하려면 어떻게 해야 합니까?\n",
            "다른 데이터베이스와 달리 Amazon DocumentDB에서는 데이터베이스 충돌이 발생한 후 데이터베이스를 다시 작업에 사용하기 전에 최종 데이터베이스 체크포인트의 재실행 로그를 재생하여(대개 5분) 모든 변경 사항이 적용되었는지 확인할 필요가 없습니다. 따라서 대부분의 경우 데이터베이스 재시작 시간이 60초 미만으로 줄어듭니다. Amazon DocumentDB는 데이터베이스 프로세스에서 캐시를 제외하여 재시작 시 즉시 사용할 수 있도록 합니다. 이렇게 하면 캐시가 다시 채워질 때까지는 액세스를 제한할 필요가 없어 중단이 방지됩니다.\n",
            "\"category : DocumentDB, question : Amazon DocumentDB에서 데이터베이스 충돌 발생 후 복구 시간을 개선하려면 어떻게 해야 합니까?, answer : 다른 데이터베이스와 달리 Amazon DocumentDB에서는 데이터베이스 충돌이 발생한 후 데이터베이스를 다시 작업에 사용하기 전에 최종 데이터베이스 체크포인트의 재실행 로그를 재생하여(대개 5분) 모든 변경 사항이 적용되었는지 확인할 필요가 없습니다. 따라서 대부분의 경우 데이터베이스 재시작 시간이 60초 미만으로 줄어듭니다. Amazon DocumentDB는 데이터베이스 프로세스에서 캐시를 제외하여 재시작 시 즉시 사용할 수 있도록 합니다. 이렇게 하면 캐시가 다시 채워질 때까지는 액세스를 제한할 필요가 없어 중단이 방지됩니다.\"\n",
            "Amazon DocumentDB는 어떤 종류의 복제본을 지원하나요?\n",
            "Amazon DocumentDB는 기본 인스턴스와 동일한 기본 스토리지 볼륨을 공유하는 읽기 전용 복제본을 지원합니다. 기본 인스턴스에서 수행한 업데이트는 모든 Amazon DocumentDB 복제본에 표시됩니다.\n",
            "\n",
            "기능: Amazon DocumentDB 읽기 전용 복제본\n",
            "복제본 수: 최대 15개\n",
            "복제본 유형: 비동기식(일반적으로 수 밀리초)\n",
            "기본 인스턴스에 미치는 성능 영향: 낮음\n",
            "장애 조치 대상으로 작동: 예(데이터 손실 없음)\n",
            "자동 장애 조치: 예\n",
            "\"category : DocumentDB, question : Amazon DocumentDB는 어떤 종류의 복제본을 지원하나요?, answer : Amazon DocumentDB는 기본 인스턴스와 동일한 기본 스토리지 볼륨을 공유하는 읽기 전용 복제본을 지원합니다. 기본 인스턴스에서 수행한 업데이트는 모든 Amazon DocumentDB 복제본에 표시됩니다.\n",
            "\n",
            "기능: Amazon DocumentDB 읽기 전용 복제본\n",
            "복제본 수: 최대 15개\n",
            "복제본 유형: 비동기식(일반적으로 수 밀리초)\n",
            "기본 인스턴스에 미치는 성능 영향: 낮음\n",
            "장애 조치 대상으로 작동: 예(데이터 손실 없음)\n",
            "자동 장애 조치: 예\"\n",
            "Amazon DocumentDB는 교차 리전 복제를 지원하나요?\n",
            "예. 글로벌 클러스터 기능을 사용하여 여러 리전에서 데이터를 복제할 수 있습니다. 글로벌 클러스터는 여러 AWS 리전에 걸쳐 있습니다. 글로벌 클러스터는 성능에 거의 또는 전혀 영향을 미치지 않으면서 최대 5개 리전의 클러스터에 데이터를 복제합니다. 글로벌 클러스터는 전체 리전에서 발생하는 가동 중단으로부터의 재해 복구를 제공하고 지연 시간이 짧은 글로벌 읽기를 지원합니다. 자세한 내용은 블로그 게시물을 참조하세요.\n",
            "\"category : DocumentDB, question : Amazon DocumentDB는 교차 리전 복제를 지원하나요?, answer : 예. 글로벌 클러스터 기능을 사용하여 여러 리전에서 데이터를 복제할 수 있습니다. 글로벌 클러스터는 여러 AWS 리전에 걸쳐 있습니다. 글로벌 클러스터는 성능에 거의 또는 전혀 영향을 미치지 않으면서 최대 5개 리전의 클러스터에 데이터를 복제합니다. 글로벌 클러스터는 전체 리전에서 발생하는 가동 중단으로부터의 재해 복구를 제공하고 지연 시간이 짧은 글로벌 읽기를 지원합니다. 자세한 내용은 블로그 게시물을 참조하세요.\"\n",
            "장애 조치 대상인 특정 복제본에 다른 복제본보다 높은 우선순위를 지정할 수 있나요?\n",
            "예. 클러스터의 각 인스턴스에 승격 우선순위 티어를 지정할 수 있습니다. 기본 인스턴스에 장애가 발생하면, Amazon DocumentDB는 가장 우선순위가 높은 복제본을 기본 인스턴스로 승격시킵니다. 같은 우선순위 티어에 있는 2개 이상의 복제본 간에 일관성이 없는 경우, Amazon DocumentDB는 기본 인스턴스와 같은 크기의 복제본을 승격시킵니다.\n",
            "\"category : DocumentDB, question : 장애 조치 대상인 특정 복제본에 다른 복제본보다 높은 우선순위를 지정할 수 있나요?, answer : 예. 클러스터의 각 인스턴스에 승격 우선순위 티어를 지정할 수 있습니다. 기본 인스턴스에 장애가 발생하면, Amazon DocumentDB는 가장 우선순위가 높은 복제본을 기본 인스턴스로 승격시킵니다. 같은 우선순위 티어에 있는 2개 이상의 복제본 간에 일관성이 없는 경우, Amazon DocumentDB는 기본 인스턴스와 같은 크기의 복제본을 승격시킵니다.\"\n",
            "인스턴스에 대한 우선순위 티어를 생성한 후에 이를 수정할 수 있나요?\n",
            "언제든 인스턴스에 대한 우선순위 티어를 수정할 수 있습니다. 우선순위 티어를 수정하는 것만으로 장애 조치가 트리거되지 않습니다.\n",
            "\"category : DocumentDB, question : 인스턴스에 대한 우선순위 티어를 생성한 후에 이를 수정할 수 있나요?, answer : 언제든 인스턴스에 대한 우선순위 티어를 수정할 수 있습니다. 우선순위 티어를 수정하는 것만으로 장애 조치가 트리거되지 않습니다.\"\n",
            "특정 복제본이 프라이머리 인스턴스로 승격되는 것을 방지할 수 있나요?\n",
            "기본 인스턴스로 승격되기를 원하지 않는 복제본에 낮은 우선순위 티어를 지정하면 됩니다. 하지만 클러스터에서 우선순위가 더 높은 복제본이 비정상이거나 어떤 이유로 사용할 수 없는 경우에는 Amazon DocumentDB가 우선순위가 낮은 복제본을 승격시키게 됩니다.\n",
            "\"category : DocumentDB, question : 특정 복제본이 프라이머리 인스턴스로 승격되는 것을 방지할 수 있나요?, answer : 기본 인스턴스로 승격되기를 원하지 않는 복제본에 낮은 우선순위 티어를 지정하면 됩니다. 하지만 클러스터에서 우선순위가 더 높은 복제본이 비정상이거나 어떤 이유로 사용할 수 없는 경우에는 Amazon DocumentDB가 우선순위가 낮은 복제본을 승격시키게 됩니다.\"\n",
            "Amazon DocumentDB는 클러스터의 고가용성을 어떻게 보장합니까?\n",
            "Amazon DocumentDB는 여러 AWS 가용 영역의 복제본 인스턴스를 장애 조치 대상으로 사용함으로써 고가용성 구성으로 배포될 수 있습니다. 기본 인스턴스에 장애가 발생하는 경우, 서비스 중단을 최소화하면서 복제본 인스턴스가 자동으로 새로운 기본 인스턴스로 승격됩니다.\n",
            "\"category : DocumentDB, question : Amazon DocumentDB는 클러스터의 고가용성을 어떻게 보장합니까?, answer : Amazon DocumentDB는 여러 AWS 가용 영역의 복제본 인스턴스를 장애 조치 대상으로 사용함으로써 고가용성 구성으로 배포될 수 있습니다. 기본 인스턴스에 장애가 발생하는 경우, 서비스 중단을 최소화하면서 복제본 인스턴스가 자동으로 새로운 기본 인스턴스로 승격됩니다.\"\n",
            "단일 Amazon DocumentDB 인스턴스의 가용성을 개선하려면 어떻게 해야 합니까?\n",
            "더 많은 Amazon DocumentDB 복제본을 추가할 수 있습니다. Amazon DocumentDB 복제본은 기본 인스턴스와 동일한 기본 스토리지를 공유합니다. 모든 Amazon DocumentDB 복제본은 데이터 손실 없이 승격되어 기본 복제본이 될 수 있으므로 기본 인스턴스에 장애가 발생하는 경우 내결함성 향상에 사용될 수 있습니다. 클러스터 가용성을 높이려면 여러 AZ에 1~15개의 복제본을 만들면 됩니다. 그러면 인스턴스가 중단되는 경우 Amazon DocumentDB가 자동으로 이러한 복제본을 장애 조치 기본 선택에 포함합니다.\n",
            "\"category : DocumentDB, question : 단일 Amazon DocumentDB 인스턴스의 가용성을 개선하려면 어떻게 해야 합니까?, answer : 더 많은 Amazon DocumentDB 복제본을 추가할 수 있습니다. Amazon DocumentDB 복제본은 기본 인스턴스와 동일한 기본 스토리지를 공유합니다. 모든 Amazon DocumentDB 복제본은 데이터 손실 없이 승격되어 기본 복제본이 될 수 있으므로 기본 인스턴스에 장애가 발생하는 경우 내결함성 향상에 사용될 수 있습니다. 클러스터 가용성을 높이려면 여러 AZ에 1~15개의 복제본을 만들면 됩니다. 그러면 인스턴스가 중단되는 경우 Amazon DocumentDB가 자동으로 이러한 복제본을 장애 조치 기본 선택에 포함합니다.\"\n",
            "장애 조치 도중 어떤 일이 발생하며 얼마나 오래 걸리나요?\n",
            "Amazon DocumentDB가 자동으로 장애 조치를 처리하므로 관리자가 직접 개입하지 않아도 애플리케이션이 최대한 신속하게 데이터베이스 작업을 재개할 수 있습니다.\n",
            "\n",
            "Amazon DocumentDB 복제본 인스턴스가 동일한 가용 영역 또는 다른 가용 영역에 있는 경우, 장애 조치가 진행될 때 Amazon DocumentDB에서는 인스턴스의 Canonical Name Record(CNAME)가 정상적인 복제본을 가리키도록 전환하고, 이에 따라 해당 복제본은 승격되어 새로운 기본 복제본이 됩니다. 일반적으로 장애 조치는 처음부터 끝까지 30초 이내에 완료됩니다. \n",
            "Amazon DocumentDB 복제본 인스턴스(즉, 단일 인스턴스 클러스터)가 없는 경우, Amazon DocumentDB는 원래 인스턴스와 동일한 가용 영역에 새 인스턴스를 생성하려고 시도합니다. 이와 같은 원래 인스턴스 대체가 최선의 방법이며, 가용 영역에 크게 영향을 주는 문제가 발생하는 경우 등에는 성공하지 못할 수도 있습니다. \n",
            "\n",
            "데이터베이스 연결이 끊어지는 경우 애플리케이션에서 연결을 다시 시도해야 합니다.\n",
            "\"category : DocumentDB, question : 장애 조치 도중 어떤 일이 발생하며 얼마나 오래 걸리나요?, answer : Amazon DocumentDB가 자동으로 장애 조치를 처리하므로 관리자가 직접 개입하지 않아도 애플리케이션이 최대한 신속하게 데이터베이스 작업을 재개할 수 있습니다.\n",
            "\n",
            "Amazon DocumentDB 복제본 인스턴스가 동일한 가용 영역 또는 다른 가용 영역에 있는 경우, 장애 조치가 진행될 때 Amazon DocumentDB에서는 인스턴스의 Canonical Name Record(CNAME)가 정상적인 복제본을 가리키도록 전환하고, 이에 따라 해당 복제본은 승격되어 새로운 기본 복제본이 됩니다. 일반적으로 장애 조치는 처음부터 끝까지 30초 이내에 완료됩니다. \n",
            "Amazon DocumentDB 복제본 인스턴스(즉, 단일 인스턴스 클러스터)가 없는 경우, Amazon DocumentDB는 원래 인스턴스와 동일한 가용 영역에 새 인스턴스를 생성하려고 시도합니다. 이와 같은 원래 인스턴스 대체가 최선의 방법이며, 가용 영역에 크게 영향을 주는 문제가 발생하는 경우 등에는 성공하지 못할 수도 있습니다. \n",
            "\n",
            "데이터베이스 연결이 끊어지는 경우 애플리케이션에서 연결을 다시 시도해야 합니다.\"\n",
            "기본 인스턴스가 있고 Amazon DocumentDB 복제본 인스턴스가 읽기 트래픽을 활발하게 처리하는 동안 장애 조치가 수행되면 어떤 일이 발생하나요?\n",
            "Amazon DocumentDB가 기본 인스턴스의 문제를 자동으로 감지하고 Amazon DocumentDB 복제본 인스턴스로 읽기/쓰기 트래픽을 라우팅하기 시작합니다. 평균적으로 이러한 장애 조치는 30초 이내에 완료됩니다. 또한, Amazon DocumentDB 복제본에서 제공하던 읽기 트래픽이 일시적으로 중단됩니다.\n",
            "\"category : DocumentDB, question : 기본 인스턴스가 있고 Amazon DocumentDB 복제본 인스턴스가 읽기 트래픽을 활발하게 처리하는 동안 장애 조치가 수행되면 어떤 일이 발생하나요?, answer : Amazon DocumentDB가 기본 인스턴스의 문제를 자동으로 감지하고 Amazon DocumentDB 복제본 인스턴스로 읽기/쓰기 트래픽을 라우팅하기 시작합니다. 평균적으로 이러한 장애 조치는 30초 이내에 완료됩니다. 또한, Amazon DocumentDB 복제본에서 제공하던 읽기 트래픽이 일시적으로 중단됩니다.\"\n",
            "프라이머리에 비해 복제본의 지연 시간은 얼마나 되나요?\n",
            "Amazon DocumentDB 복제본은 기본 인스턴스와 동일한 데이터 볼륨을 공유하므로 사실상 복제 지연이 없습니다. 일반적인 지연 시간은 10밀리초 이내입니다.\n",
            "\"category : DocumentDB, question : 프라이머리에 비해 복제본의 지연 시간은 얼마나 되나요?, answer : Amazon DocumentDB 복제본은 기본 인스턴스와 동일한 데이터 볼륨을 공유하므로 사실상 복제 지연이 없습니다. 일반적인 지연 시간은 10밀리초 이내입니다.\"\n",
            "Amazon Virtual Private Cloud(VPC)에서 Amazon DocumentDB를 사용할 수 있나요?\n",
            "예. 모든 Amazon DocumentDB 클러스터는 VPC에서 생성되어야 합니다. Amazon VPC를 사용하면 사용자의 데이터 센터에서 운영하는 기존 네트워크와 매우 유사한 가상 네트워크 토폴로지를 정의할 수 있습니다. 이를 통해 Amazon DocumentDB 클러스터에 액세스할 수 있는 사용자를 완벽하게 제어할 수 있습니다.\n",
            "\"category : DocumentDB, question : Amazon Virtual Private Cloud(VPC)에서 Amazon DocumentDB를 사용할 수 있나요?, answer : 예. 모든 Amazon DocumentDB 클러스터는 VPC에서 생성되어야 합니다. Amazon VPC를 사용하면 사용자의 데이터 센터에서 운영하는 기존 네트워크와 매우 유사한 가상 네트워크 토폴로지를 정의할 수 있습니다. 이를 통해 Amazon DocumentDB 클러스터에 액세스할 수 있는 사용자를 완벽하게 제어할 수 있습니다.\"\n",
            "Amazon DocumentDB에서 역할 기반 액세스 제어(RBAC)를 지원하나요?\n",
            "Amazon DocumentDB에서는 기본 제공 역할을 통한 RBAC를 지원합니다. RBAC를 사용하면 사용자가 수행할 수 있는 작업을 제한하여 가장 낮은 수준의 권한을 모범 사례로 적용할 수 있습니다. 자세한 내용은 Amazon DocumentDB 역할 기반 액세스 제어를 참조하세요.\n",
            "\"category : DocumentDB, question : Amazon DocumentDB에서 역할 기반 액세스 제어(RBAC)를 지원하나요?, answer : Amazon DocumentDB에서는 기본 제공 역할을 통한 RBAC를 지원합니다. RBAC를 사용하면 사용자가 수행할 수 있는 작업을 제한하여 가장 낮은 수준의 권한을 모범 사례로 적용할 수 있습니다. 자세한 내용은 Amazon DocumentDB 역할 기반 액세스 제어를 참조하세요.\"\n",
            "기존 MongoDB 인증 모드는 Amazon DocumentDB와 어떻게 연동되나요?\n",
            "Amazon DocumentDB는 VPC의 엄격한 네트워크 및 권한 부여 경계를 활용합니다. Amazon DocumentDB 관리 API에 대한 인증과 권한 부여는 IAM 사용자, 역할 및 정책에 의해 제공됩니다. Amazon DocumentDB 데이터베이스에 대한 인증은 MongoDB용 기본 인증 메커니즘인 SCRAM(Salted Challenge Response Authentication Mechanism)이 지원되는 표준 MongoDB 도구 및 드라이버를 통해 수행됩니다.\n",
            "\"category : DocumentDB, question : 기존 MongoDB 인증 모드는 Amazon DocumentDB와 어떻게 연동되나요?, answer : Amazon DocumentDB는 VPC의 엄격한 네트워크 및 권한 부여 경계를 활용합니다. Amazon DocumentDB 관리 API에 대한 인증과 권한 부여는 IAM 사용자, 역할 및 정책에 의해 제공됩니다. Amazon DocumentDB 데이터베이스에 대한 인증은 MongoDB용 기본 인증 메커니즘인 SCRAM(Salted Challenge Response Authentication Mechanism)이 지원되는 표준 MongoDB 도구 및 드라이버를 통해 수행됩니다.\"\n",
            "Amazon DocumentDB는 저장 데이터 암호화를 지원하나요?\n",
            "예. Amazon DocumentDB를 사용하면 사용자가 AWS Key Management Service(KMS)를 통해 관리하는 키를 사용해 클러스터를 암호화할 수 있습니다. Amazon DocumentDB 암호화를 실행 중인 클러스터에서는 동일한 클러스터에 있는 자동 백업, 스냅샷 및 복제본과 마찬가지로 기본 스토리지에 저장된 데이터가 암호화됩니다. 암호화와 복호화는 원활하게 처리됩니다. Amazon DocumentDB에서 KMS를 사용하는 방법에 관한 자세한 내용은 Amazon DocumentDB 저장 데이터 암호화를 참조하세요.\n",
            "\"category : DocumentDB, question : Amazon DocumentDB는 저장 데이터 암호화를 지원하나요?, answer : 예. Amazon DocumentDB를 사용하면 사용자가 AWS Key Management Service(KMS)를 통해 관리하는 키를 사용해 클러스터를 암호화할 수 있습니다. Amazon DocumentDB 암호화를 실행 중인 클러스터에서는 동일한 클러스터에 있는 자동 백업, 스냅샷 및 복제본과 마찬가지로 기본 스토리지에 저장된 데이터가 암호화됩니다. 암호화와 복호화는 원활하게 처리됩니다. Amazon DocumentDB에서 KMS를 사용하는 방법에 관한 자세한 내용은 Amazon DocumentDB 저장 데이터 암호화를 참조하세요.\"\n",
            "암호화되지 않은 기존 클러스터를 암호화할 수 있나요?\n",
            "암호화되지 않은 기존 Amazon DocumentDB 클러스터를 암호화하는 기능은 현재 지원되지 않습니다. 암호화되지 않은 기존 클러스터에 Amazon DocumentDB 암호화를 사용하려면 암호화가 활성화된 새로운 클러스터를 생성하고, 데이터를 이 클러스터로 마이그레이션합니다.\n",
            "\"category : DocumentDB, question : 암호화되지 않은 기존 클러스터를 암호화할 수 있나요?, answer : 암호화되지 않은 기존 Amazon DocumentDB 클러스터를 암호화하는 기능은 현재 지원되지 않습니다. 암호화되지 않은 기존 클러스터에 Amazon DocumentDB 암호화를 사용하려면 암호화가 활성화된 새로운 클러스터를 생성하고, 데이터를 이 클러스터로 마이그레이션합니다.\"\n",
            "Amazon DocumentDB에서 지원하는 규정 준수 인증은 무엇입니까?\n",
            "Amazon DocumentDB는 가장 엄격한 보안 표준을 충족하며 사용자가 손쉽게 AWS의 보안을 확인하고 자체 규제 및 규정 요구 사항을 충족할 수 있도록 설계되었습니다. Amazon DocumentDB는 HIPAA 적격이며 PCI DSS, ISO 9001, 27017, 27018 및 27001, SOC 1, 2, 3과 Health Information Trust Alliance(HITRUST) Common Security Framework(CSF) 인증을 준수하는 것으로 평가되었습니다. AWS 규정 준수 보고서는 AWS Artifact에서 다운로드할 수 있습니다.\n",
            "\"category : DocumentDB, question : Amazon DocumentDB에서 지원하는 규정 준수 인증은 무엇입니까?, answer : Amazon DocumentDB는 가장 엄격한 보안 표준을 충족하며 사용자가 손쉽게 AWS의 보안을 확인하고 자체 규제 및 규정 요구 사항을 충족할 수 있도록 설계되었습니다. Amazon DocumentDB는 HIPAA 적격이며 PCI DSS, ISO 9001, 27017, 27018 및 27001, SOC 1, 2, 3과 Health Information Trust Alliance(HITRUST) Common Security Framework(CSF) 인증을 준수하는 것으로 평가되었습니다. AWS 규정 준수 보고서는 AWS Artifact에서 다운로드할 수 있습니다.\"\n",
            "인플레이스 주 버전 업그레이드란 무엇인가요?\n",
            "인플레이스 주 버전 업그레이드(MVU)를 사용하면 AWS Console, 소프트웨어 개발 키트(SDK) 또는 명령줄 인터페이스(CLI)를 사용하여 Amazon DocumentDB 3.6 또는 4.0 클러스터를 Amazon DocumentDB 5.0으로 업그레이드할 수 있습니다. 인플레이스 MVU를 사용하면 새 클러스터를 생성하거나 엔드포인트를 변경할 필요가 없습니다. 인플레이스 MVU는 Amazon DocumentDB 5.0이 가능한 모든 리전에서 사용할 수 있습니다. 인플레이스 MVU를 시작하려면 인플레이스 MVU 설명서를 검토하세요.\n",
            "\"category : DocumentDB, question : 인플레이스 주 버전 업그레이드란 무엇인가요?, answer : 인플레이스 주 버전 업그레이드(MVU)를 사용하면 AWS Console, 소프트웨어 개발 키트(SDK) 또는 명령줄 인터페이스(CLI)를 사용하여 Amazon DocumentDB 3.6 또는 4.0 클러스터를 Amazon DocumentDB 5.0으로 업그레이드할 수 있습니다. 인플레이스 MVU를 사용하면 새 클러스터를 생성하거나 엔드포인트를 변경할 필요가 없습니다. 인플레이스 MVU는 Amazon DocumentDB 5.0이 가능한 모든 리전에서 사용할 수 있습니다. 인플레이스 MVU를 시작하려면 인플레이스 MVU 설명서를 검토하세요.\"\n",
            "인플레이스 MVU를 사용해야 하는 이유는 무엇인가요?\n",
            "인플레이스 MVU를 사용하면 다른 클러스터로 백업 및 복원을 수행하거나 다른 데이터 마이그레이션 도구를 사용하지 않고도 Amazon DocumentDB 3.6 또는 4.0 클러스터를 버전 5.0으로 원활하게 업그레이드할 수 있습니다. 이렇게 하면 소스 및 대상 엔드포인트 구성, 인덱스 및 데이터 마이그레이션, 애플리케이션 코드 변경 등을 수반하는 일반적인 업그레이드 프로세스와 관련된 시간과 작업을 줄일 수 있습니다.\n",
            "업그레이드 후에 애플리케이션의 엔드포인트를 변경할 필요가 없습니다. 데이터가 동일한 클러스터에 유지되므로 기능을 사용하여 업그레이드하는 데 추가 비용이 들지 않습니다.\n",
            "\"category : DocumentDB, question : 인플레이스 MVU를 사용해야 하는 이유는 무엇인가요?, answer : 인플레이스 MVU를 사용하면 다른 클러스터로 백업 및 복원을 수행하거나 다른 데이터 마이그레이션 도구를 사용하지 않고도 Amazon DocumentDB 3.6 또는 4.0 클러스터를 버전 5.0으로 원활하게 업그레이드할 수 있습니다. 이렇게 하면 소스 및 대상 엔드포인트 구성, 인덱스 및 데이터 마이그레이션, 애플리케이션 코드 변경 등을 수반하는 일반적인 업그레이드 프로세스와 관련된 시간과 작업을 줄일 수 있습니다.\n",
            "업그레이드 후에 애플리케이션의 엔드포인트를 변경할 필요가 없습니다. 데이터가 동일한 클러스터에 유지되므로 기능을 사용하여 업그레이드하는 데 추가 비용이 들지 않습니다.\"\n",
            "인플레이스 MVU로 업그레이드할 때 가동 중지 시간은 얼마나 되나요?\n",
            "가동 중지 시간은 컬렉션, 인덱스, 데이터베이스 및 인스턴스의 수에 따라 클러스터별로 다를 수 있습니다. 프로덕션 클러스터에서 인플레이스 주 버전 업그레이드를 실행하기 전에 더 낮은 환경에서 실행하여 가동 중지 시간과 성능을 테스트함으로써 업그레이드 후 애플리케이션이 예상대로 작동하는지 확인하는 것이 좋습니다.\n",
            "Amazon DocumentDB의 빠른 복제 기능을 활용하여 테스트용 클러스터 데이터를 복제할 수도 있습니다. 기존 Amazon DocumentDB 구현의 복잡성에 따라 Database Solutions Architect에게 추가 지원을 요청할 수 있습니다.\n",
            "\"category : DocumentDB, question : 인플레이스 MVU로 업그레이드할 때 가동 중지 시간은 얼마나 되나요?, answer : 가동 중지 시간은 컬렉션, 인덱스, 데이터베이스 및 인스턴스의 수에 따라 클러스터별로 다를 수 있습니다. 프로덕션 클러스터에서 인플레이스 주 버전 업그레이드를 실행하기 전에 더 낮은 환경에서 실행하여 가동 중지 시간과 성능을 테스트함으로써 업그레이드 후 애플리케이션이 예상대로 작동하는지 확인하는 것이 좋습니다.\n",
            "Amazon DocumentDB의 빠른 복제 기능을 활용하여 테스트용 클러스터 데이터를 복제할 수도 있습니다. 기존 Amazon DocumentDB 구현의 복잡성에 따라 Database Solutions Architect에게 추가 지원을 요청할 수 있습니다.\"\n",
            "현재 인플레이스 MVU는 어떤 엔진 버전을 지원하나요?\n",
            "인플레이스 MVU는 Amazon DocumentDB 3.6 또는 4.0을 소스로 사용하고 버전 5.0을 대상으로 사용하는 경우에만 지원됩니다. Amazon DocumentDB 글로벌 클러스터 또는 Elastic Clusters에는 지원되지 않으며 DocumentDB 4.0을 대상으로 사용하는 경우에도 지원되지 않습니다.\n",
            "\"category : DocumentDB, question : 현재 인플레이스 MVU는 어떤 엔진 버전을 지원하나요?, answer : 인플레이스 MVU는 Amazon DocumentDB 3.6 또는 4.0을 소스로 사용하고 버전 5.0을 대상으로 사용하는 경우에만 지원됩니다. Amazon DocumentDB 글로벌 클러스터 또는 Elastic Clusters에는 지원되지 않으며 DocumentDB 4.0을 대상으로 사용하는 경우에도 지원되지 않습니다.\"\n",
            "Amazon DocumentDB에 있는 데이터를 사용하여 기계 학습 모델을 구축하려면 어떻게 해야 하나요?\n",
            "Amazon DocumentDB를 Amazon SageMaker Canvas와 통합하면 코드를 한 줄도 작성하지 않고도 기계 학습(ML) 모델을 손쉽게 구축하고 Amazon DocumentDB에 저장된 데이터를 사용하여 파운데이션 모델을 사용자 지정할 수 있습니다. Amazon DocumentDB와 SageMaker Canvas 간의 ML 파이프라인과 사용자 지정 데이터를 개발할 필요가 없습니다. Amazon DocumentDB 콘솔 내에서 SageMaker Canvas를 시작하고 기존 Amazon DocumentDB 데이터베이스를 데이터 소스로 추가하여 기계 학습 모델 구축을 시작할 수 있습니다. SageMaker Canvas에서 DocumentDB의 데이터를 사용하여 고객 이탈을 예측하고, 사기를 탐지하고, 유지 관리 장애를 예측하고, 재무 지표 및 판매를 예측하고, 재고를 최적화하고, 콘텐츠를 요약하며, 콘텐츠를 생성하는 모델을 구축할 수 있습니다.\n",
            "\"category : DocumentDB, question : Amazon DocumentDB에 있는 데이터를 사용하여 기계 학습 모델을 구축하려면 어떻게 해야 하나요?, answer : Amazon DocumentDB를 Amazon SageMaker Canvas와 통합하면 코드를 한 줄도 작성하지 않고도 기계 학습(ML) 모델을 손쉽게 구축하고 Amazon DocumentDB에 저장된 데이터를 사용하여 파운데이션 모델을 사용자 지정할 수 있습니다. Amazon DocumentDB와 SageMaker Canvas 간의 ML 파이프라인과 사용자 지정 데이터를 개발할 필요가 없습니다. Amazon DocumentDB 콘솔 내에서 SageMaker Canvas를 시작하고 기존 Amazon DocumentDB 데이터베이스를 데이터 소스로 추가하여 기계 학습 모델 구축을 시작할 수 있습니다. SageMaker Canvas에서 DocumentDB의 데이터를 사용하여 고객 이탈을 예측하고, 사기를 탐지하고, 유지 관리 장애를 예측하고, 재무 지표 및 판매를 예측하고, 재고를 최적화하고, 콘텐츠를 요약하며, 콘텐츠를 생성하는 모델을 구축할 수 있습니다.\"\n",
            "Amazon DocumentDB를 Amazon SageMaker Canvas의 데이터 소스로 사용하여 기계 학습 모델을 구축하는 데 드는 비용은 얼마인가요?\n",
            "Amazon SageMaker Canvas는 Amazon DocumentDB를 비롯한 다양한 데이터 소스의 데이터를 사용하여 기계 학습 모델을 구축할 수 있는 노코드 인터페이스를 제공합니다. SageMaker Canvas 사용 요금과 SageMaker Canvas에서 Amazon DocumentDB 인스턴스의 데이터를 읽을 때 발생하는 I/O에 대한 요금이 청구됩니다. DocumentDB를 Amazon SageMaker Canvas의 데이터 소스로 사용하는 데 드는 추가 비용은 없습니다. 자세한 내용은 Amazon DocumentDB 요금 페이지 및 SageMaker Canvas 요금 페이지를 참조하세요.\n",
            "\"category : DocumentDB, question : Amazon DocumentDB를 Amazon SageMaker Canvas의 데이터 소스로 사용하여 기계 학습 모델을 구축하는 데 드는 비용은 얼마인가요?, answer : Amazon SageMaker Canvas는 Amazon DocumentDB를 비롯한 다양한 데이터 소스의 데이터를 사용하여 기계 학습 모델을 구축할 수 있는 노코드 인터페이스를 제공합니다. SageMaker Canvas 사용 요금과 SageMaker Canvas에서 Amazon DocumentDB 인스턴스의 데이터를 읽을 때 발생하는 I/O에 대한 요금이 청구됩니다. DocumentDB를 Amazon SageMaker Canvas의 데이터 소스로 사용하는 데 드는 추가 비용은 없습니다. 자세한 내용은 Amazon DocumentDB 요금 페이지 및 SageMaker Canvas 요금 페이지를 참조하세요.\"\n",
            "벡터 검색이란?\n",
            "벡터 검색이란 기계 학습(ML)에서 거리 또는 유사성 지표를 사용하여 벡터 표현을 비교하여 주어진 데이터 포인트와 유사한 데이터 포인트를 찾는 데 사용되는 방법입니다. 두 벡터가 벡터 공간에서 가까울수록 기본 항목이 더 유사한 것으로 간주됩니다. 이 기법은 데이터의 의미 또는 의미 체계를 파악하는 데 도움이 됩니다. 이 접근 방식은 추천 시스템, 자연어 처리 및 이미지 인식과 같은 다양한 애플리케이션에서 유용합니다.\n",
            "\"category : DocumentDB, question : 벡터 검색이란?, answer : 벡터 검색이란 기계 학습(ML)에서 거리 또는 유사성 지표를 사용하여 벡터 표현을 비교하여 주어진 데이터 포인트와 유사한 데이터 포인트를 찾는 데 사용되는 방법입니다. 두 벡터가 벡터 공간에서 가까울수록 기본 항목이 더 유사한 것으로 간주됩니다. 이 기법은 데이터의 의미 또는 의미 체계를 파악하는 데 도움이 됩니다. 이 접근 방식은 추천 시스템, 자연어 처리 및 이미지 인식과 같은 다양한 애플리케이션에서 유용합니다.\"\n",
            "Amazon DocumentDB에서 벡터 검색을 사용해야 하는 이유는 무엇인가요?\n",
            "Amazon DocumentDB용 벡터 검색은 JSON 기반 문서 데이터베이스의 유연성 및 풍부한 쿼리 기능과 벡터 검색 기능을 결합합니다. 기존 Amazon DocumentDB 데이터 또는 유연한 문서 데이터 구조를 사용하여 시맨틱 검색 경험, 제품 추천, 개인화, 챗봇, 사기 탐지 및 이상 탐지 같은 기계 학습 및 생성형 AI 사용 사례를 구축할 수 있습니다. 자세한 내용은 Amazon DocumentDB에 대한 벡터 검색 설명서를 참조하세요.\n",
            "\"category : DocumentDB, question : Amazon DocumentDB에서 벡터 검색을 사용해야 하는 이유는 무엇인가요?, answer : Amazon DocumentDB용 벡터 검색은 JSON 기반 문서 데이터베이스의 유연성 및 풍부한 쿼리 기능과 벡터 검색 기능을 결합합니다. 기존 Amazon DocumentDB 데이터 또는 유연한 문서 데이터 구조를 사용하여 시맨틱 검색 경험, 제품 추천, 개인화, 챗봇, 사기 탐지 및 이상 탐지 같은 기계 학습 및 생성형 AI 사용 사례를 구축할 수 있습니다. 자세한 내용은 Amazon DocumentDB에 대한 벡터 검색 설명서를 참조하세요.\"\n",
            "Amazon DocumentDB를 Amazon SageMaker Canvas의 데이터 소스로 사용하여 기계 학습 모델을 구축하는 데 드는 비용은 얼마인가요?\n",
            "Amazon SageMaker Canvas는 Amazon DocumentDB를 비롯한 다양한 데이터 소스의 데이터를 사용하여 기계 학습 모델을 구축할 수 있는 노코드 인터페이스를 제공합니다. SageMaker Canvas 사용 요금과 SageMaker Canvas에서 Amazon DocumentDB 인스턴스의 데이터를 읽을 때 발생하는 I/O에 대한 요금이 청구됩니다. DocumentDB를 Amazon SageMaker Canvas의 데이터 소스로 사용하는 데 드는 추가 비용은 없습니다. 자세한 내용은 Amazon DocumentDB 요금 페이지 및 SageMaker Canvas 요금 페이지를 참조하세요.\n",
            "\"category : DocumentDB, question : Amazon DocumentDB를 Amazon SageMaker Canvas의 데이터 소스로 사용하여 기계 학습 모델을 구축하는 데 드는 비용은 얼마인가요?, answer : Amazon SageMaker Canvas는 Amazon DocumentDB를 비롯한 다양한 데이터 소스의 데이터를 사용하여 기계 학습 모델을 구축할 수 있는 노코드 인터페이스를 제공합니다. SageMaker Canvas 사용 요금과 SageMaker Canvas에서 Amazon DocumentDB 인스턴스의 데이터를 읽을 때 발생하는 I/O에 대한 요금이 청구됩니다. DocumentDB를 Amazon SageMaker Canvas의 데이터 소스로 사용하는 데 드는 추가 비용은 없습니다. 자세한 내용은 Amazon DocumentDB 요금 페이지 및 SageMaker Canvas 요금 페이지를 참조하세요.\"\n",
            "어떤 버전의 Amazon DocumentDB가 벡터 검색을 지원하나요?\n",
            "Amazon DocumentDB에 대한 벡터 검색은 Amazon DocumentDB 5.0 인스턴스 기반 클러스터에서 사용할 수 있습니다.\n",
            "\"category : DocumentDB, question : 어떤 버전의 Amazon DocumentDB가 벡터 검색을 지원하나요?, answer : Amazon DocumentDB에 대한 벡터 검색은 Amazon DocumentDB 5.0 인스턴스 기반 클러스터에서 사용할 수 있습니다.\"\n",
            "시맨틱 검색의 구현은 Amazon DocumentDB를 사용한 키워드 검색과 어떻게 다른가요?\n",
            "Amazon DocumentDB의 벡터 검색을 사용하면 시맨틱 검색을 사용할 수 있으므로 데이터의 의미, 컨텍스트 및 의도를 캡처할 수 있습니다. 키워드 검색은 실제 텍스트 또는 사전 정의된 동의어 매핑을 기반으로 문서를 찾습니다. 예를 들어 기존 전자 상거래 애플리케이션에서는 빨간색 드레스의 설명에 ‘빨간색’과 ‘드레스’라는 단어가 포함된 제품을 반환할 수 있습니다. 시맨틱 검색은 다양한 빨간색 음영의 드레스에 대한 결과를 검색하므로 사용자 경험을 개선할 수 있습니다.\n",
            "\"category : DocumentDB, question : 시맨틱 검색의 구현은 Amazon DocumentDB를 사용한 키워드 검색과 어떻게 다른가요?, answer : Amazon DocumentDB의 벡터 검색을 사용하면 시맨틱 검색을 사용할 수 있으므로 데이터의 의미, 컨텍스트 및 의도를 캡처할 수 있습니다. 키워드 검색은 실제 텍스트 또는 사전 정의된 동의어 매핑을 기반으로 문서를 찾습니다. 예를 들어 기존 전자 상거래 애플리케이션에서는 빨간색 드레스의 설명에 ‘빨간색’과 ‘드레스’라는 단어가 포함된 제품을 반환할 수 있습니다. 시맨틱 검색은 다양한 빨간색 음영의 드레스에 대한 결과를 검색하므로 사용자 경험을 개선할 수 있습니다.\"\n",
            "Amazon DocumentDB에서 벡터 검색을 사용하는 데 드는 비용은 얼마인가요?\n",
            "Amazon DocumentDB에서 벡터 검색을 사용하는 데는 추가 비용이 들지 않습니다. Amazon DocumentDB에서 벡터를 저장, 인덱싱 및 검색할 때 표준 컴퓨팅, I/O, 스토리지 및 백업 요금이 적용됩니다. 자세한 내용은 Amazon DocumentDB 요금 페이지를 참조하세요.\n",
            "\"category : DocumentDB, question : Amazon DocumentDB에서 벡터 검색을 사용하는 데 드는 비용은 얼마인가요?, answer : Amazon DocumentDB에서 벡터 검색을 사용하는 데는 추가 비용이 들지 않습니다. Amazon DocumentDB에서 벡터를 저장, 인덱싱 및 검색할 때 표준 컴퓨팅, I/O, 스토리지 및 백업 요금이 적용됩니다. 자세한 내용은 Amazon DocumentDB 요금 페이지를 참조하세요.\"\n",
            "Amazon DocumentDB 및 Amazon SageMaker Canvas에서 노코드 기계 학습을 사용해야 하는 이유는 무엇인가요?\n",
            "Amazon DocumentDB는 Amazon SageMaker Canvas와 통합되므로 Amazon DocumentDB에 저장된 데이터를 사용하여 생성형 인공 지능(AI) 및 기계 학습(ML) 애플리케이션을 쉽게 구축할 수 있습니다. Amazon DocumentDB와 SageMaker Canvas 간의 ML 파이프라인과 사용자 지정 데이터를 개발할 필요가 없습니다. 콘솔 내에서 통합할 수 있으므로 데이터 연결 및 액세스에 대한 획일적인 부담을 없애고 로우 코드 노코드(LCNC) 환경을 통해 ML 개발을 가속화합니다. Amazon DocumentDB 콘솔 내에서 SageMaker Canvas를 시작하고 기존 Amazon DocumentDB 데이터베이스를 데이터 소스로 추가할 수 있습니다.\n",
            "\"category : DocumentDB, question : Amazon DocumentDB 및 Amazon SageMaker Canvas에서 노코드 기계 학습을 사용해야 하는 이유는 무엇인가요?, answer : Amazon DocumentDB는 Amazon SageMaker Canvas와 통합되므로 Amazon DocumentDB에 저장된 데이터를 사용하여 생성형 인공 지능(AI) 및 기계 학습(ML) 애플리케이션을 쉽게 구축할 수 있습니다. Amazon DocumentDB와 SageMaker Canvas 간의 ML 파이프라인과 사용자 지정 데이터를 개발할 필요가 없습니다. 콘솔 내에서 통합할 수 있으므로 데이터 연결 및 액세스에 대한 획일적인 부담을 없애고 로우 코드 노코드(LCNC) 환경을 통해 ML 개발을 가속화합니다. Amazon DocumentDB 콘솔 내에서 SageMaker Canvas를 시작하고 기존 Amazon DocumentDB 데이터베이스를 데이터 소스로 추가할 수 있습니다.\"\n",
            "========== Config  :  https://aws.amazon.com/ko/config/faqs/ 사이트 크롤링 진행중 ==========\n",
            "60\n",
            "AWS Config란 무엇인가요?\n",
            "AWS Config는 리소스 인벤토리, 구성 기록, 구성 변경 알림을 제공하여 보안 및 거버넌스를 사용할 수 있도록 하는 완전관리형 서비스입니다. AWS Config를 사용하면 기존 AWS 리소스를 검색하고, 서드 파티 리소스의 구성을 기록하고, 모든 세부 구성 정보가 포함된 리소스의 완전한 인벤토리를 내보내며, 특정 시점의 리소스 구성 방식을 확인할 수 있습니다. 이러한 기능에는 규정 준수 감사, 보안 분석, 리소스 변경 추적 및 문제 해결이 사용됩니다.\n",
            "\"category : Config, question : AWS Config란 무엇인가요?, answer : AWS Config는 리소스 인벤토리, 구성 기록, 구성 변경 알림을 제공하여 보안 및 거버넌스를 사용할 수 있도록 하는 완전관리형 서비스입니다. AWS Config를 사용하면 기존 AWS 리소스를 검색하고, 서드 파티 리소스의 구성을 기록하고, 모든 세부 구성 정보가 포함된 리소스의 완전한 인벤토리를 내보내며, 특정 시점의 리소스 구성 방식을 확인할 수 있습니다. 이러한 기능에는 규정 준수 감사, 보안 분석, 리소스 변경 추적 및 문제 해결이 사용됩니다.\"\n",
            "AWS Config 규칙이란 무엇인가요?\n",
            "AWS Config 규칙은 원하는 리소스 구성을 나타내며 AWS Config에 기록된 관련 리소스의 구성 변경 사항과 비교하여 평가됩니다. 규칙을 리소스 구성과 비교하여 평가한 결과는 대시보드에서 확인할 수 있습니다. AWS Config 규칙을 사용하면 전반적인 규칙 준수 및 위험 상태를 구성 측면에서 평가하고, 시간에 따른 규칙 준수 추세를 확인하여 리소스의 규칙 미준수를 초래한 구성 변경이 무엇인지 정확히 찾아낼 수 있습니다.\n",
            "\"category : Config, question : AWS Config 규칙이란 무엇인가요?, answer : AWS Config 규칙은 원하는 리소스 구성을 나타내며 AWS Config에 기록된 관련 리소스의 구성 변경 사항과 비교하여 평가됩니다. 규칙을 리소스 구성과 비교하여 평가한 결과는 대시보드에서 확인할 수 있습니다. AWS Config 규칙을 사용하면 전반적인 규칙 준수 및 위험 상태를 구성 측면에서 평가하고, 시간에 따른 규칙 준수 추세를 확인하여 리소스의 규칙 미준수를 초래한 구성 변경이 무엇인지 정확히 찾아낼 수 있습니다.\"\n",
            "적합성 팩이란 무엇인가요?\n",
            "적합성 팩은 AWS Config의 공통 프레임워크 및 패키징 모델을 사용하여 구축된 AWS Config 규칙과 수정 조치의 모음입니다. 위의 AWS Config 아티팩트를 패키징하면 여러 계정 및 리전에서 거버넌스 정책 및 구성 규칙 준수의 배포 및 보고 특징을 단순화하고 리소스가 규칙 미준수 상태에 있는 시간을 줄일 수 있습니다.\n",
            "\"category : Config, question : 적합성 팩이란 무엇인가요?, answer : 적합성 팩은 AWS Config의 공통 프레임워크 및 패키징 모델을 사용하여 구축된 AWS Config 규칙과 수정 조치의 모음입니다. 위의 AWS Config 아티팩트를 패키징하면 여러 계정 및 리전에서 거버넌스 정책 및 구성 규칙 준수의 배포 및 보고 특징을 단순화하고 리소스가 규칙 미준수 상태에 있는 시간을 줄일 수 있습니다.\"\n",
            "AWS Config를 사용하면 어떤 이점이 있나요?\n",
            "AWS Config를 사용하면 초기 투자 없이 리소스 구성을 손쉽게 추적할 수 있으며, 데이터 수집을 위해 에이전트를 설치하고 업데이트하거나 대용량 데이터베이스를 유지 관리하는 복잡성도 피할 수 있습니다. AWS Config를 사용하면 AWS 리소스와 연관된 모든 구성 속성에 대해 지속적으로 업데이트된 세부 정보를 볼 수 있습니다. 모든 구성이 변경되면 Amazon Simple Notification Service(SNS)를 통해 알림을 받게 됩니다.\n",
            "\"category : Config, question : AWS Config를 사용하면 어떤 이점이 있나요?, answer : AWS Config를 사용하면 초기 투자 없이 리소스 구성을 손쉽게 추적할 수 있으며, 데이터 수집을 위해 에이전트를 설치하고 업데이트하거나 대용량 데이터베이스를 유지 관리하는 복잡성도 피할 수 있습니다. AWS Config를 사용하면 AWS 리소스와 연관된 모든 구성 속성에 대해 지속적으로 업데이트된 세부 정보를 볼 수 있습니다. 모든 구성이 변경되면 Amazon Simple Notification Service(SNS)를 통해 알림을 받게 됩니다.\"\n",
            "AWS Config는 감사에 어떻게 도움이 되나요?\n",
            "AWS Config를 사용하면 리소스 구성 기록에 액세스할 수 있습니다. 구성 변경을 일으킨 AWS CloudTrail 이벤트와 구성 변경 사항을 연결할 수 있습니다. 이 정보를 통해 ‘변경한 사용자’, ‘변경한 IP 주소’ 등의 세부 정보에서 AWS 리소스와 관련 리소스에 대한 변경 결과에 이르기까지 전체적으로 파악할 수 있습니다. 이 정보를 사용하여 시간 경과에 따라 감사 및 규정 준수 평가에 도움이 되는 보고서를 생성할 수 있습니다.\n",
            "\"category : Config, question : AWS Config는 감사에 어떻게 도움이 되나요?, answer : AWS Config를 사용하면 리소스 구성 기록에 액세스할 수 있습니다. 구성 변경을 일으킨 AWS CloudTrail 이벤트와 구성 변경 사항을 연결할 수 있습니다. 이 정보를 통해 ‘변경한 사용자’, ‘변경한 IP 주소’ 등의 세부 정보에서 AWS 리소스와 관련 리소스에 대한 변경 결과에 이르기까지 전체적으로 파악할 수 있습니다. 이 정보를 사용하여 시간 경과에 따라 감사 및 규정 준수 평가에 도움이 되는 보고서를 생성할 수 있습니다.\"\n",
            "AWS Config 및 AWS Config 규칙은 누가 사용해야 하나요?\n",
            "리소스 구성을 지속적으로 평가하여 AWS 상의 보안 및 거버넌스 상태를 개선하고자 하는 AWS 고객이라면 누구나 이 기능을 활용할 수 있습니다. 리소스 구성에 대한 모범 사례를 권장하는 대규모 조직의 관리자는 이러한 규칙을 AWS Config 규칙으로 체계화하여 사용자가 자체적으로 거버넌스를 실현하도록 할 수 있습니다. 취약성을 찾아내기 위해 사용 활동과 구성을 모니터링하는 정보 보안 전문가도 AWS Config 규칙의 이점을 누릴 수 있습니다. 특정 표준(예: PCI-DSS 또는 HIPAA)을 준수해야 하는 워크로드가 있는 경우 이 기능을 사용하여 AWS 인프라 구성의 규정 준수를 평가하고 감사자를 위한 보고서를 생성할 수 있습니다. 또한 대규모 AWS 인프라 또는 자주 변경되는 구성 요소를 관리하는 운영자는 AWS Config 규칙을 문제 해결에 활용할 수 있습니다. 리소스 구성에 대한 변경 사항을 추적하고 리소스 구성에 대한 질문에 답변하며 규정 준수를 보여주고 문제를 해결하거나 보안 분석을 수행하려는 경우 AWS Config를 활성화해야 합니다.\n",
            "\"category : Config, question : AWS Config 및 AWS Config 규칙은 누가 사용해야 하나요?, answer : 리소스 구성을 지속적으로 평가하여 AWS 상의 보안 및 거버넌스 상태를 개선하고자 하는 AWS 고객이라면 누구나 이 기능을 활용할 수 있습니다. 리소스 구성에 대한 모범 사례를 권장하는 대규모 조직의 관리자는 이러한 규칙을 AWS Config 규칙으로 체계화하여 사용자가 자체적으로 거버넌스를 실현하도록 할 수 있습니다. 취약성을 찾아내기 위해 사용 활동과 구성을 모니터링하는 정보 보안 전문가도 AWS Config 규칙의 이점을 누릴 수 있습니다. 특정 표준(예: PCI-DSS 또는 HIPAA)을 준수해야 하는 워크로드가 있는 경우 이 기능을 사용하여 AWS 인프라 구성의 규정 준수를 평가하고 감사자를 위한 보고서를 생성할 수 있습니다. 또한 대규모 AWS 인프라 또는 자주 변경되는 구성 요소를 관리하는 운영자는 AWS Config 규칙을 문제 해결에 활용할 수 있습니다. 리소스 구성에 대한 변경 사항을 추적하고 리소스 구성에 대한 질문에 답변하며 규정 준수를 보여주고 문제를 해결하거나 보안 분석을 수행하려는 경우 AWS Config를 활성화해야 합니다.\"\n",
            "AWS Config 적합성 팩은 누가 사용해야 하나요?\n",
            "여러 계정의 AWS 리소스 구성에 대한 규정 준수 패키지를 구축하고 배포하기 위한 프레임워크를 찾고 있다면 적합성 팩을 사용해야 합니다. 이 프레임워크는 보안, DevOps 및 기타 페르소나에 대한 사용자 지정 팩을 구축하는 데 사용될 수 있으며, 샘플 규정 준수 팩 템플릿 중 하나를 사용하여 빠르게 시작할 수 있습니다.\n",
            "\"category : Config, question : AWS Config 적합성 팩은 누가 사용해야 하나요?, answer : 여러 계정의 AWS 리소스 구성에 대한 규정 준수 패키지를 구축하고 배포하기 위한 프레임워크를 찾고 있다면 적합성 팩을 사용해야 합니다. 이 프레임워크는 보안, DevOps 및 기타 페르소나에 대한 사용자 지정 팩을 구축하는 데 사용될 수 있으며, 샘플 규정 준수 팩 템플릿 중 하나를 사용하여 빠르게 시작할 수 있습니다.\"\n",
            "Config Ruels는 구성이 규칙을 미준수하는 일이 발생하지 않도록 보장하나요?\n",
            "AWS Config 규칙 및 적합성 팩은 리소스가 고객이 지정한 구성 규칙을 준수하는지에 관한 정보를 제공합니다. 규칙 구성 방법에 따라 주기적으로 AWS Config 규칙에 대해 리소스 구성을 평가하거나 구성 변경이 감지될 때 리소스 구성을 평가합니다. 리소스가 규칙을 준수하도록 보장하거나 사용자가 규칙에 위반되는 작업을 하지 못하도록 차단하지는 않습니다. 그러나 각 AWS Config 규칙에 대해 적절한 수정 조치를 구성하여 규정 미준수 리소스를 다시 준수하도록 하는 데 사용할 수 있습니다.\n",
            "\"category : Config, question : Config Ruels는 구성이 규칙을 미준수하는 일이 발생하지 않도록 보장하나요?, answer : AWS Config 규칙 및 적합성 팩은 리소스가 고객이 지정한 구성 규칙을 준수하는지에 관한 정보를 제공합니다. 규칙 구성 방법에 따라 주기적으로 AWS Config 규칙에 대해 리소스 구성을 평가하거나 구성 변경이 감지될 때 리소스 구성을 평가합니다. 리소스가 규칙을 준수하도록 보장하거나 사용자가 규칙에 위반되는 작업을 하지 못하도록 차단하지는 않습니다. 그러나 각 AWS Config 규칙에 대해 적절한 수정 조치를 구성하여 규정 미준수 리소스를 다시 준수하도록 하는 데 사용할 수 있습니다.\"\n",
            "Config Rules는 사용자가 규칙에 위반되는 작업을 하지 못하도록 차단합니까?\n",
            "AWS Config 규칙은 최종 사용자가 AWS를 사용하는 방법에 직접적인 영향을 주지 않습니다. AWS Config 규칙은 구성 변경이 완료되고 AWS Config에서 이를 기록한 후에만 리소스 구성을 평가합니다. AWS Config 규칙은 사용자가 규칙에 위반될 수 있는 구성 변경을 하지 못하도록 제한하지 않습니다. AWS에서 프로비저닝할 수 있는 것과 프로비저닝에 사용되는 구성 파라미터를 제어하려면 AWS Identity and Access Management(IAM) 정책 및 AWS Service Catalog를 각각 참조하세요.\n",
            "\"category : Config, question : Config Rules는 사용자가 규칙에 위반되는 작업을 하지 못하도록 차단합니까?, answer : AWS Config 규칙은 최종 사용자가 AWS를 사용하는 방법에 직접적인 영향을 주지 않습니다. AWS Config 규칙은 구성 변경이 완료되고 AWS Config에서 이를 기록한 후에만 리소스 구성을 평가합니다. AWS Config 규칙은 사용자가 규칙에 위반될 수 있는 구성 변경을 하지 못하도록 제한하지 않습니다. AWS에서 프로비저닝할 수 있는 것과 프로비저닝에 사용되는 구성 파라미터를 제어하려면 AWS Identity and Access Management(IAM) 정책 및 AWS Service Catalog를 각각 참조하세요.\"\n",
            "리소스를 프로비저닝하기 전에 규칙을 평가할 수 있나요?\n",
            "예. AWS Config 규칙을 사전 예방 전용, 탐지 전용 또는 사전 예방 및 탐지 모드 모두로 설정할 수 있습니다. 이러한 규칙의 전체 목록은 설명서를 참조하세요.\n",
            "\"category : Config, question : 리소스를 프로비저닝하기 전에 규칙을 평가할 수 있나요?, answer : 예. AWS Config 규칙을 사전 예방 전용, 탐지 전용 또는 사전 예방 및 탐지 모드 모두로 설정할 수 있습니다. 이러한 규칙의 전체 목록은 설명서를 참조하세요.\"\n",
            "리소스 사후 프로비저닝에 이미 AWS Config 규칙을 사용하고 있습니다. 동일한 규칙을 사전 예방 모드에서 실행하려면 어떻게 해야 하나요?\n",
            "기존 PutConfigRule API 또는 AWS Config 콘솔을 사용하여 계정의 AWS Config 규칙에 사전 예방 모드를 사용하도록 설정할 수 있습니다.\n",
            "\"category : Config, question : 리소스 사후 프로비저닝에 이미 AWS Config 규칙을 사용하고 있습니다. 동일한 규칙을 사전 예방 모드에서 실행하려면 어떻게 해야 하나요?, answer : 기존 PutConfigRule API 또는 AWS Config 콘솔을 사용하여 계정의 AWS Config 규칙에 사전 예방 모드를 사용하도록 설정할 수 있습니다.\"\n",
            "AWS Config로 온프레미스 또는 다른 클라우드의 리소스 구성을 기록할 수 있나요?\n",
            "AWS Config를 사용하면 서드 파티 리소스 또는 사용자 지정 리소스 유형(예: 온프레미스 서버, 서비스형 소프트웨어[SaaS] 모니터링 도구 및 버전 제어 시스템)에 대한 구성을 기록할 수 있습니다. 이렇게 하려면 리소스 유형의 구성을 준수하고 검증하는 리소스 공급자 스키마를 만들어야 합니다. AWS CloudFormation 또는 코드형 인프라(IaC) 도구를 사용하여 사용자 지정 리소스를 등록해야 합니다.\n",
            "모든 리소스 유형을 기록하도록 AWS Config를 구성한 경우 AWS CloudFormation을 통해 관리(생성, 업데이트 또는 삭제)되는 서드 파티 리소스는 AWS Config에서 구성 항목으로 자동 추적됩니다. 이에 필요한 단계를 자세히 살펴보고 어느 AWS 리전에서 사용할 수 있는지 알아보려면 AWS Config 개발자 안내서: 타사 리소스에 대한 레코드 구성을 참조하세요.\n",
            "\"category : Config, question : AWS Config로 온프레미스 또는 다른 클라우드의 리소스 구성을 기록할 수 있나요?, answer : AWS Config를 사용하면 서드 파티 리소스 또는 사용자 지정 리소스 유형(예: 온프레미스 서버, 서비스형 소프트웨어[SaaS] 모니터링 도구 및 버전 제어 시스템)에 대한 구성을 기록할 수 있습니다. 이렇게 하려면 리소스 유형의 구성을 준수하고 검증하는 리소스 공급자 스키마를 만들어야 합니다. AWS CloudFormation 또는 코드형 인프라(IaC) 도구를 사용하여 사용자 지정 리소스를 등록해야 합니다.\n",
            "모든 리소스 유형을 기록하도록 AWS Config를 구성한 경우 AWS CloudFormation을 통해 관리(생성, 업데이트 또는 삭제)되는 서드 파티 리소스는 AWS Config에서 구성 항목으로 자동 추적됩니다. 이에 필요한 단계를 자세히 살펴보고 어느 AWS 리전에서 사용할 수 있는지 알아보려면 AWS Config 개발자 안내서: 타사 리소스에 대한 레코드 구성을 참조하세요.\"\n",
            "AWS Config는 AWS CloudTrail과 어떻게 연동되나요?\n",
            "AWS CloudTrail은 계정의 사용자 API 활동을 기록하고 이러한 활동에 대한 정보에 액세스하는 데 도움이 됩니다. 호출자 자격 증명, API 호출 시간, 요청 파라미터, AWS 서비스에서 반환한 응답 요소 등 API 작업에 대한 전체 세부 정보를 가져올 수 있습니다. AWS Config는 AWS 리소스에 대한 특정 시점 구성 세부 정보를 구성 항목(CI)으로 기록합니다. CI를 사용하면 특정 시점에 ‘내 AWS 리소스는 어떤 형태입니까?’라는 질문에 대답할 수 있습니다. CloudTrail을 사용하여 ‘누가 API를 호출하여 이 리소스를 수정했습니까?’라는 질문에 대답할 수 있습니다. 예를 들어 AWS Config용 AWS Management Console을 사용하여 'Production-DB' 보안 그룹이 이전에 잘못 구성되었음을 감지할 수 있습니다. 통합 CloudTrail 정보를 사용하여 'Production-DB' 보안 그룹을 잘못 구성한 사용자를 찾아낼 수 있습니다.\n",
            "\"category : Config, question : AWS Config는 AWS CloudTrail과 어떻게 연동되나요?, answer : AWS CloudTrail은 계정의 사용자 API 활동을 기록하고 이러한 활동에 대한 정보에 액세스하는 데 도움이 됩니다. 호출자 자격 증명, API 호출 시간, 요청 파라미터, AWS 서비스에서 반환한 응답 요소 등 API 작업에 대한 전체 세부 정보를 가져올 수 있습니다. AWS Config는 AWS 리소스에 대한 특정 시점 구성 세부 정보를 구성 항목(CI)으로 기록합니다. CI를 사용하면 특정 시점에 ‘내 AWS 리소스는 어떤 형태입니까?’라는 질문에 대답할 수 있습니다. CloudTrail을 사용하여 ‘누가 API를 호출하여 이 리소스를 수정했습니까?’라는 질문에 대답할 수 있습니다. 예를 들어 AWS Config용 AWS Management Console을 사용하여 'Production-DB' 보안 그룹이 이전에 잘못 구성되었음을 감지할 수 있습니다. 통합 CloudTrail 정보를 사용하여 'Production-DB' 보안 그룹을 잘못 구성한 사용자를 찾아낼 수 있습니다.\"\n",
            "중앙 계정에서 여러 계정 및 리전의 규정 준수 정보를 모니터링할 수 있나요?\n",
            "AWS Config를 사용하면 다중 계정, 다중 리전 데이터 집계 기능을 통해 간편하게 여러 계정과 리전 전체의 규정 준수 상태를 모니터링할 수 있습니다. 어느 계정에나 구성 집계자를 생성하고 다른 계정의 규정 준수 세부 정보를 집계할 수 있습니다. 이 기능은 AWS Organizations에서도 활용되므로 조직 내 모든 계정의 데이터를 집계할 수 있습니다.\n",
            "\"category : Config, question : 중앙 계정에서 여러 계정 및 리전의 규정 준수 정보를 모니터링할 수 있나요?, answer : AWS Config를 사용하면 다중 계정, 다중 리전 데이터 집계 기능을 통해 간편하게 여러 계정과 리전 전체의 규정 준수 상태를 모니터링할 수 있습니다. 어느 계정에나 구성 집계자를 생성하고 다른 계정의 규정 준수 세부 정보를 집계할 수 있습니다. 이 기능은 AWS Organizations에서도 활용되므로 조직 내 모든 계정의 데이터를 집계할 수 있습니다.\"\n",
            "ServiceNow 인스턴스와 Jira Service Desk 인스턴스를 AWS Config에 연결할 수 있나요?\n",
            "예. AWS Service Management Connector for ServiceNow 및 Jira Service Desk는 ServiceNow 및 Jira Service Desk 최종 사용자가 ServiceNow 및 Jira Service Desk를 통해 AWS 리소스를 기본적으로 프로비저닝 및 관리하고 운영하는 데 도움이 됩니다. ServiceNow 사용자는 AWS Service Management Connector를 사용하여 ServiceNow에서 AWS Config가 제공하는 구성 항목 보기에서 리소스를 추적할 수 있습니다. Jira Service Desk 사용자는 AWS Service Management Connector를 사용하여 발급 요청 내에서 리소스를 추적할 수 있습니다. 따라서 ServiceNow 및 Jira Service Desk 사용자는 간단하게 AWS 제품 요청 작업을 실행할 수 있고 ServiceNow 및 Jira Service Desk 관리자는 AWS 제품에 대한 거버넌스를 제어하고 감독할 수 있습니다.\n",
            "AWS Service Management Connector for ServiceNow는 ServiceNow Store에서 무료로 제공됩니다. 이 새로운 기능은 AWS Service Catalog가 제공되는 모든 AWS 리전에서 사용할 수 있습니다. 자세한 내용은 설명서를 참조하세요.\n",
            "AWS Service Management Connector for Jira Service Desk는 Atlassian Marketplace에서 무료로 제공됩니다. 이 새로운 기능은 AWS Service Catalog가 제공되는 모든 AWS 리전에서 사용할 수 있습니다. 자세한 내용은 설명서를 참조하세요.\n",
            "\"category : Config, question : ServiceNow 인스턴스와 Jira Service Desk 인스턴스를 AWS Config에 연결할 수 있나요?, answer : 예. AWS Service Management Connector for ServiceNow 및 Jira Service Desk는 ServiceNow 및 Jira Service Desk 최종 사용자가 ServiceNow 및 Jira Service Desk를 통해 AWS 리소스를 기본적으로 프로비저닝 및 관리하고 운영하는 데 도움이 됩니다. ServiceNow 사용자는 AWS Service Management Connector를 사용하여 ServiceNow에서 AWS Config가 제공하는 구성 항목 보기에서 리소스를 추적할 수 있습니다. Jira Service Desk 사용자는 AWS Service Management Connector를 사용하여 발급 요청 내에서 리소스를 추적할 수 있습니다. 따라서 ServiceNow 및 Jira Service Desk 사용자는 간단하게 AWS 제품 요청 작업을 실행할 수 있고 ServiceNow 및 Jira Service Desk 관리자는 AWS 제품에 대한 거버넌스를 제어하고 감독할 수 있습니다.\n",
            "AWS Service Management Connector for ServiceNow는 ServiceNow Store에서 무료로 제공됩니다. 이 새로운 기능은 AWS Service Catalog가 제공되는 모든 AWS 리전에서 사용할 수 있습니다. 자세한 내용은 설명서를 참조하세요.\n",
            "AWS Service Management Connector for Jira Service Desk는 Atlassian Marketplace에서 무료로 제공됩니다. 이 새로운 기능은 AWS Service Catalog가 제공되는 모든 AWS 리전에서 사용할 수 있습니다. 자세한 내용은 설명서를 참조하세요.\"\n",
            "이 서비스를 시작하려면 어떻게 해야 하나요?\n",
            "AWS Config를 가장 빨리 시작하는 방법은 AWS Management Console을 사용하는 것입니다. 몇 가지 항목을 선택하여 AWS Config를 활성화할 수 있습니다. 자세한 내용은 시작하기 설명서를 참조하세요.\n",
            "\"category : Config, question : 이 서비스를 시작하려면 어떻게 해야 하나요?, answer : AWS Config를 가장 빨리 시작하는 방법은 AWS Management Console을 사용하는 것입니다. 몇 가지 항목을 선택하여 AWS Config를 활성화할 수 있습니다. 자세한 내용은 시작하기 설명서를 참조하세요.\"\n",
            "내 리소스 구성에 액세스하려면 어떻게 해야 하나요?\n",
            "AWS Management Console, AWS Command Line Interface 또는 SDK를 사용하여 현재와 이전 리소스 구성을 조회할 수 있습니다.\n",
            "자세한 내용은 AWS Config 설명서를 참조하세요.\n",
            "\"category : Config, question : 내 리소스 구성에 액세스하려면 어떻게 해야 하나요?, answer : AWS Management Console, AWS Command Line Interface 또는 SDK를 사용하여 현재와 이전 리소스 구성을 조회할 수 있습니다.\n",
            "자세한 내용은 AWS Config 설명서를 참조하세요.\"\n",
            "AWS Config는 리전별로 활성화합니까? 아니면 전 세계적으로 활성화합니까?\n",
            "AWS Config는 계정에 대해 리전별로 활성화합니다.\n",
            "\"category : Config, question : AWS Config는 리전별로 활성화합니까? 아니면 전 세계적으로 활성화합니까?, answer : AWS Config는 계정에 대해 리전별로 활성화합니다.\"\n",
            "AWS Config는 다른 AWS 계정 전체의 데이터를 집계할 수 있습니까?\n",
            "예, 적절한 IAM 정책을 Amazon S3 버킷에 적용한 후 서로 다른 계정의 구성 업데이트를 하나의 Amazon Simple Storage Service(S3) 버킷으로 전송하도록 AWS Config를 설정할 수 있습니다. 또한 적절한 IAM 정책을 SNS 주제에 적용한 후 같은 리전 내에서 하나의 SNS 주제로 알림을 게시할 수 있습니다.\n",
            "\"category : Config, question : AWS Config는 다른 AWS 계정 전체의 데이터를 집계할 수 있습니까?, answer : 예, 적절한 IAM 정책을 Amazon S3 버킷에 적용한 후 서로 다른 계정의 구성 업데이트를 하나의 Amazon Simple Storage Service(S3) 버킷으로 전송하도록 AWS Config를 설정할 수 있습니다. 또한 적절한 IAM 정책을 SNS 주제에 적용한 후 같은 리전 내에서 하나의 SNS 주제로 알림을 게시할 수 있습니다.\"\n",
            "CloudTrail은 AWS Config 자체에 대한 API 활동을 기록하나요?\n",
            "예. 구성 데이터를 읽기 위해 AWS Config API 작업을 사용하는 등의 모든 AWS Config API 활동을 CloudTrail에서 기록합니다.\n",
            "\"category : Config, question : CloudTrail은 AWS Config 자체에 대한 API 활동을 기록하나요?, answer : 예. 구성 데이터를 읽기 위해 AWS Config API 작업을 사용하는 등의 모든 AWS Config API 활동을 CloudTrail에서 기록합니다.\"\n",
            "리소스의 타임라인 보기에는 어떤 시간과 시간대가 표시되나요? 서머타임은 어떻게 처리합니까?\n",
            "AWS Config는 리소스에 대해 구성 항목(CI)이 기록된 시간을 타임라인에 표시합니다. 모든 시간은 협정 세계시(UTC)로 캡처됩니다. 타임라인이 관리 콘솔에서 시각화되면 서비스는 현재 시간대(필요한 경우 일광 절약 시간용으로 조정)를 사용하여 모든 시간을 타임라인 보기에 표시합니다.\n",
            "\"category : Config, question : 리소스의 타임라인 보기에는 어떤 시간과 시간대가 표시되나요? 서머타임은 어떻게 처리합니까?, answer : AWS Config는 리소스에 대해 구성 항목(CI)이 기록된 시간을 타임라인에 표시합니다. 모든 시간은 협정 세계시(UTC)로 캡처됩니다. 타임라인이 관리 콘솔에서 시각화되면 서비스는 현재 시간대(필요한 경우 일광 절약 시간용으로 조정)를 사용하여 모든 시간을 타임라인 보기에 표시합니다.\"\n",
            "구성 항목이란 무엇인가요?\n",
            "구성 항목(CI)은 특정 시점의 리소스 구성을 말합니다. CI는 5개 섹션으로 구성됩니다.\n",
            "서로 다른 리소스 유형의 일반적인 리소스에 대한 기본 정보(예: Amazon 리소스 이름, 태그),\n",
            "리소스 관련 구성 데이터(예: EC2 인스턴스 유형),\n",
            "다른 리소스와의 관계 매핑(예: EC2::Volume vol-3434df43은 EC2 Instance i-3432ee3a와 '연결된 인스턴스'임),\n",
            "이 상태와 관련된 AWS CloudTrail 이벤트 ID (AWS 리소스인 경우만 해당)\n",
            "CI 버전 및 CI가 캡처된 시기 등 CI에 대한 정보 식별에 도움이 되는 메타데이터.\n",
            "구성 항목에 대해 자세히 알아보세요.\n",
            "\"category : Config, question : 구성 항목이란 무엇인가요?, answer : 구성 항목(CI)은 특정 시점의 리소스 구성을 말합니다. CI는 5개 섹션으로 구성됩니다.\n",
            "서로 다른 리소스 유형의 일반적인 리소스에 대한 기본 정보(예: Amazon 리소스 이름, 태그),\n",
            "리소스 관련 구성 데이터(예: EC2 인스턴스 유형),\n",
            "다른 리소스와의 관계 매핑(예: EC2::Volume vol-3434df43은 EC2 Instance i-3432ee3a와 '연결된 인스턴스'임),\n",
            "이 상태와 관련된 AWS CloudTrail 이벤트 ID (AWS 리소스인 경우만 해당)\n",
            "CI 버전 및 CI가 캡처된 시기 등 CI에 대한 정보 식별에 도움이 되는 메타데이터.\n",
            "구성 항목에 대해 자세히 알아보세요.\"\n",
            "사용자 지정 구성 항목이란 무엇인가요?\n",
            "사용자 지정 구성 항목은 서드 파티 또는 사용자 지정 리소스의 구성 항목입니다. 예를 들어 온프레미스 데이터베이스, Active Directory 서버, GitHub와 같은 버전 제어 시스템 및 Datadog와 같은 서드 파티 모니터링 도구가 있습니다.\n",
            "\"category : Config, question : 사용자 지정 구성 항목이란 무엇인가요?, answer : 사용자 지정 구성 항목은 서드 파티 또는 사용자 지정 리소스의 구성 항목입니다. 예를 들어 온프레미스 데이터베이스, Active Directory 서버, GitHub와 같은 버전 제어 시스템 및 Datadog와 같은 서드 파티 모니터링 도구가 있습니다.\"\n",
            "AWS Config 관계는 무엇이며 어떻게 사용되나요?\n",
            "AWS Config는 기록이 변경되면 리소스의 관계를 고려합니다. 예를 들어 새 EC2 보안 그룹이 EC2 인스턴스와 연결되면 AWS Config는 기본 리소스인 EC2 보안 그룹과 관련 리소스의 업데이트된 구성을 기록합니다(이러한 리소스가 변경된 경우).\n",
            "\"category : Config, question : AWS Config 관계는 무엇이며 어떻게 사용되나요?, answer : AWS Config는 기록이 변경되면 리소스의 관계를 고려합니다. 예를 들어 새 EC2 보안 그룹이 EC2 인스턴스와 연결되면 AWS Config는 기본 리소스인 EC2 보안 그룹과 관련 리소스의 업데이트된 구성을 기록합니다(이러한 리소스가 변경된 경우).\"\n",
            "AWS Config는 리소스의 모든 상태를 기록하나요?\n",
            "AWS Config는 리소스 구성 변경을 감지하여 그 변경에 따른 구성 상태를 기록합니다. 리소스 구성에 여러 가지 변경 사항이 연달아 발생하면 AWS Config는 여러 변경 사항의 누적된 영향을 나타내는 가장 최근의 리소스 구성만 기록합니다. 이 상황에서 AWS Config는 구성 항목의 relatedEvents 필드에 최근 변경 사항만 표시합니다. 이는 AWS Config에서 중간 임시 상태를 기록하기를 기다리지 않고도 사용자 및 프로그램이 인프라 구성을 계속 변경하는 데 도움이 됩니다.\n",
            "\"category : Config, question : AWS Config는 리소스의 모든 상태를 기록하나요?, answer : AWS Config는 리소스 구성 변경을 감지하여 그 변경에 따른 구성 상태를 기록합니다. 리소스 구성에 여러 가지 변경 사항이 연달아 발생하면 AWS Config는 여러 변경 사항의 누적된 영향을 나타내는 가장 최근의 리소스 구성만 기록합니다. 이 상황에서 AWS Config는 구성 항목의 relatedEvents 필드에 최근 변경 사항만 표시합니다. 이는 AWS Config에서 중간 임시 상태를 기록하기를 기다리지 않고도 사용자 및 프로그램이 인프라 구성을 계속 변경하는 데 도움이 됩니다.\"\n",
            "AWS Config가 구성 변경을 기록하는 빈도를 선택하려면 어떻게 해야 하나요?\n",
            "주기적 기록을 사용하면 환경의 변경 사항을 기록할 빈도를 결정하여 자주 변경되는 리소스의 구성 항목을 줄일 수 있습니다. 업데이트를 지속적으로 수신하는 대신, 주기적 기록을 통해 사용 사례에 맞게 24시간마다 구성 변경 사항을 수신할 수 있습니다.\n",
            "\"category : Config, question : AWS Config가 구성 변경을 기록하는 빈도를 선택하려면 어떻게 해야 하나요?, answer : 주기적 기록을 사용하면 환경의 변경 사항을 기록할 빈도를 결정하여 자주 변경되는 리소스의 구성 항목을 줄일 수 있습니다. 업데이트를 지속적으로 수신하는 대신, 주기적 기록을 통해 사용 사례에 맞게 24시간마다 구성 변경 사항을 수신할 수 있습니다.\"\n",
            "연속 기록 대신 주기적 기록을 사용해야 하는 경우는 언제인가요?\n",
            "주기적 기록을 사용하면 리소스 구성에 대한 업데이트를 수신할 빈도를 결정할 수 있습니다. 이 기능을 활성화하면 AWS Config는 리소스의 구성이 변경된 경우 24시간이 지난 시점에 최신 구성만 제공하므로 구성 데이터의 빈도가 줄어들고 운영 계획 및 감사와 같은 사용 사례에서 이 데이터를 수집하는 데 드는 비용을 더 예측할 수 있습니다. 보안 및 규정 준수를 위해 리소스를 지속적으로 모니터링해야 하는 경우 연속 기록을 사용해야 합니다.\n",
            "\"category : Config, question : 연속 기록 대신 주기적 기록을 사용해야 하는 경우는 언제인가요?, answer : 주기적 기록을 사용하면 리소스 구성에 대한 업데이트를 수신할 빈도를 결정할 수 있습니다. 이 기능을 활성화하면 AWS Config는 리소스의 구성이 변경된 경우 24시간이 지난 시점에 최신 구성만 제공하므로 구성 데이터의 빈도가 줄어들고 운영 계획 및 감사와 같은 사용 사례에서 이 데이터를 수집하는 데 드는 비용을 더 예측할 수 있습니다. 보안 및 규정 준수를 위해 리소스를 지속적으로 모니터링해야 하는 경우 연속 기록을 사용해야 합니다.\"\n",
            "AWS Config는 리소스의 API 활동으로 인해 발생한 것이 아닌 구성 변경 사항도 기록하나요?\n",
            "예. AWS Config는 아직 기록되지 않은 리소스 구성 변경 사항이 있는지 정기적으로 스캔하여 이러한 변경 사항을 기록합니다. 이러한 스캔을 통해 기록된 CI는 메시지에 relatedEvent 필드가 없으며 이미 기록된 상태와 다른 최신 상태만 표시됩니다.\n",
            "\"category : Config, question : AWS Config는 리소스의 API 활동으로 인해 발생한 것이 아닌 구성 변경 사항도 기록하나요?, answer : 예. AWS Config는 아직 기록되지 않은 리소스 구성 변경 사항이 있는지 정기적으로 스캔하여 이러한 변경 사항을 기록합니다. 이러한 스캔을 통해 기록된 CI는 메시지에 relatedEvent 필드가 없으며 이미 기록된 상태와 다른 최신 상태만 표시됩니다.\"\n",
            "AWS Config는 EC2 인스턴스 내 소프트웨어에 대한 구성 변경을 기록하나요?\n",
            "예. AWS Config는 AWS 계정의 EC2 인스턴스 및 온프레미스 환경의 가상 머신(VM) 또는 서버 내 소프트웨어에 대한 구성 변경을 기록하는 데 도움이 됩니다. AWS Config에서 기록하는 구성 정보에는 운영 체제 업데이트, 네트워크 구성, 설치된 애플리케이션 등이 있습니다. AWS Config 규칙을 사용하면 인스턴스, VM 및 서버가 지침을 준수하는지 평가할 수 있습니다. AWS Config에서 제공하는 심층적 가시성과 지속적 모니터링 기능은 규정 준수 상태를 평가하고 운영 문제를 해결하는 데 도움이 됩니다.\n",
            "\"category : Config, question : AWS Config는 EC2 인스턴스 내 소프트웨어에 대한 구성 변경을 기록하나요?, answer : 예. AWS Config는 AWS 계정의 EC2 인스턴스 및 온프레미스 환경의 가상 머신(VM) 또는 서버 내 소프트웨어에 대한 구성 변경을 기록하는 데 도움이 됩니다. AWS Config에서 기록하는 구성 정보에는 운영 체제 업데이트, 네트워크 구성, 설치된 애플리케이션 등이 있습니다. AWS Config 규칙을 사용하면 인스턴스, VM 및 서버가 지침을 준수하는지 평가할 수 있습니다. AWS Config에서 제공하는 심층적 가시성과 지속적 모니터링 기능은 규정 준수 상태를 평가하고 운영 문제를 해결하는 데 도움이 됩니다.\"\n",
            "이전에 규정 미준수였던 리소스가 정기 규칙 평가 후에도 여전히 규정 미준수 상태인 경우 AWS Config에서 알림을 계속 전송하나요?\n",
            "AWS Config는 규정 준수 상태가 변경될 때만 알림을 전송합니다. 이전에 규정 미준수였던 리소스가 여전히 규정 미준수 상태인 경우 AWS Config는 새로운 알림을 전송하지 않습니다. 규정 준수 상태가 'compliant(규정 준수)'로 변경되면 상태 변경에 대한 알림을 받게 됩니다.\n",
            "\"category : Config, question : 이전에 규정 미준수였던 리소스가 정기 규칙 평가 후에도 여전히 규정 미준수 상태인 경우 AWS Config에서 알림을 계속 전송하나요?, answer : AWS Config는 규정 준수 상태가 변경될 때만 알림을 전송합니다. 이전에 규정 미준수였던 리소스가 여전히 규정 미준수 상태인 경우 AWS Config는 새로운 알림을 전송하지 않습니다. 규정 준수 상태가 'compliant(규정 준수)'로 변경되면 상태 변경에 대한 알림을 받게 됩니다.\"\n",
            "리소스가 AWS Config 규칙에 의해 평가되지 않도록 플래그를 지정하거나 면제 또는 제외할 수 있나요?\n",
            "예, 콘솔에서 AWS Config '레코더 설정' 페이지로 이동하여 '리소스 유형 제외' 옵션을 선택하고 원하는 제외를 지정하여 리소스를 제외할 수 있습니다. 또는 PutConfigurationRecorder API를 통해 이 기능에 액세스할 수 있습니다. 이 API는 해당 리소스 유형에 대한 구성 레코딩을 비활성화합니다. 또한 AWS Config 규칙을 구성할 때 규칙이 지정된 리소스 유형을 평가할지 아니면 특정 태그가 있는 리소스를 평가할지 지정할 수 있습니다.\n",
            "\"category : Config, question : 리소스가 AWS Config 규칙에 의해 평가되지 않도록 플래그를 지정하거나 면제 또는 제외할 수 있나요?, answer : 예, 콘솔에서 AWS Config '레코더 설정' 페이지로 이동하여 '리소스 유형 제외' 옵션을 선택하고 원하는 제외를 지정하여 리소스를 제외할 수 있습니다. 또는 PutConfigurationRecorder API를 통해 이 기능에 액세스할 수 있습니다. 이 API는 해당 리소스 유형에 대한 구성 레코딩을 비활성화합니다. 또한 AWS Config 규칙을 구성할 때 규칙이 지정된 리소스 유형을 평가할지 아니면 특정 태그가 있는 리소스를 평가할지 지정할 수 있습니다.\"\n",
            "리소스의 구성이란 무엇인가요?\n",
            "리소스 구성은 AWS Config의 구성 항목(CI)에 포함된 데이터에 의해 정의됩니다. AWS Config 규칙의 최초 릴리스를 사용하면 관련 규칙에서 리소스에 대한 CI를 사용할 수 있습니다. AWS Config 규칙은 다른 연결된 리소스, 업무 시간 등 기타 관련 정보와 함께 이 정보를 사용하여 리소스의 구성 규정 준수를 평가할 수 있습니다.\n",
            "\"category : Config, question : 리소스의 구성이란 무엇인가요?, answer : 리소스 구성은 AWS Config의 구성 항목(CI)에 포함된 데이터에 의해 정의됩니다. AWS Config 규칙의 최초 릴리스를 사용하면 관련 규칙에서 리소스에 대한 CI를 사용할 수 있습니다. AWS Config 규칙은 다른 연결된 리소스, 업무 시간 등 기타 관련 정보와 함께 이 정보를 사용하여 리소스의 구성 규정 준수를 평가할 수 있습니다.\"\n",
            "규칙이란 무엇인가요?\n",
            "규칙은 리소스에 대한 원하는 구성 항목(CI)의 속성 값을 나타내며, 이러한 속성 값을 AWS Config에서 기록한 CI와 비교하여 규칙을 평가합니다. 다음과 같은 두 가지 유형의 규칙이 있습니다.\n",
            "AWS 관리형 규칙: AWS 관리형 규칙은 AWS에서 사전에 구축하고 관리합니다. 시작하려면 활성화하려는 규칙을 선택한 다음 몇 가지 구성 파라미터를 제공하면 됩니다. 자세히 알아보기 » \n",
            "고객 관리형 규칙: 고객 관리형 규칙은 사용자 정의 규칙으로서, 고객이 정의하고 구축합니다. AWS Lambda에서 사용자 지정 규칙의 일부로 호출될 수 있는 함수를 생성하고 이러한 함수를 계정에서 적용할 수 있습니다. 자세히 알아보기 » \n",
            "AWS Config를 가장 빨리 시작하는 방법은 AWS Management Console을 사용하는 것입니다. 몇 가지 항목을 선택하여 AWS Config를 활성화할 수 있습니다. 자세한 내용은 시작하기 설명서를 참조하세요.\n",
            "\"category : Config, question : 규칙이란 무엇인가요?, answer : 규칙은 리소스에 대한 원하는 구성 항목(CI)의 속성 값을 나타내며, 이러한 속성 값을 AWS Config에서 기록한 CI와 비교하여 규칙을 평가합니다. 다음과 같은 두 가지 유형의 규칙이 있습니다.\n",
            "AWS 관리형 규칙: AWS 관리형 규칙은 AWS에서 사전에 구축하고 관리합니다. 시작하려면 활성화하려는 규칙을 선택한 다음 몇 가지 구성 파라미터를 제공하면 됩니다. 자세히 알아보기 » \n",
            "고객 관리형 규칙: 고객 관리형 규칙은 사용자 정의 규칙으로서, 고객이 정의하고 구축합니다. AWS Lambda에서 사용자 지정 규칙의 일부로 호출될 수 있는 함수를 생성하고 이러한 함수를 계정에서 적용할 수 있습니다. 자세히 알아보기 » \n",
            "AWS Config를 가장 빨리 시작하는 방법은 AWS Management Console을 사용하는 것입니다. 몇 가지 항목을 선택하여 AWS Config를 활성화할 수 있습니다. 자세한 내용은 시작하기 설명서를 참조하세요.\"\n",
            "규칙은 어떻게 생성되나요?\n",
            "규칙은 일반적으로 AWS 계정 관리자가 설정합니다. 규칙은 AWS에서 제공하는 사전에 정의된 규칙 세트인 AWS 관리형 규칙 또는 고객 관리형 규칙을 활용하여 생성할 수 있습니다. AWS 관리형 규칙의 경우 규칙에 대한 업데이트는 해당 규칙을 사용하는 모든 계정에 자동으로 적용됩니다. 고객 관리형 모델에서는 고객이 규칙의 전체 복사본을 가지고 자신의 계정 내에서 규칙을 적용합니다. 이러한 규칙은 고객이 관리합니다.\n",
            "\"category : Config, question : 규칙은 어떻게 생성되나요?, answer : 규칙은 일반적으로 AWS 계정 관리자가 설정합니다. 규칙은 AWS에서 제공하는 사전에 정의된 규칙 세트인 AWS 관리형 규칙 또는 고객 관리형 규칙을 활용하여 생성할 수 있습니다. AWS 관리형 규칙의 경우 규칙에 대한 업데이트는 해당 규칙을 사용하는 모든 계정에 자동으로 적용됩니다. 고객 관리형 모델에서는 고객이 규칙의 전체 복사본을 가지고 자신의 계정 내에서 규칙을 적용합니다. 이러한 규칙은 고객이 관리합니다.\"\n",
            "규칙은 몇 개까지 생성할 수 있습니까?\n",
            "기본적으로 AWS 계정에 최대 150개의 규칙을 생성할 수 있습니다. 또한 AWS Service Limits 페이지에서 계정에서 생성할 수 있는 규칙 수에 대한 한도 증가를 요청할 수 있습니다.\n",
            "\"category : Config, question : 규칙은 몇 개까지 생성할 수 있습니까?, answer : 기본적으로 AWS 계정에 최대 150개의 규칙을 생성할 수 있습니다. 또한 AWS Service Limits 페이지에서 계정에서 생성할 수 있는 규칙 수에 대한 한도 증가를 요청할 수 있습니다.\"\n",
            "규칙은 어떻게 평가하나요?\n",
            "모든 규칙은 변경으로 트리거되는 규칙 또는 주기적인 규칙으로 설정될 수 있습니다. 변경으로 트리거되는 규칙은 지정된 리소스에 대한 구성 변경을 AWS Config가 기록할 때 적용됩니다. 또한 다음 중 하나는 반드시 지정되어야 합니다.\n",
            "태그 키:(선택적 값): 태그 키:값은 지정된 태그 키:값을 갖는 리소스에 대한 구성 변경이 기록될 때 규칙 평가가 시작된다는 것을 의미합니다.\n",
            "리소스 유형: 지정된 리소스 유형에 해당하는 리소스에 대한 구성 변경이 기록될 때 규칙 평가가 시작됩니다.\n",
            "리소스 ID: 리소스 유형 및 리소스 ID에서 지정한 리소스에 대한 구성 변경이 기록될 때 규칙 평가가 시작됩니다.\n",
            "주기적인 규칙은 지정된 주기에 따라 시작됩니다. 사용 가능한 빈도는 1시간, 3시간, 6시간, 12시간 또는 24시간입니다. 주기적인 규칙은 해당 규칙에서 사용할 수 있는 모든 리소스에 대한 현재 구성 항목(CI)의 전체 스냅샷을 포함합니다.\n",
            "\"category : Config, question : 규칙은 어떻게 평가하나요?, answer : 모든 규칙은 변경으로 트리거되는 규칙 또는 주기적인 규칙으로 설정될 수 있습니다. 변경으로 트리거되는 규칙은 지정된 리소스에 대한 구성 변경을 AWS Config가 기록할 때 적용됩니다. 또한 다음 중 하나는 반드시 지정되어야 합니다.\n",
            "태그 키:(선택적 값): 태그 키:값은 지정된 태그 키:값을 갖는 리소스에 대한 구성 변경이 기록될 때 규칙 평가가 시작된다는 것을 의미합니다.\n",
            "리소스 유형: 지정된 리소스 유형에 해당하는 리소스에 대한 구성 변경이 기록될 때 규칙 평가가 시작됩니다.\n",
            "리소스 ID: 리소스 유형 및 리소스 ID에서 지정한 리소스에 대한 구성 변경이 기록될 때 규칙 평가가 시작됩니다.\n",
            "주기적인 규칙은 지정된 주기에 따라 시작됩니다. 사용 가능한 빈도는 1시간, 3시간, 6시간, 12시간 또는 24시간입니다. 주기적인 규칙은 해당 규칙에서 사용할 수 있는 모든 리소스에 대한 현재 구성 항목(CI)의 전체 스냅샷을 포함합니다.\"\n",
            "평가란 무엇인가요?\n",
            "규칙 평가는 특정 시점에 리소스가 규칙을 준수하는지 확인합니다. 이는 규칙을 리소스 구성과 비교하여 평가한 결과입니다. AWS Config 규칙은 각 평가의 결과를 캡처하여 저장합니다. 이러한 결과는 리소스, 규칙, 평가 시간과 더불어 규칙 미준수의 원인이 되는 구성 항목(CI)에 대한 링크를 포함합니다.\n",
            "\"category : Config, question : 평가란 무엇인가요?, answer : 규칙 평가는 특정 시점에 리소스가 규칙을 준수하는지 확인합니다. 이는 규칙을 리소스 구성과 비교하여 평가한 결과입니다. AWS Config 규칙은 각 평가의 결과를 캡처하여 저장합니다. 이러한 결과는 리소스, 규칙, 평가 시간과 더불어 규칙 미준수의 원인이 되는 구성 항목(CI)에 대한 링크를 포함합니다.\"\n",
            "규정 준수란 무엇인가요?\n",
            "리소스가 해당 리소스에 적용되는 모든 규정을 준수하는 경우 리소스가 규정을 준수하는 것이고 그렇지 않으면 규정을 준수하지 않는 것입니다. 이와 마찬가지로 규칙에서 평가하는 모든 리소스가 규칙을 준수하는 경우, 규칙이 준수되었다고 하며, 그렇지 않은 경우 규칙 미준수입니다. 일부 경우(예: 규칙에서 부적절한 권한을 사용할 수 있는 경우) 리소스에 대한 평가가 존재하지 않을 수 있으며 이는 불충분한 데이터 상태로 이어집니다. 이러한 상태는 리소스 또는 규칙의 준수 상태를 결정하는 작업에서 제외됩니다.\n",
            "\"category : Config, question : 규정 준수란 무엇인가요?, answer : 리소스가 해당 리소스에 적용되는 모든 규정을 준수하는 경우 리소스가 규정을 준수하는 것이고 그렇지 않으면 규정을 준수하지 않는 것입니다. 이와 마찬가지로 규칙에서 평가하는 모든 리소스가 규칙을 준수하는 경우, 규칙이 준수되었다고 하며, 그렇지 않은 경우 규칙 미준수입니다. 일부 경우(예: 규칙에서 부적절한 권한을 사용할 수 있는 경우) 리소스에 대한 평가가 존재하지 않을 수 있으며 이는 불충분한 데이터 상태로 이어집니다. 이러한 상태는 리소스 또는 규칙의 준수 상태를 결정하는 작업에서 제외됩니다.\"\n",
            "AWS Config 규칙 대시보드에서는 어떤 정보를 제공하나요?\n",
            "AWS Config 규칙 대시보드는 AWS Config에서 추적한 리소스의 개요와 리소스 및 규칙의 현재 준수 상태에 대한 요약을 제공합니다. 리소스의 규칙 준수 상태를 보면, 해당 리소스에 적용된 규칙 중 현재 준수되지 않은 규칙이 있는지 확인할 수 있습니다. 규칙의 준수 상태를 보면, 규칙이 적용되는 리소스 중 현재 규칙을 준수하지 않는 리소스가 있는지 확인할 수 있습니다. 이러한 요약 정보를 사용하여 리소스의 AWS Config 타임라인 보기를 추가로 살펴봄으로써 변경된 구성 파라미터를 확인할 수 있습니다. 이 대시보드에서는 개요에서부터 구성 변경의 규칙 준수 상태에 관한 전체 정보, 규칙 미준수를 초래한 구성 변경이 무엇인지까지 세분화하여 볼 수 있습니다.\n",
            "\"category : Config, question : AWS Config 규칙 대시보드에서는 어떤 정보를 제공하나요?, answer : AWS Config 규칙 대시보드는 AWS Config에서 추적한 리소스의 개요와 리소스 및 규칙의 현재 준수 상태에 대한 요약을 제공합니다. 리소스의 규칙 준수 상태를 보면, 해당 리소스에 적용된 규칙 중 현재 준수되지 않은 규칙이 있는지 확인할 수 있습니다. 규칙의 준수 상태를 보면, 규칙이 적용되는 리소스 중 현재 규칙을 준수하지 않는 리소스가 있는지 확인할 수 있습니다. 이러한 요약 정보를 사용하여 리소스의 AWS Config 타임라인 보기를 추가로 살펴봄으로써 변경된 구성 파라미터를 확인할 수 있습니다. 이 대시보드에서는 개요에서부터 구성 변경의 규칙 준수 상태에 관한 전체 정보, 규칙 미준수를 초래한 구성 변경이 무엇인지까지 세분화하여 볼 수 있습니다.\"\n",
            "AWS Config 규칙과 적합성 팩은 언제 사용해야 하나요?\n",
            "개별 AWS Config 규칙을 사용하여 하나 이상의 계정에서 리소스 구성 규칙 준수를 평가할 수 있습니다. 적합성 팩은 한 번의 선택으로 전체 조직에 배포할 수 있는 단일 개체에 대한 수정 조치와 함께 패키징 규칙의 추가적인 이점을 제공합니다. 적합성 팩은 여러 계정을 관리할 때 규정 준수 관리 및 대규모 보고를 단순화하기 위한 것입니다. 적합성 팩은 팩 수준 및 불변성에서 집계된 규정 준수 보고를 제공하도록 설계되었습니다. 이는 적합성 팩 내의 관리형 AWS Config 규칙 및 수정 문서가 조직의 개별 구성원 계정에 의해 수정되거나 삭제되지 않도록 하는 데 도움이 됩니다.\n",
            "\"category : Config, question : AWS Config 규칙과 적합성 팩은 언제 사용해야 하나요?, answer : 개별 AWS Config 규칙을 사용하여 하나 이상의 계정에서 리소스 구성 규칙 준수를 평가할 수 있습니다. 적합성 팩은 한 번의 선택으로 전체 조직에 배포할 수 있는 단일 개체에 대한 수정 조치와 함께 패키징 규칙의 추가적인 이점을 제공합니다. 적합성 팩은 여러 계정을 관리할 때 규정 준수 관리 및 대규모 보고를 단순화하기 위한 것입니다. 적합성 팩은 팩 수준 및 불변성에서 집계된 규정 준수 보고를 제공하도록 설계되었습니다. 이는 적합성 팩 내의 관리형 AWS Config 규칙 및 수정 문서가 조직의 개별 구성원 계정에 의해 수정되거나 삭제되지 않도록 하는 데 도움이 됩니다.\"\n",
            "AWS Config 및 AWS Config 규칙은 AWS Security Hub와 어떻게 관련되나요?\n",
            "AWS Security Hub는 보안 및 규정 준수 상태 관리를 서비스 형태로 제공하는 보안 및 규정 준수 서비스입니다. 이 서비스는 AWS Config 및 AWS Config 규칙을 기본 메커니즘으로 사용하여 AWS 리소스의 구성을 평가합니다. AWS Config 규칙은 또한 리소스 구성을 직접 평가하는 데 사용될 수 있습니다. AWS Config 규칙은 AWS Control Tower 및 AWS Firewall Manager와 같은 다른 AWS 서비스에서도 사용됩니다.\n",
            "\"category : Config, question : AWS Config 및 AWS Config 규칙은 AWS Security Hub와 어떻게 관련되나요?, answer : AWS Security Hub는 보안 및 규정 준수 상태 관리를 서비스 형태로 제공하는 보안 및 규정 준수 서비스입니다. 이 서비스는 AWS Config 및 AWS Config 규칙을 기본 메커니즘으로 사용하여 AWS 리소스의 구성을 평가합니다. AWS Config 규칙은 또한 리소스 구성을 직접 평가하는 데 사용될 수 있습니다. AWS Config 규칙은 AWS Control Tower 및 AWS Firewall Manager와 같은 다른 AWS 서비스에서도 사용됩니다.\"\n",
            "언제 AWS Security Hub와 AWS Config 적합성 팩을 사용해야 하나요?\n",
            "Security Hub에 PCI DSS와 같은 규정 준수 표준이 이미 있다면, 완전관리형 Security Hub 서비스가 표준을 운용하는 데 가장 쉬운 방법입니다. Amazon Detective와 Security Hub의 통합을 통해 조사 결과를 조사하고, Amazon EventBridge와 Security Hub의 통합을 사용해 자동화 또는 반자동화 방식의 수정 조치를 구성할 수 있습니다. 하지만 보안, 운용 또는 비용 최적화 확인을 포함할 수 있는 자체 규정 준수 또는 보안 표준을 결합하려는 경우에는 AWS Config 적합성 팩이 적합합니다. AWS Config 적합성 팩은 AWS Config 규칙 그룹과 관련 수정 조치를 단일 엔터티로 패키징하여 AWS Config 규칙의 관리를 단순화합니다. 이 패키징은 조직 전체에 걸친 규칙 배포와 수정 조치를 단순화합니다. 또한 규정 준수 요약이 팩 수준에서 보고될 수 있으므로, 집계 보고를 사용할 수 있게 합니다. AWS에서 제공하는 AWS Config 적합성 팩 샘플로 시작하고 적절하게 사용자 지정할 수 있습니다.\n",
            "\"category : Config, question : 언제 AWS Security Hub와 AWS Config 적합성 팩을 사용해야 하나요?, answer : Security Hub에 PCI DSS와 같은 규정 준수 표준이 이미 있다면, 완전관리형 Security Hub 서비스가 표준을 운용하는 데 가장 쉬운 방법입니다. Amazon Detective와 Security Hub의 통합을 통해 조사 결과를 조사하고, Amazon EventBridge와 Security Hub의 통합을 사용해 자동화 또는 반자동화 방식의 수정 조치를 구성할 수 있습니다. 하지만 보안, 운용 또는 비용 최적화 확인을 포함할 수 있는 자체 규정 준수 또는 보안 표준을 결합하려는 경우에는 AWS Config 적합성 팩이 적합합니다. AWS Config 적합성 팩은 AWS Config 규칙 그룹과 관련 수정 조치를 단일 엔터티로 패키징하여 AWS Config 규칙의 관리를 단순화합니다. 이 패키징은 조직 전체에 걸친 규칙 배포와 수정 조치를 단순화합니다. 또한 규정 준수 요약이 팩 수준에서 보고될 수 있으므로, 집계 보고를 사용할 수 있게 합니다. AWS에서 제공하는 AWS Config 적합성 팩 샘플로 시작하고 적절하게 사용자 지정할 수 있습니다.\"\n",
            "Security Hub와 AWS Config 적합성 팩 모두 지속적인 규정 준수 모니터링을 지원하나요?\n",
            "예. Security Hub와 AWS Config 적합성 팩 모두 AWS Config 및 AWS Config 규칙을 기반으로 하여 지속적인 규정 준수 모니터링을 지원합니다. 기본적인 AWS Config 규칙은 주기적으로 또는 리소스 구성 변경이 감지될 때 트리거될 수 있습니다. 이는 조직의 정책 및 지침을 기준으로 AWS 리소스 구성의 전반적인 규정 준수 여부를 지속적으로 감사하고 평가하는 데 도움이 됩니다.\n",
            "\"category : Config, question : Security Hub와 AWS Config 적합성 팩 모두 지속적인 규정 준수 모니터링을 지원하나요?, answer : 예. Security Hub와 AWS Config 적합성 팩 모두 AWS Config 및 AWS Config 규칙을 기반으로 하여 지속적인 규정 준수 모니터링을 지원합니다. 기본적인 AWS Config 규칙은 주기적으로 또는 리소스 구성 변경이 감지될 때 트리거될 수 있습니다. 이는 조직의 정책 및 지침을 기준으로 AWS 리소스 구성의 전반적인 규정 준수 여부를 지속적으로 감사하고 평가하는 데 도움이 됩니다.\"\n",
            "적합성 팩을 시작하려면 어떻게 해야 하나요?\n",
            "가장 빠른 시작 방법은 CLI 또는 AWS Config 콘솔을 통해 샘플 템플릿 중 하나를 사용하여 적합성 팩을 만드는 것입니다. 일부 샘플 템플릿에는 S3 운영 모범 사례, Amazon DynamoDB 운영 모범 사례 및 PCI 운영 모범 사례가 포함되어 있습니다. 이러한 템플릿은 YAML로 작성되어 있습니다. 설명서 사이트에서 이러한 템플릿을 다운로드하여 선호하는 텍스트 편집기를 통해 환경에 맞게 수정할 수 있습니다. 이전에 팩에 작성한 사용자 지정 AWS Config 규칙을 추가할 수도 있습니다.\n",
            "\"category : Config, question : 적합성 팩을 시작하려면 어떻게 해야 하나요?, answer : 가장 빠른 시작 방법은 CLI 또는 AWS Config 콘솔을 통해 샘플 템플릿 중 하나를 사용하여 적합성 팩을 만드는 것입니다. 일부 샘플 템플릿에는 S3 운영 모범 사례, Amazon DynamoDB 운영 모범 사례 및 PCI 운영 모범 사례가 포함되어 있습니다. 이러한 템플릿은 YAML로 작성되어 있습니다. 설명서 사이트에서 이러한 템플릿을 다운로드하여 선호하는 텍스트 편집기를 통해 환경에 맞게 수정할 수 있습니다. 이전에 팩에 작성한 사용자 지정 AWS Config 규칙을 추가할 수도 있습니다.\"\n",
            "AWS Config에서 이 기능을 사용하는 것과 관련된 비용이 있나요?\n",
            "적합성 팩은 계층화된 요금제 모델을 사용하여 청구됩니다. 자세한 내용은 AWS Config 요금 페이지를 참조하세요.\n",
            "\"category : Config, question : AWS Config에서 이 기능을 사용하는 것과 관련된 비용이 있나요?, answer : 적합성 팩은 계층화된 요금제 모델을 사용하여 청구됩니다. 자세한 내용은 AWS Config 요금 페이지를 참조하세요.\"\n",
            "다중 계정, 다중 리전 데이터 집계란 무엇인가요?\n",
            "AWS Config의 데이터 집계 기능은 여러 계정과 리전의 AWS Config 데이터를 단일 계정 및 단일 리전에 집계하는 데 도움이 됩니다. 다중 계정 데이터 집계는 중앙 IT 관리자가 엔터프라이즈의 여러 AWS 계정에 대한 규정 준수를 모니터링하는 데 유용합니다.\n",
            "\"category : Config, question : 다중 계정, 다중 리전 데이터 집계란 무엇인가요?, answer : AWS Config의 데이터 집계 기능은 여러 계정과 리전의 AWS Config 데이터를 단일 계정 및 단일 리전에 집계하는 데 도움이 됩니다. 다중 계정 데이터 집계는 중앙 IT 관리자가 엔터프라이즈의 여러 AWS 계정에 대한 규정 준수를 모니터링하는 데 유용합니다.\"\n",
            "데이터 집계 기능을 사용하여 중앙에서 여러 계정에 걸쳐 AWS Conﬁg 규칙을 프로비저닝할 수 있나요?\n",
            "데이터 집계 기능은 여러 계정에서 규칙을 프로비저닝하는 데 사용할 수 없습니다. 규정 준수에 대한 가시성을 제공하는 보고 기능만 제공합니다. 계정과 리전에서 규칙을 프로비저닝하려면 AWS CloudFormation StackSets를 사용할 수 있습니다. 이 블로그 링크에서 자세히 알아보세요.\n",
            "\"category : Config, question : 데이터 집계 기능을 사용하여 중앙에서 여러 계정에 걸쳐 AWS Conﬁg 규칙을 프로비저닝할 수 있나요?, answer : 데이터 집계 기능은 여러 계정에서 규칙을 프로비저닝하는 데 사용할 수 없습니다. 규정 준수에 대한 가시성을 제공하는 보고 기능만 제공합니다. 계정과 리전에서 규칙을 프로비저닝하려면 AWS CloudFormation StackSets를 사용할 수 있습니다. 이 블로그 링크에서 자세히 알아보세요.\"\n",
            "내 계정에서 데이터 집계를 활성화하려면 어떻게 해야 하나요?\n",
            "계정에서 AWS Config와 AWS Config 규칙이 활성화되고 계정이 집계되기 시작되면, 계정에 집계자를 생성하여 데이터 집계를 활성화할 수 있습니다. 자세히 알아보세요.\n",
            "\"category : Config, question : 내 계정에서 데이터 집계를 활성화하려면 어떻게 해야 하나요?, answer : 계정에서 AWS Config와 AWS Config 규칙이 활성화되고 계정이 집계되기 시작되면, 계정에 집계자를 생성하여 데이터 집계를 활성화할 수 있습니다. 자세히 알아보세요.\"\n",
            "집계자란 무엇인가요?\n",
            "집계자는 여러 계정과 리전에서 AWS Config 데이터를 수집하는 AWS Config 리소스 유형입니다. 집계자를 사용하면 여러 계정과 리전에 대해 AWS Config에 기록된 리소스 구성과 규정 준수 데이터를 확인할 수 있습니다.\n",
            "\"category : Config, question : 집계자란 무엇인가요?, answer : 집계자는 여러 계정과 리전에서 AWS Config 데이터를 수집하는 AWS Config 리소스 유형입니다. 집계자를 사용하면 여러 계정과 리전에 대해 AWS Config에 기록된 리소스 구성과 규정 준수 데이터를 확인할 수 있습니다.\"\n",
            "집계 보기 페이지에서는 어떤 정보를 제공하나요?\n",
            "집계 보기에는 조직 전체의 총 미준수 규칙 수, 리소스 수를 기준으로 상위 5개 미준수 규칙, 미준수 규칙이 가장 많은 상위 5개 AWS 계정이 표시됩니다. 그러면 사용자가 드릴다운하여 규칙을 위반한 리소스에 대한 세부 정보와 계정에서 위반하고 있는 규칙 목록을 확인할 수 있습니다.\n",
            "\"category : Config, question : 집계 보기 페이지에서는 어떤 정보를 제공하나요?, answer : 집계 보기에는 조직 전체의 총 미준수 규칙 수, 리소스 수를 기준으로 상위 5개 미준수 규칙, 미준수 규칙이 가장 많은 상위 5개 AWS 계정이 표시됩니다. 그러면 사용자가 드릴다운하여 규칙을 위반한 리소스에 대한 세부 정보와 계정에서 위반하고 있는 규칙 목록을 확인할 수 있습니다.\"\n",
            "AWS Organizations 고객이 아닙니다. 데이터 집계 기능을 사용할 수 있습니까?\n",
            "파일을 업로드하거나 개별적으로 계정을 입력하여 AWS Config 데이터를 집계할 계정을 지정할 수 있습니다. 이러한 계정은 어느 AWS Organizations에도 속해 있지 않으므로 각 계정에서 집계자 계정에 권한을 명시적으로 부여해야 합니다. 자세히 알아보세요.\n",
            "\"category : Config, question : AWS Organizations 고객이 아닙니다. 데이터 집계 기능을 사용할 수 있습니까?, answer : 파일을 업로드하거나 개별적으로 계정을 입력하여 AWS Config 데이터를 집계할 계정을 지정할 수 있습니다. 이러한 계정은 어느 AWS Organizations에도 속해 있지 않으므로 각 계정에서 집계자 계정에 권한을 명시적으로 부여해야 합니다. 자세히 알아보세요.\"\n",
            "계정이 1개만 있는 경우에도 데이터 집계 기능을 사용할 수 있나요?\n",
            "데이터 집계 기능은 다중 리전 집계에도 유용합니다. 따라서 이 기능을 사용하여 여러 리전 전체에서 계정의 AWS Config 데이터를 집계할 수 있습니다.\n",
            "\"category : Config, question : 계정이 1개만 있는 경우에도 데이터 집계 기능을 사용할 수 있나요?, answer : 데이터 집계 기능은 다중 리전 집계에도 유용합니다. 따라서 이 기능을 사용하여 여러 리전 전체에서 계정의 AWS Config 데이터를 집계할 수 있습니다.\"\n",
            "어느 리전에서 다중 계정, 다중 리전 데이터 집계 기능을 사용할 수 있습니까?\n",
            "다중 계정, 다중 리전 데이터 집계가 가능한 리전에 대한 자세한 내용은 AWS Config 개발자 안내서: 다중 계정 다중 리전 데이터 집계를 참조하세요.\n",
            "\"category : Config, question : 어느 리전에서 다중 계정, 다중 리전 데이터 집계 기능을 사용할 수 있습니까?, answer : 다중 계정, 다중 리전 데이터 집계가 가능한 리전에 대한 자세한 내용은 AWS Config 개발자 안내서: 다중 계정 다중 리전 데이터 집계를 참조하세요.\"\n",
            "내 계정에 이 기능을 지원하지 않는 리전이 포함되어 있으면 어떻게 되나요?\n",
            "집계자를 생성할 때 데이터를 집계할 수 있는 리전을 지정합니다. 이 목록에는 이 기능을 사용할 수 있는 리전만 표시됩니다. 또한 ‘all Regions(모든 리전)’를 선택할 수 있으며, 이 경우 다른 리전에 이 기능에 대한 지원이 추가되는 대로 자동으로 데이터를 집계합니다.\n",
            "\"category : Config, question : 내 계정에 이 기능을 지원하지 않는 리전이 포함되어 있으면 어떻게 되나요?, answer : 집계자를 생성할 때 데이터를 집계할 수 있는 리전을 지정합니다. 이 목록에는 이 기능을 사용할 수 있는 리전만 표시됩니다. 또한 ‘all Regions(모든 리전)’를 선택할 수 있으며, 이 경우 다른 리전에 이 기능에 대한 지원이 추가되는 대로 자동으로 데이터를 집계합니다.\"\n",
            "어떤 AWS 리소스 유형에 AWS Config가 적용되나요?\n",
            "지원되는 리소스 유형의 전체 목록은 설명서를 참조하세요.\n",
            "\"category : Config, question : 어떤 AWS 리소스 유형에 AWS Config가 적용되나요?, answer : 지원되는 리소스 유형의 전체 목록은 설명서를 참조하세요.\"\n",
            "어떤 리전에서 AWS Config를 사용할 수 있습니까?\n",
            "AWS Config를 사용할 수 있는 AWS 리전에 대한 자세한 내용은 AWS 리전 표를 참조하세요.\n",
            "\"category : Config, question : 어떤 리전에서 AWS Config를 사용할 수 있습니까?, answer : AWS Config를 사용할 수 있는 AWS 리전에 대한 자세한 내용은 AWS 리전 표를 참조하세요.\"\n",
            "AWS Config에 대한 요금은 어떻게 청구되나요?\n",
            "AWS Config를 사용하면 계정에 기록된 구성 항목 수, 활성 AWS Config 규칙 평가 수 및 적합성 팩 평가 수에 따라 요금이 청구됩니다. 구성 항목은 AWS 계정에 있는 리소스 구성 상태의 레코드를 말합니다. AWS Config가 구성 항목을 제공할 수 있는 빈도는 연속과 주기적 두 가지가 있습니다. 연속 기록은 변경이 발생할 때마다 구성 변경 사항을 기록하고 전달합니다. 주기적 기록은 변경이 발생한 경우에만 24시간마다 한 번 구성 데이터를 제공합니다. AWS Config 규칙 평가는 AWS 계정에서 AWS Config 규칙을 통해 리소스의 규정 준수 상태를 평가하는 것을 말합니다. 적합성 팩 평가는 적합성 팩 내에서 AWS Config 규칙을 통해 리소스를 평가하는 것입니다. 자세한 내용과 예를 보려면 https://aws.amazon.com/config/pricing/을 참조하세요.\n",
            "\"category : Config, question : AWS Config에 대한 요금은 어떻게 청구되나요?, answer : AWS Config를 사용하면 계정에 기록된 구성 항목 수, 활성 AWS Config 규칙 평가 수 및 적합성 팩 평가 수에 따라 요금이 청구됩니다. 구성 항목은 AWS 계정에 있는 리소스 구성 상태의 레코드를 말합니다. AWS Config가 구성 항목을 제공할 수 있는 빈도는 연속과 주기적 두 가지가 있습니다. 연속 기록은 변경이 발생할 때마다 구성 변경 사항을 기록하고 전달합니다. 주기적 기록은 변경이 발생한 경우에만 24시간마다 한 번 구성 데이터를 제공합니다. AWS Config 규칙 평가는 AWS 계정에서 AWS Config 규칙을 통해 리소스의 규정 준수 상태를 평가하는 것을 말합니다. 적합성 팩 평가는 적합성 팩 내에서 AWS Config 규칙을 통해 리소스를 평가하는 것입니다. 자세한 내용과 예를 보려면 https://aws.amazon.com/config/pricing/을 참조하세요.\"\n",
            "AWS Config 규칙 요금에는 Lambda 함수에 대한 비용이 포함되어 있나요?\n",
            "AWS에서 제공하는 관리형 규칙 세트에서 선택하거나, Lambda 함수를 사용하여 사용자 지정 규칙을 작성할 수 있습니다. 관리형 규칙은 AWS에서 모두 관리 및 유지 보수하며 이를 실행하기 위한 Lambda 요금이 추가로 부과되지 않습니다. 관리형 규칙을 활성화하고, 필요한 파라미터를 제공한 다음, 지정된 달의 활성 AWS Config 규칙별로 단일 요금을 지불할 수 있습니다. 사용자 지정 규칙을 사용하면 규칙이 계정의 Lambda 함수로 적용될 때 완벽하게 제어할 수 있습니다. 사용자 지정 AWS Config 규칙에는 활성 규칙에 대한 월별 요금 외에도 표준 Lambda 프리 티어* 및 함수 적용 요금이 적용됩니다.\n",
            "*AWS 프리 티어는 AWS 중국(베이징) 리전이나 AWS 중국(닝샤) 리전에 제공되지 않습니다.\n",
            "\"category : Config, question : AWS Config 규칙 요금에는 Lambda 함수에 대한 비용이 포함되어 있나요?, answer : AWS에서 제공하는 관리형 규칙 세트에서 선택하거나, Lambda 함수를 사용하여 사용자 지정 규칙을 작성할 수 있습니다. 관리형 규칙은 AWS에서 모두 관리 및 유지 보수하며 이를 실행하기 위한 Lambda 요금이 추가로 부과되지 않습니다. 관리형 규칙을 활성화하고, 필요한 파라미터를 제공한 다음, 지정된 달의 활성 AWS Config 규칙별로 단일 요금을 지불할 수 있습니다. 사용자 지정 규칙을 사용하면 규칙이 계정의 Lambda 함수로 적용될 때 완벽하게 제어할 수 있습니다. 사용자 지정 AWS Config 규칙에는 활성 규칙에 대한 월별 요금 외에도 표준 Lambda 프리 티어* 및 함수 적용 요금이 적용됩니다.\n",
            "*AWS 프리 티어는 AWS 중국(베이징) 리전이나 AWS 중국(닝샤) 리전에 제공되지 않습니다.\"\n",
            "사용자 지정 AWS Config 규칙에 대한 Lambda 함수를 변경하려고 합니다. 권장되는 접근 방식은 무엇입니까?\n",
            "새 규칙이 생성되어 활성화될 때마다 요금이 청구됩니다. 규칙과 연관된 Lambda 함수를 업데이트하거나 대체해야 할 경우 권장되는 접근 방식은 규칙을 삭제한 후 새 규칙을 생성하는 대신 규칙을 업데이트하는 것입니다.\n",
            "\"category : Config, question : 사용자 지정 AWS Config 규칙에 대한 Lambda 함수를 변경하려고 합니다. 권장되는 접근 방식은 무엇입니까?, answer : 새 규칙이 생성되어 활성화될 때마다 요금이 청구됩니다. 규칙과 연관된 Lambda 함수를 업데이트하거나 대체해야 할 경우 권장되는 접근 방식은 규칙을 삭제한 후 새 규칙을 생성하는 대신 규칙을 업데이트하는 것입니다.\"\n",
            "AWS Config에서 어떤 AWS 파트너 솔루션을 사용할 수 있습니까?\n",
            "Splunk, ServiceNow, Evident.io, CloudCheckr, Redseal 및 RedHat CloudForms 등 APN 파트너 솔루션은 AWS Config의 데이터와 완전히 통합된 서비스를 제공합니다. 2nd Watch, Cloudnexa 등의 관리형 서비스 공급자도 AWS Config와의 통합을 발표했습니다. 또한 AWS Config 규칙의 경우 CloudHealth Technologies, Alert Logic, Trend Micro 등과 같은 파트너가 제공하는 통합 솔루션도 사용할 수 있습니다. 이러한 솔루션에는 변경 관리와 보안 분석 등의 기능이 포함되어 있으며, AWS 리소스 구성을 시각화, 모니터링 및 관리하는 데 도움이 됩니다.\n",
            "\"category : Config, question : AWS Config에서 어떤 AWS 파트너 솔루션을 사용할 수 있습니까?, answer : Splunk, ServiceNow, Evident.io, CloudCheckr, Redseal 및 RedHat CloudForms 등 APN 파트너 솔루션은 AWS Config의 데이터와 완전히 통합된 서비스를 제공합니다. 2nd Watch, Cloudnexa 등의 관리형 서비스 공급자도 AWS Config와의 통합을 발표했습니다. 또한 AWS Config 규칙의 경우 CloudHealth Technologies, Alert Logic, Trend Micro 등과 같은 파트너가 제공하는 통합 솔루션도 사용할 수 있습니다. 이러한 솔루션에는 변경 관리와 보안 분석 등의 기능이 포함되어 있으며, AWS 리소스 구성을 시각화, 모니터링 및 관리하는 데 도움이 됩니다.\"\n",
            "========== Guardduty  :  https://aws.amazon.com/ko/guardduty/faqs/ 사이트 크롤링 진행중 ==========\n",
            "96\n",
            "Amazon GuardDuty란 무엇인가요?\n",
            "GuardDuty는 AWS 계정, 워크로드, 런타임 활동, 데이터에서 악의적인 활동을 지속적으로 모니터링하는 지능형 위협 탐지 서비스입니다. 비정상 동작, 보안 인증 정보 유출 또는 명령 및 제어(C2) 인프라 통신과 같은 잠재적인 악의적 활동이 탐지되면 보안 가시성 및 수정 지원에 사용할 수 있는 상세한 보안 조사 결과가 GuardDuty에서 생성됩니다.\n",
            "\"category : Guardduty, question : Amazon GuardDuty란 무엇인가요?, answer : GuardDuty는 AWS 계정, 워크로드, 런타임 활동, 데이터에서 악의적인 활동을 지속적으로 모니터링하는 지능형 위협 탐지 서비스입니다. 비정상 동작, 보안 인증 정보 유출 또는 명령 및 제어(C2) 인프라 통신과 같은 잠재적인 악의적 활동이 탐지되면 보안 가시성 및 수정 지원에 사용할 수 있는 상세한 보안 조사 결과가 GuardDuty에서 생성됩니다.\"\n",
            "GuardDuty의 주요 이점은 무엇인가요?\n",
            "GuardDuty를 사용하면 AWS 계정, 워크로드, 런타임 활동을 지속적으로 모니터링할 수 있습니다. GuardDuty는 리소스와 완전히 독립적으로 작동하도록 설계되었으며 워크로드의 성능 또는 가용성에 영향을 미치지 않습니다. 이 서비스는 통합 위협 인텔리전스, 기계 학습(ML) 이상 탐지 및 맬웨어 스캔이 포함된 완전관리형 서비스입니다. GuardDuty는 기존 이벤트 관리 및 워크플로 시스템과 손쉽게 통합되도록 설계된 상세하고 실행 가능한 알림을 제공합니다. 선결제 비용이 없고 추가로 소프트웨어를 배포하거나 위협 인텔리전스 피드를 구독할 필요가 없으며 분석한 이벤트에 대해서만 비용을 지불하면 됩니다.\n",
            "\"category : Guardduty, question : GuardDuty의 주요 이점은 무엇인가요?, answer : GuardDuty를 사용하면 AWS 계정, 워크로드, 런타임 활동을 지속적으로 모니터링할 수 있습니다. GuardDuty는 리소스와 완전히 독립적으로 작동하도록 설계되었으며 워크로드의 성능 또는 가용성에 영향을 미치지 않습니다. 이 서비스는 통합 위협 인텔리전스, 기계 학습(ML) 이상 탐지 및 맬웨어 스캔이 포함된 완전관리형 서비스입니다. GuardDuty는 기존 이벤트 관리 및 워크플로 시스템과 손쉽게 통합되도록 설계된 상세하고 실행 가능한 알림을 제공합니다. 선결제 비용이 없고 추가로 소프트웨어를 배포하거나 위협 인텔리전스 피드를 구독할 필요가 없으며 분석한 이벤트에 대해서만 비용을 지불하면 됩니다.\"\n",
            "GuardDuty의 비용은 얼마인가요?\n",
            "GuardDuty 요금은 분석된 서비스 로그의 볼륨, 가상 CPU(vCPU) 또는 Amazon RDS 이벤트 분석을 위한 Aurora Serverless v2 인스턴스 Aurora 용량 단위(ACU), 런타임 시 모니터링되는 Amazon EKS 워크로드의 수와 크기, 맬웨어를 검사한 데이터의 양을 기준으로 합니다. 분석된 서비스 로그는 비용 최적화를 위해 필터링되고 GuardDuty와 직접 통합되므로 따로 활성화하거나 요금을 지불하지 않아도 됩니다. \n",
            "추가 세부 정보와 요금 예제는 Amazon GuardDuty 요금 페이지를 참조하세요.\n",
            "\"category : Guardduty, question : GuardDuty의 비용은 얼마인가요?, answer : GuardDuty 요금은 분석된 서비스 로그의 볼륨, 가상 CPU(vCPU) 또는 Amazon RDS 이벤트 분석을 위한 Aurora Serverless v2 인스턴스 Aurora 용량 단위(ACU), 런타임 시 모니터링되는 Amazon EKS 워크로드의 수와 크기, 맬웨어를 검사한 데이터의 양을 기준으로 합니다. 분석된 서비스 로그는 비용 최적화를 위해 필터링되고 GuardDuty와 직접 통합되므로 따로 활성화하거나 요금을 지불하지 않아도 됩니다. \n",
            "추가 세부 정보와 요금 예제는 Amazon GuardDuty 요금 페이지를 참조하세요.\"\n",
            "GuardDuty 지급인 계정에 나오는 예상 비용은 연결된 계정에 대해 집계된 총 비용을 나타내나요, 아니면 개별 지급인 계정에만 대한 것인가요?\n",
            "예상 비용은 개별 지급인 계정에 대한 비용을 나타내며 GuardDuty 관리자 계정에서 각 멤버 계정에 대해 청구된 사용량과 평균 일 비용을 확인할 수 있습니다. 자세한 사용량 정보를 보려면 개별 계정으로 이동해야 합니다.\n",
            "\"category : Guardduty, question : GuardDuty 지급인 계정에 나오는 예상 비용은 연결된 계정에 대해 집계된 총 비용을 나타내나요, 아니면 개별 지급인 계정에만 대한 것인가요?, answer : 예상 비용은 개별 지급인 계정에 대한 비용을 나타내며 GuardDuty 관리자 계정에서 각 멤버 계정에 대해 청구된 사용량과 평균 일 비용을 확인할 수 있습니다. 자세한 사용량 정보를 보려면 개별 계정으로 이동해야 합니다.\"\n",
            "GuardDuty의 무료 평가판이 있나요?\n",
            "예. GuardDuty를 처음 사용하는 계정은 30일 동안 무료로 서비스를 사용해 볼 수 있습니다. 무료 평가판에서는 모든 기능 세트와 탐지 서비스에 액세스할 수 있습니다. 평가 기간 동안 GuardDuty 콘솔 사용량 페이지에서 평가 기간 후 예상 비용을 확인할 수 있습니다. GuardDuty 관리자인 경우 멤버 계정의 예상 비용이 표시됩니다. 30일이 지나면 AWS 빌링 콘솔에서 이 기능의 실제 비용을 확인할 수 있습니다.\n",
            "\"category : Guardduty, question : GuardDuty의 무료 평가판이 있나요?, answer : 예. GuardDuty를 처음 사용하는 계정은 30일 동안 무료로 서비스를 사용해 볼 수 있습니다. 무료 평가판에서는 모든 기능 세트와 탐지 서비스에 액세스할 수 있습니다. 평가 기간 동안 GuardDuty 콘솔 사용량 페이지에서 평가 기간 후 예상 비용을 확인할 수 있습니다. GuardDuty 관리자인 경우 멤버 계정의 예상 비용이 표시됩니다. 30일이 지나면 AWS 빌링 콘솔에서 이 기능의 실제 비용을 확인할 수 있습니다.\"\n",
            "GuardDuty와 Amazon Macie의 차이점은 무엇인가요?\n",
            "GuardDuty는 AWS 계정, 워크로드 및 데이터의 광범위한 보안 모니터링을 제공하여 공격자 정찰, 인스턴스, 계정 및 버킷 또는 Amazon EKS 클러스터 침해 및 맬웨어와 같은 위협을 식별하는 데 도움을 줍니다. Macie는 ML 및 패턴 일치를 사용하여 Amazon S3의 민감한 데이터를 검색하는 완전관리형 민감한 데이터 검색 서비스입니다.\n",
            "\"category : Guardduty, question : GuardDuty와 Amazon Macie의 차이점은 무엇인가요?, answer : GuardDuty는 AWS 계정, 워크로드 및 데이터의 광범위한 보안 모니터링을 제공하여 공격자 정찰, 인스턴스, 계정 및 버킷 또는 Amazon EKS 클러스터 침해 및 맬웨어와 같은 위협을 식별하는 데 도움을 줍니다. Macie는 ML 및 패턴 일치를 사용하여 Amazon S3의 민감한 데이터를 검색하는 완전관리형 민감한 데이터 검색 서비스입니다.\"\n",
            "GuardDuty는 리전별 서비스인가요? 아니면 글로벌 서비스인가요?\n",
            "GuardDuty는 리전별 서비스입니다. 여러 계정이 사용되고 여러 AWS 리전이 사용되더라도 GuardDuty 보안 탐지 결과는 해당 데이터가 생성된 리전과 동일한 리전에 유지됩니다. 따라서 분석된 모든 데이터가 리전을 기반으로 하고 AWS 리전 경계를 벗어나지 않습니다. 그러나 리전 전체에서 GuardDuty를 통해 생성된 보안 조사 결과를 집계하도록 선택할 수 있습니다. Amazon EventBridge를 사용하거나 Amazon S3와 같은 데이터 스토어로 결과를 푸시한 다음 적절한 조사 결과를 집계할 수 있습니다. GuardDuty 조사 결과를 AWS Security Hub로 보내고 크로스 리전 집계 활성화 기능을 사용할 수도 있습니다.\n",
            "\"category : Guardduty, question : GuardDuty는 리전별 서비스인가요? 아니면 글로벌 서비스인가요?, answer : GuardDuty는 리전별 서비스입니다. 여러 계정이 사용되고 여러 AWS 리전이 사용되더라도 GuardDuty 보안 탐지 결과는 해당 데이터가 생성된 리전과 동일한 리전에 유지됩니다. 따라서 분석된 모든 데이터가 리전을 기반으로 하고 AWS 리전 경계를 벗어나지 않습니다. 그러나 리전 전체에서 GuardDuty를 통해 생성된 보안 조사 결과를 집계하도록 선택할 수 있습니다. Amazon EventBridge를 사용하거나 Amazon S3와 같은 데이터 스토어로 결과를 푸시한 다음 적절한 조사 결과를 집계할 수 있습니다. GuardDuty 조사 결과를 AWS Security Hub로 보내고 크로스 리전 집계 활성화 기능을 사용할 수도 있습니다.\"\n",
            "GuardDuty는 어느 리전을 지원합니까?\n",
            "GuardDuty 리전 가용성은 AWS 리전 서비스 목록에 나열되어 있습니다.\n",
            "\"category : Guardduty, question : GuardDuty는 어느 리전을 지원합니까?, answer : GuardDuty 리전 가용성은 AWS 리전 서비스 목록에 나열되어 있습니다.\"\n",
            "GuardDuty의 협력 파트너는 누구입니까?\n",
            "많은 기술 파트너가 GuardDuty에 통합되어 있고 GuardDuty를 기반으로 구축되어 있습니다. 또한 GuardDuty를 전문적으로 다루는 컨설팅, 시스템 통합 사업자 및 관리형 보안 서비스 제공업체도 있습니다. 자세한 내용은 Amazon GuardDuty 파트너 페이지를 참조하십시오.\n",
            "\"category : Guardduty, question : GuardDuty의 협력 파트너는 누구입니까?, answer : 많은 기술 파트너가 GuardDuty에 통합되어 있고 GuardDuty를 기반으로 구축되어 있습니다. 또한 GuardDuty를 전문적으로 다루는 컨설팅, 시스템 통합 사업자 및 관리형 보안 서비스 제공업체도 있습니다. 자세한 내용은 Amazon GuardDuty 파트너 페이지를 참조하십시오.\"\n",
            "GuardDuty는 지불 카드 업계 데이터 보안 표준(PCI DSS)의 요구 사항을 해결하는 데 도움이 되나요?\n",
            "Foregenix에서 발표한 백서에서는 규정 준수 요구 사항의 준수를 지원하는 데 GuardDuty가 얼마나 효과적인지 자세한 평가를 제공합니다. 예를 들어 네트워크의 중요한 지점에서 침입 탐지 기술을 적용하도록 요구하는 PCI DSS 요구 사항 11.4가 포함됩니다.\n",
            "\"category : Guardduty, question : GuardDuty는 지불 카드 업계 데이터 보안 표준(PCI DSS)의 요구 사항을 해결하는 데 도움이 되나요?, answer : Foregenix에서 발표한 백서에서는 규정 준수 요구 사항의 준수를 지원하는 데 GuardDuty가 얼마나 효과적인지 자세한 평가를 제공합니다. 예를 들어 네트워크의 중요한 지점에서 침입 탐지 기술을 적용하도록 요구하는 PCI DSS 요구 사항 11.4가 포함됩니다.\"\n",
            "GuardDuty를 사용하려면 어떻게 해야 하나요?\n",
            "AWS Management Console에서 클릭 몇 번으로 GuardDuty를 설정하고 배포할 수 있습니다. 사용하도록 설정하면 GuardDuty가 즉시 거의 실시간 및 대규모로 계정과 네트워크 활동의 연속 스트림을 분석하기 시작합니다. 배포하거나 관리할 추가 보안 소프트웨어, 센서 또는 네트워크 어플라이언스는 없습니다. 위협 인텔리전스가 서비스에 사전 통합되어 있고 지속적으로 업데이트 및 유지 관리됩니다.\n",
            "\"category : Guardduty, question : GuardDuty를 사용하려면 어떻게 해야 하나요?, answer : AWS Management Console에서 클릭 몇 번으로 GuardDuty를 설정하고 배포할 수 있습니다. 사용하도록 설정하면 GuardDuty가 즉시 거의 실시간 및 대규모로 계정과 네트워크 활동의 연속 스트림을 분석하기 시작합니다. 배포하거나 관리할 추가 보안 소프트웨어, 센서 또는 네트워크 어플라이언스는 없습니다. 위협 인텔리전스가 서비스에 사전 통합되어 있고 지속적으로 업데이트 및 유지 관리됩니다.\"\n",
            "GuardDuty로 여러 계정을 관리할 수 있나요?\n",
            "예. GuardDuty에는 다중 계정 관리 기능이 있어 단일 관리자 계정에서 여러 AWS 계정을 연결하고 관리할 수 있습니다. 이 기능을 사용하면 모든 보안 탐지 결과가 검토 및 개선을 위해 관리자 계정으로 집계됩니다. 이 구성을 사용하면 Amazon EventBridge 이벤트도 GuardDuty 관리자 계정으로 집계됩니다. 또한 GuardDuty를 AWS Organizations와 통합하면 조직의 GuardDuty에 대한 관리자 계정을 위임할 수 있습니다. 이 위임된 관리자(DA) 계정은 모든 결과를 통합하고 모든 구성원 계정을 구성할 수 있는 중앙 집중식 계정입니다.\n",
            "\"category : Guardduty, question : GuardDuty로 여러 계정을 관리할 수 있나요?, answer : 예. GuardDuty에는 다중 계정 관리 기능이 있어 단일 관리자 계정에서 여러 AWS 계정을 연결하고 관리할 수 있습니다. 이 기능을 사용하면 모든 보안 탐지 결과가 검토 및 개선을 위해 관리자 계정으로 집계됩니다. 이 구성을 사용하면 Amazon EventBridge 이벤트도 GuardDuty 관리자 계정으로 집계됩니다. 또한 GuardDuty를 AWS Organizations와 통합하면 조직의 GuardDuty에 대한 관리자 계정을 위임할 수 있습니다. 이 위임된 관리자(DA) 계정은 모든 결과를 통합하고 모든 구성원 계정을 구성할 수 있는 중앙 집중식 계정입니다.\"\n",
            "GuardDuty는 어떤 데이터 소스를 분석합니까?\n",
            "GuardDuty가 분석하는 기본 데이터 소스에는 AWS CloudTrail 관리 이벤트 로그, CloudTrail 관리 이벤트, Amazon EC2 VPC 흐름 로그 및 DNS 쿼리 로그가 포함됩니다. GuardDuty 보호 플랜은 CloudTrail S3 데이터 이벤트(S3 보호), Amazon EKS 감사 로그 및 Amazon EKS의 런타임 활동(EKS 보호), Amazon ECS의 런타임 활동(ECS 런타임 모니터링), Amazon EC2의 런타임 활동(EC2 런타임 모니터링), Amazon EBS 볼륨 데이터(맬웨어 방지), Amazon Aurora 로그인 이벤트(RDS 보호), 네트워크 활동 로그(Lambda 보호)와 같은 다른 리소스 유형을 모니터링합니다. 이 서비스는 거의 실시간 보안 탐지 처리를 위해 대규모 데이터 볼륨을 소비하도록 최적화되었습니다. GuardDuty에서는 클라우드용으로 개발 및 최적화되었고 GuardDuty 엔지니어링 팀에서 유지 관리하고 지속적으로 개선하는 내장된 탐지 기법에 액세스할 수 있습니다.\n",
            "\"category : Guardduty, question : GuardDuty는 어떤 데이터 소스를 분석합니까?, answer : GuardDuty가 분석하는 기본 데이터 소스에는 AWS CloudTrail 관리 이벤트 로그, CloudTrail 관리 이벤트, Amazon EC2 VPC 흐름 로그 및 DNS 쿼리 로그가 포함됩니다. GuardDuty 보호 플랜은 CloudTrail S3 데이터 이벤트(S3 보호), Amazon EKS 감사 로그 및 Amazon EKS의 런타임 활동(EKS 보호), Amazon ECS의 런타임 활동(ECS 런타임 모니터링), Amazon EC2의 런타임 활동(EC2 런타임 모니터링), Amazon EBS 볼륨 데이터(맬웨어 방지), Amazon Aurora 로그인 이벤트(RDS 보호), 네트워크 활동 로그(Lambda 보호)와 같은 다른 리소스 유형을 모니터링합니다. 이 서비스는 거의 실시간 보안 탐지 처리를 위해 대규모 데이터 볼륨을 소비하도록 최적화되었습니다. GuardDuty에서는 클라우드용으로 개발 및 최적화되었고 GuardDuty 엔지니어링 팀에서 유지 관리하고 지속적으로 개선하는 내장된 탐지 기법에 액세스할 수 있습니다.\"\n",
            "GuardDuty로 작업을 시작하는 데 얼마나 걸리나요?\n",
            "사용하도록 설정하면 Amazon GuardDuty가 악의적 또는 무단 활동에 대한 분석을 시작합니다. 결과를 수신하기 시작할 때까지 걸리는 시간은 계정의 활동 수준에 따라 달라집니다. GuardDuty는 기록 데이터는 확인하지 않으며 서비스가 사용된 이후에 시작된 활동만 확인합니다. GuardDuty가 잠재적 위협을 파악하면 GuardDuty 콘솔에서 결과를 볼 수 있습니다.\n",
            "\"category : Guardduty, question : GuardDuty로 작업을 시작하는 데 얼마나 걸리나요?, answer : 사용하도록 설정하면 Amazon GuardDuty가 악의적 또는 무단 활동에 대한 분석을 시작합니다. 결과를 수신하기 시작할 때까지 걸리는 시간은 계정의 활동 수준에 따라 달라집니다. GuardDuty는 기록 데이터는 확인하지 않으며 서비스가 사용된 이후에 시작된 활동만 확인합니다. GuardDuty가 잠재적 위협을 파악하면 GuardDuty 콘솔에서 결과를 볼 수 있습니다.\"\n",
            "GuardDuty가 작동하려면 CloudTrail, VPC 흐름 로그, DNS 쿼리 로그 또는 Amazon EKS 감사 로그를 사용해야 하나요?\n",
            "아니요. GuardDuty는 CloudTrail, VPC 흐름 로그, DNS 쿼리 로그 및 Amazon EKS에서 직접 독립된 데이터 스트림을 가져옵니다. Amazon S3 버킷 정책을 관리하거나 로그를 수집 및 저장하는 방법을 수정할 필요가 없습니다. GuardDuty 권한은 서비스 연결 역할로 관리됩니다. 언제든지 GuardDuty를 사용 중지할 수 있으며 사용 중지하면 모든 GuardDuty 권한이 제거됩니다. 복잡한 구성이 필요하지 않으므로 서비스를 더 쉽게 사용할 수 있습니다. 또한 서비스 연결 역할을 사용하면 AWS Identity and Access Management(IAM) 권한 구성 오류나 Amazon S3 버킷 정책 변경으로 서비스 운영이 영향을 받는 일이 발생하지 않습니다. 마지막으로 서비스 연결 역할은 GuardDuty에서 계정 또는 워크로드의 성능 및 가용성에 영향을 미치지 않거나 그 영향을 최소화하면서 거의 실시간으로 다량의 데이터를 효율적으로 사용할 수 있도록 합니다.\n",
            "\"category : Guardduty, question : GuardDuty가 작동하려면 CloudTrail, VPC 흐름 로그, DNS 쿼리 로그 또는 Amazon EKS 감사 로그를 사용해야 하나요?, answer : 아니요. GuardDuty는 CloudTrail, VPC 흐름 로그, DNS 쿼리 로그 및 Amazon EKS에서 직접 독립된 데이터 스트림을 가져옵니다. Amazon S3 버킷 정책을 관리하거나 로그를 수집 및 저장하는 방법을 수정할 필요가 없습니다. GuardDuty 권한은 서비스 연결 역할로 관리됩니다. 언제든지 GuardDuty를 사용 중지할 수 있으며 사용 중지하면 모든 GuardDuty 권한이 제거됩니다. 복잡한 구성이 필요하지 않으므로 서비스를 더 쉽게 사용할 수 있습니다. 또한 서비스 연결 역할을 사용하면 AWS Identity and Access Management(IAM) 권한 구성 오류나 Amazon S3 버킷 정책 변경으로 서비스 운영이 영향을 받는 일이 발생하지 않습니다. 마지막으로 서비스 연결 역할은 GuardDuty에서 계정 또는 워크로드의 성능 및 가용성에 영향을 미치지 않거나 그 영향을 최소화하면서 거의 실시간으로 다량의 데이터를 효율적으로 사용할 수 있도록 합니다.\"\n",
            "내 계정에서 GuardDuty를 사용하면 성능이나 가용성에 영향을 미치게 되나요?\n",
            "GuardDuty를 처음으로 사용하도록 설정하면 AWS 리소스와 완전히 독립적으로 작동합니다. GuardDuty 보안 에이전트를 자동으로 배포하도록 GuardDuty EKS Runtime Monitoring을 구성하면 리소스 사용률이 높아질 수 있으며 Amazon EKS 클러스터를 실행하는 데 사용되는 VPC에 VPC 엔드포인트가 생성될 수도 있습니다.\n",
            "\"category : Guardduty, question : 내 계정에서 GuardDuty를 사용하면 성능이나 가용성에 영향을 미치게 되나요?, answer : GuardDuty를 처음으로 사용하도록 설정하면 AWS 리소스와 완전히 독립적으로 작동합니다. GuardDuty 보안 에이전트를 자동으로 배포하도록 GuardDuty EKS Runtime Monitoring을 구성하면 리소스 사용률이 높아질 수 있으며 Amazon EKS 클러스터를 실행하는 데 사용되는 VPC에 VPC 엔드포인트가 생성될 수도 있습니다.\"\n",
            "GuardDuty는 내 로그를 관리하거나 유지하나요?\n",
            "아니요. GuardDuty는 로그를 관리하거나 유지하지 않습니다. GuardDuty에서 소비하는 모든 데이터는 거의 실시간으로 분석되고 분석 후 폐기됩니다. 따라서 GuardDuty가 매우 효율적이고 비용 효과적이 될 수 있고 데이터 잔류의 위험도 줄일 수 있습니다. 로그 전달 및 보존의 경우 모든 기능을 갖춘 전달 및 보존 옵션을 제공하는 AWS 로깅 및 모니터링 서비스를 직접 사용해야 합니다.\n",
            "\"category : Guardduty, question : GuardDuty는 내 로그를 관리하거나 유지하나요?, answer : 아니요. GuardDuty는 로그를 관리하거나 유지하지 않습니다. GuardDuty에서 소비하는 모든 데이터는 거의 실시간으로 분석되고 분석 후 폐기됩니다. 따라서 GuardDuty가 매우 효율적이고 비용 효과적이 될 수 있고 데이터 잔류의 위험도 줄일 수 있습니다. 로그 전달 및 보존의 경우 모든 기능을 갖춘 전달 및 보존 옵션을 제공하는 AWS 로깅 및 모니터링 서비스를 직접 사용해야 합니다.\"\n",
            "GuardDuty가 내 로그 및 데이터 소스를 확인할 수 없도록 하려면 어떻게 해야 하나요?\n",
            "언제든지 일반 설정에서 서비스 일시 중단을 선택하여 GuardDuty가 데이터 소스를 분석하지 못하도록 할 수 있습니다. 그러면 서비스가 데이터 분석을 즉시 중단하지만, 기존 탐지 결과 또는 구성은 삭제하지 않습니다. 또한, 일반 설정에서 서비스 사용 중지를 선택할 수 있습니다. 그러면 서비스 권한을 반납하고 서비스를 재설정하기 전에 기존 탐지 결과와 구성을 비롯하여 모든 남아 있는 데이터가 삭제됩니다. Management Console 또는 AWS CLI를 통해 GuardDuty S3 Protection이나 GuardDuty EKS Protection 같은 기능을 따로 사용 중지할 수도 있습니다.\n",
            "\"category : Guardduty, question : GuardDuty가 내 로그 및 데이터 소스를 확인할 수 없도록 하려면 어떻게 해야 하나요?, answer : 언제든지 일반 설정에서 서비스 일시 중단을 선택하여 GuardDuty가 데이터 소스를 분석하지 못하도록 할 수 있습니다. 그러면 서비스가 데이터 분석을 즉시 중단하지만, 기존 탐지 결과 또는 구성은 삭제하지 않습니다. 또한, 일반 설정에서 서비스 사용 중지를 선택할 수 있습니다. 그러면 서비스 권한을 반납하고 서비스를 재설정하기 전에 기존 탐지 결과와 구성을 비롯하여 모든 남아 있는 데이터가 삭제됩니다. Management Console 또는 AWS CLI를 통해 GuardDuty S3 Protection이나 GuardDuty EKS Protection 같은 기능을 따로 사용 중지할 수도 있습니다.\"\n",
            "GuardDuty는 무엇을 탐지할 수 있나요?\n",
            "GuardDuty에서는 클라우드용으로 개발 및 최적화된 내장된 탐지 기법에 액세스할 수 있습니다. 탐지 알고리즘은 AWS GuardDuty 엔지니어가 유지 관리하고 지속적으로 개선합니다. 기본 탐지 카테고리는 다음과 같습니다.\n",
            "\n",
            "정찰: 비정상적인 API 활동, VPC 내 포트 스캐닝, 실패한 로그인 요청의 특이한 패턴, 알려진 악성 IP에서 차단되지 않은 포트 탐색 등 공격자의 정찰로 보이는 활동입니다.\n",
            "인스턴스 침해: 암호 화폐 마이닝, Domain Generation Algorithm(DGA)을 사용하는 맬웨어, 아웃바운드 DoS 활동, 비정상적으로 높은 네트워크 트래픽 볼륨, 비정상적인 네트워크 프로토콜, 알려진 악의적 IP를 통한 아웃바운드 인스턴스 통신, 외부 IP 주소에서 임시 Amazon EC2 보안 인증 사용, DNS를 사용한 데이터 탈출 등 인스턴스 침해를 나타내는 활동입니다.\n",
            "계정 침해: 계정 침해를 나타내는 일반적인 패턴에는 특이한 지리적 위치 또는 익명 프록시로부터 API 호출, CloudTrail 로깅을 비활성화하려는 시도, 비정상적인 인스턴스 또는 인프라 시작, 특이한 리전에 인프라 배포, 보안 인증 정보 유출, 의심스러운 데이터베이스 로그인 활동 및 알려진 악의적 IP 주소로부터 API 호출이 포함됩니다.\n",
            "버킷 침해: 버킷 침해를 나타내는 활동입니다. 예를 들어 보안 인증 정보 악용을 나타내는 의심스러운 데이터 액세스 패턴, 원격 호스트의 비정상적인 Amazon S3 API 활동, 알려진 IP 주소로부터의 무단 Amazon S3 액세스, 버킷에 액세스한 기록이 없거나 비정상적인 위치에서 호출된 사용자가 Amazon S3 버킷에서 데이터를 검색하기 위한 API 호출 등이 있습니다. GuardDuty는 CloudTrail S3 데이터 이벤트(예: GetObject, ListObjects, DeleteObject)를 지속적으로 모니터링 및 분석하여 모든 Amazon S3 버킷에서 의심스러운 활동을 탐지합니다.\n",
            "맬웨어 탐지: GuardDuty는 Amazon EC2 인스턴스 또는 컨테이너 워크로드에서 맬웨어를 나타내는 의심스러운 동작이 식별될 때 맬웨어 탐지 스캔을 시작합니다. GuardDuty는 이러한 Amazon EC2 인스턴스 또는 컨테이너 워크로드에 연결된 Amazon EBS 볼륨의 임시 복제본을 생성하고 이 볼륨 복제본에서 워크로드를 침해하고 리소스를 악의적인 용도로 재사용하며 데이터에 무단으로 액세스하는 데 사용될 수 있는 트로이 목마, 웜, 암호 화폐 채굴기, 루트킷, 봇 등을 스캔합니다. GuardDuty 맬웨어 방지는 의심스러운 동작의 소스를 검증할 수 있는 상황별 조사 결과를 생성합니다. 이러한 조사 결과를 적절한 관리자에게 라우팅하여 자동화된 수정 작업을 시작할 수 있습니다.\n",
            "컨테이너 침해: 컨테이너 워크로드에서 악의적의거나 의심스러운 동작을 식별하는 활동은 Amazon EKS 클러스터의 지속적인 모니터링 및 프로파일링과 Amazon EKS 감사 로그 및 컨테이너 런타임 활동 분석을 통해 탐지됩니다.\n",
            "\"category : Guardduty, question : GuardDuty는 무엇을 탐지할 수 있나요?, answer : GuardDuty에서는 클라우드용으로 개발 및 최적화된 내장된 탐지 기법에 액세스할 수 있습니다. 탐지 알고리즘은 AWS GuardDuty 엔지니어가 유지 관리하고 지속적으로 개선합니다. 기본 탐지 카테고리는 다음과 같습니다.\n",
            "\n",
            "정찰: 비정상적인 API 활동, VPC 내 포트 스캐닝, 실패한 로그인 요청의 특이한 패턴, 알려진 악성 IP에서 차단되지 않은 포트 탐색 등 공격자의 정찰로 보이는 활동입니다.\n",
            "인스턴스 침해: 암호 화폐 마이닝, Domain Generation Algorithm(DGA)을 사용하는 맬웨어, 아웃바운드 DoS 활동, 비정상적으로 높은 네트워크 트래픽 볼륨, 비정상적인 네트워크 프로토콜, 알려진 악의적 IP를 통한 아웃바운드 인스턴스 통신, 외부 IP 주소에서 임시 Amazon EC2 보안 인증 사용, DNS를 사용한 데이터 탈출 등 인스턴스 침해를 나타내는 활동입니다.\n",
            "계정 침해: 계정 침해를 나타내는 일반적인 패턴에는 특이한 지리적 위치 또는 익명 프록시로부터 API 호출, CloudTrail 로깅을 비활성화하려는 시도, 비정상적인 인스턴스 또는 인프라 시작, 특이한 리전에 인프라 배포, 보안 인증 정보 유출, 의심스러운 데이터베이스 로그인 활동 및 알려진 악의적 IP 주소로부터 API 호출이 포함됩니다.\n",
            "버킷 침해: 버킷 침해를 나타내는 활동입니다. 예를 들어 보안 인증 정보 악용을 나타내는 의심스러운 데이터 액세스 패턴, 원격 호스트의 비정상적인 Amazon S3 API 활동, 알려진 IP 주소로부터의 무단 Amazon S3 액세스, 버킷에 액세스한 기록이 없거나 비정상적인 위치에서 호출된 사용자가 Amazon S3 버킷에서 데이터를 검색하기 위한 API 호출 등이 있습니다. GuardDuty는 CloudTrail S3 데이터 이벤트(예: GetObject, ListObjects, DeleteObject)를 지속적으로 모니터링 및 분석하여 모든 Amazon S3 버킷에서 의심스러운 활동을 탐지합니다.\n",
            "맬웨어 탐지: GuardDuty는 Amazon EC2 인스턴스 또는 컨테이너 워크로드에서 맬웨어를 나타내는 의심스러운 동작이 식별될 때 맬웨어 탐지 스캔을 시작합니다. GuardDuty는 이러한 Amazon EC2 인스턴스 또는 컨테이너 워크로드에 연결된 Amazon EBS 볼륨의 임시 복제본을 생성하고 이 볼륨 복제본에서 워크로드를 침해하고 리소스를 악의적인 용도로 재사용하며 데이터에 무단으로 액세스하는 데 사용될 수 있는 트로이 목마, 웜, 암호 화폐 채굴기, 루트킷, 봇 등을 스캔합니다. GuardDuty 맬웨어 방지는 의심스러운 동작의 소스를 검증할 수 있는 상황별 조사 결과를 생성합니다. 이러한 조사 결과를 적절한 관리자에게 라우팅하여 자동화된 수정 작업을 시작할 수 있습니다.\n",
            "컨테이너 침해: 컨테이너 워크로드에서 악의적의거나 의심스러운 동작을 식별하는 활동은 Amazon EKS 클러스터의 지속적인 모니터링 및 프로파일링과 Amazon EKS 감사 로그 및 컨테이너 런타임 활동 분석을 통해 탐지됩니다.\"\n",
            "GuardDuty 위협 인텔리전스란 무엇인가요?\n",
            "GuardDuty 위협 인텔리전스는 공격자가 사용하는 것으로 알려진 IP 주소와 도메인으로 구성되어 있습니다. GuardDuty 위협 인텔리전스는 AWS 및 서드 파티 제공업체(Proofpoint, CrowdStrike 등)가 제공합니다. 이러한 위협 인텔리전스 피드는 사전에 통합되어 있으며 지속적으로 GuardDuty에 업데이트되고 추가 비용은 부과되지 않습니다.\n",
            "\"category : Guardduty, question : GuardDuty 위협 인텔리전스란 무엇인가요?, answer : GuardDuty 위협 인텔리전스는 공격자가 사용하는 것으로 알려진 IP 주소와 도메인으로 구성되어 있습니다. GuardDuty 위협 인텔리전스는 AWS 및 서드 파티 제공업체(Proofpoint, CrowdStrike 등)가 제공합니다. 이러한 위협 인텔리전스 피드는 사전에 통합되어 있으며 지속적으로 GuardDuty에 업데이트되고 추가 비용은 부과되지 않습니다.\"\n",
            "자체 위협 인텔리전스를 제공할 수 있나요?\n",
            "예. GuardDuty에서 자체 위협 인텔리전스 또는 신뢰할 수 있는 IP 주소 목록을 업로드할 수 있습니다. 이 기능을 사용하면, 해당 목록은 사용자 계정에만 적용되고 다른 고객과 공유되지 않습니다.\n",
            "\"category : Guardduty, question : 자체 위협 인텔리전스를 제공할 수 있나요?, answer : 예. GuardDuty에서 자체 위협 인텔리전스 또는 신뢰할 수 있는 IP 주소 목록을 업로드할 수 있습니다. 이 기능을 사용하면, 해당 목록은 사용자 계정에만 적용되고 다른 고객과 공유되지 않습니다.\"\n",
            "보안 탐지 결과는 어떻게 전달되나요?\n",
            "잠재적 위협이 탐지되면, GuardDuty에서 상세한 보안 탐지 결과를 GuardDuty 콘솔과 EventBridge로 전달합니다. 그러면 알림을 토대로 조치를 취할 수 있고 기존 이벤트 관리 또는 워크로드 시스템에 손쉽게 통합할 수 있습니다. 이러한 탐지 결과에는 카테고리, 영향을 받은 리소스, 리소스와 연결된 메타데이터, 심각도 수준이 포함됩니다.\n",
            "\"category : Guardduty, question : 보안 탐지 결과는 어떻게 전달되나요?, answer : 잠재적 위협이 탐지되면, GuardDuty에서 상세한 보안 탐지 결과를 GuardDuty 콘솔과 EventBridge로 전달합니다. 그러면 알림을 토대로 조치를 취할 수 있고 기존 이벤트 관리 또는 워크로드 시스템에 손쉽게 통합할 수 있습니다. 이러한 탐지 결과에는 카테고리, 영향을 받은 리소스, 리소스와 연결된 메타데이터, 심각도 수준이 포함됩니다.\"\n",
            "GuardDuty 탐지 결과는 형식이 어떻게 되나요?\n",
            "GuardDuty 탐지 결과는 Macie와 Amazon Inspector에서 사용하는 것과 같은 일반적인 JavaScript Object Notation(JSON) 형식으로 제공됩니다. 따라서 고객과 파트너는 손쉽게 세 가지 서비스 모두의 보안 탐지 결과를 소비하고 이를 좀 더 광범위한 이벤트 관리, 워크플로 또는 보안 솔루션에 통합할 수 있습니다.\n",
            "\"category : Guardduty, question : GuardDuty 탐지 결과는 형식이 어떻게 되나요?, answer : GuardDuty 탐지 결과는 Macie와 Amazon Inspector에서 사용하는 것과 같은 일반적인 JavaScript Object Notation(JSON) 형식으로 제공됩니다. 따라서 고객과 파트너는 손쉽게 세 가지 서비스 모두의 보안 탐지 결과를 소비하고 이를 좀 더 광범위한 이벤트 관리, 워크플로 또는 보안 솔루션에 통합할 수 있습니다.\"\n",
            "보안 탐지 결과는 GuardDuty에서 얼마 동안 사용할 수 있나요?\n",
            "보안 탐지 결과는 GuardDuty 콘솔 및 API에서 90일 동안 사용할 수 있도록 보존됩니다. 90일 이후에는 조사 결과가 폐기됩니다. 90일 넘게 조사 결과를 보존하려면 조사 결과를 계정 내 Amazon S3 버킷 또는 장기 보존용 다른 데이터 스토어로 자동으로 푸시하도록 EventBridge를 설정하면 됩니다.\n",
            "\"category : Guardduty, question : 보안 탐지 결과는 GuardDuty에서 얼마 동안 사용할 수 있나요?, answer : 보안 탐지 결과는 GuardDuty 콘솔 및 API에서 90일 동안 사용할 수 있도록 보존됩니다. 90일 이후에는 조사 결과가 폐기됩니다. 90일 넘게 조사 결과를 보존하려면 조사 결과를 계정 내 Amazon S3 버킷 또는 장기 보존용 다른 데이터 스토어로 자동으로 푸시하도록 EventBridge를 설정하면 됩니다.\"\n",
            "GuardDuty 조사 결과를 집계할 수 있나요?\n",
            "예. 리전 전체에서 GuardDuty를 통해 생성된 보안 조사 결과를 집계하도록 선택할 수 있습니다. EventBridge를 사용하거나 Amazon S3와 같은 데이터 스토어로 결과를 푸시한 다음 적절한 조사 결과를 집계할 수 있습니다. GuardDuty 조사 결과를 Security Hub로 보내고 크로스 리전 집계 활성화 기능을 사용할 수도 있습니다.\n",
            "\"category : Guardduty, question : GuardDuty 조사 결과를 집계할 수 있나요?, answer : 예. 리전 전체에서 GuardDuty를 통해 생성된 보안 조사 결과를 집계하도록 선택할 수 있습니다. EventBridge를 사용하거나 Amazon S3와 같은 데이터 스토어로 결과를 푸시한 다음 적절한 조사 결과를 집계할 수 있습니다. GuardDuty 조사 결과를 Security Hub로 보내고 크로스 리전 집계 활성화 기능을 사용할 수도 있습니다.\"\n",
            "GuardDuty를 사용해 자동 예방 조치를 수행할 수 있나요?\n",
            "GuardDuty, EventBridge 및 AWS Lambda를 사용하면 보안 탐지 결과에 따라 자동 수정 조치를 설정할 수 있습니다. 예를 들어 보안 조사 결과에 따라 AWS 보안 그룹 규칙을 수정하도록 Lambda 함수를 생성할 수 있습니다. 알려진 악의적 IP가 Amazon EC2 인스턴스 중 하나를 탐색하고 있다는 GuardDuty 조사 결과가 나오면, EventBridge 규칙을 통해 Lambda 함수를 시작하여 자동으로 보안 그룹 규칙을 수정하고 해당 포트에 대한 액세스를 제한할 수 있습니다.\n",
            "\"category : Guardduty, question : GuardDuty를 사용해 자동 예방 조치를 수행할 수 있나요?, answer : GuardDuty, EventBridge 및 AWS Lambda를 사용하면 보안 탐지 결과에 따라 자동 수정 조치를 설정할 수 있습니다. 예를 들어 보안 조사 결과에 따라 AWS 보안 그룹 규칙을 수정하도록 Lambda 함수를 생성할 수 있습니다. 알려진 악의적 IP가 Amazon EC2 인스턴스 중 하나를 탐색하고 있다는 GuardDuty 조사 결과가 나오면, EventBridge 규칙을 통해 Lambda 함수를 시작하여 자동으로 보안 그룹 규칙을 수정하고 해당 포트에 대한 액세스를 제한할 수 있습니다.\"\n",
            "GuardDuty 탐지 기능은 어떻게 개발 및 관리되나요?\n",
            "GuardDuty에는 탐지 기능의 엔지니어링, 관리 및 반복을 담당하는 팀이 있습니다. 따라서 서비스의 새로운 탐지 기능을 꾸준히 공급하고 기존 탐지 기능에 대한 지속적 반복을 생성할 수 있습니다. GuardDuty 사용자 인터페이스(UI)에 있는 보안 탐지 결과별 좋아요 및 싫어요와 같은 몇 가지 피드백 메커니즘이 서비스에 탑재되어 있습니다. 이를 통해 피드백을 제공할 수 있고 그 결과는 향후 GuardDuty 탐지 기능 반복에 반영될 수 있습니다.\n",
            "\"category : Guardduty, question : GuardDuty 탐지 기능은 어떻게 개발 및 관리되나요?, answer : GuardDuty에는 탐지 기능의 엔지니어링, 관리 및 반복을 담당하는 팀이 있습니다. 따라서 서비스의 새로운 탐지 기능을 꾸준히 공급하고 기존 탐지 기능에 대한 지속적 반복을 생성할 수 있습니다. GuardDuty 사용자 인터페이스(UI)에 있는 보안 탐지 결과별 좋아요 및 싫어요와 같은 몇 가지 피드백 메커니즘이 서비스에 탑재되어 있습니다. 이를 통해 피드백을 제공할 수 있고 그 결과는 향후 GuardDuty 탐지 기능 반복에 반영될 수 있습니다.\"\n",
            "Amazon GuardDuty에 사용자 지정 탐지 기능을 작성할 수 있나요?\n",
            "아니요. GuardDuty는 자체 사용자 지정 규칙 세트를 개발하고 유지 관리하는 복잡성과 부담을 없애줍니다. 고객 피드백과 AWS 보안 엔지니어 및 GuardDuty 엔지니어링 팀의 연구 결과에 따라 새로운 탐지 기능이 지속적으로 추가됩니다. 그러나 고객이 구성할 수 있는 사용자 지정 기능에는 자체 위협 목록 및 신뢰할 수 있는 IP 주소 목록이 있습니다.\n",
            "\"category : Guardduty, question : Amazon GuardDuty에 사용자 지정 탐지 기능을 작성할 수 있나요?, answer : 아니요. GuardDuty는 자체 사용자 지정 규칙 세트를 개발하고 유지 관리하는 복잡성과 부담을 없애줍니다. 고객 피드백과 AWS 보안 엔지니어 및 GuardDuty 엔지니어링 팀의 연구 결과에 따라 새로운 탐지 기능이 지속적으로 추가됩니다. 그러나 고객이 구성할 수 있는 사용자 지정 기능에는 자체 위협 목록 및 신뢰할 수 있는 IP 주소 목록이 있습니다.\"\n",
            "현재 GuardDuty를 사용하고 있는 경우 S3 Protection을 시작하려면 어떻게 해야 하나요?\n",
            "현재 GuardDuty 계정의 경우 콘솔의 S3 Protection 페이지에서 또는 API를 통해 S3 Protection을 활성화할 수 있습니다. 그러면 GuardDuty S3 Protection 기능의 30일 무료 평가판이 시작됩니다.\n",
            "\"category : Guardduty, question : 현재 GuardDuty를 사용하고 있는 경우 S3 Protection을 시작하려면 어떻게 해야 하나요?, answer : 현재 GuardDuty 계정의 경우 콘솔의 S3 Protection 페이지에서 또는 API를 통해 S3 Protection을 활성화할 수 있습니다. 그러면 GuardDuty S3 Protection 기능의 30일 무료 평가판이 시작됩니다.\"\n",
            "GuardDuty S3 Protection의 무료 평가판이 있나요?\n",
            "예. 30일 무료 평가판이 있습니다. 각 리전에서 계정별로 S3 Protection 기능이 포함된 GuardDuty의 30일 무료 평가판을 사용할 수 있습니다. GuardDuty를 이미 사용하는 계정도 S3 Protection 기능의 30일 무료 평가판을 처음 활성화하는 경우 평가판을 사용할 수 있습니다.\n",
            "\"category : Guardduty, question : GuardDuty S3 Protection의 무료 평가판이 있나요?, answer : 예. 30일 무료 평가판이 있습니다. 각 리전에서 계정별로 S3 Protection 기능이 포함된 GuardDuty의 30일 무료 평가판을 사용할 수 있습니다. GuardDuty를 이미 사용하는 계정도 S3 Protection 기능의 30일 무료 평가판을 처음 활성화하는 경우 평가판을 사용할 수 있습니다.\"\n",
            "GuardDuty를 처음 사용하는 사용자인데, 계정에 기본적으로 S3 Protection이 사용되나요?\n",
            "예. 콘솔 또는 API를 통해 GuardDuty를 사용하도록 설정한 신규 계정에는 S3 Protection이 기본적으로 사용됩니다. AWS Organizations의 자동 사용 기능을 사용하여 생성된 신규 GuardDuty 계정은 S3 자동 사용(Auto-enable for S3) 옵션을 활성화해야 S3 Protection이 활성화됩니다.\n",
            "\"category : Guardduty, question : GuardDuty를 처음 사용하는 사용자인데, 계정에 기본적으로 S3 Protection이 사용되나요?, answer : 예. 콘솔 또는 API를 통해 GuardDuty를 사용하도록 설정한 신규 계정에는 S3 Protection이 기본적으로 사용됩니다. AWS Organizations의 자동 사용 기능을 사용하여 생성된 신규 GuardDuty 계정은 S3 자동 사용(Auto-enable for S3) 옵션을 활성화해야 S3 Protection이 활성화됩니다.\"\n",
            "전체 GuardDuty 서비스(예: VPC 흐름 로그, DNS 쿼리 로그 및 CloudTrail 관리 이벤트 분석)를 사용하지 않고 GuardDuty S3 Protection을 사용할 수 있나요?\n",
            "아니요. S3 Protection을 사용하려면 GuardDuty 서비스를 사용해야 합니다. 현재 GuardDuty 계정에는 S3 Protection을 사용하도록 설정할 수 있는 옵션이 있고 신규 GuardDuty 계정의 경우 GuardDuty 서비스를 사용하도록 설정하면 기본적으로 이 기능이 포함됩니다.\n",
            "\"category : Guardduty, question : 전체 GuardDuty 서비스(예: VPC 흐름 로그, DNS 쿼리 로그 및 CloudTrail 관리 이벤트 분석)를 사용하지 않고 GuardDuty S3 Protection을 사용할 수 있나요?, answer : 아니요. S3 Protection을 사용하려면 GuardDuty 서비스를 사용해야 합니다. 현재 GuardDuty 계정에는 S3 Protection을 사용하도록 설정할 수 있는 옵션이 있고 신규 GuardDuty 계정의 경우 GuardDuty 서비스를 사용하도록 설정하면 기본적으로 이 기능이 포함됩니다.\"\n",
            "GuardDuty는 Amazon S3 배포를 보호하기 위해 계정의 모든 버킷을 모니터링하나요?\n",
            "예. S3 Protection은 환경의 모든 Amazon S3 버킷을 기본적으로 모니터링합니다.\n",
            "\"category : Guardduty, question : GuardDuty는 Amazon S3 배포를 보호하기 위해 계정의 모든 버킷을 모니터링하나요?, answer : 예. S3 Protection은 환경의 모든 Amazon S3 버킷을 기본적으로 모니터링합니다.\"\n",
            "S3 Protection에 대해 CloudTrail S3 데이터 이벤트 로깅을 활성화해야 하나요?\n",
            "아니요. GuardDuty는 CloudTrail S3 데이터 이벤트 로그에 직접 액세스할 수 있습니다. CloudTrail에서 S3 데이터 이벤트 로깅을 사용하도록 설정할 필요가 없으며 따라서 관련 비용도 발생하지 않습니다. GuardDuty는 로그를 저장하지 않고 분석 용도로만 사용합니다.\n",
            "\"category : Guardduty, question : S3 Protection에 대해 CloudTrail S3 데이터 이벤트 로깅을 활성화해야 하나요?, answer : 아니요. GuardDuty는 CloudTrail S3 데이터 이벤트 로그에 직접 액세스할 수 있습니다. CloudTrail에서 S3 데이터 이벤트 로깅을 사용하도록 설정할 필요가 없으며 따라서 관련 비용도 발생하지 않습니다. GuardDuty는 로그를 저장하지 않고 분석 용도로만 사용합니다.\"\n",
            "GuardDuty EKS Protection은 어떤 방식으로 작동하나요?\n",
            "GuardDuty EKS Protection은 Amazon EKS 감사 로그를 분석하여 Amazon EKS 클러스터 컨트롤 플레인 활동을 모니터링하는 GuardDuty 기능입니다. GuardDuty는 Amazon EKS와 통합하여, Amazon EKS 감사 로그에 직접 액세스하므로 이러한 로그를 활성화하거나 저장할 필요가 없습니다. 이러한 감사 로그는 Amazon EKS 컨트롤 플레인에서 수행한 일련의 작업을 시간순으로 기록한 보안 관련 기록입니다. 이러한 Amazon EKS 감사 로그를 통해 GuardDuty는 Amazon EKS API 활동을 지속적으로 모니터링하고 검증된 위협 인텔리전스 및 이상 탐지 기능을 적용하여 Amazon EKS 클러스터를 무단 액세스에 노출시킬 수 있는 악성 활동 또는 구성 변경을 식별합니다. 위협이 식별되면 GuardDuty는 위협 유형, 심각도 수준 및 포드 ID, 컨테이너 이미지 ID 및 관련 태그와 같은 컨테이너 수준 세부 정보가 포함된 보안 결과를 생성합니다.\n",
            "\"category : Guardduty, question : GuardDuty EKS Protection은 어떤 방식으로 작동하나요?, answer : GuardDuty EKS Protection은 Amazon EKS 감사 로그를 분석하여 Amazon EKS 클러스터 컨트롤 플레인 활동을 모니터링하는 GuardDuty 기능입니다. GuardDuty는 Amazon EKS와 통합하여, Amazon EKS 감사 로그에 직접 액세스하므로 이러한 로그를 활성화하거나 저장할 필요가 없습니다. 이러한 감사 로그는 Amazon EKS 컨트롤 플레인에서 수행한 일련의 작업을 시간순으로 기록한 보안 관련 기록입니다. 이러한 Amazon EKS 감사 로그를 통해 GuardDuty는 Amazon EKS API 활동을 지속적으로 모니터링하고 검증된 위협 인텔리전스 및 이상 탐지 기능을 적용하여 Amazon EKS 클러스터를 무단 액세스에 노출시킬 수 있는 악성 활동 또는 구성 변경을 식별합니다. 위협이 식별되면 GuardDuty는 위협 유형, 심각도 수준 및 포드 ID, 컨테이너 이미지 ID 및 관련 태그와 같은 컨테이너 수준 세부 정보가 포함된 보안 결과를 생성합니다.\"\n",
            "GuardDuty EKS Protection은 Amazon EKS 워크로드에서 어떤 유형의 위협을 탐지할 수 있습니까?\n",
            "GuardDuty EKS Protection은 Amazon EKS 감사 로그에 캡처된 사용자와 애플리케이션 활동과 관련된 위협을 탐지할 수 있습니다. Amazon EKS 위협 탐지는 알려진 악성 행위자나 Tor 노드에 의해 액세스된 Amazon EKS 클러스터, 잘못된 구성을 나타낼 수 있는 익명 사용자가 수행한 API 작업, Amazon EKS 클러스터로의 승인되지 않은 액세스를 초래할 수 있는 잘못된 구성을 포함합니다. 또한 GuardDuty는 ML 모델을 사용하여 기본 Amazon EC2 호스트로의 루트 수준 액세스 권한이 있는 컨테이너의 의심스러운 시작과 같은 권한 에스컬레이션 테크닉과 일치하는 패턴을 식별할 수 있습니다. 모든 신규 탐지의 전체 목록을 보려면 Amazon GuardDuty 조사 결과 유형을 참조하십시오.\n",
            "\"category : Guardduty, question : GuardDuty EKS Protection은 Amazon EKS 워크로드에서 어떤 유형의 위협을 탐지할 수 있습니까?, answer : GuardDuty EKS Protection은 Amazon EKS 감사 로그에 캡처된 사용자와 애플리케이션 활동과 관련된 위협을 탐지할 수 있습니다. Amazon EKS 위협 탐지는 알려진 악성 행위자나 Tor 노드에 의해 액세스된 Amazon EKS 클러스터, 잘못된 구성을 나타낼 수 있는 익명 사용자가 수행한 API 작업, Amazon EKS 클러스터로의 승인되지 않은 액세스를 초래할 수 있는 잘못된 구성을 포함합니다. 또한 GuardDuty는 ML 모델을 사용하여 기본 Amazon EC2 호스트로의 루트 수준 액세스 권한이 있는 컨테이너의 의심스러운 시작과 같은 권한 에스컬레이션 테크닉과 일치하는 패턴을 식별할 수 있습니다. 모든 신규 탐지의 전체 목록을 보려면 Amazon GuardDuty 조사 결과 유형을 참조하십시오.\"\n",
            "Amazon EKS 감사 로그를 활성화해야 하나요?\n",
            "아니요. GuardDuty는 Amazon EKS 감사 로그에 직접 액세스할 수 있습니다. GuardDuty는 이러한 로그를 분석에만 사용하고, 로그는 저장되지 않으며, GuardDuty와 공유하기 위해 이러한 Amazon EKS 감사 로그를 활성화하거나 요금을 지불할 필요도 없습니다. 비용을 최적화하기 위해 GuardDuty는 지능형 필터를 적용하여 보안 위협 탐지와 관련한 감사 로그의 일부만 사용합니다.\n",
            "\"category : Guardduty, question : Amazon EKS 감사 로그를 활성화해야 하나요?, answer : 아니요. GuardDuty는 Amazon EKS 감사 로그에 직접 액세스할 수 있습니다. GuardDuty는 이러한 로그를 분석에만 사용하고, 로그는 저장되지 않으며, GuardDuty와 공유하기 위해 이러한 Amazon EKS 감사 로그를 활성화하거나 요금을 지불할 필요도 없습니다. 비용을 최적화하기 위해 GuardDuty는 지능형 필터를 적용하여 보안 위협 탐지와 관련한 감사 로그의 일부만 사용합니다.\"\n",
            "GuardDuty EKS Protection의 무료 평가판이 있나요?\n",
            "예. 30일 무료 평가판이 있습니다. 각 리전에서 신규 GuardDuty 계정별로 GuardDuty EKS Protection 기능을 포함한 GuardDuty의 30일 무료 평가판이 제공됩니다. 기존 GuardDuty 계정은 추가 비용 없이 30일 동안 GuardDuty EKS Protection 평가판을 이용할 수 있습니다. 평가 기간 동안 GuardDuty 콘솔 사용량 페이지에서 평가 기간 후 예상 비용을 확인할 수 있습니다. GuardDuty 관리자인 경우 멤버 계정의 예상 비용이 표시됩니다. 30일이 지나면 AWS 빌링 콘솔에서 이 기능의 실제 비용을 확인할 수 있습니다.\n",
            "\"category : Guardduty, question : GuardDuty EKS Protection의 무료 평가판이 있나요?, answer : 예. 30일 무료 평가판이 있습니다. 각 리전에서 신규 GuardDuty 계정별로 GuardDuty EKS Protection 기능을 포함한 GuardDuty의 30일 무료 평가판이 제공됩니다. 기존 GuardDuty 계정은 추가 비용 없이 30일 동안 GuardDuty EKS Protection 평가판을 이용할 수 있습니다. 평가 기간 동안 GuardDuty 콘솔 사용량 페이지에서 평가 기간 후 예상 비용을 확인할 수 있습니다. GuardDuty 관리자인 경우 멤버 계정의 예상 비용이 표시됩니다. 30일이 지나면 AWS 빌링 콘솔에서 이 기능의 실제 비용을 확인할 수 있습니다.\"\n",
            "현재 GuardDuty를 사용하고 있는 경우 GuardDuty EKS Protection을 시작하려면 어떻게 해야 하나요?\n",
            "GuardDuty EKS Protection은 개별 계정마다 활성화해야 합니다. GuardDuty 콘솔의 GuardDuty EKS Protection 콘솔 페이지에서 단일 작업으로 계정에 이 기능을 활성화할 수 있습니다. GuardDuty 다중 계정 구성으로 운영하는 경우 GuardDuty 관리자 계정 GuardDuty EKS Protection 페이지에서 전체 조직의 GuardDuty EKS Protection을 활성화할 수 있습니다. 그러면 모든 개별 멤버 계정에서 Amazon EKS에 대한 지속적인 모니터링이 활성화됩니다. AWS Organizations의 자동 사용 기능을 통해 생성된 GuardDuty 계정의 경우, Auto-activate for Amazon EKS(Amazon EKS에 대해 자동 활성화)을 명시적으로 설정해야 합니다. 계정에 대해 활성화되면, Amazon EKS 클러스터에서 구성하지 않아도 계정의 모든 기존 및 미래의 Amazon EKS 클러스터에서 위협이 모니터링됩니다.\n",
            "\"category : Guardduty, question : 현재 GuardDuty를 사용하고 있는 경우 GuardDuty EKS Protection을 시작하려면 어떻게 해야 하나요?, answer : GuardDuty EKS Protection은 개별 계정마다 활성화해야 합니다. GuardDuty 콘솔의 GuardDuty EKS Protection 콘솔 페이지에서 단일 작업으로 계정에 이 기능을 활성화할 수 있습니다. GuardDuty 다중 계정 구성으로 운영하는 경우 GuardDuty 관리자 계정 GuardDuty EKS Protection 페이지에서 전체 조직의 GuardDuty EKS Protection을 활성화할 수 있습니다. 그러면 모든 개별 멤버 계정에서 Amazon EKS에 대한 지속적인 모니터링이 활성화됩니다. AWS Organizations의 자동 사용 기능을 통해 생성된 GuardDuty 계정의 경우, Auto-activate for Amazon EKS(Amazon EKS에 대해 자동 활성화)을 명시적으로 설정해야 합니다. 계정에 대해 활성화되면, Amazon EKS 클러스터에서 구성하지 않아도 계정의 모든 기존 및 미래의 Amazon EKS 클러스터에서 위협이 모니터링됩니다.\"\n",
            "신규 GuardDuty 사용자인 경우 계정에 GuardDuty EKS Protection이 기본적으로 사용되나요?\n",
            "예. 콘솔 또는 API를 통해 GuardDuty를 활성화한 신규 계정에는 기본적으로 GuardDuty EKS Protection이 활성화됩니다. AWS Organizations의 자동 사용 기능을 사용하여 생성된 신규 GuardDuty 계정의 경우, auto-enable for the EKS Protection(EKS Protection 자동 사용) 옵션을 명시적으로 사용하도록 설정해야 합니다.\n",
            "\"category : Guardduty, question : 신규 GuardDuty 사용자인 경우 계정에 GuardDuty EKS Protection이 기본적으로 사용되나요?, answer : 예. 콘솔 또는 API를 통해 GuardDuty를 활성화한 신규 계정에는 기본적으로 GuardDuty EKS Protection이 활성화됩니다. AWS Organizations의 자동 사용 기능을 사용하여 생성된 신규 GuardDuty 계정의 경우, auto-enable for the EKS Protection(EKS Protection 자동 사용) 옵션을 명시적으로 사용하도록 설정해야 합니다.\"\n",
            "GuardDuty EKS Protection을 사용 중지하려면 어떻게 해야 하나요?\n",
            "콘솔 또는 API를 사용하여 기능을 사용 중지할 수 있습니다. GuardDuty 콘솔의 GuardDuty EKS Protection 콘솔 페이지에서 계정의 GuardDuty EKS Protection을 사용 중지할 수 있습니다. GuardDuty 관리자 계정이 있는 경우 멤버 계정에 대해 이 기능을 사용 중지할 수도 있습니다.\n",
            "\"category : Guardduty, question : GuardDuty EKS Protection을 사용 중지하려면 어떻게 해야 하나요?, answer : 콘솔 또는 API를 사용하여 기능을 사용 중지할 수 있습니다. GuardDuty 콘솔의 GuardDuty EKS Protection 콘솔 페이지에서 계정의 GuardDuty EKS Protection을 사용 중지할 수 있습니다. GuardDuty 관리자 계정이 있는 경우 멤버 계정에 대해 이 기능을 사용 중지할 수도 있습니다.\"\n",
            "GuardDuty EKS Protection을 사용 중지한 경우 다시 사용하려면 어떻게 해야 하나요?\n",
            "이전에 GuardDuty EKS Protection을 사용 중지한 경우 콘솔 또는 API를 사용하여 기능을 다시 사용하도록 설정할 수 있습니다. GuardDuty 콘솔의 GuardDuty EKS Protection 콘솔 페이지에서 계정에 GuardDuty EKS Protection을 사용하도록 설정할 수 있습니다.\n",
            "\"category : Guardduty, question : GuardDuty EKS Protection을 사용 중지한 경우 다시 사용하려면 어떻게 해야 하나요?, answer : 이전에 GuardDuty EKS Protection을 사용 중지한 경우 콘솔 또는 API를 사용하여 기능을 다시 사용하도록 설정할 수 있습니다. GuardDuty 콘솔의 GuardDuty EKS Protection 콘솔 페이지에서 계정에 GuardDuty EKS Protection을 사용하도록 설정할 수 있습니다.\"\n",
            "GuardDuty EKS Protection을 각 AWS 계정과 Amazon EKS 클러스터에서 개별적으로 사용하도록 설정해야 하나요?\n",
            "GuardDuty EKS Protection은 개별 계정마다 사용하도록 설정해야 합니다. GuardDuty 다중 계정 구성으로 운영하는 경우 GuardDuty 관리자 계정 GuardDuty EKS Protection 콘솔 페이지에서 클릭 한 번으로 전체 조직의 Amazon EKS에 위협 탐지를 사용할 수 있습니다. 그러면 모든 개별 멤버 계정에서 Amazon EKS에 대한 위협 탐지가 활성화됩니다. 계정에 대해 활성화되면, 계정의 모든 기존 및 미래의 Amazon EKS 클러스터에서 위협이 모니터링되며 Amazon EKS 클러스터에서 수동으로 구성할 필요가 없습니다.\n",
            "\"category : Guardduty, question : GuardDuty EKS Protection을 각 AWS 계정과 Amazon EKS 클러스터에서 개별적으로 사용하도록 설정해야 하나요?, answer : GuardDuty EKS Protection은 개별 계정마다 사용하도록 설정해야 합니다. GuardDuty 다중 계정 구성으로 운영하는 경우 GuardDuty 관리자 계정 GuardDuty EKS Protection 콘솔 페이지에서 클릭 한 번으로 전체 조직의 Amazon EKS에 위협 탐지를 사용할 수 있습니다. 그러면 모든 개별 멤버 계정에서 Amazon EKS에 대한 위협 탐지가 활성화됩니다. 계정에 대해 활성화되면, 계정의 모든 기존 및 미래의 Amazon EKS 클러스터에서 위협이 모니터링되며 Amazon EKS 클러스터에서 수동으로 구성할 필요가 없습니다.\"\n",
            "Amazon EKS를 사용하지 않고 GuardDuty에서 GuardDuty EKS Protection을 사용하도록 설정하면 요금이 부과됩니까?\n",
            "Amazon EKS를 사용하지 않고 GuardDuty EKS Protection을 활성화했다면 GuardDuty EKS Protection 비용이 발생하지 않습니다. 그러나 Amazon EKS 사용을 시작하면 GuardDuty가 클러스터를 자동으로 모니터링하고 식별된 문제에 대한 결과를 생성하기 때문에 이 모니터링에 대한 요금이 부과됩니다.\n",
            "\"category : Guardduty, question : Amazon EKS를 사용하지 않고 GuardDuty에서 GuardDuty EKS Protection을 사용하도록 설정하면 요금이 부과됩니까?, answer : Amazon EKS를 사용하지 않고 GuardDuty EKS Protection을 활성화했다면 GuardDuty EKS Protection 비용이 발생하지 않습니다. 그러나 Amazon EKS 사용을 시작하면 GuardDuty가 클러스터를 자동으로 모니터링하고 식별된 문제에 대한 결과를 생성하기 때문에 이 모니터링에 대한 요금이 부과됩니다.\"\n",
            "전체 GuardDuty 서비스(예: VPC 흐름 로그, DNS 쿼리 로그 및 CloudTrail Management Events 분석)를 사용하지 않고 GuardDuty EKS Protection을 사용할 수 있나요?\n",
            "아니요. GuardDuty EKS Protection을 사용하려면 GuardDuty 서비스가 사용되어야 합니다.\n",
            "\"category : Guardduty, question : 전체 GuardDuty 서비스(예: VPC 흐름 로그, DNS 쿼리 로그 및 CloudTrail Management Events 분석)를 사용하지 않고 GuardDuty EKS Protection을 사용할 수 있나요?, answer : 아니요. GuardDuty EKS Protection을 사용하려면 GuardDuty 서비스가 사용되어야 합니다.\"\n",
            "GuardDuty EKS Protection은 AWS Fargate의 Amazon EKS 배포에 대한 Amazon EKS 감사 로그를 모니터링하나요?\n",
            "예, GuardDuty EKS Protection은 Amazon EC2 인스턴스에 배포된 Amazon EKS 클러스터와 Fargate에 배포된 Amazon EKS 클러스터의 Amazon EKS 감사 로그를 모니터링합니다.\n",
            "\"category : Guardduty, question : GuardDuty EKS Protection은 AWS Fargate의 Amazon EKS 배포에 대한 Amazon EKS 감사 로그를 모니터링하나요?, answer : 예, GuardDuty EKS Protection은 Amazon EC2 인스턴스에 배포된 Amazon EKS 클러스터와 Fargate에 배포된 Amazon EKS 클러스터의 Amazon EKS 감사 로그를 모니터링합니다.\"\n",
            "GuardDuty는 Amazon EC2 또는 Amazon EKS Anywhere의 비관리형 Amazon EKS를 모니터링하나요?\n",
            "현재 이 기능은 계정의 Amazon EC2 인스턴스 또는 Fargate에서 실행되는 Amazon EKS 배포만 지원합니다.\n",
            "\"category : Guardduty, question : GuardDuty는 Amazon EC2 또는 Amazon EKS Anywhere의 비관리형 Amazon EKS를 모니터링하나요?, answer : 현재 이 기능은 계정의 Amazon EC2 인스턴스 또는 Fargate에서 실행되는 Amazon EKS 배포만 지원합니다.\"\n",
            "GuardDuty EKS Protection을 사용하면 Amazon EKS의 컨테이너 실행 성능이나 비용에 영향을 미칩니까?\n",
            "아니요. GuardDuty EKS Protection은 Amazon EKS 워크로드 배포의 성능, 가용성 또는 비용에 영향을 미치지 않도록 설계되었습니다.\n",
            "\"category : Guardduty, question : GuardDuty EKS Protection을 사용하면 Amazon EKS의 컨테이너 실행 성능이나 비용에 영향을 미칩니까?, answer : 아니요. GuardDuty EKS Protection은 Amazon EKS 워크로드 배포의 성능, 가용성 또는 비용에 영향을 미치지 않도록 설계되었습니다.\"\n",
            "각 AWS 리전에서 GuardDuty EKS Protection을 개별적으로 활성화해야 하나요?\n",
            "예. GuardDuty는 리전별 서비스이기 때문에 각 AWS 리전에서 GuardDuty EKS Protection을 개별적으로 사용하도록 설정해야 합니다.\n",
            "\"category : Guardduty, question : 각 AWS 리전에서 GuardDuty EKS Protection을 개별적으로 활성화해야 하나요?, answer : 예. GuardDuty는 리전별 서비스이기 때문에 각 AWS 리전에서 GuardDuty EKS Protection을 개별적으로 사용하도록 설정해야 합니다.\"\n",
            "GuardDuty Runtime Monitoring은 어떻게 작동하나요?\n",
            "GuardDuty Runtime Monitoring은 경량의 완전 관리형 보안 에이전트를 사용하여 모니터링이 적용되는 리소스의 포드 또는 인스턴스 수준에서 파일 액세스, 프로세스 실행, 네트워크 연결과 같은 런타임 활동에 대한 가시성을 제공합니다. 보안 에이전트는 런타임 이벤트를 수집해서 보안 분석 처리를 위해 GuardDuty에 전달하는 대몬 세트로 자동 배포됩니다. 이를 통해 GuardDuty는 AWS 환경 내에서 손상되었을 가능성이 있는 특정 인스턴스 또는 컨테이너를 식별하고 더 광범위한 AWS 환경으로 권한을 에스컬레이션하려는 시도를 탐지할 수 있습니다. GuardDuty를 통해 잠재적 위협이 탐지되면 인스턴스, 컨테이너, 포드, 프로세스 세부 정보를 담은 메타데이터 컨텍스트가 포함된 보안 조사 결과가 생성됩니다.\n",
            "\"category : Guardduty, question : GuardDuty Runtime Monitoring은 어떻게 작동하나요?, answer : GuardDuty Runtime Monitoring은 경량의 완전 관리형 보안 에이전트를 사용하여 모니터링이 적용되는 리소스의 포드 또는 인스턴스 수준에서 파일 액세스, 프로세스 실행, 네트워크 연결과 같은 런타임 활동에 대한 가시성을 제공합니다. 보안 에이전트는 런타임 이벤트를 수집해서 보안 분석 처리를 위해 GuardDuty에 전달하는 대몬 세트로 자동 배포됩니다. 이를 통해 GuardDuty는 AWS 환경 내에서 손상되었을 가능성이 있는 특정 인스턴스 또는 컨테이너를 식별하고 더 광범위한 AWS 환경으로 권한을 에스컬레이션하려는 시도를 탐지할 수 있습니다. GuardDuty를 통해 잠재적 위협이 탐지되면 인스턴스, 컨테이너, 포드, 프로세스 세부 정보를 담은 메타데이터 컨텍스트가 포함된 보안 조사 결과가 생성됩니다.\"\n",
            "GuardDuty Runtime Monitoring은 어떤 AWS 서비스를 지원하나요?\n",
            "Runtime Monitoring은 Amazon EC2에서 실행되는 Amazon EKS 리소스, Amazon EC2 또는 AWS Fargate에서 실행되는 Amazon ECS 클러스터, Amazon EC2 인스턴스에 사용할 수 있습니다.\n",
            "\"category : Guardduty, question : GuardDuty Runtime Monitoring은 어떤 AWS 서비스를 지원하나요?, answer : Runtime Monitoring은 Amazon EC2에서 실행되는 Amazon EKS 리소스, Amazon EC2 또는 AWS Fargate에서 실행되는 Amazon ECS 클러스터, Amazon EC2 인스턴스에 사용할 수 있습니다.\"\n",
            "현재 GuardDuty를 사용하고 있는 경우 Runtime Monitoring을 시작하려면 어떻게 해야 하나요?\n",
            "현재 GuardDuty 계정의 경우 GuardDuty 콘솔의 Runtime Monitoring 페이지에서 또는 API를 통해 기능을 활성화할 수 있습니다. GuardDuty Runtime Monitoring에 대해 자세히 알아보세요.\n",
            "\"category : Guardduty, question : 현재 GuardDuty를 사용하고 있는 경우 Runtime Monitoring을 시작하려면 어떻게 해야 하나요?, answer : 현재 GuardDuty 계정의 경우 GuardDuty 콘솔의 Runtime Monitoring 페이지에서 또는 API를 통해 기능을 활성화할 수 있습니다. GuardDuty Runtime Monitoring에 대해 자세히 알아보세요.\"\n",
            "GuardDuty를 처음 사용하는 경우 내 계정에서 Runtime Monitoring이 기본적으로 활성화되나요?\n",
            "아니요. GuardDuty Runtime Monitoring은 GuardDuty를 처음으로 활성화할 때 기본적으로 사용되지 않는 유일한 보호 계획입니다. 이 기능은 GuardDuty 콘솔의 Runtime Monitoring 페이지에서 또는 API를 통해 활성화할 수 있습니다. AWS Organizations의 자동 사용 기능으로 생성된 새로운 GuardDuty 계정은 Runtime Monitoring 자동 사용 옵션을 활성화해야 Runtime Monitoring이 활성화됩니다.\n",
            "\"category : Guardduty, question : GuardDuty를 처음 사용하는 경우 내 계정에서 Runtime Monitoring이 기본적으로 활성화되나요?, answer : 아니요. GuardDuty Runtime Monitoring은 GuardDuty를 처음으로 활성화할 때 기본적으로 사용되지 않는 유일한 보호 계획입니다. 이 기능은 GuardDuty 콘솔의 Runtime Monitoring 페이지에서 또는 API를 통해 활성화할 수 있습니다. AWS Organizations의 자동 사용 기능으로 생성된 새로운 GuardDuty 계정은 Runtime Monitoring 자동 사용 옵션을 활성화해야 Runtime Monitoring이 활성화됩니다.\"\n",
            "GuardDuty 보안 에이전트의 배포 옵션에는 어떤 것이 있나요?\n",
            "Amazon ECS Runtime Monitoring을 활성화하면 GuardDuty는 작업의 런타임 이벤트를 사용할 준비가 됩니다. 이러한 작업은 Amazon ECS 클러스터 내에서 실행되며, 이 클러스터는 곧 AWS Fargate 인스턴스에서 실행됩니다. GuardDuty가 이러한 런타임 이벤트를 수신하려면 자동화된 에이전트 구성을 사용해야 합니다.\n",
            "Amazon EC2 또는 Amazon EKS에 대해 Runtime Monitoring을 활성화하면 GuardDuty 보안 에이전트를 수동으로 배포하거나 자동화된 에이전트 구성을 통해 GuardDuty가 사용자 대신 관리하도록 허용할 수 있는 옵션이 있습니다.\n",
            "자세한 내용은 GuardDuty 사용 설명서에서 주요 개념 - GuardDuty 보안 에이전트 관리 방법을 참조하세요.\n",
            "\"category : Guardduty, question : GuardDuty 보안 에이전트의 배포 옵션에는 어떤 것이 있나요?, answer : Amazon ECS Runtime Monitoring을 활성화하면 GuardDuty는 작업의 런타임 이벤트를 사용할 준비가 됩니다. 이러한 작업은 Amazon ECS 클러스터 내에서 실행되며, 이 클러스터는 곧 AWS Fargate 인스턴스에서 실행됩니다. GuardDuty가 이러한 런타임 이벤트를 수신하려면 자동화된 에이전트 구성을 사용해야 합니다.\n",
            "Amazon EC2 또는 Amazon EKS에 대해 Runtime Monitoring을 활성화하면 GuardDuty 보안 에이전트를 수동으로 배포하거나 자동화된 에이전트 구성을 통해 GuardDuty가 사용자 대신 관리하도록 허용할 수 있는 옵션이 있습니다.\n",
            "자세한 내용은 GuardDuty 사용 설명서에서 주요 개념 - GuardDuty 보안 에이전트 관리 방법을 참조하세요.\"\n",
            "전체 GuardDuty 서비스를 활성화하지 않고 GuardDuty Runtime Monitoring을 사용할 수 있나요?\n",
            "아니요. GuardDuty Runtime Monitoring을 사용하려면 GuardDuty 서비스를 활성화해야 합니다.\n",
            "\"category : Guardduty, question : 전체 GuardDuty 서비스를 활성화하지 않고 GuardDuty Runtime Monitoring을 사용할 수 있나요?, answer : 아니요. GuardDuty Runtime Monitoring을 사용하려면 GuardDuty 서비스를 활성화해야 합니다.\"\n",
            "현재 GuardDuty가 제공되는 모든 리전에서 GuardDuty Runtime Monitoring을 사용할 수 있나요?\n",
            "Runtime Monitoring을 사용할 수 있는 리전의 전체 목록은 리전별 기능 가용성 페이지를 참조하세요.\n",
            "\"category : Guardduty, question : 현재 GuardDuty가 제공되는 모든 리전에서 GuardDuty Runtime Monitoring을 사용할 수 있나요?, answer : Runtime Monitoring을 사용할 수 있는 리전의 전체 목록은 리전별 기능 가용성 페이지를 참조하세요.\"\n",
            "각 AWS 계정에서 개별적으로 GuardDuty Runtime Monitoring을 활성화해야 하나요?\n",
            "GuardDuty Runtime Monitoring은 각 개별 계정에 대해 활성화되어야 합니다. GuardDuty 다중 계정 구성으로 운영하는 경우 GuardDuty 관리자 계정 GuardDuty Runtime Monitoring 콘솔 페이지에서 한 번의 단계로 전체 조직에 이 기능을 활성화할 수 있습니다. 그러면 모든 개별 멤버 계정의 원하는 워크로드에 대한 런타임 모니터링이 활성화됩니다. 계정에 대해 활성화되면, 계정에서 기존 및 향후 선택한 모든 워크로드에 대해 런타임 위협이 모니터링되며 수동으로 구성할 필요가 없습니다.\n",
            "\"category : Guardduty, question : 각 AWS 계정에서 개별적으로 GuardDuty Runtime Monitoring을 활성화해야 하나요?, answer : GuardDuty Runtime Monitoring은 각 개별 계정에 대해 활성화되어야 합니다. GuardDuty 다중 계정 구성으로 운영하는 경우 GuardDuty 관리자 계정 GuardDuty Runtime Monitoring 콘솔 페이지에서 한 번의 단계로 전체 조직에 이 기능을 활성화할 수 있습니다. 그러면 모든 개별 멤버 계정의 원하는 워크로드에 대한 런타임 모니터링이 활성화됩니다. 계정에 대해 활성화되면, 계정에서 기존 및 향후 선택한 모든 워크로드에 대해 런타임 위협이 모니터링되며 수동으로 구성할 필요가 없습니다.\"\n",
            "Amazon EKS 또는 Amazon ECS의 특정 클러스터를 모니터링하는 방법을 수정할 수 있나요?\n",
            "GuardDuty Runtime Monitoring을 사용하면 위협 탐지를 위해 모니터링할 Amazon EKS 클러스터 또는 Amazon ECS 클러스터를 선택적으로 구성할 수 있습니다. 클러스터 수준 구성 기능을 사용하면 위협 탐지를 위해 특정 클러스터를 선택적으로 모니터링하거나, 계정 수준 구성 기능을 계속 사용하여 특정 계정 및 리전의 모든 EKS 또는 ECS 클러스터를 각각 모니터링할 수 있습니다.\n",
            "\"category : Guardduty, question : Amazon EKS 또는 Amazon ECS의 특정 클러스터를 모니터링하는 방법을 수정할 수 있나요?, answer : GuardDuty Runtime Monitoring을 사용하면 위협 탐지를 위해 모니터링할 Amazon EKS 클러스터 또는 Amazon ECS 클러스터를 선택적으로 구성할 수 있습니다. 클러스터 수준 구성 기능을 사용하면 위협 탐지를 위해 특정 클러스터를 선택적으로 모니터링하거나, 계정 수준 구성 기능을 계속 사용하여 특정 계정 및 리전의 모든 EKS 또는 ECS 클러스터를 각각 모니터링할 수 있습니다.\"\n",
            "GuardDuty Runtime Monitoring을 사용하면 AWS 워크로드 실행의 성능이나 비용에 영향을 주나요?\n",
            "호스트상의 에이전트가 필요한 모든 보안, 관찰성, 기타 사용 사례와 마찬가지로 GuardDuty 보안 에이전트는 리소스 사용률 오버헤드를 야기합니다. GuardDuty 보안 에이전트는 경량으로 설계되었으며 GuardDuty를 통해 주의 깊게 모니터링되어 해당 워크로드에 대한 사용률 및 비용 영향을 최소화합니다. 애플리케이션 및 보안 팀은 정확한 리소스 사용률 지표를 사용하여 Amazon CloudWatch에서 모니터링할 수 있습니다.\n",
            "GuardDuty 보안 에이전트를 자동으로 배포하도록 GuardDuty Runtime Monitoring을 구성하면 리소스 사용률이 높아질 수 있으며 AWS 워크로드를 실행하는 데 사용되는 VPC에 VPC 엔드포인트가 생성될 수도 있습니다.\n",
            "\"category : Guardduty, question : GuardDuty Runtime Monitoring을 사용하면 AWS 워크로드 실행의 성능이나 비용에 영향을 주나요?, answer : 호스트상의 에이전트가 필요한 모든 보안, 관찰성, 기타 사용 사례와 마찬가지로 GuardDuty 보안 에이전트는 리소스 사용률 오버헤드를 야기합니다. GuardDuty 보안 에이전트는 경량으로 설계되었으며 GuardDuty를 통해 주의 깊게 모니터링되어 해당 워크로드에 대한 사용률 및 비용 영향을 최소화합니다. 애플리케이션 및 보안 팀은 정확한 리소스 사용률 지표를 사용하여 Amazon CloudWatch에서 모니터링할 수 있습니다.\n",
            "GuardDuty 보안 에이전트를 자동으로 배포하도록 GuardDuty Runtime Monitoring을 구성하면 리소스 사용률이 높아질 수 있으며 AWS 워크로드를 실행하는 데 사용되는 VPC에 VPC 엔드포인트가 생성될 수도 있습니다.\"\n",
            "Amazon EKS, Amazon ECS 또는 Amazon EC2를 사용하지 않고 이러한 워크로드 중 하나에 대해 GuardDuty Runtime Monitoring을 켜면 GuardDuty Runtime Monitoring 요금이 발생하나요?\n",
            "실행 중이 아닌 워크로드에 대해 GuardDuty Runtime Monitoring을 활성화한 경우 GuardDuty Runtime Monitoring 요금이 발생하지 않습니다. 하지만 Amazon EKS, Amazon ECS 또는 Amazon EC2 사용을 시작하고 해당 워크로드에 대해 GuardDuty Runtime Monitoring이 활성화되면 GuardDuty가 클러스터, 작업 및 인스턴스를 자동으로 모니터링하고 식별된 문제에 대한 결과를 생성할 때 요금이 부과됩니다.\n",
            "\"category : Guardduty, question : Amazon EKS, Amazon ECS 또는 Amazon EC2를 사용하지 않고 이러한 워크로드 중 하나에 대해 GuardDuty Runtime Monitoring을 켜면 GuardDuty Runtime Monitoring 요금이 발생하나요?, answer : 실행 중이 아닌 워크로드에 대해 GuardDuty Runtime Monitoring을 활성화한 경우 GuardDuty Runtime Monitoring 요금이 발생하지 않습니다. 하지만 Amazon EKS, Amazon ECS 또는 Amazon EC2 사용을 시작하고 해당 워크로드에 대해 GuardDuty Runtime Monitoring이 활성화되면 GuardDuty가 클러스터, 작업 및 인스턴스를 자동으로 모니터링하고 식별된 문제에 대한 결과를 생성할 때 요금이 부과됩니다.\"\n",
            "GuardDuty Runtime Monitoring을 비활성화하려면 어떻게 해야 하나요?\n",
            "GuardDuty 콘솔의 Runtime Monitoring 페이지에서 AWS 계정 또는 조직에 대한 GuardDuty Runtime Monitoring을 비활성화할 수 있습니다. GuardDuty가 보안 에이전트를 자동으로 배포한 경우 이 기능이 비활성화되면 GuardDuty가 보안 에이전트도 제거합니다.\n",
            "GuardDuty 에이전트를 수동으로 배포하기로 선택한 경우(EKS Runtime Monitoring 및 EC2 Runtime Monitoring에만 해당) 에이전트를 수동으로 제거해야 하며 생성된 모든 VPC 엔드포인트도 수동으로 삭제해야 합니다. EKS Runtime Monitoring 및 EC2 Runtime Monitoring의 수동 제거 단계는 GuardDuty 사용 설명서에 자세히 설명되어 있습니다.\n",
            "\"category : Guardduty, question : GuardDuty Runtime Monitoring을 비활성화하려면 어떻게 해야 하나요?, answer : GuardDuty 콘솔의 Runtime Monitoring 페이지에서 AWS 계정 또는 조직에 대한 GuardDuty Runtime Monitoring을 비활성화할 수 있습니다. GuardDuty가 보안 에이전트를 자동으로 배포한 경우 이 기능이 비활성화되면 GuardDuty가 보안 에이전트도 제거합니다.\n",
            "GuardDuty 에이전트를 수동으로 배포하기로 선택한 경우(EKS Runtime Monitoring 및 EC2 Runtime Monitoring에만 해당) 에이전트를 수동으로 제거해야 하며 생성된 모든 VPC 엔드포인트도 수동으로 삭제해야 합니다. EKS Runtime Monitoring 및 EC2 Runtime Monitoring의 수동 제거 단계는 GuardDuty 사용 설명서에 자세히 설명되어 있습니다.\"\n",
            "Amazon GuardDuty 맬웨어 방지는 어떤 식으로 작동하나요?\n",
            "GuardDuty는 Amazon EC2 인스턴스 또는 컨테이너 워크로드에서 맬웨어를 나타내는 의심스러운 동작이 식별될 때 맬웨어 탐지 스캔을 시작합니다. Amazon EBS 볼륨의 스냅샷을 기반으로 GuardDuty가 생성하는 Amazon EBS 볼륨 복제본에서 트로이 목마, 웜, 암호 화폐 채굴기, 루트킷, 봇 등을 스캔합니다. GuardDuty 맬웨어 방지는 의심스러운 동작의 소스를 검증하는 데 도움이 될 수 있는 상황별 조사 결과를 생성합니다. 이러한 조사 결과를 적절한 관리자에게 라우팅하여 자동화된 수정 작업을 시작할 수도 있습니다.\n",
            "\"category : Guardduty, question : Amazon GuardDuty 맬웨어 방지는 어떤 식으로 작동하나요?, answer : GuardDuty는 Amazon EC2 인스턴스 또는 컨테이너 워크로드에서 맬웨어를 나타내는 의심스러운 동작이 식별될 때 맬웨어 탐지 스캔을 시작합니다. Amazon EBS 볼륨의 스냅샷을 기반으로 GuardDuty가 생성하는 Amazon EBS 볼륨 복제본에서 트로이 목마, 웜, 암호 화폐 채굴기, 루트킷, 봇 등을 스캔합니다. GuardDuty 맬웨어 방지는 의심스러운 동작의 소스를 검증하는 데 도움이 될 수 있는 상황별 조사 결과를 생성합니다. 이러한 조사 결과를 적절한 관리자에게 라우팅하여 자동화된 수정 작업을 시작할 수도 있습니다.\"\n",
            "Amazon EC2에 대해 맬웨어 스캔이 시작되는 GuardDuty 조사 결과 유형은 무엇입니까?\n",
            "Amazon EC2에 대해 멀웨어 스캔을 시작하는 GuardDuty 조사 결과는 GuardDuty 사용 설명서에서 확인할 수 있습니다.\n",
            "\"category : Guardduty, question : Amazon EC2에 대해 맬웨어 스캔이 시작되는 GuardDuty 조사 결과 유형은 무엇입니까?, answer : Amazon EC2에 대해 멀웨어 스캔을 시작하는 GuardDuty 조사 결과는 GuardDuty 사용 설명서에서 확인할 수 있습니다.\"\n",
            "GuardDuty 맬웨어 방지로 스캔할 수 있는 리소스 및 파일 유형은 무엇입니까?\n",
            "맬웨어 방지는 Amazon EC2 인스턴스에 연결된 Amazon EBS를 스캔하여 악성 파일의 탐지를 지원합니다. 볼륨에 있는 모든 파일을 스캔할 수 있으며 지원되는 파일 시스템 유형은 GuardDuty 사용 설명서에서 찾을 수 있습니다.\n",
            "\"category : Guardduty, question : GuardDuty 맬웨어 방지로 스캔할 수 있는 리소스 및 파일 유형은 무엇입니까?, answer : 맬웨어 방지는 Amazon EC2 인스턴스에 연결된 Amazon EBS를 스캔하여 악성 파일의 탐지를 지원합니다. 볼륨에 있는 모든 파일을 스캔할 수 있으며 지원되는 파일 시스템 유형은 GuardDuty 사용 설명서에서 찾을 수 있습니다.\"\n",
            "GuardDuty Malware Protection은 어떤 유형의 위협을 탐지할 수 있습니까?\n",
            "Malware Protection은 트로이 목마, 웜, 암호 화폐 채굴기, 루트킷 및 봇과 같이 워크로드를 손상시키고 리소스를 악의적인 용도로 재사용하며 데이터에 무단으로 액세스하는 데 사용될 수 있는 위협을 스캔합니다.\n",
            "\"category : Guardduty, question : GuardDuty Malware Protection은 어떤 유형의 위협을 탐지할 수 있습니까?, answer : Malware Protection은 트로이 목마, 웜, 암호 화폐 채굴기, 루트킷 및 봇과 같이 워크로드를 손상시키고 리소스를 악의적인 용도로 재사용하며 데이터에 무단으로 액세스하는 데 사용될 수 있는 위협을 스캔합니다.\"\n",
            "GuardDuty Malware Protection이 작동하려면 로깅을 활성화해야 하나요?\n",
            "GuardDuty 또는 Malware Protection 기능의 작동을 위해 서비스 로깅을 사용하도록 설정할 필요는 없습니다. Malware Protection 기능은 통합된 내부 및 외부 소스의 인텔리전스를 사용하는 AWS 서비스인 GuardDuty의 일부입니다.\n",
            "\"category : Guardduty, question : GuardDuty Malware Protection이 작동하려면 로깅을 활성화해야 하나요?, answer : GuardDuty 또는 Malware Protection 기능의 작동을 위해 서비스 로깅을 사용하도록 설정할 필요는 없습니다. Malware Protection 기능은 통합된 내부 및 외부 소스의 인텔리전스를 사용하는 AWS 서비스인 GuardDuty의 일부입니다.\"\n",
            "GuardDuty 맬웨어 방지는 에이전트 없이 스캔을 어떻게 수행하나요?\n",
            "GuardDuty 맬웨어 방지는 보안 에이전트를 사용하는 대신 계정의 감염 가능성이 있는 Amazon EC2 인스턴스 또는 컨테이너 워크로드에 연결된 Amazon EBS 볼륨의 스냅샷을 기반으로 복제본을 만들고 이 복제본을 스캔합니다. GuardDuty는 서비스 연결 역할을 통해 GuardDuty에 부여된 권한을 사용하여 사용자 계정에 남아 있는 스냅샷으로부터 암호화된 볼륨 복제본을 GuardDuty의 서비스 계정에 만들 수 있습니다. GuardDuty Malware Protection은 이 볼륨 복제본에서 맬웨어를 스캔합니다.\n",
            "\"category : Guardduty, question : GuardDuty 맬웨어 방지는 에이전트 없이 스캔을 어떻게 수행하나요?, answer : GuardDuty 맬웨어 방지는 보안 에이전트를 사용하는 대신 계정의 감염 가능성이 있는 Amazon EC2 인스턴스 또는 컨테이너 워크로드에 연결된 Amazon EBS 볼륨의 스냅샷을 기반으로 복제본을 만들고 이 복제본을 스캔합니다. GuardDuty는 서비스 연결 역할을 통해 GuardDuty에 부여된 권한을 사용하여 사용자 계정에 남아 있는 스냅샷으로부터 암호화된 볼륨 복제본을 GuardDuty의 서비스 계정에 만들 수 있습니다. GuardDuty Malware Protection은 이 볼륨 복제본에서 맬웨어를 스캔합니다.\"\n",
            "GuardDuty Malware Protection의 무료 평가판이 있나요?\n",
            "예. 각 리전에서 신규 Amazon GuardDuty 계정별로 Malware Protection 기능을 포함한 GuardDuty의 30일 무료 평가판이 제공됩니다. 기존 GuardDuty 계정의 경우 계정에서 Malware Protection을 처음으로 사용하도록 설정할 때 추가 비용 없이 30일 무료 평가판을 받을 수 있습니다. 평가 기간 동안 GuardDuty 콘솔 사용량 페이지에서 평가 기간 후 예상 비용을 확인할 수 있습니다. GuardDuty 관리자인 경우 멤버 계정의 예상 비용이 표시됩니다. 30일이 지나면 AWS 빌링 콘솔에서 이 기능의 실제 비용을 확인할 수 있습니다.\n",
            "\"category : Guardduty, question : GuardDuty Malware Protection의 무료 평가판이 있나요?, answer : 예. 각 리전에서 신규 Amazon GuardDuty 계정별로 Malware Protection 기능을 포함한 GuardDuty의 30일 무료 평가판이 제공됩니다. 기존 GuardDuty 계정의 경우 계정에서 Malware Protection을 처음으로 사용하도록 설정할 때 추가 비용 없이 30일 무료 평가판을 받을 수 있습니다. 평가 기간 동안 GuardDuty 콘솔 사용량 페이지에서 평가 기간 후 예상 비용을 확인할 수 있습니다. GuardDuty 관리자인 경우 멤버 계정의 예상 비용이 표시됩니다. 30일이 지나면 AWS 빌링 콘솔에서 이 기능의 실제 비용을 확인할 수 있습니다.\"\n",
            "현재 GuardDuty를 사용하고 있는 경우 GuardDuty Malware Protection을 시작하려면 어떻게 해야 하나요?\n",
            "GuardDuty 콘솔에서 Malware Protection 페이지로 이동하거나 API를 사용하여 Malware Protection을 사용하도록 설정할 수 있습니다. GuardDuty 다중 계정 구성으로 운영하는 경우 GuardDuty 관리자 계정의 Malware Protection 콘솔 페이지에서 전체 조직에 대해 이 기능을 사용하도록 설정할 수 있습니다. 그러면 모든 개별 멤버 계정에서 맬웨어가 모니터링됩니다. AWS Organizations의 자동 사용 기능을 사용하여 생성된 GuardDuty 계정의 경우, Malware Protection 자동 사용(auto-enable for the Malware Protection) 옵션을 명시적으로 사용하도록 설정해야 합니다.\n",
            "\"category : Guardduty, question : 현재 GuardDuty를 사용하고 있는 경우 GuardDuty Malware Protection을 시작하려면 어떻게 해야 하나요?, answer : GuardDuty 콘솔에서 Malware Protection 페이지로 이동하거나 API를 사용하여 Malware Protection을 사용하도록 설정할 수 있습니다. GuardDuty 다중 계정 구성으로 운영하는 경우 GuardDuty 관리자 계정의 Malware Protection 콘솔 페이지에서 전체 조직에 대해 이 기능을 사용하도록 설정할 수 있습니다. 그러면 모든 개별 멤버 계정에서 맬웨어가 모니터링됩니다. AWS Organizations의 자동 사용 기능을 사용하여 생성된 GuardDuty 계정의 경우, Malware Protection 자동 사용(auto-enable for the Malware Protection) 옵션을 명시적으로 사용하도록 설정해야 합니다.\"\n",
            "GuardDuty를 처음 사용하는 경우 계정에 기본적으로 Malware Protection이 사용되나요?\n",
            "예. 콘솔 또는 API를 통해 GuardDuty를 사용하도록 설정한 신규 계정의 경우 GuardDuty Malware Protection이 기본적으로 사용됩니다. AWS Organizations의 자동 사용 기능을 사용하여 생성된 신규 GuardDuty 계정의 경우, Malware Protection 자동 사용(auto-enable for the Malware Protection) 옵션을 명시적으로 사용하도록 설정해야 합니다.\n",
            "\"category : Guardduty, question : GuardDuty를 처음 사용하는 경우 계정에 기본적으로 Malware Protection이 사용되나요?, answer : 예. 콘솔 또는 API를 통해 GuardDuty를 사용하도록 설정한 신규 계정의 경우 GuardDuty Malware Protection이 기본적으로 사용됩니다. AWS Organizations의 자동 사용 기능을 사용하여 생성된 신규 GuardDuty 계정의 경우, Malware Protection 자동 사용(auto-enable for the Malware Protection) 옵션을 명시적으로 사용하도록 설정해야 합니다.\"\n",
            "GuardDuty Malware Protection을 사용 중지하려면 어떻게 해야 하나요?\n",
            "콘솔 또는 API를 사용하여 기능을 사용 중지할 수 있습니다. GuardDuty 콘솔의 Malware Protection 콘솔 페이지에서 계정의 Malware Protection을 사용 중지하는 옵션을 볼 수 있습니다. GuardDuty 관리자 계정이 있는 경우 멤버 계정에 대해 Malware Protection을 사용 중지할 수도 있습니다.\n",
            "\"category : Guardduty, question : GuardDuty Malware Protection을 사용 중지하려면 어떻게 해야 하나요?, answer : 콘솔 또는 API를 사용하여 기능을 사용 중지할 수 있습니다. GuardDuty 콘솔의 Malware Protection 콘솔 페이지에서 계정의 Malware Protection을 사용 중지하는 옵션을 볼 수 있습니다. GuardDuty 관리자 계정이 있는 경우 멤버 계정에 대해 Malware Protection을 사용 중지할 수도 있습니다.\"\n",
            "GuardDuty Malware Protection을 사용 중지한 경우 다시 사용하려면 어떻게 해야 하나요?\n",
            "Malware Protection이 사용 중지된 경우 콘솔 또는 API를 사용하여 기능을 사용하도록 설정할 수 있습니다. GuardDuty 콘솔의 Malware Protection 콘솔 페이지에서 계정에 Malware Protection을 사용하도록 설정할 수 있습니다.\n",
            "\"category : Guardduty, question : GuardDuty Malware Protection을 사용 중지한 경우 다시 사용하려면 어떻게 해야 하나요?, answer : Malware Protection이 사용 중지된 경우 콘솔 또는 API를 사용하여 기능을 사용하도록 설정할 수 있습니다. GuardDuty 콘솔의 Malware Protection 콘솔 페이지에서 계정에 Malware Protection을 사용하도록 설정할 수 있습니다.\"\n",
            "청구 기간 중에 GuardDuty 맬웨어 스캔이 수행되지 않은 경우 요금이 발생하나요?\n",
            "아니요. 청구 기간 중에 맬웨어가 스캔되지 않은 경우 Malware Protection 요금이 발생하지 않습니다. AWS 빌링 콘솔에서 이 기능의 비용을 확인할 수 있습니다.\n",
            "\"category : Guardduty, question : 청구 기간 중에 GuardDuty 맬웨어 스캔이 수행되지 않은 경우 요금이 발생하나요?, answer : 아니요. 청구 기간 중에 맬웨어가 스캔되지 않은 경우 Malware Protection 요금이 발생하지 않습니다. AWS 빌링 콘솔에서 이 기능의 비용을 확인할 수 있습니다.\"\n",
            "GuardDuty Malware Protection은 다중 계정 관리를 지원하나요?\n",
            "예. GuardDuty에는 다중 계정 관리 기능이 있어 단일 관리자 계정에서 여러 AWS 계정을 연결하고 관리할 수 있습니다. GuardDuty는 AWS Organizations 통합을 통한 다중 계정 관리 기능을 제공합니다. 이 통합 기능을 통해 보안 및 규정 준수 팀은 조직의 모든 계정에 걸쳐 Malware Protection을 포함한 GuardDuty의 완벽한 지원을 보장할 수 있습니다.\n",
            "\"category : Guardduty, question : GuardDuty Malware Protection은 다중 계정 관리를 지원하나요?, answer : 예. GuardDuty에는 다중 계정 관리 기능이 있어 단일 관리자 계정에서 여러 AWS 계정을 연결하고 관리할 수 있습니다. GuardDuty는 AWS Organizations 통합을 통한 다중 계정 관리 기능을 제공합니다. 이 통합 기능을 통해 보안 및 규정 준수 팀은 조직의 모든 계정에 걸쳐 Malware Protection을 포함한 GuardDuty의 완벽한 지원을 보장할 수 있습니다.\"\n",
            "구성을 변경하거나 소프트웨어를 배포하거나 AWS 배포를 수정해야 하나요?\n",
            "아니요. 기능을 사용하도록 설정하면 GuardDuty 맬웨어 방지가 관련 Amazon EC2 조사 결과에 따라 맬웨어 스캔을 시작합니다. 에이전트를 배포할 필요가 없고 로그 소스를 사용하도록 설정하지 않아도 되며 다른 구성을 변경하지 않아도 됩니다.\n",
            "\"category : Guardduty, question : 구성을 변경하거나 소프트웨어를 배포하거나 AWS 배포를 수정해야 하나요?, answer : 아니요. 기능을 사용하도록 설정하면 GuardDuty 맬웨어 방지가 관련 Amazon EC2 조사 결과에 따라 맬웨어 스캔을 시작합니다. 에이전트를 배포할 필요가 없고 로그 소스를 사용하도록 설정하지 않아도 되며 다른 구성을 변경하지 않아도 됩니다.\"\n",
            "GuardDuty Malware Protection을 사용하면 실행 중인 워크로드의 성능이 영향을 받습니까?\n",
            "GuardDuty 맬웨어 방지는 워크로드의 성능에 영향을 미치지 않도록 설계되었습니다. 예를 들어 맬웨어 분석을 위해 생성되는 Amazon EBS 볼륨 스냅샷은 24시간 기간에 한 번만 생성할 수 있고 GuardDuty 맬웨어 방지는 암호화된 복제본 및 스냅샷 스캔을 완료한 후 몇 분간만 유지합니다. 또한 GuardDuty 맬웨어 방지는 맬웨어 스캔에 고객의 컴퓨팅 리소스 대신 GuardDuty 컴퓨팅 리소스를 사용합니다.\n",
            "\"category : Guardduty, question : GuardDuty Malware Protection을 사용하면 실행 중인 워크로드의 성능이 영향을 받습니까?, answer : GuardDuty 맬웨어 방지는 워크로드의 성능에 영향을 미치지 않도록 설계되었습니다. 예를 들어 맬웨어 분석을 위해 생성되는 Amazon EBS 볼륨 스냅샷은 24시간 기간에 한 번만 생성할 수 있고 GuardDuty 맬웨어 방지는 암호화된 복제본 및 스냅샷 스캔을 완료한 후 몇 분간만 유지합니다. 또한 GuardDuty 맬웨어 방지는 맬웨어 스캔에 고객의 컴퓨팅 리소스 대신 GuardDuty 컴퓨팅 리소스를 사용합니다.\"\n",
            "각 AWS 리전에서 GuardDuty Malware Protection을 개별적으로 사용하도록 설정해야 하나요?\n",
            "예. GuardDuty는 리전별 서비스이며, 각 AWS 리전에서 Malware Protection을 개별적으로 사용하도록 설정해야 합니다.\n",
            "\"category : Guardduty, question : 각 AWS 리전에서 GuardDuty Malware Protection을 개별적으로 사용하도록 설정해야 하나요?, answer : 예. GuardDuty는 리전별 서비스이며, 각 AWS 리전에서 Malware Protection을 개별적으로 사용하도록 설정해야 합니다.\"\n",
            "GuardDuty 맬웨어 방지는 암호화를 사용하나요?\n",
            "GuardDuty 맬웨어 방지는 계정에서 감염 가능성이 있는 Amazon EC2 인스턴스 또는 컨테이너 워크로드에 연결된 Amazon EBS 볼륨의 스냅샷을 기반으로 하는 복제본을 스캔합니다. Amazon EBS 볼륨이 고객 관리형 키로 암호화되는 경우 AWS Key Management Service(KMS) 키를 GuardDuty와 공유할 수 있습니다. 그러면 서비스에서 Amazon EBS 볼륨 복제본을 암호화할 때 동일한 키가 사용됩니다. 암호화되지 않은 Amazon EBS 볼륨의 경우 GuardDuty는 자체 키를 사용하여 Amazon EBS 볼륨 복제본을 암호화합니다.\n",
            "\"category : Guardduty, question : GuardDuty 맬웨어 방지는 암호화를 사용하나요?, answer : GuardDuty 맬웨어 방지는 계정에서 감염 가능성이 있는 Amazon EC2 인스턴스 또는 컨테이너 워크로드에 연결된 Amazon EBS 볼륨의 스냅샷을 기반으로 하는 복제본을 스캔합니다. Amazon EBS 볼륨이 고객 관리형 키로 암호화되는 경우 AWS Key Management Service(KMS) 키를 GuardDuty와 공유할 수 있습니다. 그러면 서비스에서 Amazon EBS 볼륨 복제본을 암호화할 때 동일한 키가 사용됩니다. 암호화되지 않은 Amazon EBS 볼륨의 경우 GuardDuty는 자체 키를 사용하여 Amazon EBS 볼륨 복제본을 암호화합니다.\"\n",
            "Amazon EBS 볼륨 복제본은 원래 볼륨과 동일한 리전에서 분석됩니까?\n",
            "예. 모든 Amazon EBS 볼륨 복제본 데이터(및 복제본 볼륨의 기반이 되는 스냅샷)는 원래 Amazon EBS 볼륨과 동일한 리전에 유지됩니다.\n",
            "\"category : Guardduty, question : Amazon EBS 볼륨 복제본은 원래 볼륨과 동일한 리전에서 분석됩니까?, answer : 예. 모든 Amazon EBS 볼륨 복제본 데이터(및 복제본 볼륨의 기반이 되는 스냅샷)는 원래 Amazon EBS 볼륨과 동일한 리전에 유지됩니다.\"\n",
            "GuardDuty 맬웨어 방지에 대한 비용을 예상하고 제어하려면 어떻게 해야 하나요?\n",
            "각 리전에서 신규 Amazon GuardDuty 계정별로 GuardDuty Malware Protection을 포함한 GuardDuty의 30일 무료 평가판이 제공됩니다. 기존 GuardDuty 계정의 경우 계정에서 Malware Protection을 처음으로 사용하도록 설정할 때 추가 비용 없이 30일 무료 평가판을 받을 수 있습니다. 평가 기간 동안 GuardDuty 콘솔 사용량 페이지에서 평가 기간 후 예상 비용을 추정할 수 있습니다. GuardDuty 관리자인 경우 멤버 계정의 예상 비용이 표시됩니다. 30일이 지나면 AWS 빌링 콘솔에서 이 기능의 실제 비용을 확인할 수 있습니다.\n",
            "이 기능의 요금은 볼륨에서 스캔된 데이터의 GB를 기준으로 부과됩니다. 콘솔에서 스캔 옵션을 사용하여 사용자 지정을 적용할 수 있습니다. 태그를 사용하여 스캔에 포함하거나 제외할 Amazon EC2 인스턴스를 표시하여 비용을 제어할 수 있습니다. 또한 GuardDuty는 24시간에 한 번만 Amazon EC2 인스턴스를 스캔합니다. GuardDuty에서 24시간 내에 Amazon EC2 인스턴스에 대한 여러 Amazon EC2 조사 결과가 생성되는 경우 스캔은 첫 번째 관련 Amazon EC2 조사 결과에 대해서만 수행됩니다. 인스턴스의 Amazon EC2 조사 결과가 마지막 맬웨어 스캔으로부터 24시간 후에 계속되는 경우 이 인스턴스에 대해 새 맬웨어 스캔이 시작됩니다.\n",
            "\"category : Guardduty, question : GuardDuty 맬웨어 방지에 대한 비용을 예상하고 제어하려면 어떻게 해야 하나요?, answer : 각 리전에서 신규 Amazon GuardDuty 계정별로 GuardDuty Malware Protection을 포함한 GuardDuty의 30일 무료 평가판이 제공됩니다. 기존 GuardDuty 계정의 경우 계정에서 Malware Protection을 처음으로 사용하도록 설정할 때 추가 비용 없이 30일 무료 평가판을 받을 수 있습니다. 평가 기간 동안 GuardDuty 콘솔 사용량 페이지에서 평가 기간 후 예상 비용을 추정할 수 있습니다. GuardDuty 관리자인 경우 멤버 계정의 예상 비용이 표시됩니다. 30일이 지나면 AWS 빌링 콘솔에서 이 기능의 실제 비용을 확인할 수 있습니다.\n",
            "이 기능의 요금은 볼륨에서 스캔된 데이터의 GB를 기준으로 부과됩니다. 콘솔에서 스캔 옵션을 사용하여 사용자 지정을 적용할 수 있습니다. 태그를 사용하여 스캔에 포함하거나 제외할 Amazon EC2 인스턴스를 표시하여 비용을 제어할 수 있습니다. 또한 GuardDuty는 24시간에 한 번만 Amazon EC2 인스턴스를 스캔합니다. GuardDuty에서 24시간 내에 Amazon EC2 인스턴스에 대한 여러 Amazon EC2 조사 결과가 생성되는 경우 스캔은 첫 번째 관련 Amazon EC2 조사 결과에 대해서만 수행됩니다. 인스턴스의 Amazon EC2 조사 결과가 마지막 맬웨어 스캔으로부터 24시간 후에 계속되는 경우 이 인스턴스에 대해 새 맬웨어 스캔이 시작됩니다.\"\n",
            "GuardDuty 맬웨어 방지로 생성한 스냅샷을 유지할 수 있나요?\n",
            "예. Malware Protection 스캔에서 맬웨어가 탐지될 때 스냅샷 보존을 사용하도록 하는 설정이 있습니다. GuardDuty 콘솔의 설정(Settings) 페이지에서 이 설정을 사용하도록 설정할 수 있습니다. 기본적으로 스냅샷은 스캔이 완료되고 몇 분 후에 삭제되며 스캔이 완료되지 않은 경우 24시간 후에 삭제됩니다.\n",
            "\"category : Guardduty, question : GuardDuty 맬웨어 방지로 생성한 스냅샷을 유지할 수 있나요?, answer : 예. Malware Protection 스캔에서 맬웨어가 탐지될 때 스냅샷 보존을 사용하도록 하는 설정이 있습니다. GuardDuty 콘솔의 설정(Settings) 페이지에서 이 설정을 사용하도록 설정할 수 있습니다. 기본적으로 스냅샷은 스캔이 완료되고 몇 분 후에 삭제되며 스캔이 완료되지 않은 경우 24시간 후에 삭제됩니다.\"\n",
            "기본적으로 Amazon EBS 볼륨 복제본이 보존되는 최대 기간은 얼마인가요?\n",
            "GuardDuty 맬웨어 방지는 생성되고 스캔되는 각 Amazon EBS 볼륨 복제본을 최대 24시간 동안 보존합니다. 기본적으로 Amazon EBS 볼륨 복제본은 GuardDuty 맬웨어 방지의 스캔이 완료되고 몇 분 후에 삭제됩니다. 그러나 서비스 중단 또는 연결 문제로 인해 맬웨어 스캔이 중단되는 경우 GuardDuty 맬웨어 방지에서 24시간을 초과하여 Amazon EBS 볼륨 복제본을 보존해야 할 수도 있습니다. 이 경우 GuardDuty 맬웨어 방지는 Amazon EBS 볼륨 복제본을 최대 7일간 보존하여 중단 또는 연결 문제를 선별하고 해결할 충분한 시간을 확보합니다. GuardDuty 맬웨어 방지는 중단 또는 장애가 해결되거나 연장된 보존 기간이 경과한 후 Amazon EBS 볼륨 복제본을 삭제합니다.\n",
            "\"category : Guardduty, question : 기본적으로 Amazon EBS 볼륨 복제본이 보존되는 최대 기간은 얼마인가요?, answer : GuardDuty 맬웨어 방지는 생성되고 스캔되는 각 Amazon EBS 볼륨 복제본을 최대 24시간 동안 보존합니다. 기본적으로 Amazon EBS 볼륨 복제본은 GuardDuty 맬웨어 방지의 스캔이 완료되고 몇 분 후에 삭제됩니다. 그러나 서비스 중단 또는 연결 문제로 인해 맬웨어 스캔이 중단되는 경우 GuardDuty 맬웨어 방지에서 24시간을 초과하여 Amazon EBS 볼륨 복제본을 보존해야 할 수도 있습니다. 이 경우 GuardDuty 맬웨어 방지는 Amazon EBS 볼륨 복제본을 최대 7일간 보존하여 중단 또는 연결 문제를 선별하고 해결할 충분한 시간을 확보합니다. GuardDuty 맬웨어 방지는 중단 또는 장애가 해결되거나 연장된 보존 기간이 경과한 후 Amazon EBS 볼륨 복제본을 삭제합니다.\"\n",
            "맬웨어 가능성을 나타내는 단일 Amazon EC2 인스턴스 또는 컨테이너 워크로드에 대한 여러 GuardDuty 조사 결과가 여러 맬웨어 검사를 시작합니까?\n",
            "아니요. GuardDuty는 감염 가능성이 있는 Amazon EC2 인스턴스 또는 컨테이너 워크로드에 연결된 Amazon EBS 볼륨의 스냅샷을 기반으로 하는 복제본을 24시간에 한 번만 스캔합니다. GuardDuty에서 맬웨어 스캔을 시작하기에 적합한 여러 조사 결과가 생성되더라도 이전 스캔 이후 24시간이 지나지 않았다면 추가 스캔이 시작되지 않습니다. GuardDuty에서 이전 맬웨어 스캔으로부터 24시간 후에 적격 결과가 생성되는 경우 GuardDuty Malware Protection은 해당 워크로드에 대해 새 맬웨어 스캔을 시작합니다.\n",
            "\"category : Guardduty, question : 맬웨어 가능성을 나타내는 단일 Amazon EC2 인스턴스 또는 컨테이너 워크로드에 대한 여러 GuardDuty 조사 결과가 여러 맬웨어 검사를 시작합니까?, answer : 아니요. GuardDuty는 감염 가능성이 있는 Amazon EC2 인스턴스 또는 컨테이너 워크로드에 연결된 Amazon EBS 볼륨의 스냅샷을 기반으로 하는 복제본을 24시간에 한 번만 스캔합니다. GuardDuty에서 맬웨어 스캔을 시작하기에 적합한 여러 조사 결과가 생성되더라도 이전 스캔 이후 24시간이 지나지 않았다면 추가 스캔이 시작되지 않습니다. GuardDuty에서 이전 맬웨어 스캔으로부터 24시간 후에 적격 결과가 생성되는 경우 GuardDuty Malware Protection은 해당 워크로드에 대해 새 맬웨어 스캔을 시작합니다.\"\n",
            "GuardDuty를 사용 중지하는 경우 Malware Protection 기능도 사용 중지해야 하나요?\n",
            "아니요. GuardDuty 서비스를 사용 중지하면 Malware Protection 기능도 사용 중지됩니다.\n",
            "\"category : Guardduty, question : GuardDuty를 사용 중지하는 경우 Malware Protection 기능도 사용 중지해야 하나요?, answer : 아니요. GuardDuty 서비스를 사용 중지하면 Malware Protection 기능도 사용 중지됩니다.\"\n",
            "GuardDuty RDS Protection은 어떤 방식으로 작동하나요?\n",
            "GuardDuty RDS Protection은 수동으로 배포할 에이전트나 활성화할 데이터 소스, 구성할 권한 없이도 GuardDuty 콘솔에서 단일 작업으로 활성화할 수 있습니다. GuardDuty RDS Protection은 맞춤형 ML 모델을 사용하여 기존 및 신규 Amazon Aurora 데이터베이스에 대한 로그인 시도를 분석하고 프로파일링합니다. 알려진 악의적 행위자에 의한 의심스러운 동작 또는 시도가 식별되면 GuardDuty 및 Amazon Relational Database Service(RDS) 콘솔, Security Hub 및 Amazon EventBridge로 실행 가능한 보안 조사 결과가 전송되므로 기존 보안 이벤트 관리 또는 워크플로 시스템에 이 조사 결과를 통합할 수 있습니다. GuardDuty RDS Protection에서 RDS 로그인 활동 모니터링을 사용하는 방법에 대해 자세히 알아보세요.\n",
            "\"category : Guardduty, question : GuardDuty RDS Protection은 어떤 방식으로 작동하나요?, answer : GuardDuty RDS Protection은 수동으로 배포할 에이전트나 활성화할 데이터 소스, 구성할 권한 없이도 GuardDuty 콘솔에서 단일 작업으로 활성화할 수 있습니다. GuardDuty RDS Protection은 맞춤형 ML 모델을 사용하여 기존 및 신규 Amazon Aurora 데이터베이스에 대한 로그인 시도를 분석하고 프로파일링합니다. 알려진 악의적 행위자에 의한 의심스러운 동작 또는 시도가 식별되면 GuardDuty 및 Amazon Relational Database Service(RDS) 콘솔, Security Hub 및 Amazon EventBridge로 실행 가능한 보안 조사 결과가 전송되므로 기존 보안 이벤트 관리 또는 워크플로 시스템에 이 조사 결과를 통합할 수 있습니다. GuardDuty RDS Protection에서 RDS 로그인 활동 모니터링을 사용하는 방법에 대해 자세히 알아보세요.\"\n",
            "현재 GuardDuty를 사용하고 있는 경우 Aurora 데이터베이스에 대한 위협 탐지를 시작하려면 어떻게 해야 하나요?\n",
            "현재 GuardDuty 계정의 경우 RDS Protection 페이지에서 GuardDuty 콘솔이나 API를 통해 기능을 활성화할 수 있습니다. GuardDuty RDS Protection에 대해 자세히 알아보세요.\n",
            "\"category : Guardduty, question : 현재 GuardDuty를 사용하고 있는 경우 Aurora 데이터베이스에 대한 위협 탐지를 시작하려면 어떻게 해야 하나요?, answer : 현재 GuardDuty 계정의 경우 RDS Protection 페이지에서 GuardDuty 콘솔이나 API를 통해 기능을 활성화할 수 있습니다. GuardDuty RDS Protection에 대해 자세히 알아보세요.\"\n",
            "GuardDuty를 처음 사용하는 경우 계정에 기본적으로 Aurora 데이터베이스에 대한 위협 탐지 기능이 사용되나요?\n",
            "예. 콘솔 또는 API를 통해 GuardDuty를 활성화하는 신규 계정에는 RDS Protection이 기본적으로 활성화됩니다. AWS Organizations의 자동 사용 기능을 사용하여 생성된 신규 GuardDuty 계정은 Auto-enable for RDS(RDS 자동 사용) 옵션을 활성화해야 RDS Protection이 활성화됩니다.\n",
            "\"category : Guardduty, question : GuardDuty를 처음 사용하는 경우 계정에 기본적으로 Aurora 데이터베이스에 대한 위협 탐지 기능이 사용되나요?, answer : 예. 콘솔 또는 API를 통해 GuardDuty를 활성화하는 신규 계정에는 RDS Protection이 기본적으로 활성화됩니다. AWS Organizations의 자동 사용 기능을 사용하여 생성된 신규 GuardDuty 계정은 Auto-enable for RDS(RDS 자동 사용) 옵션을 활성화해야 RDS Protection이 활성화됩니다.\"\n",
            "전체 GuardDuty 서비스(예: Amazon Virtual Private Cloud(VPC) 흐름 로그, DNS 쿼리 로그 및 AWS CloudTrail 관리 이벤트 분석)를 활성화하지 않고 GuardDuty RDS Protection을 사용할 수 있나요?\n",
            "아니요. GuardDuty RDS Protection을 사용하려면 GuardDuty 서비스를 사용하도록 설정해야 합니다.\n",
            "\"category : Guardduty, question : 전체 GuardDuty 서비스(예: Amazon Virtual Private Cloud(VPC) 흐름 로그, DNS 쿼리 로그 및 AWS CloudTrail 관리 이벤트 분석)를 활성화하지 않고 GuardDuty RDS Protection을 사용할 수 있나요?, answer : 아니요. GuardDuty RDS Protection을 사용하려면 GuardDuty 서비스를 사용하도록 설정해야 합니다.\"\n",
            "현재 GuardDuty가 제공되는 모든 리전에서 GuardDuty RDS Protection을 사용할 수 있나요?\n",
            "RDS Protection이 제공되는 리전의 전체 목록을 보려면 리전별 기능 가용성 페이지를 참조하세요.\n",
            "\"category : Guardduty, question : 현재 GuardDuty가 제공되는 모든 리전에서 GuardDuty RDS Protection을 사용할 수 있나요?, answer : RDS Protection이 제공되는 리전의 전체 목록을 보려면 리전별 기능 가용성 페이지를 참조하세요.\"\n",
            "GuardDuty RDS Protection이 지원하는 Amazon Aurora 버전은 무엇인가요?\n",
            "지원되는 Amazon Aurora 데이터베이스 버전 목록을 참조하세요.\n",
            "\"category : Guardduty, question : GuardDuty RDS Protection이 지원하는 Amazon Aurora 버전은 무엇인가요?, answer : 지원되는 Amazon Aurora 데이터베이스 버전 목록을 참조하세요.\"\n",
            "GuardDuty RDS Protection을 사용하면 Aurora 데이터베이스의 실행 성능이나 비용이 영향을 받습니까?\n",
            "아니요. Aurora 데이터베이스에 대한 GuardDuty 위협 탐지 기능은 Amazon Aurora 데이터베이스의 성능, 가용성 또는 비용에 영향을 미치지 않도록 설계되었습니다.\n",
            "\"category : Guardduty, question : GuardDuty RDS Protection을 사용하면 Aurora 데이터베이스의 실행 성능이나 비용이 영향을 받습니까?, answer : 아니요. Aurora 데이터베이스에 대한 GuardDuty 위협 탐지 기능은 Amazon Aurora 데이터베이스의 성능, 가용성 또는 비용에 영향을 미치지 않도록 설계되었습니다.\"\n",
            "Amazon GuardDuty Lambda Protection은 어떤 식으로 작동하나요?\n",
            "GuardDuty Lambda Protection은 서버리스 워크로드에서 VPC 흐름 로그를 시작으로 네트워크 활동을 지속적으로 모니터링하여 무단 암호화폐 채굴용으로 악의적으로 용도가 변경된 함수나 알려진 위협 요소 서버와 통신하는 손상된 Lambda 함수와 같은 위협을 탐지합니다. GuardDuty Lambda Protection은 GuardDuty 콘솔에서 몇 단계만 거치면 활성화할 수 있으며, AWS Organizations를 사용하면 중앙에서 조직의 모든 기존 계정 및 신규 계정에 대해 사용하도록 설정할 수 있습니다. 사용하도록 설정한 후에는 계정에 있는 모든 기존 및 새로운 Lambda 함수의 네트워크 활동 데이터 모니터링을 자동으로 시작됩니다.\n",
            "\"category : Guardduty, question : Amazon GuardDuty Lambda Protection은 어떤 식으로 작동하나요?, answer : GuardDuty Lambda Protection은 서버리스 워크로드에서 VPC 흐름 로그를 시작으로 네트워크 활동을 지속적으로 모니터링하여 무단 암호화폐 채굴용으로 악의적으로 용도가 변경된 함수나 알려진 위협 요소 서버와 통신하는 손상된 Lambda 함수와 같은 위협을 탐지합니다. GuardDuty Lambda Protection은 GuardDuty 콘솔에서 몇 단계만 거치면 활성화할 수 있으며, AWS Organizations를 사용하면 중앙에서 조직의 모든 기존 계정 및 신규 계정에 대해 사용하도록 설정할 수 있습니다. 사용하도록 설정한 후에는 계정에 있는 모든 기존 및 새로운 Lambda 함수의 네트워크 활동 데이터 모니터링을 자동으로 시작됩니다.\"\n",
            "현재 GuardDuty를 사용하고 있는 경우 GuardDuty Lambda Protection을 시작하려면 어떻게 해야 하나요?\n",
            "현재 GuardDuty 계정의 경우 Lambda Protection 페이지에서 GuardDuty 콘솔이나 API를 통해 기능을 활성화할 수 있습니다. GuardDuty Lambda Protection에 대해 자세히 알아보세요.\n",
            "\"category : Guardduty, question : 현재 GuardDuty를 사용하고 있는 경우 GuardDuty Lambda Protection을 시작하려면 어떻게 해야 하나요?, answer : 현재 GuardDuty 계정의 경우 Lambda Protection 페이지에서 GuardDuty 콘솔이나 API를 통해 기능을 활성화할 수 있습니다. GuardDuty Lambda Protection에 대해 자세히 알아보세요.\"\n",
            "GuardDuty를 처음 사용하는 경우 계정에 기본적으로 GuardDuty Lambda Protection이 사용되나요?\n",
            "예. 콘솔 또는 API를 통해 GuardDuty를 활성화하는 신규 계정에는 Lambda Protection이 기본적으로 활성화됩니다. AWS Organizations의 자동 사용 기능을 사용하여 생성된 신규 GuardDuty 계정은 Auto-enable for Lambda(Lambda 자동 사용) 옵션을 활성화해야 Lambda Protection이 활성화됩니다.\n",
            "\"category : Guardduty, question : GuardDuty를 처음 사용하는 경우 계정에 기본적으로 GuardDuty Lambda Protection이 사용되나요?, answer : 예. 콘솔 또는 API를 통해 GuardDuty를 활성화하는 신규 계정에는 Lambda Protection이 기본적으로 활성화됩니다. AWS Organizations의 자동 사용 기능을 사용하여 생성된 신규 GuardDuty 계정은 Auto-enable for Lambda(Lambda 자동 사용) 옵션을 활성화해야 Lambda Protection이 활성화됩니다.\"\n",
            "현재 GuardDuty가 제공되는 모든 리전에서 GuardDuty Lambda Protection을 사용할 수 있나요?\n",
            "Lambda Protection이 제공되는 리전의 전체 목록을 보려면 리전별 기능 가용성 페이지를 참조하세요.\n",
            "\"category : Guardduty, question : 현재 GuardDuty가 제공되는 모든 리전에서 GuardDuty Lambda Protection을 사용할 수 있나요?, answer : Lambda Protection이 제공되는 리전의 전체 목록을 보려면 리전별 기능 가용성 페이지를 참조하세요.\"\n",
            "GuardDuty Lambda Protection을 사용하면 Lambda 워크로드의 실행 성능이나 비용이 영향을 받습니까?\n",
            "아니요. GuardDuty Lambda Protection은 Lambda 워크로드의 성능, 가용성 또는 비용에 영향을 미치지 않도록 설계되었습니다.\n",
            "\"category : Guardduty, question : GuardDuty Lambda Protection을 사용하면 Lambda 워크로드의 실행 성능이나 비용이 영향을 받습니까?, answer : 아니요. GuardDuty Lambda Protection은 Lambda 워크로드의 성능, 가용성 또는 비용에 영향을 미치지 않도록 설계되었습니다.\"\n",
            "========== Athena  :  https://aws.amazon.com/ko/athena/faqs/ 사이트 크롤링 진행중 ==========\n",
            "65\n",
            "Amazon Athena란 무엇인가요?\n",
            "Athena는 SQL을 사용해 Amazon Simple Storage Service(S3)의 데이터를 간편하게 분석할 수 있는 대화형 분석 서비스입니다. Athena는 서버리스 서비스이므로 설정하거나 관리할 인프라가 없으며 데이터 분석을 즉시 시작할 수 있습니다. Athena로 데이터를 로드할 필요 없이 Amazon S3에 저장된 데이터를 직접 사용하면 됩니다. Amazon Athena for SQL은 표준 SQL을 완벽하게 지원하는 Trino 및 Presto를 사용하며, CSV, JSON, Apache ORC, Apache Parquet, Apache Avro 등 다양한 표준 데이터 형식과 호환됩니다. Athena for Apache Spark는 Python을 지원하며 빅 데이터 워크로드에 사용되는 오픈 소스 분산형 처리 시스템인 Apache Spark를 사용할 수 있습니다. 시작하려면 Athena Management Console에 로그인하고 쿼리 편집기 또는 노트북을 사용하여 데이터 상호 작용을 시작합니다.\n",
            "\"category : Athena, question : Amazon Athena란 무엇인가요?, answer : Athena는 SQL을 사용해 Amazon Simple Storage Service(S3)의 데이터를 간편하게 분석할 수 있는 대화형 분석 서비스입니다. Athena는 서버리스 서비스이므로 설정하거나 관리할 인프라가 없으며 데이터 분석을 즉시 시작할 수 있습니다. Athena로 데이터를 로드할 필요 없이 Amazon S3에 저장된 데이터를 직접 사용하면 됩니다. Amazon Athena for SQL은 표준 SQL을 완벽하게 지원하는 Trino 및 Presto를 사용하며, CSV, JSON, Apache ORC, Apache Parquet, Apache Avro 등 다양한 표준 데이터 형식과 호환됩니다. Athena for Apache Spark는 Python을 지원하며 빅 데이터 워크로드에 사용되는 오픈 소스 분산형 처리 시스템인 Apache Spark를 사용할 수 있습니다. 시작하려면 Athena Management Console에 로그인하고 쿼리 편집기 또는 노트북을 사용하여 데이터 상호 작용을 시작합니다.\"\n",
            "Athena로 어떤 작업을 할 수 있나요?\n",
            "Athena를 사용하면 온프레미스 데이터 소스 또는 기타 클라우드 시스템을 포함하여 30개 이상의 다양한 데이터 소스와 S3에 저장된 데이터를 분석할 수 있습니다. Athena를 사용하면 데이터를 집계하거나 Athena로 로드할 필요 없이 ANSI SQL 또는 Python을 사용한 대화형 분석을 실행할 수 있습니다. Athena는 비정형, 반정형, 정형 데이터 세트를 처리할 수 있습니다. 예를 들어, CSV, JSON, Avro 또는 열 기반 데이터 형식(예: Parquet 및 ORC)이 여기에 해당됩니다. Amazon Athena for SQL은 데이터를 시각화하거나 대시보드를 생성할 수 있도록 Amazon QuickSight와 통합됩니다. 또한 Athena를 사용하면 보고서를 생성하거나 ODBC 또는 JDBC 드라이버를 통해 연결된 비즈니스 인텔리전스 도구나 SQL 클라이언트로 데이터를 탐색할 수도 있습니다.\n",
            "\"category : Athena, question : Athena로 어떤 작업을 할 수 있나요?, answer : Athena를 사용하면 온프레미스 데이터 소스 또는 기타 클라우드 시스템을 포함하여 30개 이상의 다양한 데이터 소스와 S3에 저장된 데이터를 분석할 수 있습니다. Athena를 사용하면 데이터를 집계하거나 Athena로 로드할 필요 없이 ANSI SQL 또는 Python을 사용한 대화형 분석을 실행할 수 있습니다. Athena는 비정형, 반정형, 정형 데이터 세트를 처리할 수 있습니다. 예를 들어, CSV, JSON, Avro 또는 열 기반 데이터 형식(예: Parquet 및 ORC)이 여기에 해당됩니다. Amazon Athena for SQL은 데이터를 시각화하거나 대시보드를 생성할 수 있도록 Amazon QuickSight와 통합됩니다. 또한 Athena를 사용하면 보고서를 생성하거나 ODBC 또는 JDBC 드라이버를 통해 연결된 비즈니스 인텔리전스 도구나 SQL 클라이언트로 데이터를 탐색할 수도 있습니다.\"\n",
            "Athena를 시작하려면 어떻게 해야 하나요?\n",
            "Athena를 시작하려면 Athena용 AWS Management Console에 로그인한 후 콘솔에서 데이터 정의 언어(DDL) 문을 작성하거나 테이블 생성 마법사를 사용하여 스키마를 작성합니다. 그러면 기본 제공된 쿼리 편집기를 사용하여 데이터 쿼리를 시작할 수 있습니다. Athena는 S3에서 직접 데이터를 쿼리하기 때문에 로드할 필요가 없습니다.\n",
            "\"category : Athena, question : Athena를 시작하려면 어떻게 해야 하나요?, answer : Athena를 시작하려면 Athena용 AWS Management Console에 로그인한 후 콘솔에서 데이터 정의 언어(DDL) 문을 작성하거나 테이블 생성 마법사를 사용하여 스키마를 작성합니다. 그러면 기본 제공된 쿼리 편집기를 사용하여 데이터 쿼리를 시작할 수 있습니다. Athena는 S3에서 직접 데이터를 쿼리하기 때문에 로드할 필요가 없습니다.\"\n",
            "Athena에 액세스하려면 어떻게 해야 하나요?\n",
            "Amazon Athena for SQL은 AWS Management Console, AWS SDK 및 CLI 또는 Athena의 ODBC 또는 JDBC 드라이버를 통해 액세스할 수 있습니다. ODBC 또는 JDBC 드라이버를 사용하여 프로그래밍 방식으로 쿼리를 실행하고 테이블 또는 파티션을 추가할 수 있습니다.\n",
            "\"category : Athena, question : Athena에 액세스하려면 어떻게 해야 하나요?, answer : Amazon Athena for SQL은 AWS Management Console, AWS SDK 및 CLI 또는 Athena의 ODBC 또는 JDBC 드라이버를 통해 액세스할 수 있습니다. ODBC 또는 JDBC 드라이버를 사용하여 프로그래밍 방식으로 쿼리를 실행하고 테이블 또는 파티션을 추가할 수 있습니다.\"\n",
            "Athena for SQL의 기반 기술은 무엇인가요?\n",
            "Athena for SQL은 표준 SQL을 완벽하게 지원하는 Trino를 사용하며 CSV, JSON, ORC, Avro, Parquet 등 다양한 표준 데이터 형식과 호환됩니다. Athena는 대규모 조인, window 함수 및 배열과 같은 복잡한 분석을 처리할 수 있습니다. Trino에 내장된 Amazon Athena SQL 엔진 버전 3을 사용하여 지속적으로 성능을 높이고 Presto에 내장된 Amazon Athena 엔진 버전 2에 대한 AWS 접근 방식과 유사한 새로운 기능을 제공합니다. v3의 가장 뛰어난 특징 중 하나는, 오픈 소스 소프트웨어 관리에 대한 새로운 지속적인 통합 접근 방식이라는 점입니다. 이를 통해 고객은 Trino와 PrestoDB 프로젝트로 최신 기술을 계속 이용할 수 있습니다. 오픈 소스 Trino 출시를 기준으로 60~90일의 기간 내 지원을 제공하는 것을 목표로 하고 있습니다. Athena 개발 팀은 다시 오픈 소스 코드 베이스에서의 버그 수정 및 보안, 확장성, 성능, 기능 개선에 적극적으로 기여합니다. 이러한 팀의 기여 노력은 Trino, Presto, Apache Iceberg를 사용하는 모든 사용자에게 혜택을 전달해줄 수 있습니다.\n",
            "\"category : Athena, question : Athena for SQL의 기반 기술은 무엇인가요?, answer : Athena for SQL은 표준 SQL을 완벽하게 지원하는 Trino를 사용하며 CSV, JSON, ORC, Avro, Parquet 등 다양한 표준 데이터 형식과 호환됩니다. Athena는 대규모 조인, window 함수 및 배열과 같은 복잡한 분석을 처리할 수 있습니다. Trino에 내장된 Amazon Athena SQL 엔진 버전 3을 사용하여 지속적으로 성능을 높이고 Presto에 내장된 Amazon Athena 엔진 버전 2에 대한 AWS 접근 방식과 유사한 새로운 기능을 제공합니다. v3의 가장 뛰어난 특징 중 하나는, 오픈 소스 소프트웨어 관리에 대한 새로운 지속적인 통합 접근 방식이라는 점입니다. 이를 통해 고객은 Trino와 PrestoDB 프로젝트로 최신 기술을 계속 이용할 수 있습니다. 오픈 소스 Trino 출시를 기준으로 60~90일의 기간 내 지원을 제공하는 것을 목표로 하고 있습니다. Athena 개발 팀은 다시 오픈 소스 코드 베이스에서의 버그 수정 및 보안, 확장성, 성능, 기능 개선에 적극적으로 기여합니다. 이러한 팀의 기여 노력은 Trino, Presto, Apache Iceberg를 사용하는 모든 사용자에게 혜택을 전달해줄 수 있습니다.\"\n",
            "Athena for SQL은 테이블 정의와 스키마를 어떻게 저장하나요?\n",
            "Athena for SQL은 S3에 저장된 데이터에 대해 사용자가 생성한 데이터베이스 및 테이블에 관한 정보와 스키마를 저장하기 위해 관리형 AWS Glue 데이터 카탈로그를 사용합니다. AWS Glue가 제공되는 리전에서는 Athena에 데이터 카탈로그를 사용하도록 업그레이드할 수 있습니다. AWS Glue가 제공되지 않는 리전에서는 Athena에 내부 카탈로그가 사용됩니다.\n",
            " \n",
            "DDL 문을 사용하거나 AWS Management Console을 통해 카탈로그를 수정할 수 있습니다. 사용자가 정의하는 모든 스키마는 명시적으로 삭제하지 않는 한, 자동으로 저장됩니다. Athena는 스키마 온 리드 기술을 사용합니다. 즉, 쿼리가 적용될 때 사용자의 테이블 정의가 S3의 데이터에 적용됩니다. 데이터 로드 또는 변환을 수행할 필요가 없습니다. S3에 저장된 기본 데이터에 영향을 주지 않으면서 테이블 정의 및 스키마를 삭제할 수 있습니다.\n",
            "\"category : Athena, question : Athena for SQL은 테이블 정의와 스키마를 어떻게 저장하나요?, answer : Athena for SQL은 S3에 저장된 데이터에 대해 사용자가 생성한 데이터베이스 및 테이블에 관한 정보와 스키마를 저장하기 위해 관리형 AWS Glue 데이터 카탈로그를 사용합니다. AWS Glue가 제공되는 리전에서는 Athena에 데이터 카탈로그를 사용하도록 업그레이드할 수 있습니다. AWS Glue가 제공되지 않는 리전에서는 Athena에 내부 카탈로그가 사용됩니다.\n",
            " \n",
            "DDL 문을 사용하거나 AWS Management Console을 통해 카탈로그를 수정할 수 있습니다. 사용자가 정의하는 모든 스키마는 명시적으로 삭제하지 않는 한, 자동으로 저장됩니다. Athena는 스키마 온 리드 기술을 사용합니다. 즉, 쿼리가 적용될 때 사용자의 테이블 정의가 S3의 데이터에 적용됩니다. 데이터 로드 또는 변환을 수행할 필요가 없습니다. S3에 저장된 기본 데이터에 영향을 주지 않으면서 테이블 정의 및 스키마를 삭제할 수 있습니다.\"\n",
            "데이터 카탈로그를 사용하도록 업그레이드해야 하는 이유는 무엇입니까?\n",
            "AWS Glue는 완전 관리형 추출, 전환, 적재(ETL) 서비스입니다. AWS Glue는 3가지 주요 요소로 구성됩니다. 1) 자동으로 데이터 소스를 스캔하고 데이터 형식을 식별하며 스키마를 추론하는 크롤러, 2) 데이터를 변환하고 다양한 대상으로 이동할 수 있는 완전 관리형 ETL 서비스, 3) S3 또는 ODBC, JDBC 호환 데이터 스토어에 저장된 데이터베이스 및 테이블에 관한 메타데이터 정보를 저장하는 데이터 카탈로그입니다. AWS Glue를 충분히 활용하기 위해서는 Athena의 내부 데이터 카탈로그를 사용하는 것에서 Glue 데이터 카탈로그를 사용하는 것으로 업그레이드해야 합니다.\n",
            "데이터 카탈로그로 업그레이드할 때의 이점은 다음과 같습니다.\n",
            "\n",
            "통합된 메타데이터 리포지토리: AWS Glue는 다양한 AWS 서비스와 통합됩니다. AWS Glue는 Amazon Aurora, Amazon Relational Database Service(RDS) for MySQL, Amazon RDS for PostgreSQL, Amazon Redshift 및 S3에 저장된 데이터와 Amazon Elastic Compute Cloud(EC2)에서 실행되는 Amazon Virtual Private Cloud(VPC)의 MySQL 및 PostgreSQL 데이터베이스에 저장된 데이터를 지원합니다. AWS Glue는 Athena, Amazon EMR, Amazon Redshift Spectrum 및 Apache Hive Metastore 호환 애플리케이션과 즉시 통합됩니다.\n",
            "자동 스키마 및 파티션 인식: AWS Glue는 자동으로 데이터 소스를 크롤링하고, 데이터 형식을 파악하며, 스키마와 변환을 제안합니다. 크롤러는 테이블 생성과 파티션 로딩을 자동화하는 데 도움이 됩니다.\n",
            "\n",
            "데이터 카탈로그에 대해 자세히 알아보려면 AWS Glue 웹 페이지를 검토하십시오.\n",
            "\"category : Athena, question : 데이터 카탈로그를 사용하도록 업그레이드해야 하는 이유는 무엇입니까?, answer : AWS Glue는 완전 관리형 추출, 전환, 적재(ETL) 서비스입니다. AWS Glue는 3가지 주요 요소로 구성됩니다. 1) 자동으로 데이터 소스를 스캔하고 데이터 형식을 식별하며 스키마를 추론하는 크롤러, 2) 데이터를 변환하고 다양한 대상으로 이동할 수 있는 완전 관리형 ETL 서비스, 3) S3 또는 ODBC, JDBC 호환 데이터 스토어에 저장된 데이터베이스 및 테이블에 관한 메타데이터 정보를 저장하는 데이터 카탈로그입니다. AWS Glue를 충분히 활용하기 위해서는 Athena의 내부 데이터 카탈로그를 사용하는 것에서 Glue 데이터 카탈로그를 사용하는 것으로 업그레이드해야 합니다.\n",
            "데이터 카탈로그로 업그레이드할 때의 이점은 다음과 같습니다.\n",
            "\n",
            "통합된 메타데이터 리포지토리: AWS Glue는 다양한 AWS 서비스와 통합됩니다. AWS Glue는 Amazon Aurora, Amazon Relational Database Service(RDS) for MySQL, Amazon RDS for PostgreSQL, Amazon Redshift 및 S3에 저장된 데이터와 Amazon Elastic Compute Cloud(EC2)에서 실행되는 Amazon Virtual Private Cloud(VPC)의 MySQL 및 PostgreSQL 데이터베이스에 저장된 데이터를 지원합니다. AWS Glue는 Athena, Amazon EMR, Amazon Redshift Spectrum 및 Apache Hive Metastore 호환 애플리케이션과 즉시 통합됩니다.\n",
            "자동 스키마 및 파티션 인식: AWS Glue는 자동으로 데이터 소스를 크롤링하고, 데이터 형식을 파악하며, 스키마와 변환을 제안합니다. 크롤러는 테이블 생성과 파티션 로딩을 자동화하는 데 도움이 됩니다.\n",
            "\n",
            "데이터 카탈로그에 대해 자세히 알아보려면 AWS Glue 웹 페이지를 검토하십시오.\"\n",
            "데이터 카탈로그로 업그레이드하는 단계별 프로세스가 있나요?\n",
            "예. 단계별 프로세스는 Amazon Athena 사용 설명서: AWS Glue와의 통합을 검토하세요.\n",
            "\"category : Athena, question : 데이터 카탈로그로 업그레이드하는 단계별 프로세스가 있나요?, answer : 예. 단계별 프로세스는 Amazon Athena 사용 설명서: AWS Glue와의 통합을 검토하세요.\"\n",
            "Athena를 사용할 수 있는 리전은 어디인가요?\n",
            "리전별 Athena 서비스 가용성에 대한 자세한 내용은 AWS 리전 서비스 목록을 검토하세요.\n",
            "\"category : Athena, question : Athena를 사용할 수 있는 리전은 어디인가요?, answer : 리전별 Athena 서비스 가용성에 대한 자세한 내용은 AWS 리전 서비스 목록을 검토하세요.\"\n",
            "Athena와 관련된 서비스 한도에는 어떤 것이 있나요?\n",
            "서비스 한도에 대해 자세히 알아보려면 Amazon Athena 사용 설명서: Service Quotas를 검토하세요.\n",
            "\"category : Athena, question : Athena와 관련된 서비스 한도에는 어떤 것이 있나요?, answer : 서비스 한도에 대해 자세히 알아보려면 Amazon Athena 사용 설명서: Service Quotas를 검토하세요.\"\n",
            "S3의 내 데이터에 대한 테이블과 스키마를 생성하려면 어떻게 해야 하나요?\n",
            "Athena는 Apache Hive DDL을 사용하여 테이블을 정의합니다. Athena 콘솔, ODBC 또는 JDBC 드라이버, API를 통해 또는 Athena 테이블 생성 마법사를 사용하여 DDL 문을 실행할 수 있습니다. Athena와 함께 데이터 카탈로그를 사용하는 경우, AWS Glue 크롤러를 사용하여 스키마와 파티션을 자동으로 추론할 수도 있습니다. AWS Glue 크롤러는 데이터 스토어에 연결하고, 우선순위가 지정된 분류자 목록을 거치면서 데이터 스키마 및 기타 통계를 추출한 후, 이러한 메타데이터로 데이터 카탈로그를 채웁니다. 크롤러는 주기적으로 실행되어 새로운 데이터의 가용성과 기존 데이터에 대한 변경 사항(데이터 정의 변경 등)을 감지할 수 있습니다. 크롤러는 새로운 테이블, 기존 테이블에 새로운 파티션, 새로운 테이블 정의 버전을 자동으로 추가합니다. 자체 파일 유형을 분류하도록 AWS Glue 크롤러를 사용자 지정할 수 있습니다. \n",
            "Athena에 새 테이블 스키마를 생성하면 이 스키마는 데이터 카탈로그에 저장되며 쿼리를 실행할 때 사용되지만 S3에 있는 사용자의 데이터를 수정하지는 않습니다. Athena는 스키마 온 리드라는 접근 방식을 사용하는데, 이 방식을 적용하면 쿼리를 실행할 때 스키마를 사용자의 데이터에 투영할 수 있습니다. 따라서 데이터를 로드하거나 변환할 필요가 줄어듭니다. 테이블 생성에 대해 자세히 알아보세요.\n",
            "\"category : Athena, question : S3의 내 데이터에 대한 테이블과 스키마를 생성하려면 어떻게 해야 하나요?, answer : Athena는 Apache Hive DDL을 사용하여 테이블을 정의합니다. Athena 콘솔, ODBC 또는 JDBC 드라이버, API를 통해 또는 Athena 테이블 생성 마법사를 사용하여 DDL 문을 실행할 수 있습니다. Athena와 함께 데이터 카탈로그를 사용하는 경우, AWS Glue 크롤러를 사용하여 스키마와 파티션을 자동으로 추론할 수도 있습니다. AWS Glue 크롤러는 데이터 스토어에 연결하고, 우선순위가 지정된 분류자 목록을 거치면서 데이터 스키마 및 기타 통계를 추출한 후, 이러한 메타데이터로 데이터 카탈로그를 채웁니다. 크롤러는 주기적으로 실행되어 새로운 데이터의 가용성과 기존 데이터에 대한 변경 사항(데이터 정의 변경 등)을 감지할 수 있습니다. 크롤러는 새로운 테이블, 기존 테이블에 새로운 파티션, 새로운 테이블 정의 버전을 자동으로 추가합니다. 자체 파일 유형을 분류하도록 AWS Glue 크롤러를 사용자 지정할 수 있습니다. \n",
            "Athena에 새 테이블 스키마를 생성하면 이 스키마는 데이터 카탈로그에 저장되며 쿼리를 실행할 때 사용되지만 S3에 있는 사용자의 데이터를 수정하지는 않습니다. Athena는 스키마 온 리드라는 접근 방식을 사용하는데, 이 방식을 적용하면 쿼리를 실행할 때 스키마를 사용자의 데이터에 투영할 수 있습니다. 따라서 데이터를 로드하거나 변환할 필요가 줄어듭니다. 테이블 생성에 대해 자세히 알아보세요.\"\n",
            "Athena는 어떤 데이터 형식을 지원하나요?\n",
            "Athena는 CSV, TSV, JSON 또는 Textfiles 등 매우 다양한 데이터 형식들을 지원하며 ORC 및 Parquet 같은 오픈 소스 열 형식도 지원합니다. Athena는 Snappy, Zlib, LZO 및 GZIP 형식으로 압축된 데이터도 지원합니다. 열 형식을 압축, 파티셔닝 및 활용하여 성능을 개선하고 비용을 절감할 수 있습니다.\n",
            "\"category : Athena, question : Athena는 어떤 데이터 형식을 지원하나요?, answer : Athena는 CSV, TSV, JSON 또는 Textfiles 등 매우 다양한 데이터 형식들을 지원하며 ORC 및 Parquet 같은 오픈 소스 열 형식도 지원합니다. Athena는 Snappy, Zlib, LZO 및 GZIP 형식으로 압축된 데이터도 지원합니다. 열 형식을 압축, 파티셔닝 및 활용하여 성능을 개선하고 비용을 절감할 수 있습니다.\"\n",
            "Athena는 어떤 종류의 데이터 유형을 지원하나요?\n",
            "Athena는 INTEGER, DOUBLE, VARCHAR 등의 단순 데이터 유형과 MAPS, ARRAY, STRUCT 등의 복합 데이터 유형을 모두 지원합니다.\n",
            "\"category : Athena, question : Athena는 어떤 종류의 데이터 유형을 지원하나요?, answer : Athena는 INTEGER, DOUBLE, VARCHAR 등의 단순 데이터 유형과 MAPS, ARRAY, STRUCT 등의 복합 데이터 유형을 모두 지원합니다.\"\n",
            "Athena에서 Hive 쿼리를 실행해도 되나요?\n",
            "Athena는 DDL과 테이블 또는 파티션의 생성, 수정, 삭제에만 Hive를 사용합니다. 지원되는 문의 전체 목록은 Amazon Athena 사용 설명서: DDL 문을 검토하세요. Athena는 S3에서 SQL 쿼리를 실행할 때 Trino 및 Presto를 사용합니다. ANSI 호환 SQL SELECT 문을 실행하여 S3에서 데이터를 쿼리할 수 있습니다.\n",
            "\"category : Athena, question : Athena에서 Hive 쿼리를 실행해도 되나요?, answer : Athena는 DDL과 테이블 또는 파티션의 생성, 수정, 삭제에만 Hive를 사용합니다. 지원되는 문의 전체 목록은 Amazon Athena 사용 설명서: DDL 문을 검토하세요. Athena는 S3에서 SQL 쿼리를 실행할 때 Trino 및 Presto를 사용합니다. ANSI 호환 SQL SELECT 문을 실행하여 S3에서 데이터를 쿼리할 수 있습니다.\"\n",
            "SerDe란 무엇인가요?\n",
            "SerDe는 데이터 형식을 해석하는 방법을 Hive에 알려주는 라이브러리인 Serializer/Deserializer를 나타냅니다. Hive DDL 문을 사용할 때는 SerDe를 지정해야 합니다. 그렇게 해야 사용자가 가리키는 데이터를 시스템에서 해석할 수 있습니다. Athena는 SerDe를 사용하여 S3에서 읽은 데이터를 해석합니다. Athena에서 SerDe의 개념은 Hive에서 사용된 개념과 동일합니다. Amazon Athena에서 지원하는 SerDe를 열거하면 다음과 같습니다.\n",
            "\n",
            "Apache 웹 로그: \"org.apache.hadoop.hive.serde2.RegexSerDe\"\n",
            "CSV: \"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\"\n",
            "TSV: \"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\"\n",
            "사용자 지정 구분 기호: \"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\"\n",
            "Parquet: \"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\"\n",
            "Orc: \"org.apache.hadoop.hive.ql.io.orc.OrcSerde\"\n",
            "JSON: “org.apache.hive.hcatalog.data.JsonSerDe” or \"org.openx.data.jsonserde.JsonSerDe\"\n",
            "\"category : Athena, question : SerDe란 무엇인가요?, answer : SerDe는 데이터 형식을 해석하는 방법을 Hive에 알려주는 라이브러리인 Serializer/Deserializer를 나타냅니다. Hive DDL 문을 사용할 때는 SerDe를 지정해야 합니다. 그렇게 해야 사용자가 가리키는 데이터를 시스템에서 해석할 수 있습니다. Athena는 SerDe를 사용하여 S3에서 읽은 데이터를 해석합니다. Athena에서 SerDe의 개념은 Hive에서 사용된 개념과 동일합니다. Amazon Athena에서 지원하는 SerDe를 열거하면 다음과 같습니다.\n",
            "\n",
            "Apache 웹 로그: \"org.apache.hadoop.hive.serde2.RegexSerDe\"\n",
            "CSV: \"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\"\n",
            "TSV: \"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\"\n",
            "사용자 지정 구분 기호: \"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\"\n",
            "Parquet: \"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\"\n",
            "Orc: \"org.apache.hadoop.hive.ql.io.orc.OrcSerde\"\n",
            "JSON: “org.apache.hive.hcatalog.data.JsonSerDe” or \"org.openx.data.jsonserde.JsonSerDe\"\"\n",
            "자체 SerDe를 Athena에 추가할 수 있나요?\n",
            "현재로서는 자체 SerDe를 Athena에 추가할 수 없습니다. 소중한 의견 감사드리며 추가하고 싶은 SerDe가 있다면 Athena 팀(athena-feedback@amazon.com)에 문의하시기 바랍니다.\n",
            "\"category : Athena, question : 자체 SerDe를 Athena에 추가할 수 있나요?, answer : 현재로서는 자체 SerDe를 Athena에 추가할 수 없습니다. 소중한 의견 감사드리며 추가하고 싶은 SerDe가 있다면 Athena 팀(athena-feedback@amazon.com)에 문의하시기 바랍니다.\"\n",
            "Spark/Hive를 사용하여 Parquet/ORC 파일을 생성한 경우 Athena에서 이 파일을 쿼리할 수 있나요?\n",
            "예. Spark를 통해 생성한 Parquet 및 ORC 파일을 Athena에서 읽을 수 있습니다.\n",
            "\"category : Athena, question : Spark/Hive를 사용하여 Parquet/ORC 파일을 생성한 경우 Athena에서 이 파일을 쿼리할 수 있나요?, answer : 예. Spark를 통해 생성한 Parquet 및 ORC 파일을 Athena에서 읽을 수 있습니다.\"\n",
            "Amazon Kinesis Data Firehose의 데이터가 있는 경우 Athena를 사용하여 이 데이터를 쿼리하려면 어떻게 해야 하나요?\n",
            "Kinesis Data Firehose 데이터가 S3에 저장되어 있는 경우 Athena를 사용하여 이 데이터를 쿼리할 수 있습니다. Athena에서 데이터에 대한 스키마를 만든 다음 쿼리를 시작하면 됩니다. 성능을 높이려면 데이터를 파티션으로 구성하는 것이 좋습니다. ALTER TABLE DDL 문을 사용하면 Data Firehose에서 만든 파티션을 추가할 수 있습니다. 데이터 파티셔닝에 대해 자세히 알아보세요.\n",
            "\"category : Athena, question : Amazon Kinesis Data Firehose의 데이터가 있는 경우 Athena를 사용하여 이 데이터를 쿼리하려면 어떻게 해야 하나요?, answer : Kinesis Data Firehose 데이터가 S3에 저장되어 있는 경우 Athena를 사용하여 이 데이터를 쿼리할 수 있습니다. Athena에서 데이터에 대한 스키마를 만든 다음 쿼리를 시작하면 됩니다. 성능을 높이려면 데이터를 파티션으로 구성하는 것이 좋습니다. ALTER TABLE DDL 문을 사용하면 Data Firehose에서 만든 파티션을 추가할 수 있습니다. 데이터 파티셔닝에 대해 자세히 알아보세요.\"\n",
            "Athena는 데이터 파티셔닝을 지원하나요?\n",
            "예. Athena를 사용하여 모든 열의 데이터를 파티셔닝할 수 있습니다. 파티션을 사용하면 각 쿼리에서 스캔하는 데이터의 양을 제한하여 비용을 절감하고 작업 시간을 단축할 수 있습니다. CREATE TABLE 문의 PARTITIONED BY 절을 사용하여 파티셔닝 스키마를 지정할 수 있습니다. Amazon Athena는 쿼리 계획을 최적화하고 쿼리 런타임을 단축하는 AWS Glue 데이터 카탈로그 파티션 인덱스를 지원합니다. 많은 수의 파티션을 포함한 테이블을 쿼리할 때, Athena는 AWS Glue 데이터 카탈로그에서 사용 가능한 파티션을 검색하고 쿼리에 어떤 것이 필요한지 결정합니다. 신규 파티션이 추가되면 파티션을 검색하는 데 필요한 시간은 증가하고, 이는 쿼리 런타임이 증가하는 원인이 될 수 있습니다. AWS Glue 데이터 카탈로그를 사용하여 고객은 파티션 인덱스를 생성할 수 있으며, 이를 통해 수만, 수십만 개의 파티션을 포함하는 테이블에서 파티션 메타데이터를 검색하고 필터링하는 데 필요한 시간을 줄일 수 있습니다.\n",
            "\"category : Athena, question : Athena는 데이터 파티셔닝을 지원하나요?, answer : 예. Athena를 사용하여 모든 열의 데이터를 파티셔닝할 수 있습니다. 파티션을 사용하면 각 쿼리에서 스캔하는 데이터의 양을 제한하여 비용을 절감하고 작업 시간을 단축할 수 있습니다. CREATE TABLE 문의 PARTITIONED BY 절을 사용하여 파티셔닝 스키마를 지정할 수 있습니다. Amazon Athena는 쿼리 계획을 최적화하고 쿼리 런타임을 단축하는 AWS Glue 데이터 카탈로그 파티션 인덱스를 지원합니다. 많은 수의 파티션을 포함한 테이블을 쿼리할 때, Athena는 AWS Glue 데이터 카탈로그에서 사용 가능한 파티션을 검색하고 쿼리에 어떤 것이 필요한지 결정합니다. 신규 파티션이 추가되면 파티션을 검색하는 데 필요한 시간은 증가하고, 이는 쿼리 런타임이 증가하는 원인이 될 수 있습니다. AWS Glue 데이터 카탈로그를 사용하여 고객은 파티션 인덱스를 생성할 수 있으며, 이를 통해 수만, 수십만 개의 파티션을 포함하는 테이블에서 파티션 메타데이터를 검색하고 필터링하는 데 필요한 시간을 줄일 수 있습니다.\"\n",
            "Athena의 기존 테이블에 새 데이터를 추가하려면 어떻게 해야 하나요?\n",
            "데이터를 파티셔닝한 경우 S3에서 새 데이터를 사용할 수 있을 때 파티션을 Athena에 추가하려면 메타데이터 쿼리(ALTER TABLE ADD PARTITION)를 실행해야 합니다. 데이터를 파티셔닝하지 않은 경우 기존 접두사에 새 데이터(또는 파일)를 추가하면 이 데이터가 Athena에 자동으로 추가됩니다. 데이터 파티셔닝에 대해 자세히 알아보세요.\n",
            "\"category : Athena, question : Athena의 기존 테이블에 새 데이터를 추가하려면 어떻게 해야 하나요?, answer : 데이터를 파티셔닝한 경우 S3에서 새 데이터를 사용할 수 있을 때 파티션을 Athena에 추가하려면 메타데이터 쿼리(ALTER TABLE ADD PARTITION)를 실행해야 합니다. 데이터를 파티셔닝하지 않은 경우 기존 접두사에 새 데이터(또는 파일)를 추가하면 이 데이터가 Athena에 자동으로 추가됩니다. 데이터 파티셔닝에 대해 자세히 알아보세요.\"\n",
            "S3에 이미 많은 양의 로그 데이터가 있는 경우 Athena를 사용하여 이 데이터를 쿼리할 수 있나요?\n",
            "예. Athena를 사용하면 기존 로그 데이터에서 표준 SQL 쿼리를 간편하게 실행할 수 있습니다. Athena는 S3에서 직접 데이터를 쿼리하기 때문에 데이터를 이동하거나 로드할 필요가 없습니다. DDL 문을 사용하여 스키마를 정의하고 데이터 쿼리를 즉시 시작하면 됩니다.\n",
            "\"category : Athena, question : S3에 이미 많은 양의 로그 데이터가 있는 경우 Athena를 사용하여 이 데이터를 쿼리할 수 있나요?, answer : 예. Athena를 사용하면 기존 로그 데이터에서 표준 SQL 쿼리를 간편하게 실행할 수 있습니다. Athena는 S3에서 직접 데이터를 쿼리하기 때문에 데이터를 이동하거나 로드할 필요가 없습니다. DDL 문을 사용하여 스키마를 정의하고 데이터 쿼리를 즉시 시작하면 됩니다.\"\n",
            "Athena는 어떤 종류의 쿼리를 지원합니까?\n",
            "Athena는 ANSI SQL 쿼리를 지원합니다. Athena는 오픈 소스 인 메모리 분산 SQL 엔진인 Trino를 사용하며 대규모 조인, window 함수 및 배열을 포함하여 복잡한 분석을 처리할 수 있습니다.\n",
            "\"category : Athena, question : Athena는 어떤 종류의 쿼리를 지원합니까?, answer : Athena는 ANSI SQL 쿼리를 지원합니다. Athena는 오픈 소스 인 메모리 분산 SQL 엔진인 Trino를 사용하며 대규모 조인, window 함수 및 배열을 포함하여 복잡한 분석을 처리할 수 있습니다.\"\n",
            "Athena와 함께 QuickSight를 사용할 수 있나요?\n",
            "예. Athena는 QuickSight와 통합되므로 S3에 저장된 데이터를 원활하게 시각화할 수 있습니다.\n",
            "\"category : Athena, question : Athena와 함께 QuickSight를 사용할 수 있나요?, answer : 예. Athena는 QuickSight와 통합되므로 S3에 저장된 데이터를 원활하게 시각화할 수 있습니다.\"\n",
            "Athena는 다른 비즈니스 인텔리전스(BI) 도구와 SQL 클라이언트를 지원하나요?\n",
            "예. Athena에는 다른 BI 도구 및 SQL 클라이언트와 함께 사용할 수 있는 ODBC 및 JDBC 드라이버가 함께 제공됩니다. Athena에서 ODBC 또는 JDBC 드라이버를 사용하는 방법에 대해 자세히 알아보세요.\n",
            "\"category : Athena, question : Athena는 다른 비즈니스 인텔리전스(BI) 도구와 SQL 클라이언트를 지원하나요?, answer : 예. Athena에는 다른 BI 도구 및 SQL 클라이언트와 함께 사용할 수 있는 ODBC 및 JDBC 드라이버가 함께 제공됩니다. Athena에서 ODBC 또는 JDBC 드라이버를 사용하는 방법에 대해 자세히 알아보세요.\"\n",
            "Athena에서 지원하는 함수에 액세스하려면 어떻게 해야 하나요?\n",
            "Athena에서 지원하는 함수에 대해 자세히 알아보세요.\n",
            "\"category : Athena, question : Athena에서 지원하는 함수에 액세스하려면 어떻게 해야 하나요?, answer : Athena에서 지원하는 함수에 대해 자세히 알아보세요.\"\n",
            "내 쿼리의 성능을 향상시키려면 어떻게 해야 하나요?\n",
            "데이터를 압축 및 파티셔닝하거나 열 형식으로 변환하여 쿼리 성능을 향상시킬 수 있습니다. Athena는 Parquet 및 ORC와 같은 오픈 소스 열 데이터 형식을 지원합니다. 데이터를 압축된 열 기반 형식으로 변환하면 Athena가 쿼리를 실행할 때 S3로부터 스캔하는 데이터 양이 줄어들므로 비용이 절감되고 쿼리 성능은 향상됩니다.\n",
            "\"category : Athena, question : 내 쿼리의 성능을 향상시키려면 어떻게 해야 하나요?, answer : 데이터를 압축 및 파티셔닝하거나 열 형식으로 변환하여 쿼리 성능을 향상시킬 수 있습니다. Athena는 Parquet 및 ORC와 같은 오픈 소스 열 데이터 형식을 지원합니다. 데이터를 압축된 열 기반 형식으로 변환하면 Athena가 쿼리를 실행할 때 S3로부터 스캔하는 데이터 양이 줄어들므로 비용이 절감되고 쿼리 성능은 향상됩니다.\"\n",
            "Athena는 사용자 정의 함수(UDF)를 지원하나요?\n",
            "예. Athena는 UDF를 지원하므로 사용자 지정 스칼라 함수를 작성하고 SQL 쿼리에서 해당 함수를 호출할 수 있습니다. Athena에서는 기본 제공 함수가 지원되지만 UDF는 데이터 압축 및 압축 해제, 민감한 데이터 삭제 또는 사용자 지정된 복호화 적용 같은 사용자 지정 처리 작업을 수행하는 데 도움이 됩니다.\n",
            "사용자는 Athena Query Federation SDK를 사용하여 Java로 UDF를 작성할 수 있습니다. Athena에 제출된 SQL 쿼리에 UDF가 사용되면 AWS Lambda에서 호출하여 실행합니다. UDF는 SQL 쿼리의 SELECT 절과 FILTER 절 모두에서 사용할 수 있습니다. 동일한 쿼리에서 여러 UDF를 호출할 수 있습니다.\n",
            "\"category : Athena, question : Athena는 사용자 정의 함수(UDF)를 지원하나요?, answer : 예. Athena는 UDF를 지원하므로 사용자 지정 스칼라 함수를 작성하고 SQL 쿼리에서 해당 함수를 호출할 수 있습니다. Athena에서는 기본 제공 함수가 지원되지만 UDF는 데이터 압축 및 압축 해제, 민감한 데이터 삭제 또는 사용자 지정된 복호화 적용 같은 사용자 지정 처리 작업을 수행하는 데 도움이 됩니다.\n",
            "사용자는 Athena Query Federation SDK를 사용하여 Java로 UDF를 작성할 수 있습니다. Athena에 제출된 SQL 쿼리에 UDF가 사용되면 AWS Lambda에서 호출하여 실행합니다. UDF는 SQL 쿼리의 SELECT 절과 FILTER 절 모두에서 사용할 수 있습니다. 동일한 쿼리에서 여러 UDF를 호출할 수 있습니다.\"\n",
            "UDF를 작성할 때의 사용자 경험은 어떤가요?\n",
            "Athena Query Federation SDK를 사용하여 UDF를 작성할 수 있습니다. UDF 예제를 검토하세요. 함수를 Lambda에 업로드한 다음 Athena 쿼리에서 호출할 수 있습니다. 시작하려면 Amazon Athena 사용 설명서: Lambda를 사용하여 UDF 생성 및 배포를 참조하세요.\n",
            "Athena는 성능을 개선하기 위해 일련의 데이터 세트 행에서 배치로 UDF를 호출합니다.\n",
            "\"category : Athena, question : UDF를 작성할 때의 사용자 경험은 어떤가요?, answer : Athena Query Federation SDK를 사용하여 UDF를 작성할 수 있습니다. UDF 예제를 검토하세요. 함수를 Lambda에 업로드한 다음 Athena 쿼리에서 호출할 수 있습니다. 시작하려면 Amazon Athena 사용 설명서: Lambda를 사용하여 UDF 생성 및 배포를 참조하세요.\n",
            "Athena는 성능을 개선하기 위해 일련의 데이터 세트 행에서 배치로 UDF를 호출합니다.\"\n",
            "Athena는 멀티클라우드 분석을 지원하나요?\n",
            "예. Athena는 데이터를 이동하거나 변환하지 않고도 다른 클라우드 서비스 공급자 및 기타 클라우드 스토리지 서비스의 데이터를 분석하는 데 사용할 수 있는 여러 데이터 소스 커넥터를 제공합니다. 데이터 소스 커넥터는 Azure Synapse, Azure 데이터 레이크 스토리지, Google BigQuery 및 Google 클라우드 스토리지를 비롯한 30개 이상의 데이터 소스에 사용할 수 있습니다. 하이브리드 및 멀티클라우드 환경을 위한 AWS 솔루션에 대해 자세히 알아보세요.\n",
            "\"category : Athena, question : Athena는 멀티클라우드 분석을 지원하나요?, answer : 예. Athena는 데이터를 이동하거나 변환하지 않고도 다른 클라우드 서비스 공급자 및 기타 클라우드 스토리지 서비스의 데이터를 분석하는 데 사용할 수 있는 여러 데이터 소스 커넥터를 제공합니다. 데이터 소스 커넥터는 Azure Synapse, Azure 데이터 레이크 스토리지, Google BigQuery 및 Google 클라우드 스토리지를 비롯한 30개 이상의 데이터 소스에 사용할 수 있습니다. 하이브리드 및 멀티클라우드 환경을 위한 AWS 솔루션에 대해 자세히 알아보세요.\"\n",
            "페더레이션 쿼리란 무엇인가요?\n",
            "S3 이외의 소스에 데이터가 있는 경우 Athena를 사용하여 데이터를 현재 위치에서 쿼리하거나, 여러 데이터 소스에서 데이터를 추출하고 S3에 저장하는 파이프라인을 구축할 수 있습니다. Athena 페더레이션 쿼리를 사용하면 관계형, 비관계형, 객체 및 사용자 지정 데이터 소스에 저장된 데이터에 대해 SQL 쿼리를 실행할 수 있습니다.\n",
            "\"category : Athena, question : 페더레이션 쿼리란 무엇인가요?, answer : S3 이외의 소스에 데이터가 있는 경우 Athena를 사용하여 데이터를 현재 위치에서 쿼리하거나, 여러 데이터 소스에서 데이터를 추출하고 S3에 저장하는 파이프라인을 구축할 수 있습니다. Athena 페더레이션 쿼리를 사용하면 관계형, 비관계형, 객체 및 사용자 지정 데이터 소스에 저장된 데이터에 대해 SQL 쿼리를 실행할 수 있습니다.\"\n",
            "Athena에서 페더레이션 쿼리를 사용해야 하는 이유는 무엇입니까?\n",
            "조직에서는 조직의 애플리케이션 또는 비즈니스 프로세스의 요구 사항을 충족하는 데이터 소스에 데이터를 저장하는 경우가 많습니다. 예를 들어 S3 데이터 레이크에 데이터를 저장하는 것에 더해 관계형, 키-값, 문서, 인 메모리, 검색, 그래프, 시계열 및 원장 데이터베이스 등에도 데이터를 저장할 수 있습니다. 이와 같이 다양한 소스에서 분석을 수행하는 작업은 복잡하고 시간 소모적일 수 있습니다. 새로운 프로그래밍 언어 또는 데이터베이스 구조를 배워야 하고 분석에 사용할 데이터를 추출, 변환 및 복제하는 복잡한 파이프라인을 구축해야 하기 때문입니다. Athena를 사용하면 데이터가 있는 위치에서 SQL 쿼리를 실행할 수 있으므로 이 복잡성이 줄어듭니다. 잘 알려진 SQL 구성을 사용하여 여러 데이터 소스에 걸쳐 데이터를 쿼리해 신속하게 분석하거나, 예약된 SQL 쿼리를 사용하여 여러 데이터 소스에서 데이터를 추출 및 변환한 후 추가 분석을 위해 S3에 저장할 수 있습니다.\n",
            "\"category : Athena, question : Athena에서 페더레이션 쿼리를 사용해야 하는 이유는 무엇입니까?, answer : 조직에서는 조직의 애플리케이션 또는 비즈니스 프로세스의 요구 사항을 충족하는 데이터 소스에 데이터를 저장하는 경우가 많습니다. 예를 들어 S3 데이터 레이크에 데이터를 저장하는 것에 더해 관계형, 키-값, 문서, 인 메모리, 검색, 그래프, 시계열 및 원장 데이터베이스 등에도 데이터를 저장할 수 있습니다. 이와 같이 다양한 소스에서 분석을 수행하는 작업은 복잡하고 시간 소모적일 수 있습니다. 새로운 프로그래밍 언어 또는 데이터베이스 구조를 배워야 하고 분석에 사용할 데이터를 추출, 변환 및 복제하는 복잡한 파이프라인을 구축해야 하기 때문입니다. Athena를 사용하면 데이터가 있는 위치에서 SQL 쿼리를 실행할 수 있으므로 이 복잡성이 줄어듭니다. 잘 알려진 SQL 구성을 사용하여 여러 데이터 소스에 걸쳐 데이터를 쿼리해 신속하게 분석하거나, 예약된 SQL 쿼리를 사용하여 여러 데이터 소스에서 데이터를 추출 및 변환한 후 추가 분석을 위해 S3에 저장할 수 있습니다.\"\n",
            "어떤 데이터 소스가 지원되나요?\n",
            "Athena는 Amazon Redshift, Amazon DynamoDB, Google BigQuery, Google 클라우드 스토리지, Azure Synapse, Azure 데이터 레이크 스토리지, Snowflake 및 SAP Hana를 포함하여 30개 이상의 인기 있는 AWS, 온프레미스 및 기타 클라우드 데이터 스토어에 기본 제공 커넥터를 제공합니다. 이 커넥터를 사용하여 정형, 반정형, 객체, 그래프, 시계열 및 기타 데이터 스토리지 유형에서 SQL 분석 사용 사례를 지원할 수 있습니다. 지원되는 소스의 목록은 Athena 데이터 소스 커넥터 사용을 참조하세요.\n",
            " 또한 Athena의 데이터 커넥터 SDK를 사용하여 사용자 지정 데이터 소스 커넥터를 생성하고 Athena를 통해 쿼리할 수 있습니다. 설명서와 예제 커넥터 구현을 검토하여 시작해 보세요.\n",
            "\"category : Athena, question : 어떤 데이터 소스가 지원되나요?, answer : Athena는 Amazon Redshift, Amazon DynamoDB, Google BigQuery, Google 클라우드 스토리지, Azure Synapse, Azure 데이터 레이크 스토리지, Snowflake 및 SAP Hana를 포함하여 30개 이상의 인기 있는 AWS, 온프레미스 및 기타 클라우드 데이터 스토어에 기본 제공 커넥터를 제공합니다. 이 커넥터를 사용하여 정형, 반정형, 객체, 그래프, 시계열 및 기타 데이터 스토리지 유형에서 SQL 분석 사용 사례를 지원할 수 있습니다. 지원되는 소스의 목록은 Athena 데이터 소스 커넥터 사용을 참조하세요.\n",
            " 또한 Athena의 데이터 커넥터 SDK를 사용하여 사용자 지정 데이터 소스 커넥터를 생성하고 Athena를 통해 쿼리할 수 있습니다. 설명서와 예제 커넥터 구현을 검토하여 시작해 보세요.\"\n",
            "페더레이션 쿼리로 지원할 수 있는 사용 사례는 무엇인가요?\n",
            "Athena를 사용할 때는 기존 SQL 지식을 사용하여 다양한 데이터 소스에서 인사이트를 추출할 수 있습니다. 새 언어를 배우거나 데이터를 추출(및 복제)하는 스크립트를 개발하거나 인프라를 관리할 필요가 없습니다. Amazon Athena를 사용하면 다음 태스크를 수행할 수 있습니다.\n",
            "\n",
            "단일 도구와 SQL 언어를 사용하여 여러 데이터 소스에 분산되어 있는 데이터에 대한 온디맨드 분석을 실행합니다.\n",
            "ODBC 및 JDBC 인터페이스를 통해 Athena의 분산 컴퓨팅 엔진으로 복잡한 다중 소스 조인을 푸시하는 BI 애플리케이션의 데이터를 시각화합니다.\n",
            "Athena를 AWS Step Functions와 통합하여 셀프 서비스 ETL 파이프라인 및 이벤트 기반 데이터 처리 워크플로를 설계합니다.\n",
            "다양한 데이터 소스를 통합하여 기계 학습 모델 훈련 워크플로를 위한 풍부한 입력 특성을 생성합니다.\n",
            "데이터 메시 아키텍처의 인사이트를 표면화하는 사용자용 제품형 데이터 애플리케이션을 개발합니다.\n",
            "조직이 온프레미스 소스에서 AWS로 마이그레이션하는 동안 분석 사용 사례를 지원합니다.\n",
            "\"category : Athena, question : 페더레이션 쿼리로 지원할 수 있는 사용 사례는 무엇인가요?, answer : Athena를 사용할 때는 기존 SQL 지식을 사용하여 다양한 데이터 소스에서 인사이트를 추출할 수 있습니다. 새 언어를 배우거나 데이터를 추출(및 복제)하는 스크립트를 개발하거나 인프라를 관리할 필요가 없습니다. Amazon Athena를 사용하면 다음 태스크를 수행할 수 있습니다.\n",
            "\n",
            "단일 도구와 SQL 언어를 사용하여 여러 데이터 소스에 분산되어 있는 데이터에 대한 온디맨드 분석을 실행합니다.\n",
            "ODBC 및 JDBC 인터페이스를 통해 Athena의 분산 컴퓨팅 엔진으로 복잡한 다중 소스 조인을 푸시하는 BI 애플리케이션의 데이터를 시각화합니다.\n",
            "Athena를 AWS Step Functions와 통합하여 셀프 서비스 ETL 파이프라인 및 이벤트 기반 데이터 처리 워크플로를 설계합니다.\n",
            "다양한 데이터 소스를 통합하여 기계 학습 모델 훈련 워크플로를 위한 풍부한 입력 특성을 생성합니다.\n",
            "데이터 메시 아키텍처의 인사이트를 표면화하는 사용자용 제품형 데이터 애플리케이션을 개발합니다.\n",
            "조직이 온프레미스 소스에서 AWS로 마이그레이션하는 동안 분석 사용 사례를 지원합니다.\"\n",
            "ETL에 페더레이션 쿼리를 사용할 수 있나요?\n",
            "Athena는 쿼리 결과를 S3의 파일에 저장합니다. 따라서 Athena를 사용하여 다른 사용자 및 애플리케이션에 페더레이션 데이터를 제공할 수 있습니다. 기반 소스를 반복적으로 쿼리할 필요 없이 Athena를 사용하여 데이터 분석을 수행하고 싶다면 Athena의 CREATE TABLE AS 함수를 사용합니다. 또한 Athena의 UNLOAD 함수를 사용하여 데이터를 쿼리하고 S3의 특정 파일 형식에 결과를 저장할 수 있습니다.\n",
            "\"category : Athena, question : ETL에 페더레이션 쿼리를 사용할 수 있나요?, answer : Athena는 쿼리 결과를 S3의 파일에 저장합니다. 따라서 Athena를 사용하여 다른 사용자 및 애플리케이션에 페더레이션 데이터를 제공할 수 있습니다. 기반 소스를 반복적으로 쿼리할 필요 없이 Athena를 사용하여 데이터 분석을 수행하고 싶다면 Athena의 CREATE TABLE AS 함수를 사용합니다. 또한 Athena의 UNLOAD 함수를 사용하여 데이터를 쿼리하고 S3의 특정 파일 형식에 결과를 저장할 수 있습니다.\"\n",
            "데이터 소스 커넥터는 어떻게 작동하나요?\n",
            "데이터 소스 커넥터는 Lambda에서 실행되어 대상 데이터 소스와 Athena 사이에서 변환을 수행하는 코드 조각입니다. 데이터 소스 커넥터를 사용하여 데이터 스토어를 Athena에 등록하면 페더레이션 데이터 스토어에서 SQL 쿼리를 실행할 수 있습니다. 페더레이션 소스에서 쿼리를 실행하면 Athena가 Lambda 함수를 호출하고 페더레이션 소스와 관련된 쿼리 일부를 실행하여 처리합니다. 자세히 알아보려면 Amazon Athena 사용 설명서: Amazon Athena 페더레이션 쿼리 사용을 검토하세요.\n",
            "\"category : Athena, question : 데이터 소스 커넥터는 어떻게 작동하나요?, answer : 데이터 소스 커넥터는 Lambda에서 실행되어 대상 데이터 소스와 Athena 사이에서 변환을 수행하는 코드 조각입니다. 데이터 소스 커넥터를 사용하여 데이터 스토어를 Athena에 등록하면 페더레이션 데이터 스토어에서 SQL 쿼리를 실행할 수 있습니다. 페더레이션 소스에서 쿼리를 실행하면 Athena가 Lambda 함수를 호출하고 페더레이션 소스와 관련된 쿼리 일부를 실행하여 처리합니다. 자세히 알아보려면 Amazon Athena 사용 설명서: Amazon Athena 페더레이션 쿼리 사용을 검토하세요.\"\n",
            "Athena가 지원하는 임베디드 기계 학습 사용 사례에는 어떤 것이 있습니까?\n",
            "Athena의 기계 학습 사용 사례는 다음 예와 같이 여러 산업을 포괄합니다. 재무 위험 데이터 분석가는 가상 분석과 몬테카를로 시뮬레이션을 실행할 수 있습니다. 비즈니스 분석가는 수익을 예측하는 풍부하고 미래 지향적인 비즈니스 대시보드를 만드는 데 도움이 되는 선형 회귀 또는 예측 모델을 실행하여 미래 가치를 예측할 수 있습니다. 마케팅 분석가는 K-평균 클러스터링 모델을 사용하여 서로 다른 고객 세그먼트를 결정할 수 있습니다. 보안 분석가는 로지스틱 회귀 모델을 사용하여 이상 징후를 찾고 로그에서 보안 인시던트를 탐지할 수 있습니다.\n",
            "\"category : Athena, question : Athena가 지원하는 임베디드 기계 학습 사용 사례에는 어떤 것이 있습니까?, answer : Athena의 기계 학습 사용 사례는 다음 예와 같이 여러 산업을 포괄합니다. 재무 위험 데이터 분석가는 가상 분석과 몬테카를로 시뮬레이션을 실행할 수 있습니다. 비즈니스 분석가는 수익을 예측하는 풍부하고 미래 지향적인 비즈니스 대시보드를 만드는 데 도움이 되는 선형 회귀 또는 예측 모델을 실행하여 미래 가치를 예측할 수 있습니다. 마케팅 분석가는 K-평균 클러스터링 모델을 사용하여 서로 다른 고객 세그먼트를 결정할 수 있습니다. 보안 분석가는 로지스틱 회귀 모델을 사용하여 이상 징후를 찾고 로그에서 보안 인시던트를 탐지할 수 있습니다.\"\n",
            "Athena에서 사용할 수 있는 기계 학습 모델은 무엇인가요?\n",
            "Athena는 SageMaker에 배포된 모든 기계 학습 모델을 호출할 수 있습니다. 독점 데이터를 사용하여 자체 모델을 훈련하거나 SageMaker에서 사전 훈련 및 배포된 모델을 사용할 수 있습니다. 예를 들어 새 레코드를 이전 레코드에 사용한 것과 동일한 범주로 분류하려는 경우 자체 데이터로 클러스터 분석을 훈련할 수 있습니다. 또는 실제 스포츠 이벤트를 예측할 때는 사용되는 훈련 데이터가 이미 공공의 자산이 되었을 것이므로 공개적으로 사용 가능한 모델을 사용할 수 있습니다. 일반적으로 도메인별 또는 산업별 예측은 SageMaker에서 자체 데이터로 훈련되며, 기계 학습 요구 사항이 획일적인 경우 외부 모델을 사용할 수 있습니다.\n",
            "\"category : Athena, question : Athena에서 사용할 수 있는 기계 학습 모델은 무엇인가요?, answer : Athena는 SageMaker에 배포된 모든 기계 학습 모델을 호출할 수 있습니다. 독점 데이터를 사용하여 자체 모델을 훈련하거나 SageMaker에서 사전 훈련 및 배포된 모델을 사용할 수 있습니다. 예를 들어 새 레코드를 이전 레코드에 사용한 것과 동일한 범주로 분류하려는 경우 자체 데이터로 클러스터 분석을 훈련할 수 있습니다. 또는 실제 스포츠 이벤트를 예측할 때는 사용되는 훈련 데이터가 이미 공공의 자산이 되었을 것이므로 공개적으로 사용 가능한 모델을 사용할 수 있습니다. 일반적으로 도메인별 또는 산업별 예측은 SageMaker에서 자체 데이터로 훈련되며, 기계 학습 요구 사항이 획일적인 경우 외부 모델을 사용할 수 있습니다.\"\n",
            "Athena를 사용하여 기계 학습 모델을 훈련할 수 있나요?\n",
            "Athena를 사용하여 SageMaker에서 기계 학습 모델을 훈련 및 배포할 수는 없습니다. 기계 학습 모델을 훈련할 수 있고, 아니면 Athena를 사용하여 SageMaker에 배포된 기존의 사전 훈련된 모델을 사용할 수 있습니다. SageMaker의 훈련 단계가 자세히 설명된 문서를 읽어보세요.\n",
            "\"category : Athena, question : Athena를 사용하여 기계 학습 모델을 훈련할 수 있나요?, answer : Athena를 사용하여 SageMaker에서 기계 학습 모델을 훈련 및 배포할 수는 없습니다. 기계 학습 모델을 훈련할 수 있고, 아니면 Athena를 사용하여 SageMaker에 배포된 기존의 사전 훈련된 모델을 사용할 수 있습니다. SageMaker의 훈련 단계가 자세히 설명된 문서를 읽어보세요.\"\n",
            "Comprehend 등의 기타 서비스에 배포된 모델, 예측 또는 내 EC2 클러스터에 배포된 모델에 대해 추론을 실행할 수 있나요?\n",
            "Athena는 SageMaker에 배포된 기계 학습 모델만 호출할 수 있습니다. Athena와 함께 사용하고자 하는 다른 서비스가 있다면 언제든지 피드백을 보내주세요. athena-feedback@amazon.com으로 피드백을 보내주세요.\n",
            "\"category : Athena, question : Comprehend 등의 기타 서비스에 배포된 모델, 예측 또는 내 EC2 클러스터에 배포된 모델에 대해 추론을 실행할 수 있나요?, answer : Athena는 SageMaker에 배포된 기계 학습 모델만 호출할 수 있습니다. Athena와 함께 사용하고자 하는 다른 서비스가 있다면 언제든지 피드백을 보내주세요. athena-feedback@amazon.com으로 피드백을 보내주세요.\"\n",
            "SageMaker 추론에 Athena 쿼리를 사용하면 성능에 어떤 영향을 미치나요?\n",
            "AWS의 기능 및 서비스에는 운영 성능 개선 사항이 지속적으로 추가됩니다. Athena ML 쿼리의 성능을 개선하기 위해 추론 시 SageMaker ML 모델을 호출할 때 행은 배치로 처리됩니다. 현재, 사용자 제공 행 배치 크기 재정의는 지원되지 않습니다.\n",
            "\"category : Athena, question : SageMaker 추론에 Athena 쿼리를 사용하면 성능에 어떤 영향을 미치나요?, answer : AWS의 기능 및 서비스에는 운영 성능 개선 사항이 지속적으로 추가됩니다. Athena ML 쿼리의 성능을 개선하기 위해 추론 시 SageMaker ML 모델을 호출할 때 행은 배치로 처리됩니다. 현재, 사용자 제공 행 배치 크기 재정의는 지원되지 않습니다.\"\n",
            "Athena 기계 학습은 어떤 기능을 지원하나요?\n",
            "Athena는 SQL 인터페이스로 래핑된 기계 학습 추론(예측) 기능을 제공합니다. Athena UDF를 호출하여 결과 세트에서 사전 또는 사후 처리 로직을 호출할 수도 있습니다. 입력값에는 열, 레코드 또는 테이블이 포함될 수 있으며, 확장성을 높이기 위해 여러 호출을 배치 처리할 수 있습니다. 선택 단계 또는 필터 단계에서 추론을 실행할 수 있습니다. 자세히 알아보려면 Amazon Athena 사용 설명서: Amazon Athena와 함께 기계 학습(ML) 사용을 참조하세요.\n",
            "\"category : Athena, question : Athena 기계 학습은 어떤 기능을 지원하나요?, answer : Athena는 SQL 인터페이스로 래핑된 기계 학습 추론(예측) 기능을 제공합니다. Athena UDF를 호출하여 결과 세트에서 사전 또는 사후 처리 로직을 호출할 수도 있습니다. 입력값에는 열, 레코드 또는 테이블이 포함될 수 있으며, 확장성을 높이기 위해 여러 호출을 배치 처리할 수 있습니다. 선택 단계 또는 필터 단계에서 추론을 실행할 수 있습니다. 자세히 알아보려면 Amazon Athena 사용 설명서: Amazon Athena와 함께 기계 학습(ML) 사용을 참조하세요.\"\n",
            "어떤 기계 학습 모델을 사용할 수 있습니까?\n",
            "SageMaker는 다양한 기계 학습 알고리즘을 지원합니다. 독점 기계 학습 모델을 만들어 SageMaker에 배포할 수도 있습니다. 예를 들어 새 레코드를 이전 레코드에 사용한 것과 동일한 범주로 분류하려는 경우 자체 데이터로 클러스터 분석을 훈련할 수 있습니다. 또는 실제 스포츠 이벤트를 예측할 때는 사용되는 훈련 데이터가 공공의 자산이 되었을 것이므로 공개적으로 사용 가능한 모델을 사용할 수 있습니다.\n",
            "도메인 또는 산업별 예측은 일반적으로 SageMaker에서 자체 데이터로 훈련되며, 기계 번역과 같이 기계 학습 요구 사항이 획일적인 경우 외부 모델을 사용할 수 있습니다.\n",
            "\"category : Athena, question : 어떤 기계 학습 모델을 사용할 수 있습니까?, answer : SageMaker는 다양한 기계 학습 알고리즘을 지원합니다. 독점 기계 학습 모델을 만들어 SageMaker에 배포할 수도 있습니다. 예를 들어 새 레코드를 이전 레코드에 사용한 것과 동일한 범주로 분류하려는 경우 자체 데이터로 클러스터 분석을 훈련할 수 있습니다. 또는 실제 스포츠 이벤트를 예측할 때는 사용되는 훈련 데이터가 공공의 자산이 되었을 것이므로 공개적으로 사용 가능한 모델을 사용할 수 있습니다.\n",
            "도메인 또는 산업별 예측은 일반적으로 SageMaker에서 자체 데이터로 훈련되며, 기계 번역과 같이 기계 학습 요구 사항이 획일적인 경우 외부 모델을 사용할 수 있습니다.\"\n",
            "내 데이터에 대한 액세스를 어떻게 제어하나요?\n",
            "Amazon Athena는 AWS Lake Formation을 통해 세분화된 액세스 제어를 지원합니다. AWS Lake Formation을 통해 S3 데이터 레이크의 데이터 카탈로그 리소스에 대한 권한 및 액세스 제어를 중앙에서 관리할 수 있습니다. Apache Iceberg, Apache Hudi, Apache Hive 같은 테이블 형식을 사용하여 모든 지원되는 파일 형식에 저장된 데이터에 대해 Athena 쿼리에서 세분화된 액세스 제어 정책을 적용할 수 있습니다. 사용 사례에 가장 적합한 테이블 및 파일 형식을 유연하게 선택하고 Athena를 사용하는 경우 중앙 집중식 데이터 거버넌스의 이점을 활용하여 데이터 액세스를 보호할 수 있습니다. 예를 들어, Lake Formation의 행 수준 보안 필터와 함께 Iceberg 테이블 형식을 사용하여 S3 데이터 레이크에 데이터를 저장함으로써 대규모 쓰기 트랜잭션을 안정적으로 수행할 수 있습니다. 덕분에 서로 다른 국가에 거주하는 데이터 분석가가 규정 요구 사항을 충족하기 위해 해당 국가에 소재한 고객만 데이터에 액세스할 수 있도록 지원할 수 있습니다. 테이블 및 파일 형식에 대해 새롭게 확장된 지원에서는 새로운 기능 및 향상된 쿼리 성능을 제공하는 Athena 엔진 버전 3을 필요로 하며, Lake Formation의 세분화된 액세스 제어 정책을 설정하는 방법을 변경할 필요가 없습니다. 또한, Athena를 사용하면 AWS Identity and Access Management(IAM) 정책, 액세스 제어 목록(ACL) 및 S3 버킷 정책을 사용하여 데이터에 대한 액세스를 제어할 수 있습니다. IAM 정책을 사용해 IAM 사용자에게 S3 버킷에 대한 세분화된 제어 권한을 부여할 수 있습니다. S3에서 데이터에 대한 액세스를 제어하면 사용자들이 Athena를 사용하여 데이터를 쿼리하지 못하도록 제한할 수 있습니다.\n",
            "\"category : Athena, question : 내 데이터에 대한 액세스를 어떻게 제어하나요?, answer : Amazon Athena는 AWS Lake Formation을 통해 세분화된 액세스 제어를 지원합니다. AWS Lake Formation을 통해 S3 데이터 레이크의 데이터 카탈로그 리소스에 대한 권한 및 액세스 제어를 중앙에서 관리할 수 있습니다. Apache Iceberg, Apache Hudi, Apache Hive 같은 테이블 형식을 사용하여 모든 지원되는 파일 형식에 저장된 데이터에 대해 Athena 쿼리에서 세분화된 액세스 제어 정책을 적용할 수 있습니다. 사용 사례에 가장 적합한 테이블 및 파일 형식을 유연하게 선택하고 Athena를 사용하는 경우 중앙 집중식 데이터 거버넌스의 이점을 활용하여 데이터 액세스를 보호할 수 있습니다. 예를 들어, Lake Formation의 행 수준 보안 필터와 함께 Iceberg 테이블 형식을 사용하여 S3 데이터 레이크에 데이터를 저장함으로써 대규모 쓰기 트랜잭션을 안정적으로 수행할 수 있습니다. 덕분에 서로 다른 국가에 거주하는 데이터 분석가가 규정 요구 사항을 충족하기 위해 해당 국가에 소재한 고객만 데이터에 액세스할 수 있도록 지원할 수 있습니다. 테이블 및 파일 형식에 대해 새롭게 확장된 지원에서는 새로운 기능 및 향상된 쿼리 성능을 제공하는 Athena 엔진 버전 3을 필요로 하며, Lake Formation의 세분화된 액세스 제어 정책을 설정하는 방법을 변경할 필요가 없습니다. 또한, Athena를 사용하면 AWS Identity and Access Management(IAM) 정책, 액세스 제어 목록(ACL) 및 S3 버킷 정책을 사용하여 데이터에 대한 액세스를 제어할 수 있습니다. IAM 정책을 사용해 IAM 사용자에게 S3 버킷에 대한 세분화된 제어 권한을 부여할 수 있습니다. S3에서 데이터에 대한 액세스를 제어하면 사용자들이 Athena를 사용하여 데이터를 쿼리하지 못하도록 제한할 수 있습니다.\"\n",
            "Athena로 S3에 있는 암호화된 데이터를 쿼리할 수 있나요?\n",
            "예. S3 관리형 암호화 키를 사용한 서버 측 암호화(SSE), AWS Key Management Service(KMS) 관리형 키를 사용한 SSE, AWS KMS로 관리되는 키를 사용한 클라이언트측 암호화(CSE)를 사용하여 암호화된 데이터를 쿼리할 수 있습니다. Athena는 AWS KMS와도 통합되며 결과 세트를 암호화하는 옵션을 제공합니다.\n",
            "\"category : Athena, question : Athena로 S3에 있는 암호화된 데이터를 쿼리할 수 있나요?, answer : 예. S3 관리형 암호화 키를 사용한 서버 측 암호화(SSE), AWS Key Management Service(KMS) 관리형 키를 사용한 SSE, AWS KMS로 관리되는 키를 사용한 클라이언트측 암호화(CSE)를 사용하여 암호화된 데이터를 쿼리할 수 있습니다. Athena는 AWS KMS와도 통합되며 결과 세트를 암호화하는 옵션을 제공합니다.\"\n",
            "Athena는 고가용성인가요?\n",
            "예. Athena는 고가용성이고 여러 시설에서 컴퓨팅 리소스를 사용하여 쿼리를 실행하며 특정 시설에 도달할 수 없는 경우에는 쿼리를 자동으로 라우팅합니다. Athena는 S3를 기본 데이터 스토어로 사용하여 데이터 가용성과 내구성을 높입니다. S3는 중요한 데이터를 저장할 수 있는 내구성 있는 인프라를 제공합니다. 데이터가 여러 시설과 각 시설의 여러 디바이스에 중복 저장됩니다.\n",
            "\"category : Athena, question : Athena는 고가용성인가요?, answer : 예. Athena는 고가용성이고 여러 시설에서 컴퓨팅 리소스를 사용하여 쿼리를 실행하며 특정 시설에 도달할 수 없는 경우에는 쿼리를 자동으로 라우팅합니다. Athena는 S3를 기본 데이터 스토어로 사용하여 데이터 가용성과 내구성을 높입니다. S3는 중요한 데이터를 저장할 수 있는 내구성 있는 인프라를 제공합니다. 데이터가 여러 시설과 각 시설의 여러 디바이스에 중복 저장됩니다.\"\n",
            "다른 사용자의 S3 버킷에 대한 크로스 계정 액세스를 제공해도 되나요?\n",
            "예. S3에 대한 크로스 계정 액세스를 제공할 수 있습니다.\n",
            "\"category : Athena, question : 다른 사용자의 S3 버킷에 대한 크로스 계정 액세스를 제공해도 되나요?, answer : 예. S3에 대한 크로스 계정 액세스를 제공할 수 있습니다.\"\n",
            "Athena의 요금은 어떻게 부과되나요?\n",
            "Athena를 사용할 때는 스캔한 데이터를 기반으로 하거나 쿼리에 필요한 컴퓨팅을 기반으로 쿼리당 비용을 지불하도록 선택할 수 있습니다. 쿼리당 요금은 쿼리에서 스캔된 데이터의 양을 기준으로 하며 쿼리당 테라바이트(TB)로 부과됩니다. 다양한 형식으로 S3에 데이터를 저장할 수 있습니다. 데이터를 압축 및 파티셔닝하거나 열 기반 저장 형식으로 변환하면 쿼리에서 스캔되는 데이터의 양이 줄어들어 비용이 절감됩니다. 데이터를 열 형식으로 변환하면 Athena에서 쿼리를 처리하는 데 필요한 열만 읽습니다. 프로비저닝된 용량을 사용하면 검색된 데이터가 아닌 쿼리 처리 용량에 대한 시간당 요금이 부과됩니다. 동일한 계정 내에서 쿼리당 청구와 컴퓨팅 기반 청구를 사용할 수 있습니다. 자세한 내용은 Amazon Athena 요금 페이지를 검토하세요.\n",
            "\"category : Athena, question : Athena의 요금은 어떻게 부과되나요?, answer : Athena를 사용할 때는 스캔한 데이터를 기반으로 하거나 쿼리에 필요한 컴퓨팅을 기반으로 쿼리당 비용을 지불하도록 선택할 수 있습니다. 쿼리당 요금은 쿼리에서 스캔된 데이터의 양을 기준으로 하며 쿼리당 테라바이트(TB)로 부과됩니다. 다양한 형식으로 S3에 데이터를 저장할 수 있습니다. 데이터를 압축 및 파티셔닝하거나 열 기반 저장 형식으로 변환하면 쿼리에서 스캔되는 데이터의 양이 줄어들어 비용이 절감됩니다. 데이터를 열 형식으로 변환하면 Athena에서 쿼리를 처리하는 데 필요한 열만 읽습니다. 프로비저닝된 용량을 사용하면 검색된 데이터가 아닌 쿼리 처리 용량에 대한 시간당 요금이 부과됩니다. 동일한 계정 내에서 쿼리당 청구와 컴퓨팅 기반 청구를 사용할 수 있습니다. 자세한 내용은 Amazon Athena 요금 페이지를 검토하세요.\"\n",
            "열 형식을 사용할 때 요금이 적게 부과되는 이유는 무엇입니까?\n",
            "쿼리당 청구를 사용하면 각 쿼리에서 스캔된 데이터 양을 기준으로 Athena 요금이 부과됩니다. 데이터를 압축하면 Athena가 스캔하는 데이터가 줄어듭니다. 데이터를 열 형식으로 변환하면 Athena가 필요한 열만 선택적으로 읽어 데이터를 처리할 수 있습니다. 또한 데이터를 파티셔닝하면 Athena는 스캔된 데이터의 양을 제한할 수도 있습니다. 그 결과, 비용은 절감되고 성능은 향상됩니다. 자세한 내용은 Amazon Athena 요금 페이지를 검토하세요.\n",
            "\"category : Athena, question : 열 형식을 사용할 때 요금이 적게 부과되는 이유는 무엇입니까?, answer : 쿼리당 청구를 사용하면 각 쿼리에서 스캔된 데이터 양을 기준으로 Athena 요금이 부과됩니다. 데이터를 압축하면 Athena가 스캔하는 데이터가 줄어듭니다. 데이터를 열 형식으로 변환하면 Athena가 필요한 열만 선택적으로 읽어 데이터를 처리할 수 있습니다. 또한 데이터를 파티셔닝하면 Athena는 스캔된 데이터의 양을 제한할 수도 있습니다. 그 결과, 비용은 절감되고 성능은 향상됩니다. 자세한 내용은 Amazon Athena 요금 페이지를 검토하세요.\"\n",
            "비용을 줄이려면 어떻게 해야 하나요?\n",
            "쿼리당 청구를 사용하면 데이터를 압축 및 파티셔닝하고 열 형식으로 변환하여 쿼리당 30%~90%를 절감하고 성능을 개선할 수 있습니다. 이러한 작업을 수행할 때마다 스캔되는 데이터의 양과 실행에 필요한 시간이 줄어듭니다. 프로비저닝된 용량을 사용할 때도 이러한 작업을 수행하는 것이 좋습니다. 쿼리 실행에 소요되는 시간이 줄어드는 경우가 많기 때문입니다.\n",
            "\"category : Athena, question : 비용을 줄이려면 어떻게 해야 하나요?, answer : 쿼리당 청구를 사용하면 데이터를 압축 및 파티셔닝하고 열 형식으로 변환하여 쿼리당 30%~90%를 절감하고 성능을 개선할 수 있습니다. 이러한 작업을 수행할 때마다 스캔되는 데이터의 양과 실행에 필요한 시간이 줄어듭니다. 프로비저닝된 용량을 사용할 때도 이러한 작업을 수행하는 것이 좋습니다. 쿼리 실행에 소요되는 시간이 줄어드는 경우가 많기 때문입니다.\"\n",
            "Athena는 실패한 쿼리에 대한 요금을 청구하나요?\n",
            "쿼리당 요금을 사용하면 실패한 쿼리에 대한 요금이 청구되지 않습니다.\n",
            "\"category : Athena, question : Athena는 실패한 쿼리에 대한 요금을 청구하나요?, answer : 쿼리당 요금을 사용하면 실패한 쿼리에 대한 요금이 청구되지 않습니다.\"\n",
            "Athena는 취소한 쿼리에 대한 요금을 청구하나요?\n",
            "예. 쿼리를 취소하면 이 쿼리를 취소한 시점까지 스캔된 데이터의 양에 대한 요금이 부과됩니다.\n",
            "\"category : Athena, question : Athena는 취소한 쿼리에 대한 요금을 청구하나요?, answer : 예. 쿼리를 취소하면 이 쿼리를 취소한 시점까지 스캔된 데이터의 양에 대한 요금이 부과됩니다.\"\n",
            "Athena와 관련된 추가 요금이 있나요?\n",
            "Athena는 S3에서 직접 데이터를 쿼리하기 때문에 사용자의 소스 데이터는 S3 요금으로 청구됩니다. Athena는 쿼리를 실행할 때 쿼리 결과를 고객이 선택한 S3 버킷에 저장합니다. 이러한 결과 세트에는 표준 S3 요금이 적용됩니다. 유지되는 데이터의 양을 제어하려면 이러한 버킷을 모니터링하고 수명 주기 정책을 사용하는 것이 좋습니다.\n",
            "\"category : Athena, question : Athena와 관련된 추가 요금이 있나요?, answer : Athena는 S3에서 직접 데이터를 쿼리하기 때문에 사용자의 소스 데이터는 S3 요금으로 청구됩니다. Athena는 쿼리를 실행할 때 쿼리 결과를 고객이 선택한 S3 버킷에 저장합니다. 이러한 결과 세트에는 표준 S3 요금이 적용됩니다. 유지되는 데이터의 양을 제어하려면 이러한 버킷을 모니터링하고 수명 주기 정책을 사용하는 것이 좋습니다.\"\n",
            "데이터 카탈로그를 사용하면 요금이 부과됩니까?\n",
            "예. 데이터 카탈로그 사용 요금이 별도로 부과됩니다. 데이터 카탈로그 요금에 대해 자세히 알아보려면 AWS Glue 요금 페이지를 검토하세요.\n",
            "\"category : Athena, question : 데이터 카탈로그를 사용하면 요금이 부과됩니까?, answer : 예. 데이터 카탈로그 사용 요금이 별도로 부과됩니다. 데이터 카탈로그 요금에 대해 자세히 알아보려면 AWS Glue 요금 페이지를 검토하세요.\"\n",
            "Amazon Athena for Apache Spark란 무엇인가요?\n",
            "Athena는 데이터 분석가 및 데이터 엔지니어가 완전관리형 Athena의 대화형 경험을 이용할 수 있도록 Apache Spark 프레임워크를 지원합니다. Apache Spark는 크기에 관계없이 모든 데이터를 대상으로 고속 분석 워크로드를 실행할 수 있도록 개선된 인기 오픈 소스 분산 처리 시스템으로, 풍부한 오픈 소스 라이브러리 시스템을 제공합니다. 이제 Athena 콘솔에서 또는 Athena API를 통해 간소화된 노트북 경험을 활용하여 Python과 같은 표현 언어로 Spark 애플리케이션을 구축할 수 있습니다. 다양한 소스의 데이터를 쿼리하고 여러 계산을 하나로 결합하며 분석 결과를 시각화할 수 있습니다. 대화형 Spark 애플리케이션의 경우 Athena를 사용하면 1초 이내에 애플리케이션을 실행할 수 있으므로 대기 시간을 줄이고 생산성을 개선할 수 있습니다. 고객은 버전 업그레이드, 성능 튜닝 및 다른 AWS 서비스와의 통합을 위해 필요한 작업을 취소화하는 간소화된 목적별 Spark 경험을 이용할 수 있습니다.\n",
            "\"category : Athena, question : Amazon Athena for Apache Spark란 무엇인가요?, answer : Athena는 데이터 분석가 및 데이터 엔지니어가 완전관리형 Athena의 대화형 경험을 이용할 수 있도록 Apache Spark 프레임워크를 지원합니다. Apache Spark는 크기에 관계없이 모든 데이터를 대상으로 고속 분석 워크로드를 실행할 수 있도록 개선된 인기 오픈 소스 분산 처리 시스템으로, 풍부한 오픈 소스 라이브러리 시스템을 제공합니다. 이제 Athena 콘솔에서 또는 Athena API를 통해 간소화된 노트북 경험을 활용하여 Python과 같은 표현 언어로 Spark 애플리케이션을 구축할 수 있습니다. 다양한 소스의 데이터를 쿼리하고 여러 계산을 하나로 결합하며 분석 결과를 시각화할 수 있습니다. 대화형 Spark 애플리케이션의 경우 Athena를 사용하면 1초 이내에 애플리케이션을 실행할 수 있으므로 대기 시간을 줄이고 생산성을 개선할 수 있습니다. 고객은 버전 업그레이드, 성능 튜닝 및 다른 AWS 서비스와의 통합을 위해 필요한 작업을 취소화하는 간소화된 목적별 Spark 경험을 이용할 수 있습니다.\"\n",
            "Athena for Apache Spark를 사용해야 하는 이유는 무엇입니까?\n",
            "대화형 및 완전 관리형 분석 경험이 필요하고 AWS 서비스와 긴밀하게 통합해야 하는 경우 Athena for Apache Spark를 사용합니다. Athena에서 Spark를 사용하여 Python과 같은 익숙한 표현 언어와 성장하는 Spark 패키지 환경을 활용하여 분석을 수행할 수 있습니다. 또한, 기반 인프라를 설정하고 튜닝하지 않고도 Athena API를 통해 Spark 애플리케이션을 시작하거나 Athena 콘솔에서 간소화된 노트북을 시작하고 1초 이내에 Spark 애플리케이션을 실행할 수 있습니다. Athena의 SQL 쿼리 기능과 마찬가지로, Athena는 완전관리형 Spark 경험을 제공하고 성능 튜닝, 머신 구성 및 소프트웨어 패치를 자동으로 처리하므로 버전 업그레이드를 통해 최신 상태를 유지해야 한다는 걱정에서 벗어날 수 있습니다. 또한, Athena는 AWS 시스템의 다른 분석 서비스(예: 데이터 카탈로그)와도 긴밀하게 통합됩니다. 따라서 데이터 카탈로그의 테이블을 참조하여 S3 데이터 레이크의 데이터를 기반으로 Spark 애플리케이션을 생성할 수 있습니다.\n",
            "\"category : Athena, question : Athena for Apache Spark를 사용해야 하는 이유는 무엇입니까?, answer : 대화형 및 완전 관리형 분석 경험이 필요하고 AWS 서비스와 긴밀하게 통합해야 하는 경우 Athena for Apache Spark를 사용합니다. Athena에서 Spark를 사용하여 Python과 같은 익숙한 표현 언어와 성장하는 Spark 패키지 환경을 활용하여 분석을 수행할 수 있습니다. 또한, 기반 인프라를 설정하고 튜닝하지 않고도 Athena API를 통해 Spark 애플리케이션을 시작하거나 Athena 콘솔에서 간소화된 노트북을 시작하고 1초 이내에 Spark 애플리케이션을 실행할 수 있습니다. Athena의 SQL 쿼리 기능과 마찬가지로, Athena는 완전관리형 Spark 경험을 제공하고 성능 튜닝, 머신 구성 및 소프트웨어 패치를 자동으로 처리하므로 버전 업그레이드를 통해 최신 상태를 유지해야 한다는 걱정에서 벗어날 수 있습니다. 또한, Athena는 AWS 시스템의 다른 분석 서비스(예: 데이터 카탈로그)와도 긴밀하게 통합됩니다. 따라서 데이터 카탈로그의 테이블을 참조하여 S3 데이터 레이크의 데이터를 기반으로 Spark 애플리케이션을 생성할 수 있습니다.\"\n",
            "Athena for Apache Spark로 작업을 시작하려면 어떻게 해야 하나요?\n",
            "Athena for Apache Spark를 시작하려면 Athena 콘솔에서 노트북을 시작하거나 AWS Command Line Interface(CLI) 또는 Athena API를 사용하여 세션을 시작하면 됩니다. 노트북에서 Python을 사용하여 Spark 애플리케이션을 시작하고 종료할 수 있습니다. Athena는 데이터 카탈로그와도 통합되므로 S3 데이터 레이크의 데이터를 직접 사용하는 것을 포함하여 카탈로그에 참조된 모든 데이터 소스를 작업에 사용할 수 있습니다. 이제 노트북을 사용하여 다양한 소스의 데이터를 쿼리하고 여러 계산을 하나로 결합하며 분석 결과를 시각화할 수 있습니다. Spark 애플리케이션에서 Athena 콘솔을 사용하여 실행 상태를 확인하고 로그 및 실행 기록을 검토할 수 있습니다.\n",
            "\"category : Athena, question : Athena for Apache Spark로 작업을 시작하려면 어떻게 해야 하나요?, answer : Athena for Apache Spark를 시작하려면 Athena 콘솔에서 노트북을 시작하거나 AWS Command Line Interface(CLI) 또는 Athena API를 사용하여 세션을 시작하면 됩니다. 노트북에서 Python을 사용하여 Spark 애플리케이션을 시작하고 종료할 수 있습니다. Athena는 데이터 카탈로그와도 통합되므로 S3 데이터 레이크의 데이터를 직접 사용하는 것을 포함하여 카탈로그에 참조된 모든 데이터 소스를 작업에 사용할 수 있습니다. 이제 노트북을 사용하여 다양한 소스의 데이터를 쿼리하고 여러 계산을 하나로 결합하며 분석 결과를 시각화할 수 있습니다. Spark 애플리케이션에서 Athena 콘솔을 사용하여 실행 상태를 확인하고 로그 및 실행 기록을 검토할 수 있습니다.\"\n",
            "Athena의 기반이 되는 Spark 버전은 무엇인가요?\n",
            "Athena for Apache Spark는 안정적인 Spark 3.2 릴리스에 기반을 둡니다. Athena는 완전관리형 엔진으로, Spark의 사용자 지정 빌드를 제공하며 대부분의 Spark 버전 업데이트를 이전 버전과 호환되는 방식으로 자동으로 처리하므로 사용자 개입이 필요하지 않습니다.\n",
            "\"category : Athena, question : Athena의 기반이 되는 Spark 버전은 무엇인가요?, answer : Athena for Apache Spark는 안정적인 Spark 3.2 릴리스에 기반을 둡니다. Athena는 완전관리형 엔진으로, Spark의 사용자 지정 빌드를 제공하며 대부분의 Spark 버전 업데이트를 이전 버전과 호환되는 방식으로 자동으로 처리하므로 사용자 개입이 필요하지 않습니다.\"\n",
            "Athena for Apache Spark 요금은 어떻게 부과되나요?\n",
            "Apache Spark 애플리케이션을 실행하는 데 걸린 시간에 대한 비용만 지불하면 됩니다. Apache Spark 애플리케이션을 실행하는 데 사용된 데이터 처리 단위(DPU)의 수에 따라 시간당 비용이 부과됩니다. 단일 DPU에서 vCPU 4개와 16GB 메모리를 제공합니다. 증분 단위는 1초이며, 시간은 반올림되어 비용이 청구됩니다.\n",
            "Athena 콘솔에서 노트북을 시작하거나 Athena API를 사용하여 Spark 세션을 시작하는 경우 애플리케이션에 대해 두 개의 노드가 프로비저닝됩니다. 하나는 노트북 노드로, 노트북 사용자 인터페이스에서 서버 역할을 합니다. 다른 노드는 Spark 드라이버 노드로, 해당 Spark 애플리케이션에 맞게 조율되며 모든 Spark 워커 노드와 통신합니다. Athena는 세션이 지속되는 동안 드라이버 및 워커 노드에 대한 비용을 청구합니다. Amazon Athena는 Apache Spark 애플리케이션을 생성, 제출, 실행하기 위한 사용자 인터페이스로 콘솔에서 노트북을 추가 비용 없이 제공합니다. Athena는 Spark 세션 중에 사용한 노트북 노드에 대한 비용은 청구하지 않습니다.\n",
            "\"category : Athena, question : Athena for Apache Spark 요금은 어떻게 부과되나요?, answer : Apache Spark 애플리케이션을 실행하는 데 걸린 시간에 대한 비용만 지불하면 됩니다. Apache Spark 애플리케이션을 실행하는 데 사용된 데이터 처리 단위(DPU)의 수에 따라 시간당 비용이 부과됩니다. 단일 DPU에서 vCPU 4개와 16GB 메모리를 제공합니다. 증분 단위는 1초이며, 시간은 반올림되어 비용이 청구됩니다.\n",
            "Athena 콘솔에서 노트북을 시작하거나 Athena API를 사용하여 Spark 세션을 시작하는 경우 애플리케이션에 대해 두 개의 노드가 프로비저닝됩니다. 하나는 노트북 노드로, 노트북 사용자 인터페이스에서 서버 역할을 합니다. 다른 노드는 Spark 드라이버 노드로, 해당 Spark 애플리케이션에 맞게 조율되며 모든 Spark 워커 노드와 통신합니다. Athena는 세션이 지속되는 동안 드라이버 및 워커 노드에 대한 비용을 청구합니다. Amazon Athena는 Apache Spark 애플리케이션을 생성, 제출, 실행하기 위한 사용자 인터페이스로 콘솔에서 노트북을 추가 비용 없이 제공합니다. Athena는 Spark 세션 중에 사용한 노트북 노드에 대한 비용은 청구하지 않습니다.\"\n",
            "Athena, Amazon EMR 및 Amazon Redshift의 차이점은 무엇인가요?\n",
            "Athena 같은 쿼리 서비스, Amazon Redshift 같은 데이터 웨어하우스 및 Amazon EMR 같은 정교한 데이터 처리 프레임워크는 모두 서로 다른 요구와 사용 사례를 처리합니다. 작업에 적합한 도구를 선택하기만 하면 됩니다. Amazon Redshift는 엔터프라이즈 보고 및 비즈니스 인텔리전스 워크로드, 그 중에서도 특히 여러 조인 및 하위 쿼리가 포함된 복잡한 SQL과 관련된 워크로드에 대해 가장 빠른 쿼리 성능을 제공합니다. Amazon EMR을 사용하면 프로세스가 간소화되며 Apache Hadoop, Spark, Presto와 같은 고도로 분산된 처리 프레임워크를 온프레미스 배포 환경보다 비용 효율적으로 실행할 수 있습니다. Amazon EMR은 유연합니다. 사용자 지정 애플리케이션 및 코드를 실행하고 특정 컴퓨팅, 메모리, 스토리지 및 애플리케이션 파라미터를 정의하여 분석 요구 사항을 개선할 수 있습니다. Athena는 서버를 설정하거나 관리할 필요 없이 S3의 데이터에 대한 대화형 쿼리를 실행하는 간소화된 방법을 제공합니다.\n",
            "\"category : Athena, question : Athena, Amazon EMR 및 Amazon Redshift의 차이점은 무엇인가요?, answer : Athena 같은 쿼리 서비스, Amazon Redshift 같은 데이터 웨어하우스 및 Amazon EMR 같은 정교한 데이터 처리 프레임워크는 모두 서로 다른 요구와 사용 사례를 처리합니다. 작업에 적합한 도구를 선택하기만 하면 됩니다. Amazon Redshift는 엔터프라이즈 보고 및 비즈니스 인텔리전스 워크로드, 그 중에서도 특히 여러 조인 및 하위 쿼리가 포함된 복잡한 SQL과 관련된 워크로드에 대해 가장 빠른 쿼리 성능을 제공합니다. Amazon EMR을 사용하면 프로세스가 간소화되며 Apache Hadoop, Spark, Presto와 같은 고도로 분산된 처리 프레임워크를 온프레미스 배포 환경보다 비용 효율적으로 실행할 수 있습니다. Amazon EMR은 유연합니다. 사용자 지정 애플리케이션 및 코드를 실행하고 특정 컴퓨팅, 메모리, 스토리지 및 애플리케이션 파라미터를 정의하여 분석 요구 사항을 개선할 수 있습니다. Athena는 서버를 설정하거나 관리할 필요 없이 S3의 데이터에 대한 대화형 쿼리를 실행하는 간소화된 방법을 제공합니다.\"\n",
            "Redshift와 비교했을 때 Athena의 SQL 지원은 어떤가요? 그리고 두 서비스 중에서 어떻게 선택해야 하나요?\n",
            "Amazon Athena와 Amazon Redshift Serverless는 모두 서버리스 서비스이고 SQL 사용자를 지원하지만 해결할 수 있는 요구 사항과 사용 사례가 서로 다릅니다.\n",
            "스토리지와 컴퓨팅 및 기계 학습 주도 자동 최적화 기능을 분리하는 MPP(Massively Parallel Processing) 아키텍처를 기반으로 Amazon Redshift(서버리스 또는 프로비저닝됨)와 같은 데이터 웨어하우스는 복잡한 BI 및 분석 워크로드를 처리하기 위해 모든 규모에서 최고의 가격 대비 성능을 필요로 하는 고객에게 탁월한 선택입니다. Redshift는 확장된 분석과 대규모의 구조화된 정형 및 반전형 데이터 세트를 처리하는 데 가장 적합합니다. 엔터프라이즈 보고 및 비즈니스 인텔리전스 워크로드, 특히 여러 조인 및 하위 쿼리가 포함된 매우 복잡한 SQL과 관련된 경우에 가장 효과적입니다. Redshift는 AWS 데이터베이스, 분석, ML 서비스와의 심층적인 통합을 지원하므로, 고객은 최소의 ETL과 코드가 필요 없는 방법을 통해 현재 위치에서 데이터를 액세스하거나 고성능 분석을 처리하도록 웨어하우스로 손쉽게 이전할 수 있습니다. 페더레이션된 쿼리 기능, Amazon Redshift Spectrum, Amazon Aurora와의 통합, AWS Data Exchange, 스트리밍 데이터 서비스 등을 활용하여 Redshift는 여러 소스에서 데이터를 사용하고 웨어하우스에서 데이터를 결합하며 무엇보다도 분석과 기계 학습을 수행할 수 있도록 지원합니다. Redshift는 인프라를 관리하지 않고도 손쉽게 분석을 시작하는 프로비저닝된 옵션과 서버리스 옵션을 모두 제공합니다.\n",
            "Athena는 사용하기 쉬운 SQL 구문을 사용하여 확장 가능한 커넥터 프레임워크(애플리케이션 및 온프레미스 또는 기타 클라우드 분석 시스템을 위한 30개가 넘는 기본 제공 커넥터 포함)를 통해 Amazon Simple Storage Service(S3) 또는 모든 데이터 소스에 있는 데이터의 대화형 분석 및 데이터 탐색에 매우 적합합니다. Amazon Athena는 Spark, Presto 및 Apache Iceberg와 같은 오픈 소스 엔진 및 프레임워크에 구축되어 고객에게 Python 또는 SQL을 사용하거나 개방형 데이터 형식으로 작업할 수 있는 유연성을 제공합니다. 고객이 오픈 소스 프레임워크와 데이터 형식을 사용하여 대화형 분석을 수행하려는 경우 Amazon Athena에서 시작하는 것이 좋습니다. 완전히 서버리스 제품이므로, 인프라를 관리하거나 설정하지 않아도 됩니다. Athena의 오픈 지원을 통해 뛰어난 데이터 이식성을 지원하므로 고객은 여러 애플리케이션, 프로그램, 심지어 클라우드 서비스 공급자 사이에서 데이터를 이전할 수 있습니다. 최근 오픈 소스 소프트웨어 관리에 대한 새로운 지속적인 통합 접근 방식을 채택하여, Trino, PrestoDB, Apache Iceberg 프로젝트의 최신 기능을 지속적으로 통합할 수 있습니다.\n",
            "\"category : Athena, question : Redshift와 비교했을 때 Athena의 SQL 지원은 어떤가요? 그리고 두 서비스 중에서 어떻게 선택해야 하나요?, answer : Amazon Athena와 Amazon Redshift Serverless는 모두 서버리스 서비스이고 SQL 사용자를 지원하지만 해결할 수 있는 요구 사항과 사용 사례가 서로 다릅니다.\n",
            "스토리지와 컴퓨팅 및 기계 학습 주도 자동 최적화 기능을 분리하는 MPP(Massively Parallel Processing) 아키텍처를 기반으로 Amazon Redshift(서버리스 또는 프로비저닝됨)와 같은 데이터 웨어하우스는 복잡한 BI 및 분석 워크로드를 처리하기 위해 모든 규모에서 최고의 가격 대비 성능을 필요로 하는 고객에게 탁월한 선택입니다. Redshift는 확장된 분석과 대규모의 구조화된 정형 및 반전형 데이터 세트를 처리하는 데 가장 적합합니다. 엔터프라이즈 보고 및 비즈니스 인텔리전스 워크로드, 특히 여러 조인 및 하위 쿼리가 포함된 매우 복잡한 SQL과 관련된 경우에 가장 효과적입니다. Redshift는 AWS 데이터베이스, 분석, ML 서비스와의 심층적인 통합을 지원하므로, 고객은 최소의 ETL과 코드가 필요 없는 방법을 통해 현재 위치에서 데이터를 액세스하거나 고성능 분석을 처리하도록 웨어하우스로 손쉽게 이전할 수 있습니다. 페더레이션된 쿼리 기능, Amazon Redshift Spectrum, Amazon Aurora와의 통합, AWS Data Exchange, 스트리밍 데이터 서비스 등을 활용하여 Redshift는 여러 소스에서 데이터를 사용하고 웨어하우스에서 데이터를 결합하며 무엇보다도 분석과 기계 학습을 수행할 수 있도록 지원합니다. Redshift는 인프라를 관리하지 않고도 손쉽게 분석을 시작하는 프로비저닝된 옵션과 서버리스 옵션을 모두 제공합니다.\n",
            "Athena는 사용하기 쉬운 SQL 구문을 사용하여 확장 가능한 커넥터 프레임워크(애플리케이션 및 온프레미스 또는 기타 클라우드 분석 시스템을 위한 30개가 넘는 기본 제공 커넥터 포함)를 통해 Amazon Simple Storage Service(S3) 또는 모든 데이터 소스에 있는 데이터의 대화형 분석 및 데이터 탐색에 매우 적합합니다. Amazon Athena는 Spark, Presto 및 Apache Iceberg와 같은 오픈 소스 엔진 및 프레임워크에 구축되어 고객에게 Python 또는 SQL을 사용하거나 개방형 데이터 형식으로 작업할 수 있는 유연성을 제공합니다. 고객이 오픈 소스 프레임워크와 데이터 형식을 사용하여 대화형 분석을 수행하려는 경우 Amazon Athena에서 시작하는 것이 좋습니다. 완전히 서버리스 제품이므로, 인프라를 관리하거나 설정하지 않아도 됩니다. Athena의 오픈 지원을 통해 뛰어난 데이터 이식성을 지원하므로 고객은 여러 애플리케이션, 프로그램, 심지어 클라우드 서비스 공급자 사이에서 데이터를 이전할 수 있습니다. 최근 오픈 소스 소프트웨어 관리에 대한 새로운 지속적인 통합 접근 방식을 채택하여, Trino, PrestoDB, Apache Iceberg 프로젝트의 최신 기능을 지속적으로 통합할 수 있습니다.\"\n",
            "Amazon EMR과 Athena는 각각 언제 사용해야 합니까?\n",
            "Amazon EMR은 SQL 쿼리를 실행하는 것 이외에도 상당히 많은 작업을 수행합니다. Amazon EMR을 사용하면 기계 학습(ML), 그래프 분석, 데이터 변환, 스트리밍 데이터 및 코딩할 수 있는 거의 모든 작업 등 애플리케이션의 다양한 확장 데이터 처리 작업을 실행할 수 있습니다. 사용자 지정 코드를 사용하여 매우 방대한 양의 데이터 세트를 Apache HBase, Spark, Hadoop 또는 Presto 같은 최신 빅 데이터 처리 프레임워크로 처리 및 분석하는 경우 Amazon EMR을 사용합니다. Amazon EMR을 사용하면 클러스터 구성 및 클러스터에 설치된 소프트웨어를 완벽하게 제어할 수 있습니다.\n",
            "인프라 또는 클러스터를 관리할 필요 없이 S3의 데이터에 대해 대화형 SQL 쿼리를 실행하려면 Athena를 사용해야 합니다.\n",
            "\"category : Athena, question : Amazon EMR과 Athena는 각각 언제 사용해야 합니까?, answer : Amazon EMR은 SQL 쿼리를 실행하는 것 이외에도 상당히 많은 작업을 수행합니다. Amazon EMR을 사용하면 기계 학습(ML), 그래프 분석, 데이터 변환, 스트리밍 데이터 및 코딩할 수 있는 거의 모든 작업 등 애플리케이션의 다양한 확장 데이터 처리 작업을 실행할 수 있습니다. 사용자 지정 코드를 사용하여 매우 방대한 양의 데이터 세트를 Apache HBase, Spark, Hadoop 또는 Presto 같은 최신 빅 데이터 처리 프레임워크로 처리 및 분석하는 경우 Amazon EMR을 사용합니다. Amazon EMR을 사용하면 클러스터 구성 및 클러스터에 설치된 소프트웨어를 완벽하게 제어할 수 있습니다.\n",
            "인프라 또는 클러스터를 관리할 필요 없이 S3의 데이터에 대해 대화형 SQL 쿼리를 실행하려면 Athena를 사용해야 합니다.\"\n",
            "Spark용 EMR Serverless와 비교했을 때 Athena의 Spark 지원은 어떤가요? 고객이 EMR Serverless 대신, Athena에서 Spark를 사용해야 하는 경우는 언제인가요?\n",
            "EMR Serverless는 클라우드에서 Spark 및 Hive 애플리케이션을 실행하는 가장 쉬운 방법이며, 업계에서 유일한 서버리스 Hive 솔루션이기도 합니다. EMR Serverless를 사용하면 튜닝, 올바른 크기 조정, 보안, 패치, 클러스터 관리와 같은 운영 오버헤드에서 벗어날 수 있으며, 애플리케이션이 실제로 사용하는 리소스에 대한 비용만 지불하면 됩니다. EMR의 성능에 최적화된 런타임을 통해 표준 오픈 소스보다 2배 이상 더 빠른 성능을 지원할 수 있으므로 애플리케이션을 더 빨리 실행하고 컴퓨팅 비용을 줄일 수 있습니다. EMR의 성능에 최적화된 런타임은 표준 오픈 소스와의 API 호환성을 완벽하게 지원하므로 EMR에서 실행하기 위해 애플리케이션을 다시 작성하지 않아도 됩니다. 또한, 기본적으로 켜져 있기 때문에 이 제품을 켜기 위해 전문적인 Spark 지식은 필요하지 않습니다. EMR은 EMR 클러스터, EKS 클러스터 또는 EMR Serverless에서 애플리케이션을 실행하는 옵션을 제공합니다. EMR 클러스터는 애플리케이션 실행 방법에 대한 최대한의 제어와 유연성을 필요로 하는 고객에게 적합합니다. 고객은 EMR 클러스터를 사용하여 EC2 인스턴스 유형을 선택하고 Amazon Linux AMI를 사용자 지정하며 EC2 인스턴스 구성을 사용자 지정하고 오픈 소스 프레임워크를 사용자 지정 및 확장하며 추가 사용자 지정 소프트웨어를 클러스터 인스턴스에 설치할 수 있습니다. EKS 기반 EMR은 EKS를 기반으로 표준화하여 애플리케이션 전체의 클러스터를 관리하거나 동일한 클러스터에서 다른 버전의 오픈 소스 프레임워크를 사용하려는 고객에게 적합합니다. EMR Serverless는 클러스터를 관리하거나 운영하지 않으면서 오픈 소스 프레임워크를 사용하여 간편하게 애플리케이션을 실행하려는 고객에게 적합합니다.\n",
            "고객이 Amazon Athena를 사용하는 대화형 SQL 기반 쿼리 경험과 비슷한 즉각적인 대화형 경험을 원하는 경우 Amazon Athena for Apache Spark를 선택할 수 있습니다. Athena에서 고객의 경험은 짧은 런타임과 1초 이내의 시작 시간을 필요로 하는 대화형 애플리케이션에 최적화되어 있습니다. Amazon Athena는 고객 개입 없이 성능 튜닝, 구성, 소프트웨어 패치 및 업데이트를 자동으로 처리합니다. 즉각적인 대화형 분석 실행과 데이터 탐색에 관심이 있으며 PySpark 프로그래밍 언어를 심층적으로 사용하는 데이터 분석가와 개발자를 위해 Amazon Athena for Apache Spark는 사용하기 쉬운 경험을 제공합니다.\n",
            "\"category : Athena, question : Spark용 EMR Serverless와 비교했을 때 Athena의 Spark 지원은 어떤가요? 고객이 EMR Serverless 대신, Athena에서 Spark를 사용해야 하는 경우는 언제인가요?, answer : EMR Serverless는 클라우드에서 Spark 및 Hive 애플리케이션을 실행하는 가장 쉬운 방법이며, 업계에서 유일한 서버리스 Hive 솔루션이기도 합니다. EMR Serverless를 사용하면 튜닝, 올바른 크기 조정, 보안, 패치, 클러스터 관리와 같은 운영 오버헤드에서 벗어날 수 있으며, 애플리케이션이 실제로 사용하는 리소스에 대한 비용만 지불하면 됩니다. EMR의 성능에 최적화된 런타임을 통해 표준 오픈 소스보다 2배 이상 더 빠른 성능을 지원할 수 있으므로 애플리케이션을 더 빨리 실행하고 컴퓨팅 비용을 줄일 수 있습니다. EMR의 성능에 최적화된 런타임은 표준 오픈 소스와의 API 호환성을 완벽하게 지원하므로 EMR에서 실행하기 위해 애플리케이션을 다시 작성하지 않아도 됩니다. 또한, 기본적으로 켜져 있기 때문에 이 제품을 켜기 위해 전문적인 Spark 지식은 필요하지 않습니다. EMR은 EMR 클러스터, EKS 클러스터 또는 EMR Serverless에서 애플리케이션을 실행하는 옵션을 제공합니다. EMR 클러스터는 애플리케이션 실행 방법에 대한 최대한의 제어와 유연성을 필요로 하는 고객에게 적합합니다. 고객은 EMR 클러스터를 사용하여 EC2 인스턴스 유형을 선택하고 Amazon Linux AMI를 사용자 지정하며 EC2 인스턴스 구성을 사용자 지정하고 오픈 소스 프레임워크를 사용자 지정 및 확장하며 추가 사용자 지정 소프트웨어를 클러스터 인스턴스에 설치할 수 있습니다. EKS 기반 EMR은 EKS를 기반으로 표준화하여 애플리케이션 전체의 클러스터를 관리하거나 동일한 클러스터에서 다른 버전의 오픈 소스 프레임워크를 사용하려는 고객에게 적합합니다. EMR Serverless는 클러스터를 관리하거나 운영하지 않으면서 오픈 소스 프레임워크를 사용하여 간편하게 애플리케이션을 실행하려는 고객에게 적합합니다.\n",
            "고객이 Amazon Athena를 사용하는 대화형 SQL 기반 쿼리 경험과 비슷한 즉각적인 대화형 경험을 원하는 경우 Amazon Athena for Apache Spark를 선택할 수 있습니다. Athena에서 고객의 경험은 짧은 런타임과 1초 이내의 시작 시간을 필요로 하는 대화형 애플리케이션에 최적화되어 있습니다. Amazon Athena는 고객 개입 없이 성능 튜닝, 구성, 소프트웨어 패치 및 업데이트를 자동으로 처리합니다. 즉각적인 대화형 분석 실행과 데이터 탐색에 관심이 있으며 PySpark 프로그래밍 언어를 심층적으로 사용하는 데이터 분석가와 개발자를 위해 Amazon Athena for Apache Spark는 사용하기 쉬운 경험을 제공합니다.\"\n",
            "Amazon EMR을 사용하여 처리되는 데이터는 Athena를 사용하여 쿼리할 수 있나요?\n",
            "예. Athena는 Amazon EMR과 동일한 데이터 형식의 상당 부분을 지원합니다. Athena 데이터 카탈로그는 Hive Metastore와 호환됩니다. 현재 Amazon EMR을 사용하고 있고 Hive Metastore가 이미 있는 경우 Athena에서 DDL 문을 실행한 다음 Amazon EMR 작업에 영향을 주지 않고 데이터 쿼리를 즉시 시작할 수 있습니다.\n",
            "\"category : Athena, question : Amazon EMR을 사용하여 처리되는 데이터는 Athena를 사용하여 쿼리할 수 있나요?, answer : 예. Athena는 Amazon EMR과 동일한 데이터 형식의 상당 부분을 지원합니다. Athena 데이터 카탈로그는 Hive Metastore와 호환됩니다. 현재 Amazon EMR을 사용하고 있고 Hive Metastore가 이미 있는 경우 Athena에서 DDL 문을 실행한 다음 Amazon EMR 작업에 영향을 주지 않고 데이터 쿼리를 즉시 시작할 수 있습니다.\"\n",
            "Athena SQL의 페더레이션 쿼리는 다른 AWS 서비스와 어떤 관련이 있나요?\n",
            "Athena의 페더레이션 쿼리를 사용하면 다양한 관계형, 비관계형 및 사용자 지정 데이터 소스에 대해 통합된 방식으로 SQL 쿼리를 실행할 수 있습니다.\n",
            "\"category : Athena, question : Athena SQL의 페더레이션 쿼리는 다른 AWS 서비스와 어떤 관련이 있나요?, answer : Athena의 페더레이션 쿼리를 사용하면 다양한 관계형, 비관계형 및 사용자 지정 데이터 소스에 대해 통합된 방식으로 SQL 쿼리를 실행할 수 있습니다.\"\n",
            "Athena의 기계 학습은 다른 AWS 서비스와 어떤 관련이 있나요?\n",
            "Athena SQL 쿼리는 Amazon SageMaker에 배포된 기계 학습 모델을 호출할 수 있습니다. 사용자는 이 Athena SQL 쿼리의 결과를 저장할 S3 위치를 지정할 수 있습니다.\n",
            "\"category : Athena, question : Athena의 기계 학습은 다른 AWS 서비스와 어떤 관련이 있나요?, answer : Athena SQL 쿼리는 Amazon SageMaker에 배포된 기계 학습 모델을 호출할 수 있습니다. 사용자는 이 Athena SQL 쿼리의 결과를 저장할 S3 위치를 지정할 수 있습니다.\"\n",
            "========== QuickSight  :  https://aws.amazon.com/ko/quicksight/resources/faqs/ 사이트 크롤링 진행중 ==========\n",
            "48\n",
            "Amazon QuickSight란 무엇인가요?\n",
            "QuickSight는 빠르고 사용이 간편한 클라우드 기반 비즈니스 분석 서비스로서, 조직 내 모든 직원이 언제든 어느 디바이스에서나 자신의 데이터를 사용해 손쉽게 시각화를 구축하고, 임시 분석을 수행하며, 신속하게 비즈니스 통찰력을 확보할 수 있습니다. CSV 파일과 Excel 파일을 업로드하고, Salesforce와 같은 SaaS 애플리케이션에 연결하고, SQL Server, MySQL 및 PostgreSQL과 같은 온프레미스 데이터베이스에 액세스하고, Amazon Redshift, Amazon Relational Database Service(Amazon RDS), Amazon Aurora, Amazon Athena 및 Amazon Simple Storage Service(Amazon S3)와 같은 AWS 데이터 소스를 원활하게 검색할 수 있습니다. 조직에서는 QuickSight를 사용하여 비즈니스 분석 기능을 수십만 명의 사용자로 확장하고, 강력한 인 메모리 엔진(SPICE)을 사용하여 빠르고 응답성이 뛰어난 쿼리 성능을 제공할 수 있습니다.\n",
            "\"category : QuickSight, question : Amazon QuickSight란 무엇인가요?, answer : QuickSight는 빠르고 사용이 간편한 클라우드 기반 비즈니스 분석 서비스로서, 조직 내 모든 직원이 언제든 어느 디바이스에서나 자신의 데이터를 사용해 손쉽게 시각화를 구축하고, 임시 분석을 수행하며, 신속하게 비즈니스 통찰력을 확보할 수 있습니다. CSV 파일과 Excel 파일을 업로드하고, Salesforce와 같은 SaaS 애플리케이션에 연결하고, SQL Server, MySQL 및 PostgreSQL과 같은 온프레미스 데이터베이스에 액세스하고, Amazon Redshift, Amazon Relational Database Service(Amazon RDS), Amazon Aurora, Amazon Athena 및 Amazon Simple Storage Service(Amazon S3)와 같은 AWS 데이터 소스를 원활하게 검색할 수 있습니다. 조직에서는 QuickSight를 사용하여 비즈니스 분석 기능을 수십만 명의 사용자로 확장하고, 강력한 인 메모리 엔진(SPICE)을 사용하여 빠르고 응답성이 뛰어난 쿼리 성능을 제공할 수 있습니다.\"\n",
            "QuickSight가 기존 비즈니스 인텔리전스(BI) 솔루션과 다른 점은 무엇입니까?\n",
            "기존 BI 솔루션은 보고서를 생성하기 전에 데이터 엔지니어 팀이 몇 달에 걸쳐 복잡한 데이터 모델을 구축해야 하는 경우가 많습니다. 일반적으로 대화식 임시 데이터 탐색 및 시각화 기능이 부족하여 사용자가 표준 보고서와 사전 선택된 쿼리만 사용할 수 있습니다. 또한, 기존 BI 솔루션은 고가의 복잡한 하드웨어와 소프트웨어에 상당한 사전 투자를 해야 하며 데이터베이스 크기가 커짐에 따라 빠른 쿼리 성능을 유지하려면 인프라에 더 많은 투자를 해야 합니다. 이러한 비용과 복잡성으로 인해 기업이 조직 전체에서 분석 솔루션을 사용하도록 지원하기가 어렵습니다. QuickSight는 AWS 클라우드의 규모와 유연성을 비즈니스 분석에 적용하여 이러한 문제를 해결하도록 설계되었습니다. 기존 BI 또는 데이터 검색 솔루션과는 달리 QuickSight는 쉽고 빠르게 시작할 수 있습니다. 로그인하면, QuickSight가 Amazon Redshift, Amazon RDS, Amazon Athena 및 Amazon S3와 같은 AWS 서비스의 데이터 소스를 원활하게 검색합니다. QuickSight에서 검색한 데이터 세트 어디에나 연결하고 몇 분 만에 해당 데이터에서 통찰력을 확보할 수 있습니다. 해당 소스의 데이터가 변경됨에 따라 QuickSight가 SPICE의 데이터를 최신으로 유지하도록 선택할 수 있습니다. SPICE는 풍부한 데이터 검색 및 비즈니스 분석 기능을 지원하므로, 고객은 인프라 프로비저닝이나 관리에 대한 걱정 없이 데이터에서 중요한 통찰력을 도출할 수 있습니다. 조직은 QuickSight 사용자당 저렴한 월별 비용을 지불하여 장기 라이선스 비용을 절약할 수 있습니다. 조직에서 QuickSight를 사용하면 큰 초기 비용 없이 모든 직원에게 풍부한 비즈니스 분석 기능을 제공할 수 있습니다.\n",
            "\"category : QuickSight, question : QuickSight가 기존 비즈니스 인텔리전스(BI) 솔루션과 다른 점은 무엇입니까?, answer : 기존 BI 솔루션은 보고서를 생성하기 전에 데이터 엔지니어 팀이 몇 달에 걸쳐 복잡한 데이터 모델을 구축해야 하는 경우가 많습니다. 일반적으로 대화식 임시 데이터 탐색 및 시각화 기능이 부족하여 사용자가 표준 보고서와 사전 선택된 쿼리만 사용할 수 있습니다. 또한, 기존 BI 솔루션은 고가의 복잡한 하드웨어와 소프트웨어에 상당한 사전 투자를 해야 하며 데이터베이스 크기가 커짐에 따라 빠른 쿼리 성능을 유지하려면 인프라에 더 많은 투자를 해야 합니다. 이러한 비용과 복잡성으로 인해 기업이 조직 전체에서 분석 솔루션을 사용하도록 지원하기가 어렵습니다. QuickSight는 AWS 클라우드의 규모와 유연성을 비즈니스 분석에 적용하여 이러한 문제를 해결하도록 설계되었습니다. 기존 BI 또는 데이터 검색 솔루션과는 달리 QuickSight는 쉽고 빠르게 시작할 수 있습니다. 로그인하면, QuickSight가 Amazon Redshift, Amazon RDS, Amazon Athena 및 Amazon S3와 같은 AWS 서비스의 데이터 소스를 원활하게 검색합니다. QuickSight에서 검색한 데이터 세트 어디에나 연결하고 몇 분 만에 해당 데이터에서 통찰력을 확보할 수 있습니다. 해당 소스의 데이터가 변경됨에 따라 QuickSight가 SPICE의 데이터를 최신으로 유지하도록 선택할 수 있습니다. SPICE는 풍부한 데이터 검색 및 비즈니스 분석 기능을 지원하므로, 고객은 인프라 프로비저닝이나 관리에 대한 걱정 없이 데이터에서 중요한 통찰력을 도출할 수 있습니다. 조직은 QuickSight 사용자당 저렴한 월별 비용을 지불하여 장기 라이선스 비용을 절약할 수 있습니다. 조직에서 QuickSight를 사용하면 큰 초기 비용 없이 모든 직원에게 풍부한 비즈니스 분석 기능을 제공할 수 있습니다.\"\n",
            "SPICE란 무엇인가요?\n",
            "QuickSight는 고속의 병렬 인 메모리 연산 엔진인 SPICE로 구축되어 있습니다. 처음부터 클라우드를 위해 구축된 SPICE는 열 형식 스토리지, 최신 하드웨어 혁신으로 가능해진 인 메모리 기술, 머신 코드 생성을 조합하여 대규모 데이터 세트에서 대화형 쿼리를 실행하고 빠른 응답을 확보할 수 있습니다. SPICE는 다양한 컴퓨팅을 지원하여 인프라 프로비저닝이나 관리에 대한 걱정 없이 분석 결과에서 중요한 통찰력을 도출하도록 지원합니다. SPICE의 데이터는 사용자가 명시적으로 삭제할 때까지 유지됩니다. SPICE는 가용성을 위해 데이터를 자동으로 복제하고, QuickSight가 다양한 AWS 데이터 소스 전체에서 빠른 대화형 분석을 동시에 수행할 수 있는 사용자를 수십만 명까지 확장할 수 있도록 지원합니다.\n",
            "\"category : QuickSight, question : SPICE란 무엇인가요?, answer : QuickSight는 고속의 병렬 인 메모리 연산 엔진인 SPICE로 구축되어 있습니다. 처음부터 클라우드를 위해 구축된 SPICE는 열 형식 스토리지, 최신 하드웨어 혁신으로 가능해진 인 메모리 기술, 머신 코드 생성을 조합하여 대규모 데이터 세트에서 대화형 쿼리를 실행하고 빠른 응답을 확보할 수 있습니다. SPICE는 다양한 컴퓨팅을 지원하여 인프라 프로비저닝이나 관리에 대한 걱정 없이 분석 결과에서 중요한 통찰력을 도출하도록 지원합니다. SPICE의 데이터는 사용자가 명시적으로 삭제할 때까지 유지됩니다. SPICE는 가용성을 위해 데이터를 자동으로 복제하고, QuickSight가 다양한 AWS 데이터 소스 전체에서 빠른 대화형 분석을 동시에 수행할 수 있는 사용자를 수십만 명까지 확장할 수 있도록 지원합니다.\"\n",
            "QuickSight를 시작하려면 어떻게 해야 하나요?\n",
            "시작하려면 30일 동안 4명의 무료 작성자를 지원하는 혜택을 받기 위해 QuickSight에 가입하세요.\n",
            "\"category : QuickSight, question : QuickSight를 시작하려면 어떻게 해야 하나요?, answer : 시작하려면 30일 동안 4명의 무료 작성자를 지원하는 혜택을 받기 위해 QuickSight에 가입하세요.\"\n",
            "QuickSight에서 Amazon SageMaker 추론을 사용하는 데 사전 조건이 있나요?\n",
            "권한이 있는 QuickSight 계정 관리자가 대신 SageMaker API를 호출할 수 있도록 QuickSight IAM 권한을 부여해야 합니다. 자세히 알아보려면 QuickSight IAM 권한 부여 설명서를 참조하세요.\n",
            "\"category : QuickSight, question : QuickSight에서 Amazon SageMaker 추론을 사용하는 데 사전 조건이 있나요?, answer : 권한이 있는 QuickSight 계정 관리자가 대신 SageMaker API를 호출할 수 있도록 QuickSight IAM 권한을 부여해야 합니다. 자세히 알아보려면 QuickSight IAM 권한 부여 설명서를 참조하세요.\"\n",
            "QuickSight 작성자란 누구인가요?\n",
            "QuickSight 작성자는 데이터 소스(AWS 내부 또는 외부)에 연결하고 시각화를 생성하고 데이터를 분석할 수 있는 사용자를 말합니다. 작성자는 파라미터, 계산된 필드와 같은 고급 QuickSight 기능을 사용하여 대화형 대시보드를 생성하고 계정의 다른 사용자와 함께 대시보드를 게시할 수 있습니다.\n",
            "\"category : QuickSight, question : QuickSight 작성자란 누구인가요?, answer : QuickSight 작성자는 데이터 소스(AWS 내부 또는 외부)에 연결하고 시각화를 생성하고 데이터를 분석할 수 있는 사용자를 말합니다. 작성자는 파라미터, 계산된 필드와 같은 고급 QuickSight 기능을 사용하여 대화형 대시보드를 생성하고 계정의 다른 사용자와 함께 대시보드를 게시할 수 있습니다.\"\n",
            "QuickSight ‘독자’란 누구인가요?\n",
            "QuickSight 독자는 대화형 대시보드를 소비하는 사용자를 말합니다. 독자는 조직의 기본 인증 메커니즘(QuickSight 사용자 이름/암호, SAML 포털 또는 AD 인증)을 통해 로그인하고, 웹 브라우저 또는 모바일 앱을 사용하여 공유 대시보드를 보고, 데이터를 필터링하고, 세부 정보로 드릴다운하거나, CSV 파일로 데이터를 내보낼 수 있습니다. 독자에게는 SPICE 용량이 할당되지 않습니다.\n",
            "개별 최종 사용자는 독자로서 QuickSight에 액세스하도록 프로비저닝될 수 있습니다. 독자 요금은 수동 세션 상호 작용에만 적용됩니다. AWS에서는 재량에 따라 고객이 독자 세션을 다른 용도(예: 프로그래밍 방식 또는 자동화된 쿼리)로 사용하고 있다고 판단하는 경우 독자에게 더 높은 월별 작성자 요금을 부과할 권리가 있습니다.\n",
            "\"category : QuickSight, question : QuickSight ‘독자’란 누구인가요?, answer : QuickSight 독자는 대화형 대시보드를 소비하는 사용자를 말합니다. 독자는 조직의 기본 인증 메커니즘(QuickSight 사용자 이름/암호, SAML 포털 또는 AD 인증)을 통해 로그인하고, 웹 브라우저 또는 모바일 앱을 사용하여 공유 대시보드를 보고, 데이터를 필터링하고, 세부 정보로 드릴다운하거나, CSV 파일로 데이터를 내보낼 수 있습니다. 독자에게는 SPICE 용량이 할당되지 않습니다.\n",
            "개별 최종 사용자는 독자로서 QuickSight에 액세스하도록 프로비저닝될 수 있습니다. 독자 요금은 수동 세션 상호 작용에만 적용됩니다. AWS에서는 재량에 따라 고객이 독자 세션을 다른 용도(예: 프로그래밍 방식 또는 자동화된 쿼리)로 사용하고 있다고 판단하는 경우 독자에게 더 높은 월별 작성자 요금을 부과할 권리가 있습니다.\"\n",
            "독자를 작성자로 업그레이드할 수 있습니까?\n",
            "예. QuickSight 사용자 관리 옵션을 통해 독자는 작성자로 손쉽게 업그레이드할 수 있습니다.\n",
            "\"category : QuickSight, question : 독자를 작성자로 업그레이드할 수 있습니까?, answer : 예. QuickSight 사용자 관리 옵션을 통해 독자는 작성자로 손쉽게 업그레이드할 수 있습니다.\"\n",
            "스탠더드 에디션 계정을 보유하고 있습니다. 독자를 추가할 수 있습니까?\n",
            "아니요. 세션당 요금제의 독자는 엔터프라이즈 에디션에서만 제공됩니다. 스탠더드 에디션 계정을 보유한 경우, QuickSight 관리 페이지를 사용하여 엔터프라이즈 에디션으로 업그레이드할 수 있습니다.\n",
            "\"category : QuickSight, question : 스탠더드 에디션 계정을 보유하고 있습니다. 독자를 추가할 수 있습니까?, answer : 아니요. 세션당 요금제의 독자는 엔터프라이즈 에디션에서만 제공됩니다. 스탠더드 에디션 계정을 보유한 경우, QuickSight 관리 페이지를 사용하여 엔터프라이즈 에디션으로 업그레이드할 수 있습니다.\"\n",
            "QuickSight 독자 계정을 사용하여 QuickSight에 프로그래밍 방식으로 액세스할 수 있습니까?\n",
            "QuickSight Reader 사용자 요금은 수동 액세스와 프로그래밍 방식 액세스 모두에 대해 개별 사용자의 대화형 데이터 소비에만 적용됩니다. 임베딩 시나리오와 같이 QuickSight에 등록되지 않은 여러 최종 사용자와 대시보드를 공유하려면 용량 요금을 사용하세요. 의 서비스 약관 40.4를 참조하세요.\n",
            "\"category : QuickSight, question : QuickSight 독자 계정을 사용하여 QuickSight에 프로그래밍 방식으로 액세스할 수 있습니까?, answer : QuickSight Reader 사용자 요금은 수동 액세스와 프로그래밍 방식 액세스 모두에 대해 개별 사용자의 대화형 데이터 소비에만 적용됩니다. 임베딩 시나리오와 같이 QuickSight에 등록되지 않은 여러 최종 사용자와 대시보드를 공유하려면 용량 요금을 사용하세요. 의 서비스 약관 40.4를 참조하세요.\"\n",
            "QuickSight 관리자란 누구인가요?\n",
            "QuickSight 관리자는 QuickSight 사용자와 계정 수준의 기본 설정을 관리하고 계정의 SPICE 용량과 연간 구독을 구매하는 사용자를 말합니다. 또한, 관리자는 모든 QuickSight 작성 기능을 사용할 수 있습니다. 필요한 경우 스탠다드 에디션 계정을 엔터프라이즈 에디션으로 업그레이드할 수도 있습니다. 청구 용도에서 QuickSight 작성자 및 관리자는 모두 작성자로 인식됩니다.\n",
            "\"category : QuickSight, question : QuickSight 관리자란 누구인가요?, answer : QuickSight 관리자는 QuickSight 사용자와 계정 수준의 기본 설정을 관리하고 계정의 SPICE 용량과 연간 구독을 구매하는 사용자를 말합니다. 또한, 관리자는 모든 QuickSight 작성 기능을 사용할 수 있습니다. 필요한 경우 스탠다드 에디션 계정을 엔터프라이즈 에디션으로 업그레이드할 수도 있습니다. 청구 용도에서 QuickSight 작성자 및 관리자는 모두 작성자로 인식됩니다.\"\n",
            "독자 또는 작성자를 관리자로 업그레이드할 수 있나요?\n",
            "언제든 QuickSight 작성자 및 독자를 관리자로 업그레이드할 수 있습니다.\n",
            "\"category : QuickSight, question : 독자 또는 작성자를 관리자로 업그레이드할 수 있나요?, answer : 언제든 QuickSight 작성자 및 독자를 관리자로 업그레이드할 수 있습니다.\"\n",
            "독자 세션의 시간은 어떻게 됩니까?\n",
            "각 QuickSight 독자 세션의 지속 시간은 30분입니다. 각 세션에는 0.30 USD가 부과되며 월별 독자당 최대 요금은 5 USD입니다.\n",
            "\"category : QuickSight, question : 독자 세션의 시간은 어떻게 됩니까?, answer : 각 QuickSight 독자 세션의 지속 시간은 30분입니다. 각 세션에는 0.30 USD가 부과되며 월별 독자당 최대 요금은 5 USD입니다.\"\n",
            "독자 세션은 언제 시작하고 종료됩니까?\n",
            "독자 세션은 사용자의 개시 작업(예: 로그인, 대시보드 로드, 페이지 갱신, 드릴다운 또는 필터링)으로 시작되어 다음 30분 동안 실행됩니다. QuickSight를 백그라운드 브라우저 창/탭에 열어두어도 독자가 페이지에서 작업을 개시하기 전에는 세션이 활성화되지 않습니다.\n",
            "\"category : QuickSight, question : 독자 세션은 언제 시작하고 종료됩니까?, answer : 독자 세션은 사용자의 개시 작업(예: 로그인, 대시보드 로드, 페이지 갱신, 드릴다운 또는 필터링)으로 시작되어 다음 30분 동안 실행됩니다. QuickSight를 백그라운드 브라우저 창/탭에 열어두어도 독자가 페이지에서 작업을 개시하기 전에는 세션이 활성화되지 않습니다.\"\n",
            "30분 세션에 종료되면 독자가 로그아웃됩니까?\n",
            "아니요. 독자 세션은 QuickSight 독자에게 투명하게 작동합니다. 독자 세션은 30분 간격으로 자동 갱신되며, 비활성화되면 시작되고 30분 후에 제한 시간이 초과됩니다. 독자는 인증이 만료되는 경우에만 QuickSight에서 로그아웃되며 이는 적용된 인증 체계에 따라 달라집니다(QuickSight 전용 사용자, SAML/Open ID Connect 또는 Active Directory 중 하나가 될 수 있음).\n",
            "\"category : QuickSight, question : 30분 세션에 종료되면 독자가 로그아웃됩니까?, answer : 아니요. 독자 세션은 QuickSight 독자에게 투명하게 작동합니다. 독자 세션은 30분 간격으로 자동 갱신되며, 비활성화되면 시작되고 30분 후에 제한 시간이 초과됩니다. 독자는 인증이 만료되는 경우에만 QuickSight에서 로그아웃되며 이는 적용된 인증 체계에 따라 달라집니다(QuickSight 전용 사용자, SAML/Open ID Connect 또는 Active Directory 중 하나가 될 수 있음).\"\n",
            "백그라운드 탭의 브라우저에 QuickSight가 열려 있는 경우 독자에게 요금이 부과됩니까?\n",
            "아니요. 백그라운드 탭에 QuickSight가 열려 있어도 사용 요금이 발생하지 않습니다. 세션은 QuickSight 웹 애플리케이션에서 명시적 독자 활동이 있는 경우에만 개시됩니다. QuickSight 페이지가 백그라운드로 이동되거나 최소화되는 경우, 독자가 다시 QuickSight와 상호 작용할 때까지 독자가 창/탭에서 활성 상태일 때 개시된 세션 이외에는 세션에 추가 요금이 부과되지 않습니다.\n",
            "\"category : QuickSight, question : 백그라운드 탭의 브라우저에 QuickSight가 열려 있는 경우 독자에게 요금이 부과됩니까?, answer : 아니요. 백그라운드 탭에 QuickSight가 열려 있어도 사용 요금이 발생하지 않습니다. 세션은 QuickSight 웹 애플리케이션에서 명시적 독자 활동이 있는 경우에만 개시됩니다. QuickSight 페이지가 백그라운드로 이동되거나 최소화되는 경우, 독자가 다시 QuickSight와 상호 작용할 때까지 독자가 창/탭에서 활성 상태일 때 개시된 세션 이외에는 세션에 추가 요금이 부과되지 않습니다.\"\n",
            "독자에 대해 월별 최대 5 USD를 부과한다는 것은 어떤 의미인가요?\n",
            "독자에게는 세션당 0.30 USD가 부과되며 월별 최대 요금은 5 USD입니다. 따라서 그 후 추가 세션에 대해서는 독자가 QuickSight에 무료로 액세스할 수 있습니다.\n",
            "\"category : QuickSight, question : 독자에 대해 월별 최대 5 USD를 부과한다는 것은 어떤 의미인가요?, answer : 독자에게는 세션당 0.30 USD가 부과되며 월별 최대 요금은 5 USD입니다. 따라서 그 후 추가 세션에 대해서는 독자가 QuickSight에 무료로 액세스할 수 있습니다.\"\n",
            "QuickSight 작성자 또는 독자가 다른 사용자를 초대할 수 있나요?\n",
            "아니요. QuickSight 작성자 및 독자는 계정 권한을 변경하거나 다른 사용자를 초대할 수 없는 사용자 유형입니다. QuickSight에서는 QuickSight 사용자와 계정 수준의 기본 설정을 관리하고 계정의 SPICE 용량과 연간 구독을 구매할 수 있는 관리자 사용자를 제공합니다. 또한, 관리자는 모든 QuickSight 작성 기능을 사용할 수 있습니다. 필요한 경우 스탠다드 에디션 계정을 엔터프라이즈 에디션으로 업그레이드할 수도 있습니다.\n",
            "\"category : QuickSight, question : QuickSight 작성자 또는 독자가 다른 사용자를 초대할 수 있나요?, answer : 아니요. QuickSight 작성자 및 독자는 계정 권한을 변경하거나 다른 사용자를 초대할 수 없는 사용자 유형입니다. QuickSight에서는 QuickSight 사용자와 계정 수준의 기본 설정을 관리하고 계정의 SPICE 용량과 연간 구독을 구매할 수 있는 관리자 사용자를 제공합니다. 또한, 관리자는 모든 QuickSight 작성 기능을 사용할 수 있습니다. 필요한 경우 스탠다드 에디션 계정을 엔터프라이즈 에디션으로 업그레이드할 수도 있습니다.\"\n",
            "QuickSight 독자 계정을 사용하여 모니터 또는 더 큰 화면에 QuickSight 대시보드를 표시하고 스크립팅된 갱신을 수행할 수 있습니까?\n",
            "데이터를 연속해서 표시하는 모니터 또는 대형 화면에 맞게 QuickSight 대시보드의 스크립팅된 갱신을 수행하려면 작성자 계정을 사용하는 것이 좋습니다. QuickSight 독자 사용자 요금은 조직 내 개별 최종 사용자의 대화형 데이터 소비에 적용됩니다. 사용자 요금과 용량 요금이 모두 적용되는 독자 세션도 30분 간격으로 사용하도록 설계되었습니다. 공정한 사용 정책이 적용되며 시스템을 남용하면 독자가 작성자로 측정됩니다.\n",
            "\"category : QuickSight, question : QuickSight 독자 계정을 사용하여 모니터 또는 더 큰 화면에 QuickSight 대시보드를 표시하고 스크립팅된 갱신을 수행할 수 있습니까?, answer : 데이터를 연속해서 표시하는 모니터 또는 대형 화면에 맞게 QuickSight 대시보드의 스크립팅된 갱신을 수행하려면 작성자 계정을 사용하는 것이 좋습니다. QuickSight 독자 사용자 요금은 조직 내 개별 최종 사용자의 대화형 데이터 소비에 적용됩니다. 사용자 요금과 용량 요금이 모두 적용되는 독자 세션도 30분 간격으로 사용하도록 설계되었습니다. 공정한 사용 정책이 적용되며 시스템을 남용하면 독자가 작성자로 측정됩니다.\"\n",
            "모바일 디바이스에서 Amazon QuickSight를 사용할 수 있나요?\n",
            "QuickSight 모바일 앱(iOS 및 Android)을 통해 데이터에 즉시 액세스하고 이동 중에 결정을 내리는 데 도움이 되는 분석 정보를 얻을 수 있습니다. 대시보드로 검색하고 상호 작용하십시오. 빠르고 쉽게 액세스하기 위해 대시보드를 즐겨찾기에 추가하십시오. 드릴 다운, 필터링 등으로 데이터를 검색하십시오. 또한 모든 모바일 디바이스에서 웹 브라우저를 사용하여 QuickSight에 액세스할 수 있습니다.\n",
            "\"category : QuickSight, question : 모바일 디바이스에서 Amazon QuickSight를 사용할 수 있나요?, answer : QuickSight 모바일 앱(iOS 및 Android)을 통해 데이터에 즉시 액세스하고 이동 중에 결정을 내리는 데 도움이 되는 분석 정보를 얻을 수 있습니다. 대시보드로 검색하고 상호 작용하십시오. 빠르고 쉽게 액세스하기 위해 대시보드를 즐겨찾기에 추가하십시오. 드릴 다운, 필터링 등으로 데이터를 검색하십시오. 또한 모든 모바일 디바이스에서 웹 브라우저를 사용하여 QuickSight에 액세스할 수 있습니다.\"\n",
            "QuickSight에서 지원하는 브라우저는 무엇인가요?\n",
            "QuickSight는 Mozilla Firefox, Chrome 및 Safari 최신 버전, Internet Explorer 버전 10 이상 및 Edge를 지원합니다.\n",
            "\"category : QuickSight, question : QuickSight에서 지원하는 브라우저는 무엇인가요?, answer : QuickSight는 Mozilla Firefox, Chrome 및 Safari 최신 버전, Internet Explorer 버전 10 이상 및 Edge를 지원합니다.\"\n",
            "QuickSight에서 지원하는 데이터 소스는 무엇인가요?\n",
            "Amazon RDS, Amazon Aurora, Amazon Redshift, Amazon Athena 및 Amazon S3를 비롯한 AWS 데이터 소스에 연결할 수 있습니다. 또한, Excel 스프레드시트 또는 플랫 파일(CSV, TSV, CLF 및 ELF)을 업로드하고, SQL Server, MySQL 및 PostgreSQL과 같은 온프레미스 데이터베이스에 연결하고, Salesforce와 같은 SaaS 애플리케이션에서 데이터를 가져올 수 있습니다.\n",
            "\"category : QuickSight, question : QuickSight에서 지원하는 데이터 소스는 무엇인가요?, answer : Amazon RDS, Amazon Aurora, Amazon Redshift, Amazon Athena 및 Amazon S3를 비롯한 AWS 데이터 소스에 연결할 수 있습니다. 또한, Excel 스프레드시트 또는 플랫 파일(CSV, TSV, CLF 및 ELF)을 업로드하고, SQL Server, MySQL 및 PostgreSQL과 같은 온프레미스 데이터베이스에 연결하고, Salesforce와 같은 SaaS 애플리케이션에서 데이터를 가져올 수 있습니다.\"\n",
            "QuickSight를 Amazon EC2 또는 온프레미스 데이터베이스에 연결할 수 있나요?\n",
            "예. QuickSight를 Amazon Elastic Compute Cloud(Amazon EC2) 또는 온프레미스 데이터베이스에 연결하려면 Amazon QuickSight IP 범위를 호스팅된 데이터베이스의 승인 목록에 추가해야 합니다.\n",
            "\"category : QuickSight, question : QuickSight를 Amazon EC2 또는 온프레미스 데이터베이스에 연결할 수 있나요?, answer : 예. QuickSight를 Amazon Elastic Compute Cloud(Amazon EC2) 또는 온프레미스 데이터베이스에 연결하려면 Amazon QuickSight IP 범위를 호스팅된 데이터베이스의 승인 목록에 추가해야 합니다.\"\n",
            "QuickSight로 데이터 파일을 업로드하려면 어떻게 해야 하나요?\n",
            "QuickSight 웹 사이트에서 직접 XLSX, CSV, TSV, CLF 및 XLF 데이터 파일을 업로드할 수 있습니다. 또한, Amazon S3 버킷에 데이터 파일을 업로드하고 QuickSight가 S3 객체를 가르키도록 할 수 있습니다.\n",
            "\"category : QuickSight, question : QuickSight로 데이터 파일을 업로드하려면 어떻게 해야 하나요?, answer : QuickSight 웹 사이트에서 직접 XLSX, CSV, TSV, CLF 및 XLF 데이터 파일을 업로드할 수 있습니다. 또한, Amazon S3 버킷에 데이터 파일을 업로드하고 QuickSight가 S3 객체를 가르키도록 할 수 있습니다.\"\n",
            "AWS 데이터 소스의 데이터에 액세스하려면 어떻게 해야 합니까?\n",
            "QuickSight는 고객의 승인하에 계정에서 사용 가능한 AWS 데이터 소스를 원활하게 검색합니다. 즉시 데이터 탐색 및 시각화 구축을 시작할 수 있습니다. 또한, 해당 데이터 소스에 대한 연결 세부 정보를 제공하여 계정에 없거나 다른 리전에 있는 다른 AWS 데이터 소스에 명시적으로 연결할 수 있습니다.\n",
            "\"category : QuickSight, question : AWS 데이터 소스의 데이터에 액세스하려면 어떻게 해야 합니까?, answer : QuickSight는 고객의 승인하에 계정에서 사용 가능한 AWS 데이터 소스를 원활하게 검색합니다. 즉시 데이터 탐색 및 시각화 구축을 시작할 수 있습니다. 또한, 해당 데이터 소스에 대한 연결 세부 정보를 제공하여 계정에 없거나 다른 리전에 있는 다른 AWS 데이터 소스에 명시적으로 연결할 수 있습니다.\"\n",
            "소스 데이터가 정리된 형식이 아닙니다. 시각화하기 전에 데이터의 형식을 지정하고 변환하려면 어떻게 해야 합니까?\n",
            "QuickSight를 사용하면 시각화할 준비가 안 된 데이터를 준비시킬 수 있습니다. 연결 대화 상자에서 [Edit/Preview Data] 버튼을 선택합니다. QuickSight는 데이터의 형식을 지정하고 변환할 수 있도록 다양한 함수를 지원합니다. 별칭 데이터 필드를 만들고, 데이터 유형을 변경할 수 있으며, 내장된 필터를 사용하여 데이터를 나누고 끌어서 놓기 동작을 통해 데이터베이스 조인 작업을 수행할 수 있습니다. 또한, 수학 연수과 내장된 함수(조건문, 문자열, 숫자형 함수, 날짜 함수 등)를 사용하여 계산 필드를 생성할 수 있습니다.\n",
            "\"category : QuickSight, question : 소스 데이터가 정리된 형식이 아닙니다. 시각화하기 전에 데이터의 형식을 지정하고 변환하려면 어떻게 해야 합니까?, answer : QuickSight를 사용하면 시각화할 준비가 안 된 데이터를 준비시킬 수 있습니다. 연결 대화 상자에서 [Edit/Preview Data] 버튼을 선택합니다. QuickSight는 데이터의 형식을 지정하고 변환할 수 있도록 다양한 함수를 지원합니다. 별칭 데이터 필드를 만들고, 데이터 유형을 변경할 수 있으며, 내장된 필터를 사용하여 데이터를 나누고 끌어서 놓기 동작을 통해 데이터베이스 조인 작업을 수행할 수 있습니다. 또한, 수학 연수과 내장된 함수(조건문, 문자열, 숫자형 함수, 날짜 함수 등)를 사용하여 계산 필드를 생성할 수 있습니다.\"\n",
            "QuickSight로 분석할 수 있는 데이터 양은 어떻게 되나요?\n",
            "QuickSight에서는 규모에 대해 걱정할 필요가 없습니다. 어떤 인프라도 관리하지 않고 몇백 기가바이트에서 수십 테라바이트까지 데이터를 원활하게 확장할 수 있습니다.\n",
            "\"category : QuickSight, question : QuickSight로 분석할 수 있는 데이터 양은 어떻게 되나요?, answer : QuickSight에서는 규모에 대해 걱정할 필요가 없습니다. 어떤 인프라도 관리하지 않고 몇백 기가바이트에서 수십 테라바이트까지 데이터를 원활하게 확장할 수 있습니다.\"\n",
            "QuickSight와 SageMaker의 통합은 어떻게 이루어지나요?\n",
            "첫 번째 단계를 데이터를 가져올 데이터 원본을 연결하는 것입니다. 데이터 원본에 연결했으면 ‘Augment with SageMaker’ 옵션을 선택합니다. 거기에서 AWS 계정에 있는 SageMaker 모델 목록 중 사용하고자 하는 모델을 선택하고 입력, 출력 및 런타임 설정이 포함되어 있는 JSON 형식 스키마 파일을 제공합니다. 데이터 세트에서 열이 있는 입력 스키마 매핑을 검토합니다. 완료했으면 이 작업을 실행해 추론 실행을 시작할 수 있습니다.\n",
            "\"category : QuickSight, question : QuickSight와 SageMaker의 통합은 어떻게 이루어지나요?, answer : 첫 번째 단계를 데이터를 가져올 데이터 원본을 연결하는 것입니다. 데이터 원본에 연결했으면 ‘Augment with SageMaker’ 옵션을 선택합니다. 거기에서 AWS 계정에 있는 SageMaker 모델 목록 중 사용하고자 하는 모델을 선택하고 입력, 출력 및 런타임 설정이 포함되어 있는 JSON 형식 스키마 파일을 제공합니다. 데이터 세트에서 열이 있는 입력 스키마 매핑을 검토합니다. 완료했으면 이 작업을 실행해 추론 실행을 시작할 수 있습니다.\"\n",
            "QuickSight는 SageMaker 모델을 이용하여 증분 데이터 또는 전체 데이터를 실행할 때마다 추론을 수행하나요?\n",
            "QuickSight는 새로 고칠 때마다 전체 데이터를 추론합니다.\n",
            "\"category : QuickSight, question : QuickSight는 SageMaker 모델을 이용하여 증분 데이터 또는 전체 데이터를 실행할 때마다 추론을 수행하나요?, answer : QuickSight는 새로 고칠 때마다 전체 데이터를 추론합니다.\"\n",
            "QuickSight에 대한 사용자 액세스를 제어하려면 어떻게 해야 하나요?\n",
            "여러분이 새로운 QuickSight 계정을 생성하면, 기본적으로 관리자 권한을 갖게 됩니다. QuickSight 사용자가 되도록 초대를 받는 경우, 여러분을 초대하는 사람이 여러분에게 관리자 또는 사용자 역할을 할당합니다. 관리자 역할이 있는 경우, 서비스 사용 외에도 사용자 계정을 생성 및 삭제하고, 연간 구독과 SPICE 용량을 구매할 수 있습니다. 사용자 계정을 생성하려면 인애플리케이션 인터페이스를 통해 사용자에게 초대 이메일을 전송합니다. 그런 다음 사용자가 암호를 지정한 후 로그인하면 계정 생성이 완료됩니다.\n",
            "\"category : QuickSight, question : QuickSight에 대한 사용자 액세스를 제어하려면 어떻게 해야 하나요?, answer : 여러분이 새로운 QuickSight 계정을 생성하면, 기본적으로 관리자 권한을 갖게 됩니다. QuickSight 사용자가 되도록 초대를 받는 경우, 여러분을 초대하는 사람이 여러분에게 관리자 또는 사용자 역할을 할당합니다. 관리자 역할이 있는 경우, 서비스 사용 외에도 사용자 계정을 생성 및 삭제하고, 연간 구독과 SPICE 용량을 구매할 수 있습니다. 사용자 계정을 생성하려면 인애플리케이션 인터페이스를 통해 사용자에게 초대 이메일을 전송합니다. 그런 다음 사용자가 암호를 지정한 후 로그인하면 계정 생성이 완료됩니다.\"\n",
            "QuickSight를 사용해 분석을 생성하려면 어떻게 해야 하나요?\n",
            "분석을 수행하는 것은 간단합니다. QuickSight는 AWS 계정 인기 있는 AWS 데이터 리포지토리의 데이터를 원활하게 검색합니다. 검색한 데이터 소스 중 하나를 QuickSight가 가리키도록 하면 됩니다. AWS 계정에 없거나 다른 리전에 있는 또 다른 AWS 데이터 소스에 연결하려면, 해당 소스에 대한 연결 세부 정보를 제공하면 됩니다. 그런 다음 테이블을 선택하고 데이터 분석을 시작합니다. 또한, 스프레드시트와 CSV 파일을 업로드하고 QuickSight로 해당 파일을 분석할 수 있습니다. 시각화를 생성하려면 분석하고자 하는 데이터 필드를 선택하거나, 시각화 캔버스로 필드를 직접 끌어오거나, 두 동작을 조합하여 시작합니다. QuickSight에서는 선택한 데이터에 따라 적절한 시각화를 자동으로 선택하여 표시합니다.\n",
            "\"category : QuickSight, question : QuickSight를 사용해 분석을 생성하려면 어떻게 해야 하나요?, answer : 분석을 수행하는 것은 간단합니다. QuickSight는 AWS 계정 인기 있는 AWS 데이터 리포지토리의 데이터를 원활하게 검색합니다. 검색한 데이터 소스 중 하나를 QuickSight가 가리키도록 하면 됩니다. AWS 계정에 없거나 다른 리전에 있는 또 다른 AWS 데이터 소스에 연결하려면, 해당 소스에 대한 연결 세부 정보를 제공하면 됩니다. 그런 다음 테이블을 선택하고 데이터 분석을 시작합니다. 또한, 스프레드시트와 CSV 파일을 업로드하고 QuickSight로 해당 파일을 분석할 수 있습니다. 시각화를 생성하려면 분석하고자 하는 데이터 필드를 선택하거나, 시각화 캔버스로 필드를 직접 끌어오거나, 두 동작을 조합하여 시작합니다. QuickSight에서는 선택한 데이터에 따라 적절한 시각화를 자동으로 선택하여 표시합니다.\"\n",
            "QuickSight에서는 내 데이터에 사용하기에 적합한 시각화를 어떻게 선택하나요?\n",
            "QuickSight에는 AutoGraph라고 부르는 혁신적인 기술이 적용되어 있어 카디널리티와 데이터 유형 같은 데이터 속성에 따라 가장 적합한 시각화를 선택할 수 있습니다. 데이터와 관계를 가장 효과적인 방법으로 표현하는 시각화 유형이 선택됩니다.\n",
            "\"category : QuickSight, question : QuickSight에서는 내 데이터에 사용하기에 적합한 시각화를 어떻게 선택하나요?, answer : QuickSight에는 AutoGraph라고 부르는 혁신적인 기술이 적용되어 있어 카디널리티와 데이터 유형 같은 데이터 속성에 따라 가장 적합한 시각화를 선택할 수 있습니다. 데이터와 관계를 가장 효과적인 방법으로 표현하는 시각화 유형이 선택됩니다.\"\n",
            "대시보드는 어떻게 생성하나요?\n",
            "시각화, 테이블 및 기타 시각적 표시 모음을 함께 정리하여 보여 주는 화면이 대시보드입니다. QuickSight에서는 시각화의 레이아웃과 크기를 조정하여 분석 내 대시보드를 구성한 다음, 조직 내 사용자에게 대시보드를 게시할 수 있습니다.\n",
            "\"category : QuickSight, question : 대시보드는 어떻게 생성하나요?, answer : 시각화, 테이블 및 기타 시각적 표시 모음을 함께 정리하여 보여 주는 화면이 대시보드입니다. QuickSight에서는 시각화의 레이아웃과 크기를 조정하여 분석 내 대시보드를 구성한 다음, 조직 내 사용자에게 대시보드를 게시할 수 있습니다.\"\n",
            "QuickSight에서는 어떤 유형의 시각화를 지원하나요?\n",
            "QuickSight는 여러 분석 접근 방식을 지원하는 다양한 시각화를 제공합니다. 비교 및 분포 막대 차트(여러 가지 다양한 종류) 시간에 따른 변화 선 그래프 영역 꺾은선형 차트 상관관계 산점도 열 지도 집계 원형 그래프 트리 맵 테이블 형식 피벗 테이블\n",
            "\"category : QuickSight, question : QuickSight에서는 어떤 유형의 시각화를 지원하나요?, answer : QuickSight는 여러 분석 접근 방식을 지원하는 다양한 시각화를 제공합니다. 비교 및 분포 막대 차트(여러 가지 다양한 종류) 시간에 따른 변화 선 그래프 영역 꺾은선형 차트 상관관계 산점도 열 지도 집계 원형 그래프 트리 맵 테이블 형식 피벗 테이블\"\n",
            "제안된 시각화란 무엇인가요? QuickSight는 제안을 어떻게 생성하나요?\n",
            "QuickSight는 해당 데이터 세트의 속성에 따라 추천 시각화를 제공하는 추천 엔진이 내장되어 제공됩니다. 추천 엔진의 제안은 분석의 첫 번째 또는 다음 단계로 사용할 수 있고 이를 통해 데이터 스키마를 조사 및 이해하는 시간 소모적인 작업을 피할 수 있습니다. 좀 더 구체적인 데이터로 작업하면, 제안은 현재 분석에 적합한 다음 단계를 반영하도록 업데이트됩니다.\n",
            "\"category : QuickSight, question : 제안된 시각화란 무엇인가요? QuickSight는 제안을 어떻게 생성하나요?, answer : QuickSight는 해당 데이터 세트의 속성에 따라 추천 시각화를 제공하는 추천 엔진이 내장되어 제공됩니다. 추천 엔진의 제안은 분석의 첫 번째 또는 다음 단계로 사용할 수 있고 이를 통해 데이터 스키마를 조사 및 이해하는 시간 소모적인 작업을 피할 수 있습니다. 좀 더 구체적인 데이터로 작업하면, 제안은 현재 분석에 적합한 다음 단계를 반영하도록 업데이트됩니다.\"\n",
            "스토리란 무엇인가요?\n",
            "스토리는 분석의 특정 관점을 쉽게 이해할 수 있게 해줍니다. 스토리는 협업을 위해 핵심 사항, 사고 과정 또는 분석의 전개를 전달하는 데 사용됩니다. QuickSight에서 분석의 특정 상태를 캡처하고 주석을 달아 스토리를 구성할 수 있습니다. 스토리를 읽는 사람이 스토리의 이미지를 선택하면, 해당 시점의 분석을 살펴보고 자체적으로 검토할 수 있습니다.\n",
            "\"category : QuickSight, question : 스토리란 무엇인가요?, answer : 스토리는 분석의 특정 관점을 쉽게 이해할 수 있게 해줍니다. 스토리는 협업을 위해 핵심 사항, 사고 과정 또는 분석의 전개를 전달하는 데 사용됩니다. QuickSight에서 분석의 특정 상태를 캡처하고 주석을 달아 스토리를 구성할 수 있습니다. 스토리를 읽는 사람이 스토리의 이미지를 선택하면, 해당 시점의 분석을 살펴보고 자체적으로 검토할 수 있습니다.\"\n",
            "QuickSight에서는 어떤 유형의 계산을 수행할 수 있나요?\n",
            "일반적인 연산 및 비교 함수, if, then과 같은 조건 함수, 날짜, 숫자 및 문자열 계산을 수행할 수 있습니다.\n",
            "\"category : QuickSight, question : QuickSight에서는 어떤 유형의 계산을 수행할 수 있나요?, answer : 일반적인 연산 및 비교 함수, if, then과 같은 조건 함수, 날짜, 숫자 및 문자열 계산을 수행할 수 있습니다.\"\n",
            "QuickSight는 AWS에서 생성한 데이터와 어떻게 통합되나요?\n",
            "현재 청구 및 비용 관리는 QuickSight와의 직접 통합을 제공합니다. QuickSight는 AWS 지출에 대한 인사이트를 얻는 데 사용할 수 있는 비용 및 사용 대시보드 생성을 자동화합니다. 이 대시보드는 오픈소스 프로젝트인 Cloud Intelligence Dashboards(CID)에서 영감을 받았습니다. 비용 및 사용 대시보드는 CID의 이점을 AWS 지원 기능인 결제 및 비용 관리 콘솔에 제공하므로 Amazon Athena 보기 또는 AWS Glue 크롤러와 같은 기본 인프라를 유지 관리할 필요가 없습니다. 비용 및 사용 대시보드는 청구 및 비용 관리 콘솔의 데이터 내보내기 페이지에서 배포할 수 있습니다.\n",
            "\"category : QuickSight, question : QuickSight는 AWS에서 생성한 데이터와 어떻게 통합되나요?, answer : 현재 청구 및 비용 관리는 QuickSight와의 직접 통합을 제공합니다. QuickSight는 AWS 지출에 대한 인사이트를 얻는 데 사용할 수 있는 비용 및 사용 대시보드 생성을 자동화합니다. 이 대시보드는 오픈소스 프로젝트인 Cloud Intelligence Dashboards(CID)에서 영감을 받았습니다. 비용 및 사용 대시보드는 CID의 이점을 AWS 지원 기능인 결제 및 비용 관리 콘솔에 제공하므로 Amazon Athena 보기 또는 AWS Glue 크롤러와 같은 기본 인프라를 유지 관리할 필요가 없습니다. 비용 및 사용 대시보드는 청구 및 비용 관리 콘솔의 데이터 내보내기 페이지에서 배포할 수 있습니다.\"\n",
            "QuickSight에서 살펴볼 샘플 데이터를 받으려면 어떻게 해야 하나요?\n",
            "편의를 위해 사용자가 Amazon QuickSight에 계정을 생성할 때 샘플 분석이 자동으로 생성됩니다. 원시 데이터는 아래 링크에서도 다운로드할 수 있습니다. 비즈니스 개요 인력 개요 영업 파이프라인 웹 및 마케팅 분석. 이러한 데이터 세트는 빅 데이터 컴피턴시를 획득한 AWS 어드밴스 컨설팅 파트너인 47Lining에서 생성했습니다.\n",
            "\"category : QuickSight, question : QuickSight에서 살펴볼 샘플 데이터를 받으려면 어떻게 해야 하나요?, answer : 편의를 위해 사용자가 Amazon QuickSight에 계정을 생성할 때 샘플 분석이 자동으로 생성됩니다. 원시 데이터는 아래 링크에서도 다운로드할 수 있습니다. 비즈니스 개요 인력 개요 영업 파이프라인 웹 및 마케팅 분석. 이러한 데이터 세트는 빅 데이터 컴피턴시를 획득한 AWS 어드밴스 컨설팅 파트너인 47Lining에서 생성했습니다.\"\n",
            "데이터는 어떻게 QuickSight로 전송되나요?\n",
            "파일 업로드, AWS 데이터 소스로 연결, JDBC/ODBC를 통해 외부 데이터 소스로 연결 또는 다른 API 기반 데이터 스토어 커넥터 통해 연결 등 몇 가지 옵션 중 선택하여 데이터를 QuickSight로 전송할 수 있습니다.\n",
            "\"category : QuickSight, question : 데이터는 어떻게 QuickSight로 전송되나요?, answer : 파일 업로드, AWS 데이터 소스로 연결, JDBC/ODBC를 통해 외부 데이터 소스로 연결 또는 다른 API 기반 데이터 스토어 커넥터 통해 연결 등 몇 가지 옵션 중 선택하여 데이터를 QuickSight로 전송할 수 있습니다.\"\n",
            "JDBC/ODBC를 통해 호스팅된 또는 온프레미스 데이터베이스에 연결할 AWS 리전을 선택할 수 있나요?\n",
            "예. 더 나은 성능과 사용자 상호 작용을 위해 데이터가 저장되어 있는 리전을 사용하는 것이 좋습니다. QuickSight의 자동 검색 기능은 연결되어 있는 QuickSight 엔드포인트의 리전 내 데이터 소스만 탐지합니다. 지원되는 QuickSight 지역 목록은 모든 AWS 글로벌 인프라를 위한 리전별 제품 및 서비스를 참조하세요.\n",
            "\"category : QuickSight, question : JDBC/ODBC를 통해 호스팅된 또는 온프레미스 데이터베이스에 연결할 AWS 리전을 선택할 수 있나요?, answer : 예. 더 나은 성능과 사용자 상호 작용을 위해 데이터가 저장되어 있는 리전을 사용하는 것이 좋습니다. QuickSight의 자동 검색 기능은 연결되어 있는 QuickSight 엔드포인트의 리전 내 데이터 소스만 탐지합니다. 지원되는 QuickSight 지역 목록은 모든 AWS 글로벌 인프라를 위한 리전별 제품 및 서비스를 참조하세요.\"\n",
            "QuickSight에서 멀티 팩터 인증을 지원하나요?\n",
            "예. AWS Management Console에서 AWS 계정의 멀티 팩터 인증(MFA)을 활성화할 수 있습니다.\n",
            "\"category : QuickSight, question : QuickSight에서 멀티 팩터 인증을 지원하나요?, answer : 예. AWS Management Console에서 AWS 계정의 멀티 팩터 인증(MFA)을 활성화할 수 있습니다.\"\n",
            "VPC를 QuickSight에 연결하려면 어떻게 해야 하나요?\n",
            "VPC가 퍼블릭 연결로 설정된 경우,QuickSight의 IP 주소 범위를 데이터베이스 인스턴스의 보안 그룹 규칙에 추가하면 트래픽이 VPC와 데이터베이스 인스턴스로 전송될 수 있습니다.\n",
            "\"category : QuickSight, question : VPC를 QuickSight에 연결하려면 어떻게 해야 하나요?, answer : VPC가 퍼블릭 연결로 설정된 경우,QuickSight의 IP 주소 범위를 데이터베이스 인스턴스의 보안 그룹 규칙에 추가하면 트래픽이 VPC와 데이터베이스 인스턴스로 전송될 수 있습니다.\"\n",
            "행 수준 보안이란 무엇입니까?\n",
            "행 수준 보안(RLS)을 사용하면 QuickSight 데이터 세트 소유자가 데이터와 상호 작용하는 사용자의 권한을 기준으로 데이터에 대한 액세스를 행 단위로 제어할 수 있습니다. RLS를 이용할 경우, QuickSight 사용자는 한 세트의 데이터만 관리하면서 적절한 행 수준 데이터 세트 규칙을 그 데이터 세트에 적용하면 됩니다. 그러면 연결된 모든 대시보드와 분석에도 이러한 규칙이 적용되기 때문에, 데이터 세트 관리가 간소해지고 다양한 데이터 액세스 권한을 가진 사용자별로 여러 가지 데이터 세트를 유지할 필요가 없습니다.\n",
            "\"category : QuickSight, question : 행 수준 보안이란 무엇입니까?, answer : 행 수준 보안(RLS)을 사용하면 QuickSight 데이터 세트 소유자가 데이터와 상호 작용하는 사용자의 권한을 기준으로 데이터에 대한 액세스를 행 단위로 제어할 수 있습니다. RLS를 이용할 경우, QuickSight 사용자는 한 세트의 데이터만 관리하면서 적절한 행 수준 데이터 세트 규칙을 그 데이터 세트에 적용하면 됩니다. 그러면 연결된 모든 대시보드와 분석에도 이러한 규칙이 적용되기 때문에, 데이터 세트 관리가 간소해지고 다양한 데이터 액세스 권한을 가진 사용자별로 여러 가지 데이터 세트를 유지할 필요가 없습니다.\"\n",
            "QuickSight 맥락에서의 프라이빗 VPC 액세스란 무슨 의미인가요?\n",
            "퍼블릭 연결 없이 AWS(Amazon Redshift, Amazon RDS 또는 EC2)에 데이터가 있거나 온프레미스의 Teradata 또는 SQL Server에 데이터가 있는 경우, 이 기능이 적합합니다. QuickSight의 프라이빗 VPC(Virtual Private Cloud) 액세스에서는 VPC 내 데이터 소스와의 안전한 프라이빗 통신을 위해 ENI(Elastic Network Interface)를 사용합니다. 또한, AWS Direct Connect를 사용하여 온프레미스 리소스와의 안전한 프라이빗 링크를 생성할 수 있습니다.\n",
            "\"category : QuickSight, question : QuickSight 맥락에서의 프라이빗 VPC 액세스란 무슨 의미인가요?, answer : 퍼블릭 연결 없이 AWS(Amazon Redshift, Amazon RDS 또는 EC2)에 데이터가 있거나 온프레미스의 Teradata 또는 SQL Server에 데이터가 있는 경우, 이 기능이 적합합니다. QuickSight의 프라이빗 VPC(Virtual Private Cloud) 액세스에서는 VPC 내 데이터 소스와의 안전한 프라이빗 통신을 위해 ENI(Elastic Network Interface)를 사용합니다. 또한, AWS Direct Connect를 사용하여 온프레미스 리소스와의 안전한 프라이빗 링크를 생성할 수 있습니다.\"\n",
            "QuickSight에서 분석, 대시보드 또는 스토리를 공유하려면 어떻게 해야 하나요?\n",
            "QuickSight 서비스 인터페이스에서 공유 아이콘을 사용하여 분석, 대시보드 또는 스토리를 공유할 수 있습니다. 다른 사용자와 콘텐츠를 공유하기 전에 수신자(이메일 주소, 사용자 이름 또는 그룹 이름), 권한 수준 및 기타 옵션을 선택할 수 있습니다.\n",
            "\"category : QuickSight, question : QuickSight에서 분석, 대시보드 또는 스토리를 공유하려면 어떻게 해야 하나요?, answer : QuickSight 서비스 인터페이스에서 공유 아이콘을 사용하여 분석, 대시보드 또는 스토리를 공유할 수 있습니다. 다른 사용자와 콘텐츠를 공유하기 전에 수신자(이메일 주소, 사용자 이름 또는 그룹 이름), 권한 수준 및 기타 옵션을 선택할 수 있습니다.\"\n",
            "스탠다드 에디션에서 엔터프라이즈 에디션으로 업그레이드할 수 있습니까?\n",
            "예. QuickSight 관리 페이지에서 스탠다드 에디션 계정을 엔터프라이즈 에디션으로 업그레이드할 수 있습니다. 기존 인증 세부 정보 및 사용자 데이터는 엔터프라이즈 에디션으로 원활하게 마이그레이션됩니다. 사용자 및 SPICE 용량에 대한 엔터프라이즈 에디션 요금이 적용됩니다.\n",
            "\"category : QuickSight, question : 스탠다드 에디션에서 엔터프라이즈 에디션으로 업그레이드할 수 있습니까?, answer : 예. QuickSight 관리 페이지에서 스탠다드 에디션 계정을 엔터프라이즈 에디션으로 업그레이드할 수 있습니다. 기존 인증 세부 정보 및 사용자 데이터는 엔터프라이즈 에디션으로 원활하게 마이그레이션됩니다. 사용자 및 SPICE 용량에 대한 엔터프라이즈 에디션 요금이 적용됩니다.\"\n",
            "엔터프라이즈 에디션에서 스탠다드 에디션으로 다운그레이드할 수 있습니까?\n",
            "아니요. Amazon QuickSight 엔터프라이즈 에디션에서 스탠다드 에디션으로 다운그레이드할 수 없습니다. Amazon QuickSight 엔터프라이즈 에디션은 QuickSight 독자, 프라이빗 VPC에서 데이터 소스에 연결, 행 수준의 보안, SPICE 데이터의 시간 단위 갱신, AD 연결, AD 계정의 그룹 기반 자산 관리와 같은 향상된 기능을 제공합니다. 기능 세트의 차이로 인해 다운그레이드는 데이터 연결 해제 및 보안 손상으로 이어질 수 있으므로 이 옵션은 지원되지 않습니다.\n",
            "\"category : QuickSight, question : 엔터프라이즈 에디션에서 스탠다드 에디션으로 다운그레이드할 수 있습니까?, answer : 아니요. Amazon QuickSight 엔터프라이즈 에디션에서 스탠다드 에디션으로 다운그레이드할 수 없습니다. Amazon QuickSight 엔터프라이즈 에디션은 QuickSight 독자, 프라이빗 VPC에서 데이터 소스에 연결, 행 수준의 보안, SPICE 데이터의 시간 단위 갱신, AD 연결, AD 계정의 그룹 기반 자산 관리와 같은 향상된 기능을 제공합니다. 기능 세트의 차이로 인해 다운그레이드는 데이터 연결 해제 및 보안 손상으로 이어질 수 있으므로 이 옵션은 지원되지 않습니다.\"\n",
            "========== SageMaker  :  https://aws.amazon.com/ko/sagemaker/faqs/ 사이트 크롤링 진행중 ==========\n",
            "168\n",
            "Amazon SageMaker란 무엇인가요?\n",
            "SageMaker는 완전관리형 인프라, 도구 및 워크플로를 사용하여 모든 사용 사례에 대해 데이터를 준비하고 기계 학습(ML) 모델을 구축, 훈련 및 배포하는 완전관리형 서비스입니다.\n",
            "\"category : SageMaker, question : Amazon SageMaker란 무엇인가요?, answer : SageMaker는 완전관리형 인프라, 도구 및 워크플로를 사용하여 모든 사용 사례에 대해 데이터를 준비하고 기계 학습(ML) 모델을 구축, 훈련 및 배포하는 완전관리형 서비스입니다.\"\n",
            "SageMaker는 어느 AWS 리전에서 사용할 수 있나요?\n",
            "지원되는 SageMaker 리전의 목록은 AWS 리전 서비스 페이지를 참조하세요. 자세한 내용은 AWS 일반 참조 가이드에서 리전 엔드포인트를 참조하세요.\n",
            "\"category : SageMaker, question : SageMaker는 어느 AWS 리전에서 사용할 수 있나요?, answer : 지원되는 SageMaker 리전의 목록은 AWS 리전 서비스 페이지를 참조하세요. 자세한 내용은 AWS 일반 참조 가이드에서 리전 엔드포인트를 참조하세요.\"\n",
            "SageMaker의 서비스 가용성은 어떤가요?\n",
            "SageMaker는 고가용성을 제공하도록 설계되었습니다. 유지 관리 기간이나 예약된 가동 중지 시간이 없습니다. SageMaker API는 입증된 고가용성 Amazon 데이터 센터에서 실행되며 각 리전의 시설 세 곳에 서비스 스택 복제가 구성되어 있어 서버 장애나 가용 영역 중단 시에 내결함성을 제공합니다.\n",
            "\"category : SageMaker, question : SageMaker의 서비스 가용성은 어떤가요?, answer : SageMaker는 고가용성을 제공하도록 설계되었습니다. 유지 관리 기간이나 예약된 가동 중지 시간이 없습니다. SageMaker API는 입증된 고가용성 Amazon 데이터 센터에서 실행되며 각 리전의 시설 세 곳에 서비스 스택 복제가 구성되어 있어 서버 장애나 가용 영역 중단 시에 내결함성을 제공합니다.\"\n",
            "SageMaker는 어떤 방식으로 코드 보안을 제공하나요?\n",
            "SageMaker는 보안 그룹과 저장 중 암호화(선택 사항)로 보호된 ML 스토리지 볼륨에 코드를 저장합니다.\n",
            "\"category : SageMaker, question : SageMaker는 어떤 방식으로 코드 보안을 제공하나요?, answer : SageMaker는 보안 그룹과 저장 중 암호화(선택 사항)로 보호된 ML 스토리지 볼륨에 코드를 저장합니다.\"\n",
            "SageMaker에는 어떤 보안 조치가 있나요?\n",
            "SageMaker는 전송 중이거나 저장 중인 ML 모델 아티팩트 및 기타 시스템 아티팩트를 암호화합니다. SageMaker API 및 콘솔에 대한 요청은 보안(SSL) 연결을 통해 전달됩니다. 사용자는 AWS Identity and Access Management 역할을 SageMaker로 전달하여 사용자 대신, 훈련 및 배포를 위해 리소스에 액세스할 수 있는 권한을 제공합니다. 모델 아티팩트 및 데이터에 대해 암호화된 Amazon Simple Storage Service(S3) 버킷을 사용하고 AWS Key Management Service(AWS KMS) 키를 SageMaker 노트북, 훈련 작업 및 엔드포인트로 전달하여 연결된 ML 스토리지 볼륨을 암호화할 수 있습니다. 또한 SageMaker는 Amazon Virtual Private Cloud(VPC) 및 AWS PrivateLink 지원도 제공합니다.\n",
            "\"category : SageMaker, question : SageMaker에는 어떤 보안 조치가 있나요?, answer : SageMaker는 전송 중이거나 저장 중인 ML 모델 아티팩트 및 기타 시스템 아티팩트를 암호화합니다. SageMaker API 및 콘솔에 대한 요청은 보안(SSL) 연결을 통해 전달됩니다. 사용자는 AWS Identity and Access Management 역할을 SageMaker로 전달하여 사용자 대신, 훈련 및 배포를 위해 리소스에 액세스할 수 있는 권한을 제공합니다. 모델 아티팩트 및 데이터에 대해 암호화된 Amazon Simple Storage Service(S3) 버킷을 사용하고 AWS Key Management Service(AWS KMS) 키를 SageMaker 노트북, 훈련 작업 및 엔드포인트로 전달하여 연결된 ML 스토리지 볼륨을 암호화할 수 있습니다. 또한 SageMaker는 Amazon Virtual Private Cloud(VPC) 및 AWS PrivateLink 지원도 제공합니다.\"\n",
            "SageMaker는 모델, 훈련 데이터 또는 알고리즘을 사용하거나 공유하나요?\n",
            "SageMaker는 고객 모델, 훈련 데이터 또는 알고리즘을 사용하거나 공유하지 않습니다. AWS는 프라이버시와 데이터 보안이 고객에게 얼마나 중요한지 잘 알고 있습니다. 따라서 AWS에서는 콘텐츠를 저장할 장소를 결정하고, 저장 콘텐츠 및 전송 콘텐츠를 보호하며, AWS 서비스와 리소스에 대한 사용자의 액세스를 관리할 수 있는 단순하지만 강력한 도구를 통해 고객에게 콘텐츠에 대한 소유권과 제어권을 제공합니다. 또한 AWS는 고객 콘텐츠에 대한 무단 액세스나 공개를 차단하도록 설계된 기술적 제어 및 물리적 제어를 구현합니다. 고객은 자신의 콘텐츠에 대한 소유권을 유지하고 콘텐츠를 처리, 저장 및 호스팅할 AWS 서비스를 선택합니다. AWS에서는 고객의 동의 없이는 어떠한 목적으로도 고객 콘텐츠에 액세스하지 않습니다.\n",
            "\"category : SageMaker, question : SageMaker는 모델, 훈련 데이터 또는 알고리즘을 사용하거나 공유하나요?, answer : SageMaker는 고객 모델, 훈련 데이터 또는 알고리즘을 사용하거나 공유하지 않습니다. AWS는 프라이버시와 데이터 보안이 고객에게 얼마나 중요한지 잘 알고 있습니다. 따라서 AWS에서는 콘텐츠를 저장할 장소를 결정하고, 저장 콘텐츠 및 전송 콘텐츠를 보호하며, AWS 서비스와 리소스에 대한 사용자의 액세스를 관리할 수 있는 단순하지만 강력한 도구를 통해 고객에게 콘텐츠에 대한 소유권과 제어권을 제공합니다. 또한 AWS는 고객 콘텐츠에 대한 무단 액세스나 공개를 차단하도록 설계된 기술적 제어 및 물리적 제어를 구현합니다. 고객은 자신의 콘텐츠에 대한 소유권을 유지하고 콘텐츠를 처리, 저장 및 호스팅할 AWS 서비스를 선택합니다. AWS에서는 고객의 동의 없이는 어떠한 목적으로도 고객 콘텐츠에 액세스하지 않습니다.\"\n",
            "SageMaker 사용 요금은 어떻게 부과되나요?\n",
            "노트북을 호스팅하고 모델을 훈련하고 예측을 수행하며 출력을 로깅하는 데 사용하는 ML 컴퓨팅, 스토리지 및 데이터 처리 리소스에 대한 요금이 부과됩니다. SageMaker를 사용할 때는 호스팅된 노트북, 훈련 및 모델 호스팅에 사용되는 인스턴스 수 및 유형을 선택할 수 있습니다. 서비스를 사용하면서 사용한 만큼만 비용을 지불하며 최소 요금 및 사전 약정은 없습니다. 자세한 내용은 Amazon SageMaker 요금 및 Amazon SageMaker 요금 계산기를 참조하세요.\n",
            "\"category : SageMaker, question : SageMaker 사용 요금은 어떻게 부과되나요?, answer : 노트북을 호스팅하고 모델을 훈련하고 예측을 수행하며 출력을 로깅하는 데 사용하는 ML 컴퓨팅, 스토리지 및 데이터 처리 리소스에 대한 요금이 부과됩니다. SageMaker를 사용할 때는 호스팅된 노트북, 훈련 및 모델 호스팅에 사용되는 인스턴스 수 및 유형을 선택할 수 있습니다. 서비스를 사용하면서 사용한 만큼만 비용을 지불하며 최소 요금 및 사전 약정은 없습니다. 자세한 내용은 Amazon SageMaker 요금 및 Amazon SageMaker 요금 계산기를 참조하세요.\"\n",
            "유휴 리소스를 검색하고 중지하여 불필요한 요금을 방지하는 것과 같이 SageMaker 비용을 최적화하려면 어떻게 해야 하나요?\n",
            "SageMaker 리소스 사용량을 최적화하는 데 사용할 수 있는 여러 모범 사례가 있습니다. 구성 최적화를 수행하는 것과 관련된 접근 방식이 있고 프로그래밍 방식 솔루션을 제공하는 접근 방식도 있습니다. 이 블로그 게시물에서 시각적 자습서 및 코드 샘플과 함께 이 개념에 대한 전체 가이드를 확인할 수 있습니다.\n",
            "\"category : SageMaker, question : 유휴 리소스를 검색하고 중지하여 불필요한 요금을 방지하는 것과 같이 SageMaker 비용을 최적화하려면 어떻게 해야 하나요?, answer : SageMaker 리소스 사용량을 최적화하는 데 사용할 수 있는 여러 모범 사례가 있습니다. 구성 최적화를 수행하는 것과 관련된 접근 방식이 있고 프로그래밍 방식 솔루션을 제공하는 접근 방식도 있습니다. 이 블로그 게시물에서 시각적 자습서 및 코드 샘플과 함께 이 개념에 대한 전체 가이드를 확인할 수 있습니다.\"\n",
            "자체 노트북, 훈련 또는 호스팅 환경이 있는 경우는 어떻게 되나요?\n",
            "SageMaker는 완전한 전체 워크플로를 제공하지만, SageMaker에서 기존 도구를 계속 사용할 수 있습니다. 비즈니스 요구 사항에 따라 필요한 대로 SageMaker에서 각 단계의 결과를 손쉽게 송수신할 수 있습니다.\n",
            "\"category : SageMaker, question : 자체 노트북, 훈련 또는 호스팅 환경이 있는 경우는 어떻게 되나요?, answer : SageMaker는 완전한 전체 워크플로를 제공하지만, SageMaker에서 기존 도구를 계속 사용할 수 있습니다. 비즈니스 요구 사항에 따라 필요한 대로 SageMaker에서 각 단계의 결과를 손쉽게 송수신할 수 있습니다.\"\n",
            "SageMaker에서 R이 지원되나요?\n",
            "예. SageMaker 노트북 인스턴스에서 R을 사용할 수 있으며, 여기에는 사전 설치된 R 커널과 reticulate 라이브러리가 포함됩니다. Reticulate는 Amazon SageMaker Python SDK를 위한 R 인터페이스를 제공하여 ML 실무자의 R 모델 구축, 훈련, 튜닝 및 배포 작업을 지원합니다.\n",
            "\"category : SageMaker, question : SageMaker에서 R이 지원되나요?, answer : 예. SageMaker 노트북 인스턴스에서 R을 사용할 수 있으며, 여기에는 사전 설치된 R 커널과 reticulate 라이브러리가 포함됩니다. Reticulate는 Amazon SageMaker Python SDK를 위한 R 인터페이스를 제공하여 ML 실무자의 R 모델 구축, 훈련, 튜닝 및 배포 작업을 지원합니다.\"\n",
            "모델에서 불균형을 확인하려면 어떻게 해야 하나요?\n",
            "Amazon SageMaker Clarify를 통해 전체 ML 워크플로에서 통계적인 바이어스를 탐지해 모델 투명성을 개선할 수 있습니다. SageMaker Clarify는 데이터 준비 도중, 훈련 후, 시간 경과에 따른 불균형을 검사하고, ML 모델 및 예측을 설명해주는 도구도 포함합니다. 조사 결과는 설명 보고서를 통해 공유할 수 있습니다.\n",
            "\"category : SageMaker, question : 모델에서 불균형을 확인하려면 어떻게 해야 하나요?, answer : Amazon SageMaker Clarify를 통해 전체 ML 워크플로에서 통계적인 바이어스를 탐지해 모델 투명성을 개선할 수 있습니다. SageMaker Clarify는 데이터 준비 도중, 훈련 후, 시간 경과에 따른 불균형을 검사하고, ML 모델 및 예측을 설명해주는 도구도 포함합니다. 조사 결과는 설명 보고서를 통해 공유할 수 있습니다.\"\n",
            "Amazon SageMaker Clarify는 어떤 종류의 바이어스를 탐지하나요?\n",
            "ML 모델에서 바이어스 평가는 바이어스를 완화하는 첫 단계입니다. 바이어스는 훈련 전후에, 그리고 배포된 모델에 대한 추론을 위해 평가할 수 있습니다. 바이어스의 각 평가는 다양한 공정성 개념을 반영합니다. 단순한 공정성 개념을 평가하는 작업도 다양한 컨텍스트에서 다양한 많은 평가를 적용할 수 있습니다. 사용자는 조사할 상황과 애플리케이션에 유효한 바이어스 개념과 지표를 선택해야 합니다. 현재 SageMaker는 훈련 데이터(SageMaker 데이터 준비의 일부), 훈련된 모델(Amazon SageMaker Experiments의 일부), 배포된 모델의 추론(Amazon SageMaker Model Monitor의 일부)을 위해 다양한 바이어스 지표 계산을 지원합니다. 예를 들어 훈련 전에 훈련 데이터가 대표성을 갖는지(즉, 한 그룹에서 대표성이 부족한지 여부)와 여러 그룹에서 레이블 분산에 차이가 있는지를 검사하는 지표를 제공합니다. 훈련 후 또는 배포 중에는 여러 그룹에서 모델의 성능이 다른지 여부와 다르다면 얼마나 다른지를 평가하는 데 이러한 지표를 유용하게 활용할 수 있습니다. 예를 들어 오류 비율(모델의 예측이 실제 레이블과 차이가 날 가능성), 더 세분화된 정밀도로 분류(긍정적인 예측이 올바를 가능성), 회수(모델이 긍정적인 예제에 레이블을 올바르게 지정할 가능성)를 비교하는 작업부터 시작할 수 있습니다.\n",
            "\"category : SageMaker, question : Amazon SageMaker Clarify는 어떤 종류의 바이어스를 탐지하나요?, answer : ML 모델에서 바이어스 평가는 바이어스를 완화하는 첫 단계입니다. 바이어스는 훈련 전후에, 그리고 배포된 모델에 대한 추론을 위해 평가할 수 있습니다. 바이어스의 각 평가는 다양한 공정성 개념을 반영합니다. 단순한 공정성 개념을 평가하는 작업도 다양한 컨텍스트에서 다양한 많은 평가를 적용할 수 있습니다. 사용자는 조사할 상황과 애플리케이션에 유효한 바이어스 개념과 지표를 선택해야 합니다. 현재 SageMaker는 훈련 데이터(SageMaker 데이터 준비의 일부), 훈련된 모델(Amazon SageMaker Experiments의 일부), 배포된 모델의 추론(Amazon SageMaker Model Monitor의 일부)을 위해 다양한 바이어스 지표 계산을 지원합니다. 예를 들어 훈련 전에 훈련 데이터가 대표성을 갖는지(즉, 한 그룹에서 대표성이 부족한지 여부)와 여러 그룹에서 레이블 분산에 차이가 있는지를 검사하는 지표를 제공합니다. 훈련 후 또는 배포 중에는 여러 그룹에서 모델의 성능이 다른지 여부와 다르다면 얼마나 다른지를 평가하는 데 이러한 지표를 유용하게 활용할 수 있습니다. 예를 들어 오류 비율(모델의 예측이 실제 레이블과 차이가 날 가능성), 더 세분화된 정밀도로 분류(긍정적인 예측이 올바를 가능성), 회수(모델이 긍정적인 예제에 레이블을 올바르게 지정할 가능성)를 비교하는 작업부터 시작할 수 있습니다.\"\n",
            "SageMaker Clarify는 모델 설명 가능성을 어떻게 개선하나요?\n",
            "SageMaker Clarify는 SageMaker Experiments에 통합되어 모델 훈련 후에 모델의 전반적인 의사 결정 프로세스에서 각 입력의 중요성을 자세히 설명하는 특성 중요도 그래프를 제공합니다. 이와 같은 세부 정보는 특정 모델 입력이 전체 모델 동작에 미쳐야 하는 것보다 더 많은 영향을 미치는지 확인하는 데 유용할 수 있습니다. SageMaker Clarify는 API를 통해 개별 예측에 대한 설명도 제공할 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker Clarify는 모델 설명 가능성을 어떻게 개선하나요?, answer : SageMaker Clarify는 SageMaker Experiments에 통합되어 모델 훈련 후에 모델의 전반적인 의사 결정 프로세스에서 각 입력의 중요성을 자세히 설명하는 특성 중요도 그래프를 제공합니다. 이와 같은 세부 정보는 특정 모델 입력이 전체 모델 동작에 미쳐야 하는 것보다 더 많은 영향을 미치는지 확인하는 데 유용할 수 있습니다. SageMaker Clarify는 API를 통해 개별 예측에 대한 설명도 제공할 수 있습니다.\"\n",
            "Amazon SageMaker Studio란 무엇인가요?\n",
            "Amazon SageMaker Studio는 JupyterLab, Visual Studio - Code Open Source(Code - OSS) 기반 코드 편집기, RStudio 등 다양한 통합 개발 환경(IDE)을 제공합니다. 선호하는 IDE를 선택하여 목적별 도구에 액세스하여, 데이터 준비부터 ML 모델 구축, 훈련 및 배포에 이르기까지 모든 기계 학습(ML) 개발 단계를 수행할 수 있습니다. 선호하는 IDE를 사용하여 데이터를 빠르게 업로드하고, 노트북으로 모델을 구축하며, 분산 훈련으로 확장할 수 있습니다.\n",
            "\"category : SageMaker, question : Amazon SageMaker Studio란 무엇인가요?, answer : Amazon SageMaker Studio는 JupyterLab, Visual Studio - Code Open Source(Code - OSS) 기반 코드 편집기, RStudio 등 다양한 통합 개발 환경(IDE)을 제공합니다. 선호하는 IDE를 선택하여 목적별 도구에 액세스하여, 데이터 준비부터 ML 모델 구축, 훈련 및 배포에 이르기까지 모든 기계 학습(ML) 개발 단계를 수행할 수 있습니다. 선호하는 IDE를 사용하여 데이터를 빠르게 업로드하고, 노트북으로 모델을 구축하며, 분산 훈련으로 확장할 수 있습니다.\"\n",
            "RStudio on Amazon SageMaker란 무엇인가요?\n",
            "RStudio on SageMaker는 클라우드 최초의 완전관리형 RStudio Workbench입니다. 익숙한 RStudio 통합 개발 환경(IDE)을 신속하게 시작할 수 있고, 작업을 중단하지 않고도 기본 컴퓨팅 리소스를 탄력적으로 늘리고 줄일 수 있으며, R에서 대규모로 ML 및 분석 솔루션을 손쉽게 구축할 수 있습니다. RStudio IDE와 R 및 Python 개발용 SageMaker Studio 노트북 간에 원활하게 전환할 수 있습니다. 코드, 데이터 세트, 리포지토리 및 기타 아티팩트를 비롯한 모든 작업이 두 환경 간에 자동으로 동기화되므로 컨텍스트 전환이 줄어들고 생산성이 개선됩니다.\n",
            "\"category : SageMaker, question : RStudio on Amazon SageMaker란 무엇인가요?, answer : RStudio on SageMaker는 클라우드 최초의 완전관리형 RStudio Workbench입니다. 익숙한 RStudio 통합 개발 환경(IDE)을 신속하게 시작할 수 있고, 작업을 중단하지 않고도 기본 컴퓨팅 리소스를 탄력적으로 늘리고 줄일 수 있으며, R에서 대규모로 ML 및 분석 솔루션을 손쉽게 구축할 수 있습니다. RStudio IDE와 R 및 Python 개발용 SageMaker Studio 노트북 간에 원활하게 전환할 수 있습니다. 코드, 데이터 세트, 리포지토리 및 기타 아티팩트를 비롯한 모든 작업이 두 환경 간에 자동으로 동기화되므로 컨텍스트 전환이 줄어들고 생산성이 개선됩니다.\"\n",
            "SageMaker Studio 요금은 어떻게 적용되나요?\n",
            "SageMaker Studio 사용에 대한 추가 요금은 없습니다. SageMaker Studio 내에서 사용하는 서비스의 기본 컴퓨팅 및 스토리지 요금만 지불하면 됩니다.\n",
            "\"category : SageMaker, question : SageMaker Studio 요금은 어떻게 적용되나요?, answer : SageMaker Studio 사용에 대한 추가 요금은 없습니다. SageMaker Studio 내에서 사용하는 서비스의 기본 컴퓨팅 및 스토리지 요금만 지불하면 됩니다.\"\n",
            "SageMaker Studio는 어느 리전에서 지원되나요?\n",
            "SageMaker Studio를 지원하는 리전은 Amazon SageMaker 개발자 안내서에서 확인할 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker Studio는 어느 리전에서 지원되나요?, answer : SageMaker Studio를 지원하는 리전은 Amazon SageMaker 개발자 안내서에서 확인할 수 있습니다.\"\n",
            "SageMaker는 어떤 ML 거버넌스 도구를 제공하나요?\n",
            "SageMaker는 ML 수명 주기 전반에 걸쳐 목적별 ML 거버넌스 도구를 제공합니다. 관리자는 Amazon SageMaker 역할 관리자를 사용하여 최소 권한을 몇 분 만에 정의할 수 있습니다. Amazon SageMaker 모델 카드를 사용하면 필수 모델 정보를 개념 단계부터 배포 단계까지 손쉽게 캡처, 검색 및 공유할 수 있고, Amazon SageMaker 모델 대시보드라는 한 장소에서 프로덕션 모델 동작에 대한 정보를 지속적으로 확인할 수 있습니다. 자세한 내용은 Amazon SageMaker를 사용한 ML 거버넌스를 참조하세요.\n",
            "\"category : SageMaker, question : SageMaker는 어떤 ML 거버넌스 도구를 제공하나요?, answer : SageMaker는 ML 수명 주기 전반에 걸쳐 목적별 ML 거버넌스 도구를 제공합니다. 관리자는 Amazon SageMaker 역할 관리자를 사용하여 최소 권한을 몇 분 만에 정의할 수 있습니다. Amazon SageMaker 모델 카드를 사용하면 필수 모델 정보를 개념 단계부터 배포 단계까지 손쉽게 캡처, 검색 및 공유할 수 있고, Amazon SageMaker 모델 대시보드라는 한 장소에서 프로덕션 모델 동작에 대한 정보를 지속적으로 확인할 수 있습니다. 자세한 내용은 Amazon SageMaker를 사용한 ML 거버넌스를 참조하세요.\"\n",
            "SageMaker 역할 관리자는 어떤 기능을 제공하나요?\n",
            "SageMaker 역할 관리자를 사용하면 최소 권한을 몇 분 만에 정의할 수 있습니다. SageMaker 역할 관리자는 사전 구축된 IAM 정책 카탈로그를 통해 ML 활동 및 페르소나에 대한 기본 권한 세트를 제공합니다. 기준 권한을 유지하거나 특정한 요구 사항에 따라 추가로 사용자 지정할 수 있습니다. 몇 가지 셀프 구성 화면을 통해 네트워크 액세스 경계, 암호화 키 등 일반적인 거버넌스 구성을 빠르게 입력할 수 있습니다. 그러면 SageMaker Role Manager가 IAM 정책을 자동으로 생성합니다. AWS IAM 콘솔을 통해 생성된 역할과 관련 정책을 검색할 수 있습니다. 사용 사례에 대한 권한을 추가로 사용자 지정하려면, SageMaker Role Manager에서 생성하는 IAM 역할에 관리형 IAM 정책을 연결합니다. 또한 AWS 서비스 전반에서 역할을 식별하고 구성하는 데 유용한 태그를 추가할 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker 역할 관리자는 어떤 기능을 제공하나요?, answer : SageMaker 역할 관리자를 사용하면 최소 권한을 몇 분 만에 정의할 수 있습니다. SageMaker 역할 관리자는 사전 구축된 IAM 정책 카탈로그를 통해 ML 활동 및 페르소나에 대한 기본 권한 세트를 제공합니다. 기준 권한을 유지하거나 특정한 요구 사항에 따라 추가로 사용자 지정할 수 있습니다. 몇 가지 셀프 구성 화면을 통해 네트워크 액세스 경계, 암호화 키 등 일반적인 거버넌스 구성을 빠르게 입력할 수 있습니다. 그러면 SageMaker Role Manager가 IAM 정책을 자동으로 생성합니다. AWS IAM 콘솔을 통해 생성된 역할과 관련 정책을 검색할 수 있습니다. 사용 사례에 대한 권한을 추가로 사용자 지정하려면, SageMaker Role Manager에서 생성하는 IAM 역할에 관리형 IAM 정책을 연결합니다. 또한 AWS 서비스 전반에서 역할을 식별하고 구성하는 데 유용한 태그를 추가할 수 있습니다.\"\n",
            "SageMaker 모델 카드는 어떤 기능을 제공하나요?\n",
            "SageMaker 모델 카드는 모델 정보의 단일 소스를 생성하여 ML 수명 주기 전반에서 모델 설명서를 중앙 집중화하고 표준화하는 데 도움이 됩니다. SageMaker 모델 카드는 훈련 세부 정보를 자동으로 입력하여 문서화 프로세스의 속도를 높여줍니다. 모델의 용도 및 성능 목표와 같은 세부 정보를 추가할 수도 있습니다. 모델 평가 결과를 모델 카드에 첨부하고, 시각화 요소를 제공하여 모델 성능에 대한 주요 인사이트를 얻을 수 있습니다. SageMaker 모델 카드를 PDF 형식으로 내보내 다른 사용자와 손쉽게 공유할 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker 모델 카드는 어떤 기능을 제공하나요?, answer : SageMaker 모델 카드는 모델 정보의 단일 소스를 생성하여 ML 수명 주기 전반에서 모델 설명서를 중앙 집중화하고 표준화하는 데 도움이 됩니다. SageMaker 모델 카드는 훈련 세부 정보를 자동으로 입력하여 문서화 프로세스의 속도를 높여줍니다. 모델의 용도 및 성능 목표와 같은 세부 정보를 추가할 수도 있습니다. 모델 평가 결과를 모델 카드에 첨부하고, 시각화 요소를 제공하여 모델 성능에 대한 주요 인사이트를 얻을 수 있습니다. SageMaker 모델 카드를 PDF 형식으로 내보내 다른 사용자와 손쉽게 공유할 수 있습니다.\"\n",
            "SageMaker 모델 대시보드는 어떤 기능을 제공하나요?\n",
            "SageMaker 모델 대시보드는 배포된 모델과 엔드포인트에 대한 전반적인 개요 정보를 제공하므로, 단일 창에서 리소스를 추적하고 동작 위반을 모델링할 수 있습니다. 대시보드에서는 4가지 차원으로 모델 동작을 모니터링할 수 있습니다. 예를 들어 데이터 및 모델 품질로 모니터링하거나 SageMaker Model Monitor 및 SageMaker Clarify와 통합할 경우 바이어스 및 특성 기여도 드리프트로 모니터링할 수 있습니다. 또한 SageMaker 모델 대시보드는 누락되고 비활성화된 모델 모니터링 작업과 모델 품질, 데이터 품질, 바이어스 드리프트 및 특성 기여도 드리프트에 대한 모델 동작의 편차에 대한 알림을 설정하고 받을 수 있는 통합 경험을 제공합니다. 개별 모델을 추가로 검사하고 시간 경과에 따라 모델 성능에 영향을 미치는 요인을 분석할 수 있습니다. 그런 다음 ML 실무자와 함께 시정 조치를 취할 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker 모델 대시보드는 어떤 기능을 제공하나요?, answer : SageMaker 모델 대시보드는 배포된 모델과 엔드포인트에 대한 전반적인 개요 정보를 제공하므로, 단일 창에서 리소스를 추적하고 동작 위반을 모델링할 수 있습니다. 대시보드에서는 4가지 차원으로 모델 동작을 모니터링할 수 있습니다. 예를 들어 데이터 및 모델 품질로 모니터링하거나 SageMaker Model Monitor 및 SageMaker Clarify와 통합할 경우 바이어스 및 특성 기여도 드리프트로 모니터링할 수 있습니다. 또한 SageMaker 모델 대시보드는 누락되고 비활성화된 모델 모니터링 작업과 모델 품질, 데이터 품질, 바이어스 드리프트 및 특성 기여도 드리프트에 대한 모델 동작의 편차에 대한 알림을 설정하고 받을 수 있는 통합 경험을 제공합니다. 개별 모델을 추가로 검사하고 시간 경과에 따라 모델 성능에 영향을 미치는 요인을 분석할 수 있습니다. 그런 다음 ML 실무자와 함께 시정 조치를 취할 수 있습니다.\"\n",
            "SageMaker를 빠르게 시작하는 방법은 무엇인가요?\n",
            "SageMaker JumpStart를 사용하면 ML을 빠르고 쉽게 시작할 수 있습니다. SageMaker JumpStart는 몇 단계로 손쉽게 배포할 수 있는, 가장 일반적인 사용 사례를 위한 솔루션 세트를 제공합니다. 솔루션은 모든 사용자 지정이 가능하며 AWS CloudFormation 템플릿과 참조 아키텍처 사용을 보여주므로 ML 여정의 속도를 높일 수 있습니다. 또한 SageMaker JumpStart는 기초 모델을 제공하고 한 단계로 배포와 변환 도구, 객체 탐지 및 이미지 분류 모델과 같은 150개가 넘는 널리 사용되는 오픈 소스 모델에 대한 세분화된 튜닝도 지원합니다.\n",
            "\"category : SageMaker, question : SageMaker를 빠르게 시작하는 방법은 무엇인가요?, answer : SageMaker JumpStart를 사용하면 ML을 빠르고 쉽게 시작할 수 있습니다. SageMaker JumpStart는 몇 단계로 손쉽게 배포할 수 있는, 가장 일반적인 사용 사례를 위한 솔루션 세트를 제공합니다. 솔루션은 모든 사용자 지정이 가능하며 AWS CloudFormation 템플릿과 참조 아키텍처 사용을 보여주므로 ML 여정의 속도를 높일 수 있습니다. 또한 SageMaker JumpStart는 기초 모델을 제공하고 한 단계로 배포와 변환 도구, 객체 탐지 및 이미지 분류 모델과 같은 150개가 넘는 널리 사용되는 오픈 소스 모델에 대한 세분화된 튜닝도 지원합니다.\"\n",
            "SageMaker JumpStart에서 사용할 수 있는 파운데이션 모델에는 어떤 것이 있나요?\n",
            "SageMaker JumpStart는 독점 모델과 공개 모델을 제공합니다. 사용 가능한 파운데이션 모델의 목록은 Amazon SageMaker JumpStart 시작하기를 참조하세요.\n",
            "\"category : SageMaker, question : SageMaker JumpStart에서 사용할 수 있는 파운데이션 모델에는 어떤 것이 있나요?, answer : SageMaker JumpStart는 독점 모델과 공개 모델을 제공합니다. 사용 가능한 파운데이션 모델의 목록은 Amazon SageMaker JumpStart 시작하기를 참조하세요.\"\n",
            "SageMaker JumpStart에서 파운데이션 모델을 시작하려면 어떻게 해야 하나요?\n",
            "SageMaker Studio, SageMaker SDK 및 AWS Management Console을 통해 기초 모델에 액세스할 수 있습니다. 독점 파운데이션 모델을 시작하려면 AWS Marketplace의 판매 약관에 동의해야 합니다.\n",
            "\"category : SageMaker, question : SageMaker JumpStart에서 파운데이션 모델을 시작하려면 어떻게 해야 하나요?, answer : SageMaker Studio, SageMaker SDK 및 AWS Management Console을 통해 기초 모델에 액세스할 수 있습니다. 독점 파운데이션 모델을 시작하려면 AWS Marketplace의 판매 약관에 동의해야 합니다.\"\n",
            "SageMaker JumpStart를 사용하는 고객에게 제공되는 기본 모델을 업데이트하는 데 내 데이터가 사용되거나 공유되나요?\n",
            "아니요. 추론 및 훈련 데이터는 SageMaker JumpStart가 고객에게 제공하는 기본 모델의 업데이트 또는 훈련에 사용되거나 공유되지 않습니다.\n",
            "\"category : SageMaker, question : SageMaker JumpStart를 사용하는 고객에게 제공되는 기본 모델을 업데이트하는 데 내 데이터가 사용되거나 공유되나요?, answer : 아니요. 추론 및 훈련 데이터는 SageMaker JumpStart가 고객에게 제공하는 기본 모델의 업데이트 또는 훈련에 사용되거나 공유되지 않습니다.\"\n",
            "SageMaker JumpStart를 통해 독점 모델의 모델 가중치 및 스크립트를 볼 수 있나요?\n",
            "아니요. 독점 모델에서는 모델 가중치 및 스크립트를 볼 수 없습니다.\n",
            "\"category : SageMaker, question : SageMaker JumpStart를 통해 독점 모델의 모델 가중치 및 스크립트를 볼 수 있나요?, answer : 아니요. 독점 모델에서는 모델 가중치 및 스크립트를 볼 수 없습니다.\"\n",
            "SageMaker JumpStart 파운데이션 모델은 어느 리전에서 사용할 수 있나요?\n",
            "SageMaker Studio를 사용할 수 있는 모든 리전에서 모델을 검색할 수 있지만 모델 배포 기능은 필요한 인스턴스 유형의 모델 및 인스턴스 가용성에 따라 다릅니다. AWS Marketplace의 모델 세부 정보 페이지에서 AWS 리전 가용성 및 필요한 인스턴스를 참조할 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker JumpStart 파운데이션 모델은 어느 리전에서 사용할 수 있나요?, answer : SageMaker Studio를 사용할 수 있는 모든 리전에서 모델을 검색할 수 있지만 모델 배포 기능은 필요한 인스턴스 유형의 모델 및 인스턴스 가용성에 따라 다릅니다. AWS Marketplace의 모델 세부 정보 페이지에서 AWS 리전 가용성 및 필요한 인스턴스를 참조할 수 있습니다.\"\n",
            "SageMaker JumpStart 파운데이션 모델의 요금은 어떻게 부과되나요?\n",
            "독점 모델의 경우 모델 제공자가 결정한 소프트웨어 요금과 사용된 인스턴스를 기준으로 한 SageMaker 인프라 요금이 부과됩니다. 공개적으로 사용 가능한 모델의 경우 사용된 인스턴스를 기준으로 한 SageMaker 인프라 요금이 부과됩니다. 자세한 내용은 Amazon SageMaker 요금 및 AWS Marketplace를 참조하세요.\n",
            "\"category : SageMaker, question : SageMaker JumpStart 파운데이션 모델의 요금은 어떻게 부과되나요?, answer : 독점 모델의 경우 모델 제공자가 결정한 소프트웨어 요금과 사용된 인스턴스를 기준으로 한 SageMaker 인프라 요금이 부과됩니다. 공개적으로 사용 가능한 모델의 경우 사용된 인스턴스를 기준으로 한 SageMaker 인프라 요금이 부과됩니다. 자세한 내용은 Amazon SageMaker 요금 및 AWS Marketplace를 참조하세요.\"\n",
            "SageMaker JumpStart는 데이터의 보호 및 보안에 어떤 도움이 되나요?\n",
            "보안은 AWS의 최우선 과제이며 SageMaker JumpStart는 안전하도록 설계되었습니다. 따라서 SageMaker에서는 콘텐츠를 저장할 장소를 결정하고, 저장 콘텐츠 및 전송 콘텐츠를 보호하며, AWS 서비스와 리소스에 대한 사용자의 액세스를 관리할 수 있는 단순하지만 강력한 도구를 통해 고객에게 콘텐츠에 대한 소유권과 제어권을 제공합니다.\n",
            "\n",
            "AWS는 고객의 훈련 및 추론 정보를 AWS Marketplace의 모델 판매자와 공유하지 않습니다. 마찬가지로 판매자의 모델 아티팩트(예: 모델 가중치)는 구매자와 공유되지 않습니다.\n",
            "SageMaker JumpStart는 고객의 모델, 훈련 데이터 또는 알고리즘을 사용하여 서비스를 개선하지 않으며 고객의 훈련 및 추론 데이터를 서드 파티와 공유하지 않습니다.\n",
            "SageMaker JumpStart에서는 ML 모델 아티팩트가 전송 및 저장 중에 암호화됩니다.\n",
            "AWS Shared Responsibility Model에 따라 AWS는 AWS의 모든 것이 실행되는 글로벌 인프라를 보호할 책임이 있습니다. 이 인프라에서 호스팅되는 콘텐츠에 대한 제어를 유지할 책임은 사용자에게 있습니다.\n",
            "\n",
            "AWS Marketplace 또는 SageMaker JumpStart의 모델을 사용할 때 사용자는 모델 출력 품질에 대한 책임을 지고 개별 모델 설명에 설명된 기능 및 한계를 인정합니다.\n",
            "\"category : SageMaker, question : SageMaker JumpStart는 데이터의 보호 및 보안에 어떤 도움이 되나요?, answer : 보안은 AWS의 최우선 과제이며 SageMaker JumpStart는 안전하도록 설계되었습니다. 따라서 SageMaker에서는 콘텐츠를 저장할 장소를 결정하고, 저장 콘텐츠 및 전송 콘텐츠를 보호하며, AWS 서비스와 리소스에 대한 사용자의 액세스를 관리할 수 있는 단순하지만 강력한 도구를 통해 고객에게 콘텐츠에 대한 소유권과 제어권을 제공합니다.\n",
            "\n",
            "AWS는 고객의 훈련 및 추론 정보를 AWS Marketplace의 모델 판매자와 공유하지 않습니다. 마찬가지로 판매자의 모델 아티팩트(예: 모델 가중치)는 구매자와 공유되지 않습니다.\n",
            "SageMaker JumpStart는 고객의 모델, 훈련 데이터 또는 알고리즘을 사용하여 서비스를 개선하지 않으며 고객의 훈련 및 추론 데이터를 서드 파티와 공유하지 않습니다.\n",
            "SageMaker JumpStart에서는 ML 모델 아티팩트가 전송 및 저장 중에 암호화됩니다.\n",
            "AWS Shared Responsibility Model에 따라 AWS는 AWS의 모든 것이 실행되는 글로벌 인프라를 보호할 책임이 있습니다. 이 인프라에서 호스팅되는 콘텐츠에 대한 제어를 유지할 책임은 사용자에게 있습니다.\n",
            "\n",
            "AWS Marketplace 또는 SageMaker JumpStart의 모델을 사용할 때 사용자는 모델 출력 품질에 대한 책임을 지고 개별 모델 설명에 설명된 기능 및 한계를 인정합니다.\"\n",
            "SageMaker JumpStart에서 지원하는 오픈 소스 모델로는 어떤 것이 있나요?\n",
            "SageMaker JumpStart에는 PyTorch Hub 및 TensorFlow Hub의 사전 훈련된 150개 이상의 오픈 소스 모델이 포함되어 있습니다. 이미지 분류 및 객체 탐지와 같은 비전 태스크의 경우 RESNET, MobileNet, Single-Shot Detector(SSD)와 같은 모델을 사용할 수 있습니다. 문장 분류, 텍스트 분류, 질문 답변과 같은 텍스트 태스크의 경우 BERT, RoBERTa, DistilBERT와 같은 모델을 사용할 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker JumpStart에서 지원하는 오픈 소스 모델로는 어떤 것이 있나요?, answer : SageMaker JumpStart에는 PyTorch Hub 및 TensorFlow Hub의 사전 훈련된 150개 이상의 오픈 소스 모델이 포함되어 있습니다. 이미지 분류 및 객체 탐지와 같은 비전 태스크의 경우 RESNET, MobileNet, Single-Shot Detector(SSD)와 같은 모델을 사용할 수 있습니다. 문장 분류, 텍스트 분류, 질문 답변과 같은 텍스트 태스크의 경우 BERT, RoBERTa, DistilBERT와 같은 모델을 사용할 수 있습니다.\"\n",
            "ML 아티팩트를 조직 내의 다른 사용자와 공유하려면 어떻게 해야 하나요?\n",
            "SageMaker JumpStart를 사용하면 노트북 및 모델을 포함한 ML 아티팩트를 조직 내에서 손쉽게 공유할 수 있습니다. 관리자는 정의된 사용자 집합이 액세스할 수 있는 리포지토리를 설정할 수 있습니다. 리포지토리 액세스 권한이 있는 모든 사용자는 모델 및 노트북과 SageMaker JumpStart 내부의 공개 콘텐츠를 찾아보고 검색하고 사용할 수 있습니다. 사용자는 SageMaker JumpStart에서 아티팩트를 선택하여 모델을 훈련하고 엔드포인트를 배포하며 노트북을 실행할 수 있습니다.\n",
            "\"category : SageMaker, question : ML 아티팩트를 조직 내의 다른 사용자와 공유하려면 어떻게 해야 하나요?, answer : SageMaker JumpStart를 사용하면 노트북 및 모델을 포함한 ML 아티팩트를 조직 내에서 손쉽게 공유할 수 있습니다. 관리자는 정의된 사용자 집합이 액세스할 수 있는 리포지토리를 설정할 수 있습니다. 리포지토리 액세스 권한이 있는 모든 사용자는 모델 및 노트북과 SageMaker JumpStart 내부의 공개 콘텐츠를 찾아보고 검색하고 사용할 수 있습니다. 사용자는 SageMaker JumpStart에서 아티팩트를 선택하여 모델을 훈련하고 엔드포인트를 배포하며 노트북을 실행할 수 있습니다.\"\n",
            "조직 내의 다른 사용자와 ML 아티팩트를 공유할 때 SageMaker JumpStart를 사용해야 하는 이유는 무엇인가요?\n",
            "SageMaker JumpStart를 사용하면 구축하는 ML 애플리케이션의 출시를 가속화할 수 있습니다. 조직 내에서 한 팀이 구축한 모델 및 노트북을 단 몇 단계로 손쉽게 조직 내의 다른 팀과 공유할 수 있습니다. 내부에서 지식을 공유하고 자산을 재사용하면 조직의 생산성이 크게 개선될 수 있습니다.\n",
            "\"category : SageMaker, question : 조직 내의 다른 사용자와 ML 아티팩트를 공유할 때 SageMaker JumpStart를 사용해야 하는 이유는 무엇인가요?, answer : SageMaker JumpStart를 사용하면 구축하는 ML 애플리케이션의 출시를 가속화할 수 있습니다. 조직 내에서 한 팀이 구축한 모델 및 노트북을 단 몇 단계로 손쉽게 조직 내의 다른 팀과 공유할 수 있습니다. 내부에서 지식을 공유하고 자산을 재사용하면 조직의 생산성이 크게 개선될 수 있습니다.\"\n",
            "파운데이션 모델을 평가하고 선택하려면 어떻게 해야 하나요?\n",
            "이제 Amazon SageMaker Clarify에서 파운데이션 모델 평가가 지원됩니다. 특정 사용 사례에 가장 적합한 파운데이션 모델을 평가, 비교, 선택할 수 있습니다. 주어진 태스크(예: 질문 답변 또는 콘텐츠 요약)에 대해 평가하려는 모델을 선택하기만 하면 됩니다. 그런 다음 평가 기준(예: 정확성, 공정성, 견고성)을 선택하고 자체 프롬프트 데이터 세트를 업로드하거나 공개적으로 사용 가능한 기본 제공 데이터 세트 중에서 선택합니다. 정교한 인간의 판단이 필요한 주관적 기준 또는 미묘한 콘텐츠의 경우 자체 인력을 활용하거나 AWS에서 제공하는 관리형 인력을 사용하여 응답을 검토할 수 있습니다. 설정 프로세스를 마치면 SageMaker Clarify에서 평가가 실행되고 보고서가 생성되므로 주요 기준에 대한 모델의 성능을 쉽게 이해할 수 있습니다. 평가 마법사를 사용하여 SageMaker JumpStart의 파운데이션 모델을 평가하거나 오픈 소스 라이브러리를 사용하여 AWS에서 호스팅되지 않는 모든 파운데이션 모델을 평가할 수 있습니다.\n",
            "\"category : SageMaker, question : 파운데이션 모델을 평가하고 선택하려면 어떻게 해야 하나요?, answer : 이제 Amazon SageMaker Clarify에서 파운데이션 모델 평가가 지원됩니다. 특정 사용 사례에 가장 적합한 파운데이션 모델을 평가, 비교, 선택할 수 있습니다. 주어진 태스크(예: 질문 답변 또는 콘텐츠 요약)에 대해 평가하려는 모델을 선택하기만 하면 됩니다. 그런 다음 평가 기준(예: 정확성, 공정성, 견고성)을 선택하고 자체 프롬프트 데이터 세트를 업로드하거나 공개적으로 사용 가능한 기본 제공 데이터 세트 중에서 선택합니다. 정교한 인간의 판단이 필요한 주관적 기준 또는 미묘한 콘텐츠의 경우 자체 인력을 활용하거나 AWS에서 제공하는 관리형 인력을 사용하여 응답을 검토할 수 있습니다. 설정 프로세스를 마치면 SageMaker Clarify에서 평가가 실행되고 보고서가 생성되므로 주요 기준에 대한 모델의 성능을 쉽게 이해할 수 있습니다. 평가 마법사를 사용하여 SageMaker JumpStart의 파운데이션 모델을 평가하거나 오픈 소스 라이브러리를 사용하여 AWS에서 호스팅되지 않는 모든 파운데이션 모델을 평가할 수 있습니다.\"\n",
            "Amazon SageMaker Autopilot이란 무엇인가요?\n",
            "SageMaker Autopilot은 업계 최초의 자동화된 기계 학습 기능으로, ML 모델을 완벽하게 제어하고 파악할 수 있는 기능을 제공합니다. SageMaker Autopilot은 원시 데이터를 자동으로 검사하고, 특성 프로세서를 적용하며, 최상의 알고리즘 세트를 선택하고, 여러 가지 모델을 훈련 및 튜닝하며, 해당 성능을 추적한 다음 성능을 기준으로 모델 등급을 지정합니다. 이 모든 작업이 클릭 몇 번으로 수행됩니다. 이 작업의 결과로 최고 성능의 모델이 나오는데, 일반적으로 모델 훈련에 필요한 시간에 비해 아주 짧은 시간으로 모델을 배포할 수 있습니다. 모델이 생성된 방식과 모델에 포함된 내용을 완벽하게 파악할 수 있으며 SageMaker Autopilot을 SageMaker Studio와 통합할 수 있습니다. SageMaker Studio 내에서 SageMaker Autopilot으로 생성된 서로 다른 모델을 50개까지 탐색할 수 있으므로 사용 사례에 가장 적합한 모델을 쉽게 선택할 수 있습니다. SageMaker Autopilot을 사용하면 ML 경험이 없는 사람도 모델을 쉽게 만들 수 있고, 숙련된 개발자라면 팀에서 나중에 반복해서 사용할 수 있는 기준 모델을 개발할 수 있습니다.\n",
            "\"category : SageMaker, question : Amazon SageMaker Autopilot이란 무엇인가요?, answer : SageMaker Autopilot은 업계 최초의 자동화된 기계 학습 기능으로, ML 모델을 완벽하게 제어하고 파악할 수 있는 기능을 제공합니다. SageMaker Autopilot은 원시 데이터를 자동으로 검사하고, 특성 프로세서를 적용하며, 최상의 알고리즘 세트를 선택하고, 여러 가지 모델을 훈련 및 튜닝하며, 해당 성능을 추적한 다음 성능을 기준으로 모델 등급을 지정합니다. 이 모든 작업이 클릭 몇 번으로 수행됩니다. 이 작업의 결과로 최고 성능의 모델이 나오는데, 일반적으로 모델 훈련에 필요한 시간에 비해 아주 짧은 시간으로 모델을 배포할 수 있습니다. 모델이 생성된 방식과 모델에 포함된 내용을 완벽하게 파악할 수 있으며 SageMaker Autopilot을 SageMaker Studio와 통합할 수 있습니다. SageMaker Studio 내에서 SageMaker Autopilot으로 생성된 서로 다른 모델을 50개까지 탐색할 수 있으므로 사용 사례에 가장 적합한 모델을 쉽게 선택할 수 있습니다. SageMaker Autopilot을 사용하면 ML 경험이 없는 사람도 모델을 쉽게 만들 수 있고, 숙련된 개발자라면 팀에서 나중에 반복해서 사용할 수 있는 기준 모델을 개발할 수 있습니다.\"\n",
            "SageMaker Autopilot 작업을 수동으로 중지할 수 있나요?\n",
            "예. 언제든지 작업을 중지할 수 있습니다. SageMaker Autopilot 작업이 중지되면 진행 중인 모든 실험이 중지되고 새 실험이 시작되지 않습니다.\n",
            "\"category : SageMaker, question : SageMaker Autopilot 작업을 수동으로 중지할 수 있나요?, answer : 예. 언제든지 작업을 중지할 수 있습니다. SageMaker Autopilot 작업이 중지되면 진행 중인 모든 실험이 중지되고 새 실험이 시작되지 않습니다.\"\n",
            "Amazon SageMaker Canvas란 무엇인가요?\n",
            "SageMaker Canvas는 마우스 클릭 방식의 직관적인 인터페이스를 제공하는 노코드 서비스로, 데이터에서 매우 정확한 ML 기반 예측을 생성하는 데 사용할 수 있습니다. SageMaker Canvas를 사용하면 드래그 앤 드롭 방식의 사용자 인터페이스를 사용하여 다양한 소스의 데이터에 액세스하고 이러한 데이터를 결합할 수 있습니다. 데이터가 자동으로 정리되고 준비되므로 수동 정리 작업이 최소화됩니다. SageMaker Canvas는 다양한 최첨단 ML 알고리즘을 적용하여 고도로 정확한 예측 모델을 찾으며, 예측을 수행할 수 있는 직관적인 인터페이스를 제공합니다. SageMaker Canvas를 사용하면 다양한 비즈니스 애플리케이션에서 훨씬 더 정확한 예측을 수행하고 모델, 데이터 및 보고서를 공유하여 기업의 데이터 사이언티스트 및 분석가와 쉽게 협업할 수 있습니다. SageMaker Canvas에 대해 자세히 알아보려면 Amazon SageMaker Canvas FAQ를 참조하세요.\n",
            "\"category : SageMaker, question : Amazon SageMaker Canvas란 무엇인가요?, answer : SageMaker Canvas는 마우스 클릭 방식의 직관적인 인터페이스를 제공하는 노코드 서비스로, 데이터에서 매우 정확한 ML 기반 예측을 생성하는 데 사용할 수 있습니다. SageMaker Canvas를 사용하면 드래그 앤 드롭 방식의 사용자 인터페이스를 사용하여 다양한 소스의 데이터에 액세스하고 이러한 데이터를 결합할 수 있습니다. 데이터가 자동으로 정리되고 준비되므로 수동 정리 작업이 최소화됩니다. SageMaker Canvas는 다양한 최첨단 ML 알고리즘을 적용하여 고도로 정확한 예측 모델을 찾으며, 예측을 수행할 수 있는 직관적인 인터페이스를 제공합니다. SageMaker Canvas를 사용하면 다양한 비즈니스 애플리케이션에서 훨씬 더 정확한 예측을 수행하고 모델, 데이터 및 보고서를 공유하여 기업의 데이터 사이언티스트 및 분석가와 쉽게 협업할 수 있습니다. SageMaker Canvas에 대해 자세히 알아보려면 Amazon SageMaker Canvas FAQ를 참조하세요.\"\n",
            "SageMaker Canvas 요금은 어떻게 부과되나요?\n",
            "SageMaker Canvas 요금은 사용량에 따라 부과됩니다. SageMaker Canvas를 사용하면 여러 소스의 데이터를 대화형으로 수집, 탐색 및 준비하고, 데이터를 사용하여 고도로 정확한 ML 모델을 훈련하고 예측을 생성할 수 있습니다. 요금은 2가지 구성 요소로 결정됩니다. 세션 요금은 SageMaker Canvas를 사용하거나 로그인한 시간에 따라 부과되며 모델 훈련 요금은 모델 구축 시 사용한 데이터 세트의 크기에 따라 부과됩니다. 자세한 내용은 Amazon SageMaker Canvas 요금을 참조하세요.\n",
            "\"category : SageMaker, question : SageMaker Canvas 요금은 어떻게 부과되나요?, answer : SageMaker Canvas 요금은 사용량에 따라 부과됩니다. SageMaker Canvas를 사용하면 여러 소스의 데이터를 대화형으로 수집, 탐색 및 준비하고, 데이터를 사용하여 고도로 정확한 ML 모델을 훈련하고 예측을 생성할 수 있습니다. 요금은 2가지 구성 요소로 결정됩니다. 세션 요금은 SageMaker Canvas를 사용하거나 로그인한 시간에 따라 부과되며 모델 훈련 요금은 모델 구축 시 사용한 데이터 세트의 크기에 따라 부과됩니다. 자세한 내용은 Amazon SageMaker Canvas 요금을 참조하세요.\"\n",
            "SageMaker를 사용하여 지속적 통합 및 전송(CI/CD) 파이프라인을 구축하려면 어떻게 해야 하나요?\n",
            "Amazon SageMaker Pipelines는 데이터 준비에서 모델 배포에 이르는 완전 자동화된 ML 워크플로를 생성하여 프로덕션 환경에서 수천 개의 ML 모델로 확장하는 데 도움이 됩니다. SageMaker Pipelines는 SageMaker Studio에 연결되는 Python SDK도 함께 제공하므로, 시각적 인터페이스를 활용하여 워크플로의 각 단계를 구축할 수 있습니다. 그런 다음, 단일 API를 사용하여 각 단계를 연결하고 완전한 워크플로를 생성할 수 있습니다. SageMaker Pipelines는 단계 사이에서 데이터를 관리하고, 코드 레시피를 패키지로 작성하며, 실행을 조정합니다. 따라서 수개월이 걸리던 코딩 작업이 몇 시간으로 줄어듭니다. 워크플로를 실행할 때마다 수행되는 작업과 처리되는 데이터의 전체 레코드가 유지되므로, 데이터 사이언티스트와 기계 학습 개발자는 문제를 빠르게 디버깅할 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker를 사용하여 지속적 통합 및 전송(CI/CD) 파이프라인을 구축하려면 어떻게 해야 하나요?, answer : Amazon SageMaker Pipelines는 데이터 준비에서 모델 배포에 이르는 완전 자동화된 ML 워크플로를 생성하여 프로덕션 환경에서 수천 개의 ML 모델로 확장하는 데 도움이 됩니다. SageMaker Pipelines는 SageMaker Studio에 연결되는 Python SDK도 함께 제공하므로, 시각적 인터페이스를 활용하여 워크플로의 각 단계를 구축할 수 있습니다. 그런 다음, 단일 API를 사용하여 각 단계를 연결하고 완전한 워크플로를 생성할 수 있습니다. SageMaker Pipelines는 단계 사이에서 데이터를 관리하고, 코드 레시피를 패키지로 작성하며, 실행을 조정합니다. 따라서 수개월이 걸리던 코딩 작업이 몇 시간으로 줄어듭니다. 워크플로를 실행할 때마다 수행되는 작업과 처리되는 데이터의 전체 레코드가 유지되므로, 데이터 사이언티스트와 기계 학습 개발자는 문제를 빠르게 디버깅할 수 있습니다.\"\n",
            "프로덕션으로 이전할 최상의 모델을 선택하기 위해 훈련된 모든 모델을 보려면 어떻게 해야 하나요?\n",
            "SageMaker Pipelines는 모델 레지스트리라고 하는 훈련된 모델의 중앙 리포지토리를 제공합니다. Python SDK를 통해 프로그래밍 방식으로 또는 SageMaker Studio를 통해 시각적으로 모델을 검색하고 모델 레지스트리에 액세스할 수 있으므로 프로덕션에 배포할 원하는 모델을 간편하게 선택할 수 있습니다.\n",
            "\"category : SageMaker, question : 프로덕션으로 이전할 최상의 모델을 선택하기 위해 훈련된 모든 모델을 보려면 어떻게 해야 하나요?, answer : SageMaker Pipelines는 모델 레지스트리라고 하는 훈련된 모델의 중앙 리포지토리를 제공합니다. Python SDK를 통해 프로그래밍 방식으로 또는 SageMaker Studio를 통해 시각적으로 모델을 검색하고 모델 레지스트리에 액세스할 수 있으므로 프로덕션에 배포할 원하는 모델을 간편하게 선택할 수 있습니다.\"\n",
            "SageMaker Pipelines에 추가할 수 있는 SageMaker 구성 요소는 무엇인가요?\n",
            "SageMaker Clarify, Amazon SageMaker Data Wrangler, Amazon SageMaker 특성 저장소, Amazon SageMaker Experiments, Amazon SageMaker Debugger 및 Amazon SageMaker Model Monitor 등 SageMaker Studio를 통해 사용 가능한 구성 요소를 SageMaker Pipelines에 추가할 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker Pipelines에 추가할 수 있는 SageMaker 구성 요소는 무엇인가요?, answer : SageMaker Clarify, Amazon SageMaker Data Wrangler, Amazon SageMaker 특성 저장소, Amazon SageMaker Experiments, Amazon SageMaker Debugger 및 Amazon SageMaker Model Monitor 등 SageMaker Studio를 통해 사용 가능한 구성 요소를 SageMaker Pipelines에 추가할 수 있습니다.\"\n",
            "전체 ML 워크플로에서 모델 구성 요소를 추적하려면 어떻게 해야 하나요?\n",
            "SageMaker Pipelines는 모든 모델 구성 요소를 자동으로 추적하고 모든 변경 사항에 대한 감사 추적을 유지하므로 수동 추적은 필요하지 않으며 규제 준수 목표를 달성하는 데 도움이 될 수 있습니다. SageMaker Pipelines에서는 데이터, 코드, 훈련된 모델 등을 추적할 수 있습니다.\n",
            "\"category : SageMaker, question : 전체 ML 워크플로에서 모델 구성 요소를 추적하려면 어떻게 해야 하나요?, answer : SageMaker Pipelines는 모든 모델 구성 요소를 자동으로 추적하고 모든 변경 사항에 대한 감사 추적을 유지하므로 수동 추적은 필요하지 않으며 규제 준수 목표를 달성하는 데 도움이 될 수 있습니다. SageMaker Pipelines에서는 데이터, 코드, 훈련된 모델 등을 추적할 수 있습니다.\"\n",
            "SageMaker Pipelines 요금은 어떻게 부과되나요?\n",
            "SageMaker Pipelines 사용에 대한 추가 비용은 없습니다. SageMaker Pipelines 내에서 사용하는 개별 AWS 서비스 또는 기본 컴퓨팅에 대한 비용만 지불하면 됩니다.\n",
            "\"category : SageMaker, question : SageMaker Pipelines 요금은 어떻게 부과되나요?, answer : SageMaker Pipelines 사용에 대한 추가 비용은 없습니다. SageMaker Pipelines 내에서 사용하는 개별 AWS 서비스 또는 기본 컴퓨팅에 대한 비용만 지불하면 됩니다.\"\n",
            "SageMaker에서 Kubeflow를 사용할 수 있나요?\n",
            "예. Amazon SageMaker Components for Kubeflow Pipelines는 Kubeflow Pipelines를 사용하여 ML 워크플로를 정의하고 SageMaker를 사용하여 데이터 레이블링, 훈련 및 추론 단계를 수행할 수 있도록 하는 오픈 소스 플러그인입니다. Kubeflow Pipelines는 확장 가능하고 이식 가능하며 완전한 ML 파이프라인을 구축 및 배포할 수 있는 Kubeflow 추가 기능입니다. 그러나 Kubeflow Pipelines를 사용할 때 ML 운영 팀은 Kubernetes 클러스터의 CPU 및 GPU 인스턴스를 관리하고 사용률을 계속 높게 유지하여 운영 비용을 절감해야 합니다. 데이터 과학 팀 전체에서 클러스터의 사용률을 극대화하는 것은 어려운 일이며 ML 운영 팀에 추가적인 운영 오버헤드가 발생합니다. ML 최적화 Kubernetes 클러스터 대신, SageMaker Components for Kubeflow Pipelines를 사용하면 기계 학습 작업을 실행하기 위해 Kubernetes 클러스터를 특별히 구성 및 관리할 필요 없이 데이터 레이블링, 완전관리형 대규모 하이퍼파라미터 튜닝 및 분산 훈련 작업, 안전하고 확장 가능한 원클릭 모델 배포, Amazon Elastic Compute Cloud(Amazon EC2) 스팟 인스턴스를 통한 비용 효율적인 훈련과 같은 강력한 SageMaker 기능을 활용할 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker에서 Kubeflow를 사용할 수 있나요?, answer : 예. Amazon SageMaker Components for Kubeflow Pipelines는 Kubeflow Pipelines를 사용하여 ML 워크플로를 정의하고 SageMaker를 사용하여 데이터 레이블링, 훈련 및 추론 단계를 수행할 수 있도록 하는 오픈 소스 플러그인입니다. Kubeflow Pipelines는 확장 가능하고 이식 가능하며 완전한 ML 파이프라인을 구축 및 배포할 수 있는 Kubeflow 추가 기능입니다. 그러나 Kubeflow Pipelines를 사용할 때 ML 운영 팀은 Kubernetes 클러스터의 CPU 및 GPU 인스턴스를 관리하고 사용률을 계속 높게 유지하여 운영 비용을 절감해야 합니다. 데이터 과학 팀 전체에서 클러스터의 사용률을 극대화하는 것은 어려운 일이며 ML 운영 팀에 추가적인 운영 오버헤드가 발생합니다. ML 최적화 Kubernetes 클러스터 대신, SageMaker Components for Kubeflow Pipelines를 사용하면 기계 학습 작업을 실행하기 위해 Kubernetes 클러스터를 특별히 구성 및 관리할 필요 없이 데이터 레이블링, 완전관리형 대규모 하이퍼파라미터 튜닝 및 분산 훈련 작업, 안전하고 확장 가능한 원클릭 모델 배포, Amazon Elastic Compute Cloud(Amazon EC2) 스팟 인스턴스를 통한 비용 효율적인 훈련과 같은 강력한 SageMaker 기능을 활용할 수 있습니다.\"\n",
            "SageMaker Components for Kubeflow Pipelines 요금은 어떻게 부과되나요?\n",
            "SageMaker Components for Kubeflow Pipelines 사용에 대한 추가 비용은 없습니다.\n",
            "\"category : SageMaker, question : SageMaker Components for Kubeflow Pipelines 요금은 어떻게 부과되나요?, answer : SageMaker Components for Kubeflow Pipelines 사용에 대한 추가 비용은 없습니다.\"\n",
            "SageMaker에서는 ML용 데이터를 어떻게 준비하나요?\n",
            "SageMaker Data Wrangler는 ML용 데이터를 집계하고 준비하는 시간을 줄여줍니다. SageMaker Studio의 단일 인터페이스를 통해 단 몇 단계로 Amazon S3, Amazon Athena, Amazon Redshift, AWS Lake Formation, Amazon SageMaker 특성 저장소 및 Snowflake에서 데이터를 찾아서 가져올 수 있습니다. 40개 이상의 데이터 소스에서 전송되어 Amazon AppFlow를 통해 AWS Glue 데이터 카탈로그에 등록된 데이터를 쿼리하고 가져올 수도 있습니다. SageMaker Data Wrangler는 원시 데이터를 자동으로 로드하고 집계한 후 표시합니다. 데이터를 SageMaker Data Wrangler로 가져오면 자동으로 생성된 열 요약 및 히스토그램이 표시됩니다. SageMaker Data Wrangler의 데이터 품질 및 인사이트 보고서를 사용하면 제공된 요약 통계 및 데이터 품질 경고를 바탕으로 데이터를 더 자세히 살펴보고 잠재적 오류를 식별할 수 있습니다. SageMaker Clarify에서 지원하는 바이어스 분석을 SageMaker Data Wrangler에서 직접 실행하여 데이터 준비 중에 잠재적 바이어스를 감지할 수도 있습니다. 거기에서 SageMaker Data Wrangler의 미리 구축된 변환을 사용하여 데이터를 준비할 수 있습니다. 데이터가 준비되면 Amazon SageMaker Pipelines에서 완전히 자동화된 ML 워크플로를 구축하거나 Amazon SageMaker 특성 저장소로 해당 데이터를 가져올 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker에서는 ML용 데이터를 어떻게 준비하나요?, answer : SageMaker Data Wrangler는 ML용 데이터를 집계하고 준비하는 시간을 줄여줍니다. SageMaker Studio의 단일 인터페이스를 통해 단 몇 단계로 Amazon S3, Amazon Athena, Amazon Redshift, AWS Lake Formation, Amazon SageMaker 특성 저장소 및 Snowflake에서 데이터를 찾아서 가져올 수 있습니다. 40개 이상의 데이터 소스에서 전송되어 Amazon AppFlow를 통해 AWS Glue 데이터 카탈로그에 등록된 데이터를 쿼리하고 가져올 수도 있습니다. SageMaker Data Wrangler는 원시 데이터를 자동으로 로드하고 집계한 후 표시합니다. 데이터를 SageMaker Data Wrangler로 가져오면 자동으로 생성된 열 요약 및 히스토그램이 표시됩니다. SageMaker Data Wrangler의 데이터 품질 및 인사이트 보고서를 사용하면 제공된 요약 통계 및 데이터 품질 경고를 바탕으로 데이터를 더 자세히 살펴보고 잠재적 오류를 식별할 수 있습니다. SageMaker Clarify에서 지원하는 바이어스 분석을 SageMaker Data Wrangler에서 직접 실행하여 데이터 준비 중에 잠재적 바이어스를 감지할 수도 있습니다. 거기에서 SageMaker Data Wrangler의 미리 구축된 변환을 사용하여 데이터를 준비할 수 있습니다. 데이터가 준비되면 Amazon SageMaker Pipelines에서 완전히 자동화된 ML 워크플로를 구축하거나 Amazon SageMaker 특성 저장소로 해당 데이터를 가져올 수 있습니다.\"\n",
            "SageMaker Data Wrangler가 지원하는 데이터 유형은 무엇인가요?\n",
            "SageMaker Data Wrangler는 테이블, 시계열, 이미지 데이터를 지원하며 이 다양한 데이터 양식을 준비할 수 있도록 300개 이상의 사전 구성된 데이터 변환을 제공합니다. Data Wrangler는 NLP 사용 사례에 맞게 Data Wrangler에서 텍스트 데이터를 준비하려는 고객을 위해 NLTK 라이브러리를 지원합니다. 따라서 Data Wrangler에서 자체 사용자 지정 변환을 작성하여 텍스트 데이터를 준비할 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker Data Wrangler가 지원하는 데이터 유형은 무엇인가요?, answer : SageMaker Data Wrangler는 테이블, 시계열, 이미지 데이터를 지원하며 이 다양한 데이터 양식을 준비할 수 있도록 300개 이상의 사전 구성된 데이터 변환을 제공합니다. Data Wrangler는 NLP 사용 사례에 맞게 Data Wrangler에서 텍스트 데이터를 준비하려는 고객을 위해 NLTK 라이브러리를 지원합니다. 따라서 Data Wrangler에서 자체 사용자 지정 변환을 작성하여 텍스트 데이터를 준비할 수 있습니다.\"\n",
            "SageMaker Data Wrangler에서 모델 특성을 생성하려면 어떻게 해야 하나요?\n",
            "SageMaker Data Wrangler를 사용하면 코드 작성 없이 데이터를 새 특성으로 자동으로 변환할 수 있습니다. SageMaker Data Wrangler를 사용할 때는 미리 구성된 데이터 변환, 누락 데이터 산입, 원-핫 인코딩, 주성분 분석(PCA)을 사용한 차원 축소와 시계열 관련 변환기를 선택할 수 있습니다. 예를 들어 한 단계로 텍스트 필드 열을 숫자 열로 변환할 수 있습니다. 또한 SageMaker Data Wrangler의 코드 조각 라이브러리를 사용하여 코드 조각을 작성할 수도 있습니다.\n",
            "\"category : SageMaker, question : SageMaker Data Wrangler에서 모델 특성을 생성하려면 어떻게 해야 하나요?, answer : SageMaker Data Wrangler를 사용하면 코드 작성 없이 데이터를 새 특성으로 자동으로 변환할 수 있습니다. SageMaker Data Wrangler를 사용할 때는 미리 구성된 데이터 변환, 누락 데이터 산입, 원-핫 인코딩, 주성분 분석(PCA)을 사용한 차원 축소와 시계열 관련 변환기를 선택할 수 있습니다. 예를 들어 한 단계로 텍스트 필드 열을 숫자 열로 변환할 수 있습니다. 또한 SageMaker Data Wrangler의 코드 조각 라이브러리를 사용하여 코드 조각을 작성할 수도 있습니다.\"\n",
            "SageMaker Data Wrangler에서 데이터를 시각화하려면 어떻게 해야 하나요?\n",
            "SageMaker Data Wrangler는 미리 구성된 강력한 시각화 템플릿 세트를 통해 데이터를 이해하고 잠재적 오류와 극단 값을 식별하는 데 도움이 됩니다. 코드 작성 없이 히스토그램, 산점도 및 ML 특정 시각화(예: 대상 유출 탐지)를 모두 사용할 수 있습니다. 고유한 시각화를 생성하고 편집할 수도 있습니다.\n",
            "\"category : SageMaker, question : SageMaker Data Wrangler에서 데이터를 시각화하려면 어떻게 해야 하나요?, answer : SageMaker Data Wrangler는 미리 구성된 강력한 시각화 템플릿 세트를 통해 데이터를 이해하고 잠재적 오류와 극단 값을 식별하는 데 도움이 됩니다. 코드 작성 없이 히스토그램, 산점도 및 ML 특정 시각화(예: 대상 유출 탐지)를 모두 사용할 수 있습니다. 고유한 시각화를 생성하고 편집할 수도 있습니다.\"\n",
            "SageMaker Data Wrangler 요금은 어떻게 부과되나요?\n",
            "SageMaker Data Wrangler에 대해 사용하는 모든 ML 컴퓨팅, 스토리지 및 데이터 처리 리소스에 대한 요금만 청구됩니다. SageMaker Data Wrangler 요금에 대한 모든 세부 정보는 여기에서 검토할 수 있습니다. AWS 프리 티어를 사용하는 고객은 SageMaker Data Wrangler를 무료로 시작할 수도 있습니다.\n",
            "\"category : SageMaker, question : SageMaker Data Wrangler 요금은 어떻게 부과되나요?, answer : SageMaker Data Wrangler에 대해 사용하는 모든 ML 컴퓨팅, 스토리지 및 데이터 처리 리소스에 대한 요금만 청구됩니다. SageMaker Data Wrangler 요금에 대한 모든 세부 정보는 여기에서 검토할 수 있습니다. AWS 프리 티어를 사용하는 고객은 SageMaker Data Wrangler를 무료로 시작할 수도 있습니다.\"\n",
            "SageMaker Data Wrangler에서 준비된 데이터로 ML 모델을 훈련하려면 어떻게 해야 하나요?\n",
            "SageMaker Data Wrangler는 SageMaker Autopilot에서 데이터를 준비하고 기계 학습 모델을 원활하게 훈련할 수 있는 통합 경험을 제공합니다. SageMaker Autopilot은 데이터를 기반으로 최상의 ML 모델을 자동으로 구축, 훈련 및 튜닝합니다. SageMaker Autopilot을 사용하면 데이터 및 모델의 완벽한 제어와 가시성을 유지 관리할 수 있습니다. SageMaker Data Wrangler에서 준비한 특성을 기존 모델에 사용할 수도 있습니다. SageMaker Data Wrangler 처리 작업을 SageMaker 훈련 파이프라인의 일부로 실행되도록 구성할 수 있습니다. 사용자 인터페이스(UI)에서 작업을 구성하거나 오케스트레이션 코드로 노트북을 내보내면 됩니다.\n",
            "\"category : SageMaker, question : SageMaker Data Wrangler에서 준비된 데이터로 ML 모델을 훈련하려면 어떻게 해야 하나요?, answer : SageMaker Data Wrangler는 SageMaker Autopilot에서 데이터를 준비하고 기계 학습 모델을 원활하게 훈련할 수 있는 통합 경험을 제공합니다. SageMaker Autopilot은 데이터를 기반으로 최상의 ML 모델을 자동으로 구축, 훈련 및 튜닝합니다. SageMaker Autopilot을 사용하면 데이터 및 모델의 완벽한 제어와 가시성을 유지 관리할 수 있습니다. SageMaker Data Wrangler에서 준비한 특성을 기존 모델에 사용할 수도 있습니다. SageMaker Data Wrangler 처리 작업을 SageMaker 훈련 파이프라인의 일부로 실행되도록 구성할 수 있습니다. 사용자 인터페이스(UI)에서 작업을 구성하거나 오케스트레이션 코드로 노트북을 내보내면 됩니다.\"\n",
            "기록 데이터를 기반으로 특성을 준비한 경우 SageMaker Data Wrangler에서 새 데이터는 어떻게 처리되나요?\n",
            "SageMaker Data Wrangler UI에서 직접 SageMaker 처리 작업을 구성하고 시작할 수 있습니다. 예를 들어 데이터 처리 작업을 예약하고 데이터 소스를 파라미터화하여 새 데이터 배치를 대규모로 손쉽게 변환할 수 있습니다.\n",
            "\"category : SageMaker, question : 기록 데이터를 기반으로 특성을 준비한 경우 SageMaker Data Wrangler에서 새 데이터는 어떻게 처리되나요?, answer : SageMaker Data Wrangler UI에서 직접 SageMaker 처리 작업을 구성하고 시작할 수 있습니다. 예를 들어 데이터 처리 작업을 예약하고 데이터 소스를 파라미터화하여 새 데이터 배치를 대규모로 손쉽게 변환할 수 있습니다.\"\n",
            "SageMaker Data Wrangler는 CI/CD 프로세스와 어떻게 연동되나요?\n",
            "데이터가 준비된 후에는 SageMaker Data Wrangler에서 다양한 옵션을 사용하여 SageMaker Data Wrangler 흐름을 프로덕션으로 승격하고 MLOps 및 CI/CD 기능과 원활하게 통합할 수 있습니다. SageMaker Data Wrangler UI에서 직접 SageMaker 처리 작업을 구성하고 시작할 수 있습니다. 예를 들어 데이터 처리 작업을 예약하고 데이터 소스를 파라미터화하여 새 데이터 배치를 대규모로 손쉽게 변환할 수 있습니다. SageMaker Data Wrangler는 SageMaker 처리 및 SageMaker Spark 컨테이너와 원활하게 통합되므로 간편하게 SageMaker SDK를 사용하여 SageMaker Data Wrangler를 프로덕션 워크플로에 통합할 수도 있습니다.\n",
            "\"category : SageMaker, question : SageMaker Data Wrangler는 CI/CD 프로세스와 어떻게 연동되나요?, answer : 데이터가 준비된 후에는 SageMaker Data Wrangler에서 다양한 옵션을 사용하여 SageMaker Data Wrangler 흐름을 프로덕션으로 승격하고 MLOps 및 CI/CD 기능과 원활하게 통합할 수 있습니다. SageMaker Data Wrangler UI에서 직접 SageMaker 처리 작업을 구성하고 시작할 수 있습니다. 예를 들어 데이터 처리 작업을 예약하고 데이터 소스를 파라미터화하여 새 데이터 배치를 대규모로 손쉽게 변환할 수 있습니다. SageMaker Data Wrangler는 SageMaker 처리 및 SageMaker Spark 컨테이너와 원활하게 통합되므로 간편하게 SageMaker SDK를 사용하여 SageMaker Data Wrangler를 프로덕션 워크플로에 통합할 수도 있습니다.\"\n",
            "SageMaker Data Wrangler Quick Model에는 어떤 모델이 사용되나요?\n",
            "SageMaker Data Wrangler에서 몇 단계를 수행하여 기본 하이퍼파라미터로 XGBoost 모델을 분할하고 훈련할 수 있습니다. SageMaker Data Wrangler는 문제 유형에 따라 모델 요약, 특성 요약 및 혼동 행렬을 제공하므로 이러한 인사이트를 빠르게 확인하여 데이터 준비 흐름을 반복할 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker Data Wrangler Quick Model에는 어떤 모델이 사용되나요?, answer : SageMaker Data Wrangler에서 몇 단계를 수행하여 기본 하이퍼파라미터로 XGBoost 모델을 분할하고 훈련할 수 있습니다. SageMaker Data Wrangler는 문제 유형에 따라 모델 요약, 특성 요약 및 혼동 행렬을 제공하므로 이러한 인사이트를 빠르게 확인하여 데이터 준비 흐름을 반복할 수 있습니다.\"\n",
            "SageMaker Data Wrangler가 지원하는 데이터 크기는 얼마인가요?\n",
            "SageMaker Data Wrangler는 데이터 가져오기에 상위 K 샘플링, 무작위 샘플링 및 층화 샘플링 등 다양한 샘플링 기법을 지원합니다. 따라서 SageMaker Data Wrangler의 UI를 사용하여 데이터를 빠르게 변환할 수 있습니다. 크거나 넓은 데이터 세트를 사용하는 경우 SageMaker Data Wrangler 인스턴스 크기를 늘려 성능을 개선할 수 있습니다. 흐름을 생성한 후에는 SageMaker Data Wrangler 처리 작업을 사용하여 전체 데이터 세트를 처리할 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker Data Wrangler가 지원하는 데이터 크기는 얼마인가요?, answer : SageMaker Data Wrangler는 데이터 가져오기에 상위 K 샘플링, 무작위 샘플링 및 층화 샘플링 등 다양한 샘플링 기법을 지원합니다. 따라서 SageMaker Data Wrangler의 UI를 사용하여 데이터를 빠르게 변환할 수 있습니다. 크거나 넓은 데이터 세트를 사용하는 경우 SageMaker Data Wrangler 인스턴스 크기를 늘려 성능을 개선할 수 있습니다. 흐름을 생성한 후에는 SageMaker Data Wrangler 처리 작업을 사용하여 전체 데이터 세트를 처리할 수 있습니다.\"\n",
            "SageMaker Data Wrangler는 SageMaker 특성 저장소와 어떻게 연동되나요?\n",
            "SageMaker 특성 저장소를 SageMaker Data Wrangler에서 준비한 특성의 대상으로 구성할 수 있습니다. UI에서 직접 이 작업을 수행하거나 SageMaker 특성 저장소를 데이터 처리 대상으로 사용하기 위해 특별히 생성된 노트북을 내보낼 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker Data Wrangler는 SageMaker 특성 저장소와 어떻게 연동되나요?, answer : SageMaker 특성 저장소를 SageMaker Data Wrangler에서 준비한 특성의 대상으로 구성할 수 있습니다. UI에서 직접 이 작업을 수행하거나 SageMaker 특성 저장소를 데이터 처리 대상으로 사용하기 위해 특별히 생성된 노트북을 내보낼 수 있습니다.\"\n",
            "기계 학습 모델에 대한 특성을 저장하려면 어떻게 해야 하나요?\n",
            "SageMaker 특성 저장소는 읽기 및 쓰기 지연 시간이 짧은(밀리초) 데이터 특성을 위한 중앙 리포지토리를 제공합니다. SageMaker 특성 저장소를 통해 특성을 저장, 검색, 검색 및 공유할 수 있으므로 보안 액세스 및 제어 기능을 통해 여러 모델과 팀에서 쉽게 재사용할 수 있습니다. SageMaker 특성 저장소는 배치 또는 스트리밍 파이프라인을 통해 생성된 온라인 및 오프라인 특성을 모두 지원합니다. 그리고 특성 다시 채우기를 지원하며, 모델 훈련 및 추론에 사용된 특성 간의 패리티 유지를 위한 온라인 및 오프라인 스토어 모두를 제공합니다.\n",
            "\"category : SageMaker, question : 기계 학습 모델에 대한 특성을 저장하려면 어떻게 해야 하나요?, answer : SageMaker 특성 저장소는 읽기 및 쓰기 지연 시간이 짧은(밀리초) 데이터 특성을 위한 중앙 리포지토리를 제공합니다. SageMaker 특성 저장소를 통해 특성을 저장, 검색, 검색 및 공유할 수 있으므로 보안 액세스 및 제어 기능을 통해 여러 모델과 팀에서 쉽게 재사용할 수 있습니다. SageMaker 특성 저장소는 배치 또는 스트리밍 파이프라인을 통해 생성된 온라인 및 오프라인 특성을 모두 지원합니다. 그리고 특성 다시 채우기를 지원하며, 모델 훈련 및 추론에 사용된 특성 간의 패리티 유지를 위한 온라인 및 오프라인 스토어 모두를 제공합니다.\"\n",
            "온라인 및 오프라인 특성 사이의 일관성을 유지하려면 어떻게 해야 하나요?\n",
            "SageMaker 특성 저장소는 추가 관리나 코드 없이 온라인 특성과 오프라인 특성의 일관성을 자동으로 유지합니다. SageMaker 특성 저장소는 훈련 및 추론 환경에서 일관성을 유지하는 완전관리형 기능입니다.\n",
            "\"category : SageMaker, question : 온라인 및 오프라인 특성 사이의 일관성을 유지하려면 어떻게 해야 하나요?, answer : SageMaker 특성 저장소는 추가 관리나 코드 없이 온라인 특성과 오프라인 특성의 일관성을 자동으로 유지합니다. SageMaker 특성 저장소는 훈련 및 추론 환경에서 일관성을 유지하는 완전관리형 기능입니다.\"\n",
            "특정 순간의 특성을 재현하려면 어떻게 해야 하나요?\n",
            "SageMaker 특성 저장소는 모든 순간에 모든 특성에 대한 타임스탬프를 유지합니다. 따라서 특정 기간의 특성을 검색하여 비즈니스 또는 규정 준수 요구 사항을 충족할 수 있습니다. 지정된 순간에 모델을 재현하여 처음 특성을 생성한 시점부터 현재까지 모델 특성과 값을 쉽게 설명할 수 있습니다.\n",
            "\"category : SageMaker, question : 특정 순간의 특성을 재현하려면 어떻게 해야 하나요?, answer : SageMaker 특성 저장소는 모든 순간에 모든 특성에 대한 타임스탬프를 유지합니다. 따라서 특정 기간의 특성을 검색하여 비즈니스 또는 규정 준수 요구 사항을 충족할 수 있습니다. 지정된 순간에 모델을 재현하여 처음 특성을 생성한 시점부터 현재까지 모델 특성과 값을 쉽게 설명할 수 있습니다.\"\n",
            "오프라인 특성이란 무엇인가요?\n",
            "오프라인 피처는 오랫동안 매우 큰 볼륨에 액세스해야 하는 경우 훈련에 사용됩니다. 이러한 특성은 처리량이 높고 대역폭이 큰 리포지토리에서 지원됩니다.\n",
            "\"category : SageMaker, question : 오프라인 특성이란 무엇인가요?, answer : 오프라인 피처는 오랫동안 매우 큰 볼륨에 액세스해야 하는 경우 훈련에 사용됩니다. 이러한 특성은 처리량이 높고 대역폭이 큰 리포지토리에서 지원됩니다.\"\n",
            "온라인 특성이란 무엇인가요?\n",
            "온라인 특성은 실시간 예측을 수행해야 하는 애플리케이션에 사용됩니다. 온라인 특성은 지연 시간이 10밀리초 미만인 고처리량 리포지토리에서 제공되어 빠른 예측을 지원합니다.\n",
            "\"category : SageMaker, question : 온라인 특성이란 무엇인가요?, answer : 온라인 특성은 실시간 예측을 수행해야 하는 애플리케이션에 사용됩니다. 온라인 특성은 지연 시간이 10밀리초 미만인 고처리량 리포지토리에서 제공되어 빠른 예측을 지원합니다.\"\n",
            "SageMaker 특성 저장소 요금은 어떻게 부과되나요?\n",
            "AWS 프리 티어의 일부로 SageMaker 특성 저장소를 무료로 시작할 수 있습니다. SageMaker 특성 저장소를 사용할 때는 특성 저장소에 대한 쓰기 작업 요금과 온라인 특성 저장소의 읽기 및 저장 작업에 대한 요금이 부과됩니다. 자세한 요금은 Amazon SageMaker 요금을 참조하세요.\n",
            "\"category : SageMaker, question : SageMaker 특성 저장소 요금은 어떻게 부과되나요?, answer : AWS 프리 티어의 일부로 SageMaker 특성 저장소를 무료로 시작할 수 있습니다. SageMaker 특성 저장소를 사용할 때는 특성 저장소에 대한 쓰기 작업 요금과 온라인 특성 저장소의 읽기 및 저장 작업에 대한 요금이 부과됩니다. 자세한 요금은 Amazon SageMaker 요금을 참조하세요.\"\n",
            "SageMaker는 데이터 레이블링을 위해 어떤 기능을 제공하나요?\n",
            "SageMaker는 Amazon SageMaker Ground Truth Plus 및 Amazon SageMaker Ground Truth라는 2가지 데이터 레이블링 기능을 제공합니다. 두 옵션 모두 이미지, 텍스트 파일 및 비디오와 같은 원시 데이터를 식별하고 정보 레이블을 추가하여 ML 모델을 위한 고품질 훈련 데이터 세트를 생성할 수 있습니다. 자세히 알아보려면 Amazon SageMaker 데이터 레이블링을 참조하세요.\n",
            "\"category : SageMaker, question : SageMaker는 데이터 레이블링을 위해 어떤 기능을 제공하나요?, answer : SageMaker는 Amazon SageMaker Ground Truth Plus 및 Amazon SageMaker Ground Truth라는 2가지 데이터 레이블링 기능을 제공합니다. 두 옵션 모두 이미지, 텍스트 파일 및 비디오와 같은 원시 데이터를 식별하고 정보 레이블을 추가하여 ML 모델을 위한 고품질 훈련 데이터 세트를 생성할 수 있습니다. 자세히 알아보려면 Amazon SageMaker 데이터 레이블링을 참조하세요.\"\n",
            "지리 공간 데이터란 무엇인가요?\n",
            "지리 공간 데이터는 지구 표면의 형상이나 물체를 나타냅니다. 지리 공간 데이터의 첫 번째 유형은 벡터 데이터로, 점, 선 또는 다각형과 같은 2차원 기하학 구조를 사용하여 도로와 토지 경계 같은 객체를 나타냅니다. 지리 공간 데이터의 두 번째 유형은 위성, 고소 플랫폼 또는 원격 감지 데이터에서 캡처된 이미지와 같은 래스터 데이터입니다. 이 데이터 유형은 픽셀 매트릭스를 사용하여 형상이 있는 위치를 정의합니다. 래스터 형식은 다양한 데이터를 저장하는 데 사용할 수 있습니다. 지리 공간 데이터의 세 번째 유형은 지오 태깅 위치 데이터입니다. 여기에는 관심 장소(예: 에펠탑), 위치가 태깅된 소셜 미디어 게시물, 위도 및 경도 좌표 또는 다양한 스타일과 형식의 상세 주소가 포함됩니다.\n",
            "\"category : SageMaker, question : 지리 공간 데이터란 무엇인가요?, answer : 지리 공간 데이터는 지구 표면의 형상이나 물체를 나타냅니다. 지리 공간 데이터의 첫 번째 유형은 벡터 데이터로, 점, 선 또는 다각형과 같은 2차원 기하학 구조를 사용하여 도로와 토지 경계 같은 객체를 나타냅니다. 지리 공간 데이터의 두 번째 유형은 위성, 고소 플랫폼 또는 원격 감지 데이터에서 캡처된 이미지와 같은 래스터 데이터입니다. 이 데이터 유형은 픽셀 매트릭스를 사용하여 형상이 있는 위치를 정의합니다. 래스터 형식은 다양한 데이터를 저장하는 데 사용할 수 있습니다. 지리 공간 데이터의 세 번째 유형은 지오 태깅 위치 데이터입니다. 여기에는 관심 장소(예: 에펠탑), 위치가 태깅된 소셜 미디어 게시물, 위도 및 경도 좌표 또는 다양한 스타일과 형식의 상세 주소가 포함됩니다.\"\n",
            "SageMaker 지리 공간 기능에는 어떤 것이 있나요?\n",
            "SageMaker 지리 공간 기능을 사용하면 지리 공간 데이터를 사용하여 예측용 ML 모델을 손쉽게 구축, 훈련 및 배포할 수 있습니다. Amazon S3에서 Planet Labs 위성 데이터와 같은 자체 데이터를 가져오거나, Open Data on AWS, Amazon Location Service 및 기타 SageMaker 지리 공간 데이터 소스에서 데이터를 가져올 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker 지리 공간 기능에는 어떤 것이 있나요?, answer : SageMaker 지리 공간 기능을 사용하면 지리 공간 데이터를 사용하여 예측용 ML 모델을 손쉽게 구축, 훈련 및 배포할 수 있습니다. Amazon S3에서 Planet Labs 위성 데이터와 같은 자체 데이터를 가져오거나, Open Data on AWS, Amazon Location Service 및 기타 SageMaker 지리 공간 데이터 소스에서 데이터를 가져올 수 있습니다.\"\n",
            "SageMaker의 지리 공간 ML을 사용해야 하는 이유는 무엇인가요?\n",
            "SageMaker 지리 공간 기능을 사용하면 DIY 솔루션보다 빠르게 지리 공간 데이터 기반 예측을 수행할 수 있습니다. SageMaker 지리 공간 기능을 사용하면 기존 고객 데이터 레이크, 오픈 소스 데이터 세트 및 기타 SageMaker 지리 공간 데이터 소스의 지리 공간 데이터에 보다 손쉽게 액세스할 수 있습니다. SageMaker 지리 공간 기능은 효율적인 데이터 준비, 모델 훈련 및 추론을 지원하는 목적별 알고리즘을 제공하므로, 맞춤형 인프라와 데이터 전처리 기능을 구축해야 하는 필요성이 최소화됩니다. 또한 SageMaker Studio에서 맞춤형 시각화와 데이터를 생성하고 조직에서 공유할 수 있습니다. SageMaker 지리 공간 기능에는 농업, 부동산, 보험 및 금융 서비스 분야에서 일반적인 용도로 사용 가능한 사전 훈련된 모델이 포함되어 있습니다.\n",
            "\"category : SageMaker, question : SageMaker의 지리 공간 ML을 사용해야 하는 이유는 무엇인가요?, answer : SageMaker 지리 공간 기능을 사용하면 DIY 솔루션보다 빠르게 지리 공간 데이터 기반 예측을 수행할 수 있습니다. SageMaker 지리 공간 기능을 사용하면 기존 고객 데이터 레이크, 오픈 소스 데이터 세트 및 기타 SageMaker 지리 공간 데이터 소스의 지리 공간 데이터에 보다 손쉽게 액세스할 수 있습니다. SageMaker 지리 공간 기능은 효율적인 데이터 준비, 모델 훈련 및 추론을 지원하는 목적별 알고리즘을 제공하므로, 맞춤형 인프라와 데이터 전처리 기능을 구축해야 하는 필요성이 최소화됩니다. 또한 SageMaker Studio에서 맞춤형 시각화와 데이터를 생성하고 조직에서 공유할 수 있습니다. SageMaker 지리 공간 기능에는 농업, 부동산, 보험 및 금융 서비스 분야에서 일반적인 용도로 사용 가능한 사전 훈련된 모델이 포함되어 있습니다.\"\n",
            "Amazon SageMaker Studio 노트북이란 무엇인가요?\n",
            "SageMaker Studio 노트북은 협업을 위해 빠르게 시작할 수 있는 관리형 Jupyter Notebook입니다. SageMaker Studio 노트북은 SageMaker 및 기타 AWS 서비스의 목적별 ML 도구와 통합되며 ML용 완전 IDE인 SageMaker Studio에서의 모든 ML 개발을 지원합니다.\n",
            "\"category : SageMaker, question : Amazon SageMaker Studio 노트북이란 무엇인가요?, answer : SageMaker Studio 노트북은 협업을 위해 빠르게 시작할 수 있는 관리형 Jupyter Notebook입니다. SageMaker Studio 노트북은 SageMaker 및 기타 AWS 서비스의 목적별 ML 도구와 통합되며 ML용 완전 IDE인 SageMaker Studio에서의 모든 ML 개발을 지원합니다.\"\n",
            "SageMaker Studio 노트북은 인스턴스 기반 노트북 제품 및 서비스와 어떻게 다른가요?\n",
            "SageMaker Studio 노트북은 인스턴스 기반 노트북과 차별화된 몇 가지 중요한 기능을 제공합니다. Studio 노트북을 사용하면 인스턴스를 수동으로 프로비저닝하거나 운영 상태가 될 때까지 기다리지 않고도 빠르게 노트북을 시작할 수 있습니다. 노트북을 읽고 실행하기 위해 UI를 시작하는 시작 시간도 인스턴스 기반 노트북보다 빠릅니다.\n",
            "또한 언제라도 UI 내의 다양한 인스턴스 유형 모음에서 탄력적으로 인스턴스를 고를 수 있습니다. 노트북에서 새 인스턴스와 포트를 시작하기 위해 AWS 관리 콘솔로 이동할 필요가 없습니다.\n",
            "사용자마다 특정 인스턴스와 독립되어 있는 격리된 홈 디렉터리가 있습니다. 이 디렉터리는 시작될 때 모든 노트북 서버와 커널에 자동으로 탑재되므로, 인스턴스를 전환해서 노트북을 보고 실행하는 경우에도 노트북과 다른 파일에 액세스할 수 있습니다.\n",
            "SageMaker Studio 노트북은 AWS IAM Identity Center(AWS SSO의 후속 모델)와 통합되며 조직 보안 인증 정보를 통해 노트북에 쉽게 액세스할 수 있습니다. 노트북 공유는 SageMaker Studio 노트북에 통합된 기능입니다. 한 단계로 동료와 노트북을 공유하거나 단일 노트북을 동시에 공동으로 편집할 수도 있습니다.\n",
            "\"category : SageMaker, question : SageMaker Studio 노트북은 인스턴스 기반 노트북 제품 및 서비스와 어떻게 다른가요?, answer : SageMaker Studio 노트북은 인스턴스 기반 노트북과 차별화된 몇 가지 중요한 기능을 제공합니다. Studio 노트북을 사용하면 인스턴스를 수동으로 프로비저닝하거나 운영 상태가 될 때까지 기다리지 않고도 빠르게 노트북을 시작할 수 있습니다. 노트북을 읽고 실행하기 위해 UI를 시작하는 시작 시간도 인스턴스 기반 노트북보다 빠릅니다.\n",
            "또한 언제라도 UI 내의 다양한 인스턴스 유형 모음에서 탄력적으로 인스턴스를 고를 수 있습니다. 노트북에서 새 인스턴스와 포트를 시작하기 위해 AWS 관리 콘솔로 이동할 필요가 없습니다.\n",
            "사용자마다 특정 인스턴스와 독립되어 있는 격리된 홈 디렉터리가 있습니다. 이 디렉터리는 시작될 때 모든 노트북 서버와 커널에 자동으로 탑재되므로, 인스턴스를 전환해서 노트북을 보고 실행하는 경우에도 노트북과 다른 파일에 액세스할 수 있습니다.\n",
            "SageMaker Studio 노트북은 AWS IAM Identity Center(AWS SSO의 후속 모델)와 통합되며 조직 보안 인증 정보를 통해 노트북에 쉽게 액세스할 수 있습니다. 노트북 공유는 SageMaker Studio 노트북에 통합된 기능입니다. 한 단계로 동료와 노트북을 공유하거나 단일 노트북을 동시에 공동으로 편집할 수도 있습니다.\"\n",
            "SageMaker Studio 노트북은 어떤 방식으로 작동하나요?\n",
            "SageMaker Studio 노트북은 한 단계로 빠르게 가동할 수 있는 Jupyter Notebook입니다. 기본 컴퓨팅 리소스가 완전히 탄력적이므로 사용 가능한 리소스를 쉽게 확장하거나 축소할 수 있으며 변경 작업이 백그라운드에서 자동으로 진행되므로 작업에 방해가 되지 않습니다. 또한 SageMaker에서 한 단계로 노트북을 공유할 수 있습니다. 노트북을 다른 사람과 쉽게 공유하여 같은 장소에 저장된 정확하게 동일한 노트북을 제공할 수 있습니다.\n",
            "SageMaker Studio 노트북을 사용할 때는 IAM Identity Center를 사용하여 사내 보안 인증 정보로 로그인할 수 있습니다. 노트북을 공유하면 해당 노트북으로 캡슐화되는 작업 이미지에서 노트북을 실행하는 데 필요한 종속성이 자동으로 추적되므로 팀 내에서는 물론, 팀 간에도 노트북을 손쉽게 공유할 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker Studio 노트북은 어떤 방식으로 작동하나요?, answer : SageMaker Studio 노트북은 한 단계로 빠르게 가동할 수 있는 Jupyter Notebook입니다. 기본 컴퓨팅 리소스가 완전히 탄력적이므로 사용 가능한 리소스를 쉽게 확장하거나 축소할 수 있으며 변경 작업이 백그라운드에서 자동으로 진행되므로 작업에 방해가 되지 않습니다. 또한 SageMaker에서 한 단계로 노트북을 공유할 수 있습니다. 노트북을 다른 사람과 쉽게 공유하여 같은 장소에 저장된 정확하게 동일한 노트북을 제공할 수 있습니다.\n",
            "SageMaker Studio 노트북을 사용할 때는 IAM Identity Center를 사용하여 사내 보안 인증 정보로 로그인할 수 있습니다. 노트북을 공유하면 해당 노트북으로 캡슐화되는 작업 이미지에서 노트북을 실행하는 데 필요한 종속성이 자동으로 추적되므로 팀 내에서는 물론, 팀 간에도 노트북을 손쉽게 공유할 수 있습니다.\"\n",
            "SageMaker에서 공유 공간이란 무엇인가요?\n",
            "ML 실무자는 팀원이 SageMaker Studio 노트북을 함께 읽고 편집할 수 있는 작업 공간을 만들 수 있습니다. 팀원들은 공유 공간을 사용하여 동일한 노트북 파일을 공동 편집하고, 노트북 코드를 동시에 실행하며, 결과를 함께 검토하여 반복 작업을 줄이고 협업을 간소화할 수 있습니다. 공유 공간에서는 BitBucket 및 AWS CodeCommit과 같은 서비스가 기본적으로 지원되기 때문에 여러 버전의 노트북을 손쉽게 관리하고 변경 사항을 지속적으로 비교할 수 있습니다. 실험 및 ML 모델과 같이 노트북 내에서 생성되는 모든 리소스가 자동으로 저장되고, 생성된 특정 작업 공간에 연결되므로 팀 작업을 보다 체계적으로 유지하고 ML 모델 개발을 가속화할 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker에서 공유 공간이란 무엇인가요?, answer : ML 실무자는 팀원이 SageMaker Studio 노트북을 함께 읽고 편집할 수 있는 작업 공간을 만들 수 있습니다. 팀원들은 공유 공간을 사용하여 동일한 노트북 파일을 공동 편집하고, 노트북 코드를 동시에 실행하며, 결과를 함께 검토하여 반복 작업을 줄이고 협업을 간소화할 수 있습니다. 공유 공간에서는 BitBucket 및 AWS CodeCommit과 같은 서비스가 기본적으로 지원되기 때문에 여러 버전의 노트북을 손쉽게 관리하고 변경 사항을 지속적으로 비교할 수 있습니다. 실험 및 ML 모델과 같이 노트북 내에서 생성되는 모든 리소스가 자동으로 저장되고, 생성된 특정 작업 공간에 연결되므로 팀 작업을 보다 체계적으로 유지하고 ML 모델 개발을 가속화할 수 있습니다.\"\n",
            "SageMaker Studio 노트북을 다른 AWS 서비스와 함께 사용할 수 있나요?\n",
            "SageMaker Studio 노트북에서 분산 훈련, 배치 변환, 호스팅, 실험 관리 같은 모든 SageMaker 기능을 활용할 수 있습니다. SageMaker 노트북에서 Amazon S3의 데이터 세트, Amazon Redshift, AWS Glue, Amazon EMR 또는 AWS Lake Formation 같은 다른 서비스에 액세스할 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker Studio 노트북을 다른 AWS 서비스와 함께 사용할 수 있나요?, answer : SageMaker Studio 노트북에서 분산 훈련, 배치 변환, 호스팅, 실험 관리 같은 모든 SageMaker 기능을 활용할 수 있습니다. SageMaker 노트북에서 Amazon S3의 데이터 세트, Amazon Redshift, AWS Glue, Amazon EMR 또는 AWS Lake Formation 같은 다른 서비스에 액세스할 수 있습니다.\"\n",
            "SageMaker Studio 노트북 요금은 어떻게 적용되나요?\n",
            "SageMaker Studio 노트북을 사용할 때는 컴퓨팅과 스토리지 비용을 모두 지불해야 합니다. 컴퓨팅 인스턴스 유형별 요금은 Amazon SageMaker 요금을 참조하세요. 노트북과 데이터 파일 및 스크립트와 같은 관련 아티팩트는 Amazon Elastic File System(Amazon EFS)에 보존됩니다. 스토리지 요금은 Amazon EFS 요금을 참조하세요. AWS 프리 티어의 일부로 SageMaker Studio 노트북을 무료로 시작할 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker Studio 노트북 요금은 어떻게 적용되나요?, answer : SageMaker Studio 노트북을 사용할 때는 컴퓨팅과 스토리지 비용을 모두 지불해야 합니다. 컴퓨팅 인스턴스 유형별 요금은 Amazon SageMaker 요금을 참조하세요. 노트북과 데이터 파일 및 스크립트와 같은 관련 아티팩트는 Amazon Elastic File System(Amazon EFS)에 보존됩니다. 스토리지 요금은 Amazon EFS 요금을 참조하세요. AWS 프리 티어의 일부로 SageMaker Studio 노트북을 무료로 시작할 수 있습니다.\"\n",
            "SageMaker Studio에서 만들고 실행하는 각 노트북에 대해 별도의 요금이 부과되나요?\n",
            "아니요. 같은 컴퓨팅 인스턴스에 여러 노트북을 생성하고 실행할 수 있습니다. 개별 항목이 아닌 사용하는 컴퓨팅에 대해서만 지불합니다. 측정 가이드에서 자세한 내용을 읽어보실 수 있습니다.\n",
            "노트북 외에도 SageMaker Studio에서 터미널과 대화형 쉘을 시작하고 실행할 수 있습니다. 모두 동일한 컴퓨팅 인스턴스에 위치합니다. 각 애플리케이션은 컨테이너나 이미지 내에서 실행됩니다. SageMaker Studio는 데이터 과학 및 기계 학습용으로 특별히 구축되고 사전 구성된 여러 가지 내장 이미지를 제공합니다. SageMaker Studio 노트북에 대한 가이드에서 SageMaker Studio 개발자 환경에 대해 자세히 읽어볼 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker Studio에서 만들고 실행하는 각 노트북에 대해 별도의 요금이 부과되나요?, answer : 아니요. 같은 컴퓨팅 인스턴스에 여러 노트북을 생성하고 실행할 수 있습니다. 개별 항목이 아닌 사용하는 컴퓨팅에 대해서만 지불합니다. 측정 가이드에서 자세한 내용을 읽어보실 수 있습니다.\n",
            "노트북 외에도 SageMaker Studio에서 터미널과 대화형 쉘을 시작하고 실행할 수 있습니다. 모두 동일한 컴퓨팅 인스턴스에 위치합니다. 각 애플리케이션은 컨테이너나 이미지 내에서 실행됩니다. SageMaker Studio는 데이터 과학 및 기계 학습용으로 특별히 구축되고 사전 구성된 여러 가지 내장 이미지를 제공합니다. SageMaker Studio 노트북에 대한 가이드에서 SageMaker Studio 개발자 환경에 대해 자세히 읽어볼 수 있습니다.\"\n",
            "노트북에 사용되는 리소스를 모니터링하고 종료하려면 어떻게 해야 하나요?\n",
            "SageMaker Studio 비주얼 인터페이스 및 AWS Management Console을 통해 SageMaker Studio 노트북에서 사용하는 리소스를 모니터링하고 종료할 수 있습니다. 자세한 내용은 설명서를 참조하세요.\n",
            "\"category : SageMaker, question : 노트북에 사용되는 리소스를 모니터링하고 종료하려면 어떻게 해야 하나요?, answer : SageMaker Studio 비주얼 인터페이스 및 AWS Management Console을 통해 SageMaker Studio 노트북에서 사용하는 리소스를 모니터링하고 종료할 수 있습니다. 자세한 내용은 설명서를 참조하세요.\"\n",
            "SageMaker Studio 노트북을 실행하고 있습니다. 브라우저를 닫거나, 노트북 탭을 닫거나, 브라우저를 열어두어도 요금이 부과되나요?\n",
            "예. 컴퓨팅 요금은 계속해서 부과됩니다. 이는 AWS 관리 콘솔에서 Amazon EC2 인스턴스를 시작한 다음 브라우저를 닫는 것과 유사합니다. Amazon EC2 인스턴스가 계속해서 실행되고 있으므로 인스턴스를 명시적으로 종료하지 않는 한 여전히 요금이 부과됩니다.\n",
            "\"category : SageMaker, question : SageMaker Studio 노트북을 실행하고 있습니다. 브라우저를 닫거나, 노트북 탭을 닫거나, 브라우저를 열어두어도 요금이 부과되나요?, answer : 예. 컴퓨팅 요금은 계속해서 부과됩니다. 이는 AWS 관리 콘솔에서 Amazon EC2 인스턴스를 시작한 다음 브라우저를 닫는 것과 유사합니다. Amazon EC2 인스턴스가 계속해서 실행되고 있으므로 인스턴스를 명시적으로 종료하지 않는 한 여전히 요금이 부과됩니다.\"\n",
            "SageMaker Studio 도메인을 만들고 설정하는 데 요금이 부과되나요?\n",
            "아니오. 사용자 프로필 추가, 업데이트, 삭제를 포함하여 SageMaker Studio 도메인을 만들거나 구성하는 데는 요금이 부과되지 않습니다.\n",
            "\"category : SageMaker, question : SageMaker Studio 도메인을 만들고 설정하는 데 요금이 부과되나요?, answer : 아니오. 사용자 프로필 추가, 업데이트, 삭제를 포함하여 SageMaker Studio 도메인을 만들거나 구성하는 데는 요금이 부과되지 않습니다.\"\n",
            "SageMaker Studio 노트북 또는 기타 SageMaker 서비스에 대한 항목별 요금은 어떻게 확인하나요?\n",
            "관리자는 AWS 빌링 콘솔에서 SageMaker Studio를 포함하여 SageMaker에 대한 항목별 요금 목록을 확인할 수 있습니다. SageMaker용 AWS Management Console의 상단 메뉴에서 ‘서비스’를 선택하고, 검색 상자에 ‘billing(결제)’을 입력하고, 드롭다운 메뉴에서 결제를 선택한 다음 왼쪽 패널에서 ‘청구서’를 선택합니다. Details(세부 정보) 섹션에서 SageMaker를 선택하여 리전 목록을 확장하고 항목별 요금으로 드릴다운할 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker Studio 노트북 또는 기타 SageMaker 서비스에 대한 항목별 요금은 어떻게 확인하나요?, answer : 관리자는 AWS 빌링 콘솔에서 SageMaker Studio를 포함하여 SageMaker에 대한 항목별 요금 목록을 확인할 수 있습니다. SageMaker용 AWS Management Console의 상단 메뉴에서 ‘서비스’를 선택하고, 검색 상자에 ‘billing(결제)’을 입력하고, 드롭다운 메뉴에서 결제를 선택한 다음 왼쪽 패널에서 ‘청구서’를 선택합니다. Details(세부 정보) 섹션에서 SageMaker를 선택하여 리전 목록을 확장하고 항목별 요금으로 드릴다운할 수 있습니다.\"\n",
            "Amazon SageMaker Studio Lab이란 무엇인가요?\n",
            "SageMaker Studio Lab은 누구나 ML을 배우고 실험할 수 있도록 컴퓨팅, 스토리지(최대 15GB) 및 보안을 모두 무료로 제공하는 무료 기계 학습 개발 환경입니다. 시작하려면 유효한 이메일 ID만 있으면 됩니다. 인프라를 구성하거나 ID 및 액세스를 관리하거나 AWS 계정에 가입할 필요가 없습니다. SageMaker Studio Lab은 GitHub 통합을 통해 모델 구축을 가속화하고 가장 인기 있는 ML 도구, 프레임워크 및 라이브러리로 사전 구성되어 제공되므로 즉시 시작할 수 있습니다. SageMaker Studio Lab은 작업을 자동으로 저장하므로 세션 사이에 다시 시작할 필요가 없습니다. 노트북을 닫았다가 나중에 다시 열고 작업하는 것처럼 쉽습니다.\n",
            "\"category : SageMaker, question : Amazon SageMaker Studio Lab이란 무엇인가요?, answer : SageMaker Studio Lab은 누구나 ML을 배우고 실험할 수 있도록 컴퓨팅, 스토리지(최대 15GB) 및 보안을 모두 무료로 제공하는 무료 기계 학습 개발 환경입니다. 시작하려면 유효한 이메일 ID만 있으면 됩니다. 인프라를 구성하거나 ID 및 액세스를 관리하거나 AWS 계정에 가입할 필요가 없습니다. SageMaker Studio Lab은 GitHub 통합을 통해 모델 구축을 가속화하고 가장 인기 있는 ML 도구, 프레임워크 및 라이브러리로 사전 구성되어 제공되므로 즉시 시작할 수 있습니다. SageMaker Studio Lab은 작업을 자동으로 저장하므로 세션 사이에 다시 시작할 필요가 없습니다. 노트북을 닫았다가 나중에 다시 열고 작업하는 것처럼 쉽습니다.\"\n",
            "SageMaker Studio Lab을 사용해야 하는 이유는 무엇인가요?\n",
            "SageMaker Studio Lab은 설정 없이 기계 학습 수업 및 실험에 사용할 수 있는 무료 노트북 개발 환경이 필요한 학생, 연구원 및 데이터 사이언티스트를 위한 것입니다. SageMaker Studio Lab은 프로덕션 환경이 필요하지는 않지만 기계 학습 기술 향상을 위해 SageMaker 기능의 일부를 원하는 사용자에게 이상적입니다. SageMaker 세션은 자동으로 저장되므로 각 사용자 세션에서 중단한 부분부터 다시 시작할 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker Studio Lab을 사용해야 하는 이유는 무엇인가요?, answer : SageMaker Studio Lab은 설정 없이 기계 학습 수업 및 실험에 사용할 수 있는 무료 노트북 개발 환경이 필요한 학생, 연구원 및 데이터 사이언티스트를 위한 것입니다. SageMaker Studio Lab은 프로덕션 환경이 필요하지는 않지만 기계 학습 기술 향상을 위해 SageMaker 기능의 일부를 원하는 사용자에게 이상적입니다. SageMaker 세션은 자동으로 저장되므로 각 사용자 세션에서 중단한 부분부터 다시 시작할 수 있습니다.\"\n",
            "SageMaker Studio Lab은 다른 AWS 서비스와 어떻게 작동하나요?\n",
            "SageMaker Studio Lab은 AWS에 구축된 서비스로, Amazon SageMaker Studio와 동일한 핵심 서비스(예: Amazon S3 및 Amazon EC2)를 다수 사용합니다. 다른 서비스와 달리 AWS 계정이 필요하지 않습니다. 대신 이메일 주소를 사용하여 SageMaker Studio Lab 전용 계정을 생성하게 됩니다. 이 전용 계정으로 ML 노트북을 실행할 수 있는 제한된 환경(15GB 스토리지 및 12시간 세션)에 액세스할 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker Studio Lab은 다른 AWS 서비스와 어떻게 작동하나요?, answer : SageMaker Studio Lab은 AWS에 구축된 서비스로, Amazon SageMaker Studio와 동일한 핵심 서비스(예: Amazon S3 및 Amazon EC2)를 다수 사용합니다. 다른 서비스와 달리 AWS 계정이 필요하지 않습니다. 대신 이메일 주소를 사용하여 SageMaker Studio Lab 전용 계정을 생성하게 됩니다. 이 전용 계정으로 ML 노트북을 실행할 수 있는 제한된 환경(15GB 스토리지 및 12시간 세션)에 액세스할 수 있습니다.\"\n",
            "SageMaker Canvas란 무엇인가요?\n",
            "SageMaker Canvas는 비즈니스 분석가가 코드를 작성하거나 기계 학습 전문 지식 없이도 기계 학습 모델을 구축하고 정확한 예측을 생성할 수 있는, 끌어서 놓기 방식의 시각적 서비스입니다. SageMaker Canvas를 사용하면 다양한 소스의 데이터에 쉽게 액세스 및 결합하고, 데이터를 자동으로 정리하고 다양한 데이터 조정을 적용하고, 기계 학습 모델을 구축하여 한 단계로 정확한 예측을 생성할 수 있습니다. 또한 간편하게 결과를 게시하고, 모델을 설명 및 해석하고, 검토를 위해 조직 내의 다른 사람들과 모델을 공유할 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker Canvas란 무엇인가요?, answer : SageMaker Canvas는 비즈니스 분석가가 코드를 작성하거나 기계 학습 전문 지식 없이도 기계 학습 모델을 구축하고 정확한 예측을 생성할 수 있는, 끌어서 놓기 방식의 시각적 서비스입니다. SageMaker Canvas를 사용하면 다양한 소스의 데이터에 쉽게 액세스 및 결합하고, 데이터를 자동으로 정리하고 다양한 데이터 조정을 적용하고, 기계 학습 모델을 구축하여 한 단계로 정확한 예측을 생성할 수 있습니다. 또한 간편하게 결과를 게시하고, 모델을 설명 및 해석하고, 검토를 위해 조직 내의 다른 사람들과 모델을 공유할 수 있습니다.\"\n",
            "SageMaker Canvas는 어떤 데이터 소스를 지원하나요?\n",
            "SageMaker Canvas는 Amazon S3 및 Amazon Redshift를 포함하여 계정에서 액세스할 수 있는 AWS 데이터 소스를 원활하게 검색하는 데 도움이 됩니다. SageMaker Canvas의 시각적 드래그 앤 드롭 인터페이스를 사용하여 데이터를 찾아보고 가져올 수 있습니다. 또한 로컬 디스크에서 파일을 끌어서 놓을 수 있으며 사전 구축된 커넥터를 사용하여 Snowflake와 같은 서드 파티 소스에서 데이터를 가져올 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker Canvas는 어떤 데이터 소스를 지원하나요?, answer : SageMaker Canvas는 Amazon S3 및 Amazon Redshift를 포함하여 계정에서 액세스할 수 있는 AWS 데이터 소스를 원활하게 검색하는 데 도움이 됩니다. SageMaker Canvas의 시각적 드래그 앤 드롭 인터페이스를 사용하여 데이터를 찾아보고 가져올 수 있습니다. 또한 로컬 디스크에서 파일을 끌어서 놓을 수 있으며 사전 구축된 커넥터를 사용하여 Snowflake와 같은 서드 파티 소스에서 데이터를 가져올 수 있습니다.\"\n",
            "SageMaker Canvas에서 정확한 예측을 생성하기 위해 기계 학습 모델을 구축하려면 어떻게 해야 하나요?\n",
            "소스를 연결하고 데이터 세트를 선택하고 데이터를 준비한 후 예측할 대상 열을 선택하여 모델 생성 작업을 시작할 수 있습니다. SageMaker Canvas는 자동으로 문제 유형을 식별하고, 새로운 관련 기능을 생성하고, 선형 회귀, 로지스틱 회귀, 딥 러닝, 시계열 예측, 그라데이션 부스팅 등의 기계 학습 기술을 사용하여 포괄적인 예측 모델 세트를 테스트하며, 데이터 세트를 기반으로 정확한 예측을 수행하는 모델을 구축합니다.\n",
            "\"category : SageMaker, question : SageMaker Canvas에서 정확한 예측을 생성하기 위해 기계 학습 모델을 구축하려면 어떻게 해야 하나요?, answer : 소스를 연결하고 데이터 세트를 선택하고 데이터를 준비한 후 예측할 대상 열을 선택하여 모델 생성 작업을 시작할 수 있습니다. SageMaker Canvas는 자동으로 문제 유형을 식별하고, 새로운 관련 기능을 생성하고, 선형 회귀, 로지스틱 회귀, 딥 러닝, 시계열 예측, 그라데이션 부스팅 등의 기계 학습 기술을 사용하여 포괄적인 예측 모델 세트를 테스트하며, 데이터 세트를 기반으로 정확한 예측을 수행하는 모델을 구축합니다.\"\n",
            "SageMaker Canvas에서 모델을 구축할 때 소요되는 시간은 어느 정도인가요? 모델 생성 중 진행 상황을 모니터링하려면 어떻게 해야 하나요?\n",
            "모델을 구축하는 데 걸리는 시간은 데이터 집합의 크기에 따라 다릅니다. 작은 데이터 세트는 30분 미만이 소요될 수 있고 큰 데이터 세트는 몇 시간이 걸릴 수 있습니다. 모델 생성 작업이 진행됨에 따라 작업 완료율과 작업 완료까지 남은 시간을 포함하여 자세한 시각적 업데이트가 SageMaker Canvas에 표시됩니다.\n",
            "\"category : SageMaker, question : SageMaker Canvas에서 모델을 구축할 때 소요되는 시간은 어느 정도인가요? 모델 생성 중 진행 상황을 모니터링하려면 어떻게 해야 하나요?, answer : 모델을 구축하는 데 걸리는 시간은 데이터 집합의 크기에 따라 다릅니다. 작은 데이터 세트는 30분 미만이 소요될 수 있고 큰 데이터 세트는 몇 시간이 걸릴 수 있습니다. 모델 생성 작업이 진행됨에 따라 작업 완료율과 작업 완료까지 남은 시간을 포함하여 자세한 시각적 업데이트가 SageMaker Canvas에 표시됩니다.\"\n",
            "Amazon SageMaker HyperPod란 무엇인가요?\n",
            "SageMaker HyperPod는 파운데이션 모델(FM)의 훈련을 가속화하기 위해 특별히 제작되었습니다. 대규모 분산 훈련에 최적화되고 복원력이 뛰어난 인프라를 제공하므로 수천 개의 액셀러레이터로 더 빠르게 훈련할 수 있습니다. 결함을 자동으로 감지, 진단, 복구하므로 중단 없이 한 번에 몇 달 동안 FM을 훈련할 수 있습니다. SageMaker HyperPod는 SageMaker 분산 훈련 라이브러리로 사전 구성되어 있기 때문에 모델 훈련 데이터를 작은 청크로 분산하여 성능을 효율적으로 개선할 수 있고 여러 액셀러레이터에서 병렬로 처리할 수 있습니다.\n",
            "\"category : SageMaker, question : Amazon SageMaker HyperPod란 무엇인가요?, answer : SageMaker HyperPod는 파운데이션 모델(FM)의 훈련을 가속화하기 위해 특별히 제작되었습니다. 대규모 분산 훈련에 최적화되고 복원력이 뛰어난 인프라를 제공하므로 수천 개의 액셀러레이터로 더 빠르게 훈련할 수 있습니다. 결함을 자동으로 감지, 진단, 복구하므로 중단 없이 한 번에 몇 달 동안 FM을 훈련할 수 있습니다. SageMaker HyperPod는 SageMaker 분산 훈련 라이브러리로 사전 구성되어 있기 때문에 모델 훈련 데이터를 작은 청크로 분산하여 성능을 효율적으로 개선할 수 있고 여러 액셀러레이터에서 병렬로 처리할 수 있습니다.\"\n",
            "Amazon SageMaker Experiments란 무엇인가요?\n",
            "SageMaker Experiments를 사용하면 기계 학습 모델에 대한 반복을 구성하고 추적할 수 있습니다. SageMaker Experiments는 입력 파라미터, 구성 및 결과를 자동으로 캡처하고 이를 ‘실험’으로 저장하여 반복을 관리하도록 돕습니다. SageMaker Studio의 시각적 인터페이스에서 작업하여 활성 실험을 탐색하고 특성별로 이전 실험을 검색하며 결과와 함께 이전 실험을 검토하고 실험 결과를 시각적으로 비교할 수 있습니다.\n",
            "\"category : SageMaker, question : Amazon SageMaker Experiments란 무엇인가요?, answer : SageMaker Experiments를 사용하면 기계 학습 모델에 대한 반복을 구성하고 추적할 수 있습니다. SageMaker Experiments는 입력 파라미터, 구성 및 결과를 자동으로 캡처하고 이를 ‘실험’으로 저장하여 반복을 관리하도록 돕습니다. SageMaker Studio의 시각적 인터페이스에서 작업하여 활성 실험을 탐색하고 특성별로 이전 실험을 검색하며 결과와 함께 이전 실험을 검토하고 실험 결과를 시각적으로 비교할 수 있습니다.\"\n",
            "언제 SageMaker HyperPod를 사용해야 하나요?\n",
            "시간이 오래 걸리고 규모가 큰 훈련 워크로드에 GPU 또는 AWS 액셀러레이터와 같이 대량의 컴퓨팅 인스턴스가 필요한 경우 SageMaker HyperPod를 사용하여 보다 탄력적인 환경을 구축하면 훈련 시간을 줄일 수 있습니다.\n",
            "\"category : SageMaker, question : 언제 SageMaker HyperPod를 사용해야 하나요?, answer : 시간이 오래 걸리고 규모가 큰 훈련 워크로드에 GPU 또는 AWS 액셀러레이터와 같이 대량의 컴퓨팅 인스턴스가 필요한 경우 SageMaker HyperPod를 사용하여 보다 탄력적인 환경을 구축하면 훈련 시간을 줄일 수 있습니다.\"\n",
            "Amazon SageMaker Debugger란 무엇인가요?\n",
            "SageMaker Debugger는 훈련 도중 혼동 지표 및 학습 경사와 같은 실시간 지표를 자동으로 캡처하여 모델 정확도를 개선합니다. SageMaker Debugger의 지표를 SageMaker Studio에서 시각화하면 지표를 더 쉽게 이해할 수 있습니다. SageMaker Debugger에서 일반적인 훈련 문제가 감지될 때 경고 및 해결 방법을 생성할 수도 있습니다. 또한 SageMaker Debugger는 CPU, GPU, 네트워크 및 메모리와 같은 시스템 리소스를 실시간으로 자동으로 모니터링하고 프로파일링하며, 이러한 리소스의 재할당에 대한 권장 사항을 제공합니다. 이 기능은 훈련 중에 리소스를 효율적으로 사용하고 비용과 리소스를 절감하는 데 도움이 됩니다.\n",
            "\"category : SageMaker, question : Amazon SageMaker Debugger란 무엇인가요?, answer : SageMaker Debugger는 훈련 도중 혼동 지표 및 학습 경사와 같은 실시간 지표를 자동으로 캡처하여 모델 정확도를 개선합니다. SageMaker Debugger의 지표를 SageMaker Studio에서 시각화하면 지표를 더 쉽게 이해할 수 있습니다. SageMaker Debugger에서 일반적인 훈련 문제가 감지될 때 경고 및 해결 방법을 생성할 수도 있습니다. 또한 SageMaker Debugger는 CPU, GPU, 네트워크 및 메모리와 같은 시스템 리소스를 실시간으로 자동으로 모니터링하고 프로파일링하며, 이러한 리소스의 재할당에 대한 권장 사항을 제공합니다. 이 기능은 훈련 중에 리소스를 효율적으로 사용하고 비용과 리소스를 절감하는 데 도움이 됩니다.\"\n",
            "SageMaker에서는 분산 훈련을 지원하나요?\n",
            "예. SageMaker는 AWS GPU 인스턴스에 걸쳐 딥 러닝 모델과 대규모 훈련 세트를 자동으로 분산할 수 있습니다. 이때 소요 시간은 이러한 분산 전략을 수동으로 빌드하고 최적화하는 데 걸리는 시간에 비해 몇 분의 일에 불과합니다. SageMaker에서 적용하는 2가지 분산 훈련 기술로는 데이터 병렬 처리와 모델 병렬 처리가 있습니다. 데이터 병렬 처리는 여러 GPU 인스턴스에서 데이터를 균등하게 나누어 훈련 속도를 개선하기 위해 적용됩니다. 이 경우 각 인스턴스를 동시에 훈련할 수 있습니다. 모델 병렬 처리는 모델이 너무 커서 단일 GPU에 저장할 수 없는 경우에 유용하며, 여러 GPU에 분산하기 전에 모델을 더 작은 부분으로 파티셔닝해야 합니다. PyTorch 및 TensorFlow 훈련 스크립트에서 추가 코드 몇 줄만 작성하면, SageMaker는 데이터 병렬 처리 또는 모델 병렬 처리를 자동으로 적용하여 모델을 더 빠르게 개발하고 배포할 수 있습니다. SageMaker는 각 GPU 계산의 균형을 맞추기 위해 그래프 분할 알고리즘을 사용하여 모델을 분할하는 최상의 접근 방법을 결정하는 동시에 GPU 인스턴스 간의 통신을 최소화합니다. 또한 SageMaker는 선형에 가까운 확장 효율성을 달성하기 위해 AWS 컴퓨팅과 네트워크를 충분히 활용하는 알고리즘을 통해 분산 훈련 작업을 최적화하므로 수동 오픈 소스 구현보다 더욱 빠르게 훈련을 완료할 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker에서는 분산 훈련을 지원하나요?, answer : 예. SageMaker는 AWS GPU 인스턴스에 걸쳐 딥 러닝 모델과 대규모 훈련 세트를 자동으로 분산할 수 있습니다. 이때 소요 시간은 이러한 분산 전략을 수동으로 빌드하고 최적화하는 데 걸리는 시간에 비해 몇 분의 일에 불과합니다. SageMaker에서 적용하는 2가지 분산 훈련 기술로는 데이터 병렬 처리와 모델 병렬 처리가 있습니다. 데이터 병렬 처리는 여러 GPU 인스턴스에서 데이터를 균등하게 나누어 훈련 속도를 개선하기 위해 적용됩니다. 이 경우 각 인스턴스를 동시에 훈련할 수 있습니다. 모델 병렬 처리는 모델이 너무 커서 단일 GPU에 저장할 수 없는 경우에 유용하며, 여러 GPU에 분산하기 전에 모델을 더 작은 부분으로 파티셔닝해야 합니다. PyTorch 및 TensorFlow 훈련 스크립트에서 추가 코드 몇 줄만 작성하면, SageMaker는 데이터 병렬 처리 또는 모델 병렬 처리를 자동으로 적용하여 모델을 더 빠르게 개발하고 배포할 수 있습니다. SageMaker는 각 GPU 계산의 균형을 맞추기 위해 그래프 분할 알고리즘을 사용하여 모델을 분할하는 최상의 접근 방법을 결정하는 동시에 GPU 인스턴스 간의 통신을 최소화합니다. 또한 SageMaker는 선형에 가까운 확장 효율성을 달성하기 위해 AWS 컴퓨팅과 네트워크를 충분히 활용하는 알고리즘을 통해 분산 훈련 작업을 최적화하므로 수동 오픈 소스 구현보다 더욱 빠르게 훈련을 완료할 수 있습니다.\"\n",
            "Amazon SageMaker Training Compiler란 무엇인가요?\n",
            "SageMaker Training Compiler는 GPU를 보다 효율적으로 사용하기 위해 그래프 및 커널 수준 최적화를 통해 딥 러닝(DL) 모델 훈련을 최대 50% 가속화하는 DL 컴파일러입니다. SageMaker Training Compiler는 SageMaker의 TensorFlow 및 PyTorch 버전과 통합되므로 최소한의 코드 변경으로 이러한 인기 있는 프레임워크에서 훈련 속도를 높일 수 있습니다.\n",
            "\"category : SageMaker, question : Amazon SageMaker Training Compiler란 무엇인가요?, answer : SageMaker Training Compiler는 GPU를 보다 효율적으로 사용하기 위해 그래프 및 커널 수준 최적화를 통해 딥 러닝(DL) 모델 훈련을 최대 50% 가속화하는 DL 컴파일러입니다. SageMaker Training Compiler는 SageMaker의 TensorFlow 및 PyTorch 버전과 통합되므로 최소한의 코드 변경으로 이러한 인기 있는 프레임워크에서 훈련 속도를 높일 수 있습니다.\"\n",
            "SageMaker Training Compiler는 어떻게 작동하나요?\n",
            "SageMaker Training Compiler는 고급 언어 표현의 DL 모델을 기본 프레임워크를 사용하는 작업보다 빠르게 훈련하는 하드웨어 최적화 지침으로 변환하여 훈련 작업을 가속화합니다. 보다 구체적으로 SageMaker Training Compiler는 그래프 수준 최적화(연산자 융합, 메모리 계획 및 대수 단순화), 데이터 흐름 수준 최적화(레이아웃 변환, 공통 하위 표현식 제거) 및 백엔드 최적화(메모리 대기 시간 숨기기, 루프 지향적 최적화)를 통해 하드웨어 리소스를 보다 효율적으로 사용하고 결과적으로 더 빠르게 훈련하는 최적화된 모델 훈련 작업을 생성합니다.\n",
            "\"category : SageMaker, question : SageMaker Training Compiler는 어떻게 작동하나요?, answer : SageMaker Training Compiler는 고급 언어 표현의 DL 모델을 기본 프레임워크를 사용하는 작업보다 빠르게 훈련하는 하드웨어 최적화 지침으로 변환하여 훈련 작업을 가속화합니다. 보다 구체적으로 SageMaker Training Compiler는 그래프 수준 최적화(연산자 융합, 메모리 계획 및 대수 단순화), 데이터 흐름 수준 최적화(레이아웃 변환, 공통 하위 표현식 제거) 및 백엔드 최적화(메모리 대기 시간 숨기기, 루프 지향적 최적화)를 통해 하드웨어 리소스를 보다 효율적으로 사용하고 결과적으로 더 빠르게 훈련하는 최적화된 모델 훈련 작업을 생성합니다.\"\n",
            "SageMaker Training Compiler는 어떻게 사용하나요?\n",
            "SageMaker Training Compiler는 SageMaker Python SDK 및 SageMaker Hugging Face Deep Learning Containers에 내장되어 있습니다. 속도 향상 이점을 활용하기 위해 워크플로를 변경할 필요가 없습니다. SageMaker 노트북 인스턴스, SageMaker Studio, AWS SDK for Python(Boto3) 및 AWS Command Line Interface(AWS CLI)와 같은 SageMaker 인터페이스를 사용하여 기존과 동일한 방식으로 훈련 작업을 실행할 수 있습니다. 프레임워크 추정기 객체를 생성할 때 TrainingCompilerConfig 클래스를 파라미터로 추가하여 SageMaker Training Compiler를 사용할 수 있습니다. 실제로 이는 단일 GPU 인스턴스에 대한 기존 훈련 작업 스크립트에 몇 줄의 코드가 추가되었음을 의미합니다. 최신 세부 설명서, 샘플 노트북 및 예제는 설명서에서 제공됩니다.\n",
            "\"category : SageMaker, question : SageMaker Training Compiler는 어떻게 사용하나요?, answer : SageMaker Training Compiler는 SageMaker Python SDK 및 SageMaker Hugging Face Deep Learning Containers에 내장되어 있습니다. 속도 향상 이점을 활용하기 위해 워크플로를 변경할 필요가 없습니다. SageMaker 노트북 인스턴스, SageMaker Studio, AWS SDK for Python(Boto3) 및 AWS Command Line Interface(AWS CLI)와 같은 SageMaker 인터페이스를 사용하여 기존과 동일한 방식으로 훈련 작업을 실행할 수 있습니다. 프레임워크 추정기 객체를 생성할 때 TrainingCompilerConfig 클래스를 파라미터로 추가하여 SageMaker Training Compiler를 사용할 수 있습니다. 실제로 이는 단일 GPU 인스턴스에 대한 기존 훈련 작업 스크립트에 몇 줄의 코드가 추가되었음을 의미합니다. 최신 세부 설명서, 샘플 노트북 및 예제는 설명서에서 제공됩니다.\"\n",
            "SageMaker Training Compiler의 요금은 얼마인가요?\n",
            "SageMaker Training Compiler는 SageMaker 훈련 기능이며 SageMaker 고객에게만 추가 비용 없이 제공됩니다. SageMaker Training Compiler를 사용하면 훈련 시간이 단축되므로 실제로 비용을 절감할 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker Training Compiler의 요금은 얼마인가요?, answer : SageMaker Training Compiler는 SageMaker 훈련 기능이며 SageMaker 고객에게만 추가 비용 없이 제공됩니다. SageMaker Training Compiler를 사용하면 훈련 시간이 단축되므로 실제로 비용을 절감할 수 있습니다.\"\n",
            "관리형 스팟 훈련이란 무엇인가요?\n",
            "SageMaker를 통해 관리형 스팟 훈련을 수행할 경우 Amazon EC2 스팟 인스턴스를 사용하여 기계 학습 모델을 훈련할 수 있으며 모델 훈련 비용을 최대 90%나 줄일 수 있습니다.\n",
            "\"category : SageMaker, question : 관리형 스팟 훈련이란 무엇인가요?, answer : SageMaker를 통해 관리형 스팟 훈련을 수행할 경우 Amazon EC2 스팟 인스턴스를 사용하여 기계 학습 모델을 훈련할 수 있으며 모델 훈련 비용을 최대 90%나 줄일 수 있습니다.\"\n",
            "관리형 스팟 훈련을 사용하려면 어떻게 해야 하나요?\n",
            "훈련 작업을 제출할 때 관리형 스팟 훈련 옵션을 사용하도록 설정하고 스팟 용량 지연 시간도 지정합니다. 그러면 SageMaker가 Amazon EC2 스팟 인스턴스를 사용하여 작업을 실행하고 스팟 용량을 관리합니다. 작업이 실행 중이거나 용량 대기 중일 때 모두 훈련 작업 상태를 완전히 파악할 수 있습니다.\n",
            "\"category : SageMaker, question : 관리형 스팟 훈련을 사용하려면 어떻게 해야 하나요?, answer : 훈련 작업을 제출할 때 관리형 스팟 훈련 옵션을 사용하도록 설정하고 스팟 용량 지연 시간도 지정합니다. 그러면 SageMaker가 Amazon EC2 스팟 인스턴스를 사용하여 작업을 실행하고 스팟 용량을 관리합니다. 작업이 실행 중이거나 용량 대기 중일 때 모두 훈련 작업 상태를 완전히 파악할 수 있습니다.\"\n",
            "관리형 스팟 훈련은 어떤 경우에 이용해야 하나요?\n",
            "관리형 스팟 훈련은 훈련 실행에 유연성이 있고 훈련 작업 비용을 최소화하려는 경우에 이상적입니다. 관리형 스팟 훈련을 사용할 경우 기계 학습 모델에 대한 훈련 비용을 최대 90%까지 절감할 수 있습니다.\n",
            "\"category : SageMaker, question : 관리형 스팟 훈련은 어떤 경우에 이용해야 하나요?, answer : 관리형 스팟 훈련은 훈련 실행에 유연성이 있고 훈련 작업 비용을 최소화하려는 경우에 이상적입니다. 관리형 스팟 훈련을 사용할 경우 기계 학습 모델에 대한 훈련 비용을 최대 90%까지 절감할 수 있습니다.\"\n",
            "관리형 스팟 훈련은 어떻게 작동하나요?\n",
            "관리형 스팟 훈련은 훈련에 Amazon EC2 스팟 인스턴스를 사용하며 스팟 인스턴스는 AWS에 용량이 필요한 경우 대체될 수 있습니다. 따라서 관리형 스팟 훈련 작업은 사용 가능한 용량이 생길 때마다 조금씩 실행될 수 있습니다. SageMaker는 최신 모델 체크포인트를 사용하여 훈련 작업을 재개할 수 있으므로 중단이 발생한 경우 훈련 작업을 처음부터 다시 시작할 필요가 없습니다. SageMaker의 기본 제공 프레임워크 및 기본 제공 컴퓨터 비전 알고리즘을 통해 정기적 체크포인트를 사용할 수 있으며 사용자 지정 모델에서 체크포인트를 사용할 수도 있습니다.\n",
            "\"category : SageMaker, question : 관리형 스팟 훈련은 어떻게 작동하나요?, answer : 관리형 스팟 훈련은 훈련에 Amazon EC2 스팟 인스턴스를 사용하며 스팟 인스턴스는 AWS에 용량이 필요한 경우 대체될 수 있습니다. 따라서 관리형 스팟 훈련 작업은 사용 가능한 용량이 생길 때마다 조금씩 실행될 수 있습니다. SageMaker는 최신 모델 체크포인트를 사용하여 훈련 작업을 재개할 수 있으므로 중단이 발생한 경우 훈련 작업을 처음부터 다시 시작할 필요가 없습니다. SageMaker의 기본 제공 프레임워크 및 기본 제공 컴퓨터 비전 알고리즘을 통해 정기적 체크포인트를 사용할 수 있으며 사용자 지정 모델에서 체크포인트를 사용할 수도 있습니다.\"\n",
            "관리형 스팟 훈련에 대해 정기적으로 체크포인트를 점검해야 하나요?\n",
            "장기적인 훈련 작업을 위한 일반적인 모범 사례로서 주기적으로 체크포인트 점검을 수행할 것을 권장합니다. 이렇게 하면 용량이 대체된 경우 관리형 스팟 훈련이 다시 시작되는 것을 방지할 수 있습니다. 체크포인트를 사용하도록 설정하면 SageMaker가 마지막 체크포인트에서 관리형 스팟 훈련 작업을 재개합니다.\n",
            "\"category : SageMaker, question : 관리형 스팟 훈련에 대해 정기적으로 체크포인트를 점검해야 하나요?, answer : 장기적인 훈련 작업을 위한 일반적인 모범 사례로서 주기적으로 체크포인트 점검을 수행할 것을 권장합니다. 이렇게 하면 용량이 대체된 경우 관리형 스팟 훈련이 다시 시작되는 것을 방지할 수 있습니다. 체크포인트를 사용하도록 설정하면 SageMaker가 마지막 체크포인트에서 관리형 스팟 훈련 작업을 재개합니다.\"\n",
            "관리형 스팟 훈련 작업을 통한 비용 절감액을 계산하려면 어떻게 해야 하나요?\n",
            "관리형 스팟 훈련 작업이 완료되면 AWS 관리 콘솔에서 절감액을 확인할 수 있고 훈련 작업 기간과 청구 대상 기간 사이의 백분율 차이로도 비용 절감액을 계산할 수 있습니다.\n",
            "관리형 스팟 훈련 작업의 중단 횟수와 관계없이 데이터를 다운로드한 기간에 대해서 한 번만 비용이 청구됩니다.\n",
            "\"category : SageMaker, question : 관리형 스팟 훈련 작업을 통한 비용 절감액을 계산하려면 어떻게 해야 하나요?, answer : 관리형 스팟 훈련 작업이 완료되면 AWS 관리 콘솔에서 절감액을 확인할 수 있고 훈련 작업 기간과 청구 대상 기간 사이의 백분율 차이로도 비용 절감액을 계산할 수 있습니다.\n",
            "관리형 스팟 훈련 작업의 중단 횟수와 관계없이 데이터를 다운로드한 기간에 대해서 한 번만 비용이 청구됩니다.\"\n",
            "관리형 스팟 훈련에 사용할 수 있는 인스턴스는 무엇인가요?\n",
            "관리형 스팟 훈련은 SageMaker에서 지원되는 모든 인스턴스에서 사용할 수 있습니다.\n",
            "\"category : SageMaker, question : 관리형 스팟 훈련에 사용할 수 있는 인스턴스는 무엇인가요?, answer : 관리형 스팟 훈련은 SageMaker에서 지원되는 모든 인스턴스에서 사용할 수 있습니다.\"\n",
            "관리형 스팟 훈련은 어느 리전에서 지원되나요?\n",
            "관리형 스팟 훈련은 현재 SageMaker가 제공되는 모든 리전에서 지원됩니다.\n",
            "\"category : SageMaker, question : 관리형 스팟 훈련은 어느 리전에서 지원되나요?, answer : 관리형 스팟 훈련은 현재 SageMaker가 제공되는 모든 리전에서 지원됩니다.\"\n",
            "훈련에 사용할 수 있는 데이터 세트의 크기에 제한이 있나요?\n",
            "SageMaker에서 모델 훈련에 사용할 수 있는 데이터 세트의 크기 제한은 없습니다.\n",
            "\"category : SageMaker, question : 훈련에 사용할 수 있는 데이터 세트의 크기에 제한이 있나요?, answer : SageMaker에서 모델 훈련에 사용할 수 있는 데이터 세트의 크기 제한은 없습니다.\"\n",
            "SageMaker에서는 모델을 생성할 때 어떤 알고리즘을 사용하나요?\n",
            "SageMaker에는 선형 회귀, 로지스틱 회귀, k-평균 클러스터링, 주성분 분석, 인수 분해 머신, 신경 주제 모델링, LDA(Latent Dirichlet Allocation), 그래디언트 부스티드 트리, sequence2sequence, 시계열 예측, word2vec 및 이미지 분류에 대한 알고리즘이 내장되어 있습니다. SageMaker는 최적화된 Apache MXNet, Tensorflow, Chainer, PyTorch, Gluon, Keras, Horovod, Scikit-learn 및 Deep Graph Library 컨테이너도 제공합니다. 그뿐 아니라 SageMaker는 문서화된 사양을 준수하는 도커 이미지를 통해 제공된 사용자 지정 훈련 알고리즘을 지원합니다.\n",
            "\"category : SageMaker, question : SageMaker에서는 모델을 생성할 때 어떤 알고리즘을 사용하나요?, answer : SageMaker에는 선형 회귀, 로지스틱 회귀, k-평균 클러스터링, 주성분 분석, 인수 분해 머신, 신경 주제 모델링, LDA(Latent Dirichlet Allocation), 그래디언트 부스티드 트리, sequence2sequence, 시계열 예측, word2vec 및 이미지 분류에 대한 알고리즘이 내장되어 있습니다. SageMaker는 최적화된 Apache MXNet, Tensorflow, Chainer, PyTorch, Gluon, Keras, Horovod, Scikit-learn 및 Deep Graph Library 컨테이너도 제공합니다. 그뿐 아니라 SageMaker는 문서화된 사양을 준수하는 도커 이미지를 통해 제공된 사용자 지정 훈련 알고리즘을 지원합니다.\"\n",
            "자동 모델 튜닝이란 무엇인가요?\n",
            "대부분 기계 학습 알고리즘에서는 기본 알고리즘이 작동하는 방식을 제어하는 다양한 파라미터를 공개합니다. 이러한 파라미터는 일반적으로 하이퍼파라미터라고 부르며 그 값은 훈련된 모델의 품질에 영향을 미칩니다. 자동 모델 튜닝은 알고리즘에서 최적의 모델을 산출할 수 있는 하이퍼파라미터 세트를 찾는 프로세스입니다.\n",
            "\"category : SageMaker, question : 자동 모델 튜닝이란 무엇인가요?, answer : 대부분 기계 학습 알고리즘에서는 기본 알고리즘이 작동하는 방식을 제어하는 다양한 파라미터를 공개합니다. 이러한 파라미터는 일반적으로 하이퍼파라미터라고 부르며 그 값은 훈련된 모델의 품질에 영향을 미칩니다. 자동 모델 튜닝은 알고리즘에서 최적의 모델을 산출할 수 있는 하이퍼파라미터 세트를 찾는 프로세스입니다.\"\n",
            "자동 모델 튜닝으로 어떤 모델을 튜닝할 수 있나요?\n",
            "과학적으로 실현 가능한 한, 내장된 SageMaker 알고리즘, 딥 신경망 또는 도커 이미지 형식으로 SageMaker에 가져오는 임의 알고리즘을 비롯하여 원하는 알고리즘에 대해 SageMaker에서 자동 모델 튜닝을 실행할 수 있습니다.\n",
            "\"category : SageMaker, question : 자동 모델 튜닝으로 어떤 모델을 튜닝할 수 있나요?, answer : 과학적으로 실현 가능한 한, 내장된 SageMaker 알고리즘, 딥 신경망 또는 도커 이미지 형식으로 SageMaker에 가져오는 임의 알고리즘을 비롯하여 원하는 알고리즘에 대해 SageMaker에서 자동 모델 튜닝을 실행할 수 있습니다.\"\n",
            "SageMaker 외부에서 자동 모델 튜닝을 사용할 수 있나요?\n",
            "현재는 지원되지 않습니다. 최상의 모델 튜닝 성능과 경험은 SageMaker 내에서 제공됩니다.\n",
            "\"category : SageMaker, question : SageMaker 외부에서 자동 모델 튜닝을 사용할 수 있나요?, answer : 현재는 지원되지 않습니다. 최상의 모델 튜닝 성능과 경험은 SageMaker 내에서 제공됩니다.\"\n",
            "자동 모델 튜닝의 기본 튜닝 알고리즘은 무엇인가요?\n",
            "현재 튜닝 하이퍼파라미터에 대한 알고리즘은 베이지안 최적화를 사용자 지정하여 구현한 것입니다. 튜닝 프로세스 전반에 걸쳐 고객이 지정한 목표 지표를 최적화하는 것을 목표로 합니다. 완료된 훈련 작업의 객체 지표를 확인하고 지식을 사용하여 다른 훈련 작업을 위한 하이퍼파라미터 조합을 추론합니다.\n",
            "\"category : SageMaker, question : 자동 모델 튜닝의 기본 튜닝 알고리즘은 무엇인가요?, answer : 현재 튜닝 하이퍼파라미터에 대한 알고리즘은 베이지안 최적화를 사용자 지정하여 구현한 것입니다. 튜닝 프로세스 전반에 걸쳐 고객이 지정한 목표 지표를 최적화하는 것을 목표로 합니다. 완료된 훈련 작업의 객체 지표를 확인하고 지식을 사용하여 다른 훈련 작업을 위한 하이퍼파라미터 조합을 추론합니다.\"\n",
            "자동 모델 튜닝에 권장되는 특정 하이퍼파라미터가 있나요?\n",
            "아니요. 특정 하이퍼파라미터가 모델 성능에 미치는 영향은 다양한 요소에 따라 다르며 하나의 하이퍼파라미터가 다른 하이퍼파라미터보다 중요하므로 튜닝을 해야 한다고 단정적으로 말하기는 어렵습니다. SageMaker에 내장된 알고리즘의 경우 AWS에서 하이퍼파라미터의 튜닝 가능 여부를 알려드립니다.\n",
            "\"category : SageMaker, question : 자동 모델 튜닝에 권장되는 특정 하이퍼파라미터가 있나요?, answer : 아니요. 특정 하이퍼파라미터가 모델 성능에 미치는 영향은 다양한 요소에 따라 다르며 하나의 하이퍼파라미터가 다른 하이퍼파라미터보다 중요하므로 튜닝을 해야 한다고 단정적으로 말하기는 어렵습니다. SageMaker에 내장된 알고리즘의 경우 AWS에서 하이퍼파라미터의 튜닝 가능 여부를 알려드립니다.\"\n",
            "하이퍼파라미터 튜닝 작업은 얼마나 걸리나요?\n",
            "하이퍼파라미터 튜닝 작업에 걸리는 시간은 데이터 크기, 기본 알고리즘, 하이퍼파라미터의 값 등 여러 요소에 따라 달라집니다. 또한 고객은 동시 훈련 작업 수와 총 훈련 작업 수를 선택할 수 있습니다. 이 모든 선택이 하이퍼파라미터 튜닝 작업에 걸리는 시간에 영향을 미칩니다.\n",
            "\"category : SageMaker, question : 하이퍼파라미터 튜닝 작업은 얼마나 걸리나요?, answer : 하이퍼파라미터 튜닝 작업에 걸리는 시간은 데이터 크기, 기본 알고리즘, 하이퍼파라미터의 값 등 여러 요소에 따라 달라집니다. 또한 고객은 동시 훈련 작업 수와 총 훈련 작업 수를 선택할 수 있습니다. 이 모든 선택이 하이퍼파라미터 튜닝 작업에 걸리는 시간에 영향을 미칩니다.\"\n",
            "모델의 속도와 정확도를 최적화하는 것처럼 여러 목표를 동시에 최적화할 수 있나요?\n",
            "현재는 지원되지 않습니다. 현재는 단일 목표 지표를 지정하여 알고리즘 코드를 최적화하거나 변경하여 2개 이상의 유용한 지표 간 가중 평균인 새로운 지표를 생성해야 하며 튜닝 프로세스를 해당 목표 지표에 맞게 최적화해야 합니다.\n",
            "\"category : SageMaker, question : 모델의 속도와 정확도를 최적화하는 것처럼 여러 목표를 동시에 최적화할 수 있나요?, answer : 현재는 지원되지 않습니다. 현재는 단일 목표 지표를 지정하여 알고리즘 코드를 최적화하거나 변경하여 2개 이상의 유용한 지표 간 가중 평균인 새로운 지표를 생성해야 하며 튜닝 프로세스를 해당 목표 지표에 맞게 최적화해야 합니다.\"\n",
            "자동 모델 튜닝은 비용이 어떻게 되나요?\n",
            "하이퍼파라미터 튜닝 작업 자체는 무료입니다. 모델 훈련 요금을 기준으로 하이퍼파라미터 튜닝 작업에서 시작하는 훈련 작업에 대한 비용이 부과됩니다.\n",
            "\"category : SageMaker, question : 자동 모델 튜닝은 비용이 어떻게 되나요?, answer : 하이퍼파라미터 튜닝 작업 자체는 무료입니다. 모델 훈련 요금을 기준으로 하이퍼파라미터 튜닝 작업에서 시작하는 훈련 작업에 대한 비용이 부과됩니다.\"\n",
            "SageMaker Autopilot이나 자동 모델 튜닝을 사용하는 상황은 어떻게 판단해야 하나요?\n",
            "SageMaker Autopilot은 분류 및 회귀 사용 사례에 중점을 두면서 기능 사전 처리, 알고리즘 선택, 하이퍼파라미터 튜닝 같은 일반적인 기계 학습 워크플로의 모든 것을 자동화합니다. 반면 자동 모델 튜닝은 모델의 기반이 기본 제공 알고리즘이든, 딥 러닝 프레임워크든, 사용자 지정 컨테이너든 관계없이 모든 모델을 튜닝하도록 설계되어 있습니다. 유연성을 높이려면 특정 알고리즘을 직접 선택하고, 튜닝할 하이퍼파라미터와 해당하는 검색 범위를 결정해야 합니다.\n",
            "\"category : SageMaker, question : SageMaker Autopilot이나 자동 모델 튜닝을 사용하는 상황은 어떻게 판단해야 하나요?, answer : SageMaker Autopilot은 분류 및 회귀 사용 사례에 중점을 두면서 기능 사전 처리, 알고리즘 선택, 하이퍼파라미터 튜닝 같은 일반적인 기계 학습 워크플로의 모든 것을 자동화합니다. 반면 자동 모델 튜닝은 모델의 기반이 기본 제공 알고리즘이든, 딥 러닝 프레임워크든, 사용자 지정 컨테이너든 관계없이 모든 모델을 튜닝하도록 설계되어 있습니다. 유연성을 높이려면 특정 알고리즘을 직접 선택하고, 튜닝할 하이퍼파라미터와 해당하는 검색 범위를 결정해야 합니다.\"\n",
            "Q: 강화 학습이란 무엇입니까?\n",
            "강화 학습은 에이전트가 자신의 작업과 경험의 피드백을 사용하여 시행착오를 통해 대화형 환경에서 학습할 수 있는 기계 학습 기법입니다.\n",
            "\"category : SageMaker, question : Q: 강화 학습이란 무엇입니까?, answer : 강화 학습은 에이전트가 자신의 작업과 경험의 피드백을 사용하여 시행착오를 통해 대화형 환경에서 학습할 수 있는 기계 학습 기법입니다.\"\n",
            "SageMaker에서 강화 학습 모델을 훈련할 수 있나요?\n",
            "예, SageMaker에서 지도 학습 및 비지도 학습에 추가하여 강화 학습 모델을 훈련할 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker에서 강화 학습 모델을 훈련할 수 있나요?, answer : 예, SageMaker에서 지도 학습 및 비지도 학습에 추가하여 강화 학습 모델을 훈련할 수 있습니다.\"\n",
            "Q: 강화 학습은 지도 학습과 어떻게 다릅니까?\n",
            "지도 학습과 강화 학습은 입력 및 출력 간의 매핑을 사용하지만, 태스크를 수행하기 위한 올바른 작업 세트가 에이전트에게 피드백으로 제공되는 지도 학습과는 달리, 강화 학습은 일련의 작업을 통해 장기적인 목표를 실현할 수 있도록 보상 신호를 최적화하는 지연된 피드백을 사용합니다.\n",
            "\"category : SageMaker, question : Q: 강화 학습은 지도 학습과 어떻게 다릅니까?, answer : 지도 학습과 강화 학습은 입력 및 출력 간의 매핑을 사용하지만, 태스크를 수행하기 위한 올바른 작업 세트가 에이전트에게 피드백으로 제공되는 지도 학습과는 달리, 강화 학습은 일련의 작업을 통해 장기적인 목표를 실현할 수 있도록 보상 신호를 최적화하는 지연된 피드백을 사용합니다.\"\n",
            "Q: 강화 학습은 언제 사용해야 합니까?\n",
            "지도 학습 기법의 목적은 훈련 데이터의 패턴을 기반으로 올바른 답변을 찾는 것인 반면, 비지도 학습 기법의 목적은 데이터 포인트 간의 유사점과 차이점을 찾는 것입니다. 반대로, 강화 학습(RL) 기법의 목적은 결과를 달성하는 방법이 확실하지 않더라도 원하는 결과를 달성하는 방법을 학습하는 것입니다. 결과적으로 RL은 로봇 공학, 자율 주행 차량, HVAC, 산업 제어 등과 같이 에이전트가 자율적 결정을 수행할 수 있는 지능형 애플리케이션을 지원하는 데 더 적합합니다.\n",
            "\"category : SageMaker, question : Q: 강화 학습은 언제 사용해야 합니까?, answer : 지도 학습 기법의 목적은 훈련 데이터의 패턴을 기반으로 올바른 답변을 찾는 것인 반면, 비지도 학습 기법의 목적은 데이터 포인트 간의 유사점과 차이점을 찾는 것입니다. 반대로, 강화 학습(RL) 기법의 목적은 결과를 달성하는 방법이 확실하지 않더라도 원하는 결과를 달성하는 방법을 학습하는 것입니다. 결과적으로 RL은 로봇 공학, 자율 주행 차량, HVAC, 산업 제어 등과 같이 에이전트가 자율적 결정을 수행할 수 있는 지능형 애플리케이션을 지원하는 데 더 적합합니다.\"\n",
            "RL 모델을 훈련하는 데 어떤 유형의 환경을 사용할 수 있나요?\n",
            "Amazon SageMaker RL은 RL 모델을 훈련할 수 있는 다양한 환경을 지원합니다. AWS RoboMaker와 같은 AWS 서비스, Open AI Gym 인터페이스를 사용하여 개발된 오픈 소스 환경 또는 사용자 지정 환경, MATLAB 및 SimuLink와 같은 상업용 시뮬레이션 환경을 사용할 수 있습니다.\n",
            "\"category : SageMaker, question : RL 모델을 훈련하는 데 어떤 유형의 환경을 사용할 수 있나요?, answer : Amazon SageMaker RL은 RL 모델을 훈련할 수 있는 다양한 환경을 지원합니다. AWS RoboMaker와 같은 AWS 서비스, Open AI Gym 인터페이스를 사용하여 개발된 오픈 소스 환경 또는 사용자 지정 환경, MATLAB 및 SimuLink와 같은 상업용 시뮬레이션 환경을 사용할 수 있습니다.\"\n",
            "RL 모델을 훈련하기 위해 고유의 RL 에이전트 알고리즘을 작성해야 하나요?\n",
            "아니요. SageMaker RL에는 DQN, PPO, A3C 등과 같은 RL 에이전트 알고리즘의 구현을 제공하는 Coach 및 Ray RLLib와 같은 RL 도구 키트가 포함되어 있습니다.\n",
            "\"category : SageMaker, question : RL 모델을 훈련하기 위해 고유의 RL 에이전트 알고리즘을 작성해야 하나요?, answer : 아니요. SageMaker RL에는 DQN, PPO, A3C 등과 같은 RL 에이전트 알고리즘의 구현을 제공하는 Coach 및 Ray RLLib와 같은 RL 도구 키트가 포함되어 있습니다.\"\n",
            "고유의 RL 라이브러리 및 알고리즘 구현을 가져와서 SageMaker RL에서 실행할 수 있나요?\n",
            "예. 고유의 RL 라이브러리 및 알고리즘 구현을 도커 컨테이너로 가져온 다음 SageMaker RL에서 실행할 수 있습니다.\n",
            "\"category : SageMaker, question : 고유의 RL 라이브러리 및 알고리즘 구현을 가져와서 SageMaker RL에서 실행할 수 있나요?, answer : 예. 고유의 RL 라이브러리 및 알고리즘 구현을 도커 컨테이너로 가져온 다음 SageMaker RL에서 실행할 수 있습니다.\"\n",
            "SageMaker RL을 사용하여 배포된 롤아웃을 수행할 수 있나요?\n",
            "예. 다른 유형의 클러스터를 선택할 수도 있습니다. 이 경우 한 GPU 인스턴스에서 훈련을 실행하고 여러 CPU 인스턴스에서 시뮬레이션을 실행할 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker RL을 사용하여 배포된 롤아웃을 수행할 수 있나요?, answer : 예. 다른 유형의 클러스터를 선택할 수도 있습니다. 이 경우 한 GPU 인스턴스에서 훈련을 실행하고 여러 CPU 인스턴스에서 시뮬레이션을 실행할 수 있습니다.\"\n",
            "SageMaker에서 제공하는 배포 옵션은 어떤 것들이 있나요?\n",
            "모델을 구축 및 훈련한 후 SageMaker에서는 3가지 옵션으로 모델을 배포하고 예측을 시작할 수 있습니다. 실시간 추론은 밀리초 단위의 지연 시간 요구 사항, 최대 6MB의 페이로드 크기, 최대 60초의 처리 시간을 요구하는 워크로드에 적합합니다. 배치 변환은 사전에 제공되는 대규모 데이터 배치에 대한 오프라인 예측에 이상적입니다. Asynchronous Inference는 1초 미만의 대기 시간 요구 사항, 최대 1GB의 페이로드 크기, 최대 15분의 처리 시간이 필요하지 않은 워크로드를 위해 설계되었습니다.\n",
            "\"category : SageMaker, question : SageMaker에서 제공하는 배포 옵션은 어떤 것들이 있나요?, answer : 모델을 구축 및 훈련한 후 SageMaker에서는 3가지 옵션으로 모델을 배포하고 예측을 시작할 수 있습니다. 실시간 추론은 밀리초 단위의 지연 시간 요구 사항, 최대 6MB의 페이로드 크기, 최대 60초의 처리 시간을 요구하는 워크로드에 적합합니다. 배치 변환은 사전에 제공되는 대규모 데이터 배치에 대한 오프라인 예측에 이상적입니다. Asynchronous Inference는 1초 미만의 대기 시간 요구 사항, 최대 1GB의 페이로드 크기, 최대 15분의 처리 시간이 필요하지 않은 워크로드를 위해 설계되었습니다.\"\n",
            "Amazon SageMaker 비동기 추론이란 무엇인가요?\n",
            "SageMaker 비동기 추론은 수신 요청을 대기열에 넣고 비동기식으로 처리합니다. 이 옵션은 페이로드 크기가 크고 처리 시간이 길어서 도착 즉시 처리되어야 하는 요청에 이상적입니다. 선택적으로 자동 크기 조정 설정을 구성하여 요청을 바쁘게 처리하지 않을 때 인스턴스 수를 0개로 축소하여 비용을 절감할 수 있습니다.\n",
            "\"category : SageMaker, question : Amazon SageMaker 비동기 추론이란 무엇인가요?, answer : SageMaker 비동기 추론은 수신 요청을 대기열에 넣고 비동기식으로 처리합니다. 이 옵션은 페이로드 크기가 크고 처리 시간이 길어서 도착 즉시 처리되어야 하는 요청에 이상적입니다. 선택적으로 자동 크기 조정 설정을 구성하여 요청을 바쁘게 처리하지 않을 때 인스턴스 수를 0개로 축소하여 비용을 절감할 수 있습니다.\"\n",
            "요청을 능동적으로 처리하지 않을 때 인스턴스 수를 0으로 스케일 다운하도록 자동 크기 조정 설정을 구성하려면 어떻게 해야 하나요?\n",
            "SageMaker 비동기 추론 엔드포인트 인스턴스 수를 0개로 스케일 다운하면 요청을 능동적으로 처리하지 않을 때 비용을 절감할 수 있습니다. 'ApproximateBacklogPerInstance' 사용자 지정 지표에서 크기를 조정할 크기 조정 정책을 정의하고 'MinCapacity' 값을 0으로 설정해야 합니다. 단계별 지침은 개발자 안내서에서 비동기 엔드포인트 자동 크기 조정 섹션을 참조하세요.\n",
            "\"category : SageMaker, question : 요청을 능동적으로 처리하지 않을 때 인스턴스 수를 0으로 스케일 다운하도록 자동 크기 조정 설정을 구성하려면 어떻게 해야 하나요?, answer : SageMaker 비동기 추론 엔드포인트 인스턴스 수를 0개로 스케일 다운하면 요청을 능동적으로 처리하지 않을 때 비용을 절감할 수 있습니다. 'ApproximateBacklogPerInstance' 사용자 지정 지표에서 크기를 조정할 크기 조정 정책을 정의하고 'MinCapacity' 값을 0으로 설정해야 합니다. 단계별 지침은 개발자 안내서에서 비동기 엔드포인트 자동 크기 조정 섹션을 참조하세요.\"\n",
            "Amazon SageMaker 서버리스 추론이란 무엇인가요?\n",
            "SageMaker 서버리스 추론은 기계 학습 모델을 쉽게 배포하고 확장할 수 있도록 특별히 제작된 서버리스 모델 제공 옵션입니다. SageMaker 서버리스 추론 엔드포인트는 컴퓨팅 리소스를 자동으로 시작하고 트래픽에 따라 리소스를 확장 및 축소하므로 인스턴스 유형을 선택하거나 프로비저닝된 용량을 실행하거나 확장을 관리할 필요가 없습니다. 서버리스 추론 엔드포인트에 대한 메모리 요구 사항을 지정할 수도 있습니다. 유휴 기간이 아닌 추론 코드를 실행하는 기간과 처리된 데이터 양에 대해서만 비용을 지불합니다.\n",
            "\"category : SageMaker, question : Amazon SageMaker 서버리스 추론이란 무엇인가요?, answer : SageMaker 서버리스 추론은 기계 학습 모델을 쉽게 배포하고 확장할 수 있도록 특별히 제작된 서버리스 모델 제공 옵션입니다. SageMaker 서버리스 추론 엔드포인트는 컴퓨팅 리소스를 자동으로 시작하고 트래픽에 따라 리소스를 확장 및 축소하므로 인스턴스 유형을 선택하거나 프로비저닝된 용량을 실행하거나 확장을 관리할 필요가 없습니다. 서버리스 추론 엔드포인트에 대한 메모리 요구 사항을 지정할 수도 있습니다. 유휴 기간이 아닌 추론 코드를 실행하는 기간과 처리된 데이터 양에 대해서만 비용을 지불합니다.\"\n",
            "SageMaker 서버리스 추론을 사용해야 하는 이유는 무엇인가요?\n",
            "SageMaker 서버리스 추론은 용량을 미리 프로비저닝하고 조정 정책을 관리할 필요가 없기 때문에 개발자 경험이 단순화됩니다. SageMaker 서버리스 추론은 사용 패턴에 따라 몇 초 안에 수십 개에서 수천 개의 추론으로 즉시 확장할 수 있으므로 트래픽이 간헐적이거나 예측할 수 없는 기계 학습 애플리케이션에 이상적입니다. 예를 들어 급여 처리 회사의 챗봇 서비스는 월말에 문의가 증가하지만 나머지 월에는 트래픽이 간헐적입니다. 이러한 상황에서 전체 달에 대한 인스턴스를 프로비저닝하는 것은 유휴 기간의 비용을 지불하게 되므로 비용 효율적이지 않습니다. SageMaker Serverless Inference는 트래픽을 미리 예측하거나 확장 정책을 관리할 필요 없이 즉시 사용 가능한 자동 확장을 제공하여 이러한 유형의 사용 사례를 해결하는 데 도움이 됩니다. 또한 추론 코드를 실행하는 컴퓨팅 시간(밀리초 단위로 청구)과 데이터 처리에 대해서만 비용을 지불하면 트래픽이 간헐적으로 발생하는 워크로드에 대해 비용 효율적인 옵션이 됩니다.\n",
            "\"category : SageMaker, question : SageMaker 서버리스 추론을 사용해야 하는 이유는 무엇인가요?, answer : SageMaker 서버리스 추론은 용량을 미리 프로비저닝하고 조정 정책을 관리할 필요가 없기 때문에 개발자 경험이 단순화됩니다. SageMaker 서버리스 추론은 사용 패턴에 따라 몇 초 안에 수십 개에서 수천 개의 추론으로 즉시 확장할 수 있으므로 트래픽이 간헐적이거나 예측할 수 없는 기계 학습 애플리케이션에 이상적입니다. 예를 들어 급여 처리 회사의 챗봇 서비스는 월말에 문의가 증가하지만 나머지 월에는 트래픽이 간헐적입니다. 이러한 상황에서 전체 달에 대한 인스턴스를 프로비저닝하는 것은 유휴 기간의 비용을 지불하게 되므로 비용 효율적이지 않습니다. SageMaker Serverless Inference는 트래픽을 미리 예측하거나 확장 정책을 관리할 필요 없이 즉시 사용 가능한 자동 확장을 제공하여 이러한 유형의 사용 사례를 해결하는 데 도움이 됩니다. 또한 추론 코드를 실행하는 컴퓨팅 시간(밀리초 단위로 청구)과 데이터 처리에 대해서만 비용을 지불하면 트래픽이 간헐적으로 발생하는 워크로드에 대해 비용 효율적인 옵션이 됩니다.\"\n",
            "SageMaker 서버리스 추론의 프로비저닝된 동시성이란 무엇인가요?\n",
            "프로비저닝된 동시성을 사용하면 지정된 수의 동시 요청에 대해 엔드포인트를 웜 상태로 유지하여 예측 가능한 성능과 높은 확장성으로 서버리스 엔드포인트에 모델을 배포할 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker 서버리스 추론의 프로비저닝된 동시성이란 무엇인가요?, answer : 프로비저닝된 동시성을 사용하면 지정된 수의 동시 요청에 대해 엔드포인트를 웜 상태로 유지하여 예측 가능한 성능과 높은 확장성으로 서버리스 엔드포인트에 모델을 배포할 수 있습니다.\"\n",
            "프로비저닝된 동시성을 사용해야 하는 이유는 무엇인가요?\n",
            "온디맨드 서버리스 엔드포인트의 경우 엔드포인트가 한동안 트래픽을 수신하지 못하다가 갑자기 새 요청을 받게 되면 컴퓨팅 리소스를 가동하여 요청을 처리하는 데 시간이 걸릴 수 있습니다. 이를 콜드 스타트라고 합니다. 동시 요청이 현재 동시 요청 사용량을 초과하는 경우에도 콜드 스타트가 발생할 수 있습니다. 콜드 스타트 시간은 모델 크기, 모델을 다운로드하는 데 걸리는 시간, 컨테이너의 시작 시간에 따라 달라집니다.\n",
            "지연 시간 프로파일의 가변성을 줄이려면 서버리스 엔드포인트에 프로비저닝된 동시성을 선택적으로 사용할 수 있습니다. 프로비저닝된 동시성을 사용하면 서버리스 엔드포인트가 항상 준비되어 있기 때문에 콜드 스타트 없이 트래픽 급증을 즉시 처리할 수 있습니다.\n",
            "\"category : SageMaker, question : 프로비저닝된 동시성을 사용해야 하는 이유는 무엇인가요?, answer : 온디맨드 서버리스 엔드포인트의 경우 엔드포인트가 한동안 트래픽을 수신하지 못하다가 갑자기 새 요청을 받게 되면 컴퓨팅 리소스를 가동하여 요청을 처리하는 데 시간이 걸릴 수 있습니다. 이를 콜드 스타트라고 합니다. 동시 요청이 현재 동시 요청 사용량을 초과하는 경우에도 콜드 스타트가 발생할 수 있습니다. 콜드 스타트 시간은 모델 크기, 모델을 다운로드하는 데 걸리는 시간, 컨테이너의 시작 시간에 따라 달라집니다.\n",
            "지연 시간 프로파일의 가변성을 줄이려면 서버리스 엔드포인트에 프로비저닝된 동시성을 선택적으로 사용할 수 있습니다. 프로비저닝된 동시성을 사용하면 서버리스 엔드포인트가 항상 준비되어 있기 때문에 콜드 스타트 없이 트래픽 급증을 즉시 처리할 수 있습니다.\"\n",
            "프로비저닝된 동시성 요금은 어떻게 부과되나요?\n",
            "온디맨드 Serverless Inference와 마찬가지로 프로비저닝된 동시성을 사용하는 경우 추론 요청을 처리하는 데 사용된 컴퓨팅 파워에 대한 요금이 부과됩니다. 요금은 밀리초 단위로 처리된 데이터 양에 따라 부과됩니다. 또한 프로비저닝된 동시성 사용 요금은 구성된 메모리, 프로비저닝된 기간 및 사용된 동시성 양에 따라서도 부과됩니다. 자세한 내용은 Amazon SageMaker 요금을 참조하세요.\n",
            "\"category : SageMaker, question : 프로비저닝된 동시성 요금은 어떻게 부과되나요?, answer : 온디맨드 Serverless Inference와 마찬가지로 프로비저닝된 동시성을 사용하는 경우 추론 요청을 처리하는 데 사용된 컴퓨팅 파워에 대한 요금이 부과됩니다. 요금은 밀리초 단위로 처리된 데이터 양에 따라 부과됩니다. 또한 프로비저닝된 동시성 사용 요금은 구성된 메모리, 프로비저닝된 기간 및 사용된 동시성 양에 따라서도 부과됩니다. 자세한 내용은 Amazon SageMaker 요금을 참조하세요.\"\n",
            "Amazon SageMaker 섀도우 테스트란 무엇인가요?\n",
            "SageMaker는 프로덕션 릴리스 전에 현재 배포된 모델을 기준으로 새로운 ML 모델의 성능을 테스트하여 새 모델을 평가하는 섀도우 테스트를 실행하는 데 도움이 됩니다. SageMaker는 새 모델을 현재 프로덕션 모델과 함께 섀도우 모드에 배포하고 프로덕션 트래픽에서 사용자가 지정한 부분을 새 모델에 미러링합니다. 필요한 경우 오프라인 비교를 위해 모델 추론을 로깅합니다. 지연 시간 및 오류율과 같은 주요 성능 지표를 프로덕션 모델과 섀도우 모델 간에 비교하여 새 모델을 프로덕션으로 승격할지 여부를 결정하는 데 도움이 되는 라이브 대시보드도 제공합니다.\n",
            "\"category : SageMaker, question : Amazon SageMaker 섀도우 테스트란 무엇인가요?, answer : SageMaker는 프로덕션 릴리스 전에 현재 배포된 모델을 기준으로 새로운 ML 모델의 성능을 테스트하여 새 모델을 평가하는 섀도우 테스트를 실행하는 데 도움이 됩니다. SageMaker는 새 모델을 현재 프로덕션 모델과 함께 섀도우 모드에 배포하고 프로덕션 트래픽에서 사용자가 지정한 부분을 새 모델에 미러링합니다. 필요한 경우 오프라인 비교를 위해 모델 추론을 로깅합니다. 지연 시간 및 오류율과 같은 주요 성능 지표를 프로덕션 모델과 섀도우 모델 간에 비교하여 새 모델을 프로덕션으로 승격할지 여부를 결정하는 데 도움이 되는 라이브 대시보드도 제공합니다.\"\n",
            "섀도우 테스트에 SageMaker를 사용해야 하는 이유는 무엇인가요?\n",
            "SageMaker를 사용하면 섀도우 변형을 설정하고 모니터링하는 프로세스가 간소화되므로 라이브 프로덕션 트래픽에서 새 ML 모델의 성능을 평가할 수 있습니다. SageMaker를 사용하면 섀도우 테스트용으로 인프라를 오케스트레이션할 필요가 없습니다. 섀도우 변형으로 미러링되는 트래픽의 비율 및 테스트 기간과 같은 테스트 파라미터를 제어할 수 있기 때문입니다. 따라서 소규모로 시작한 후 모델 성능에 대해 확신이 생기면 추론 요청을 새 모델로 늘려갈 수 있습니다. SageMaker는 주요 지표의 성능 차이를 표시하는 라이브 대시보드를 생성합니다. 따라서 모델 성능을 손쉽게 비교하여 새 모델과 프로덕션 모델 간의 차이를 평가할 수 있습니다.\n",
            "\"category : SageMaker, question : 섀도우 테스트에 SageMaker를 사용해야 하는 이유는 무엇인가요?, answer : SageMaker를 사용하면 섀도우 변형을 설정하고 모니터링하는 프로세스가 간소화되므로 라이브 프로덕션 트래픽에서 새 ML 모델의 성능을 평가할 수 있습니다. SageMaker를 사용하면 섀도우 테스트용으로 인프라를 오케스트레이션할 필요가 없습니다. 섀도우 변형으로 미러링되는 트래픽의 비율 및 테스트 기간과 같은 테스트 파라미터를 제어할 수 있기 때문입니다. 따라서 소규모로 시작한 후 모델 성능에 대해 확신이 생기면 추론 요청을 새 모델로 늘려갈 수 있습니다. SageMaker는 주요 지표의 성능 차이를 표시하는 라이브 대시보드를 생성합니다. 따라서 모델 성능을 손쉽게 비교하여 새 모델과 프로덕션 모델 간의 차이를 평가할 수 있습니다.\"\n",
            "Amazon SageMaker Inference Recommender란 무엇인가요?\n",
            "SageMaker Inference Recommender는 성능 벤치마킹을 자동화하고 SageMaker 기계 학습 인스턴스에서 모델 성능을 조정하여 프로덕션에서 ML 모델을 가져오는 데 필요한 시간을 줄여줍니다. 이제 SageMaker Inference Recommender를 사용하여 엔드포인트에 최상의 성능을 제공하고 비용을 최소화하는 모델을 배포할 수 있습니다. 몇 분 만에 SageMaker Inference Recommender를 시작하면서 인스턴스 유형을 선택하고 몇 시간 안에 최적의 엔드포인트 구성에 대한 권장 사항을 얻을 수 있으므로 몇 주에 걸친 수동 테스트 및 조정 시간이 필요 없습니다. SageMaker Inference Recommender를 사용하면 로드 테스트 중에 사용된 SageMaker 기계 학습 인스턴스에 대해서만 비용을 지불하고 추가 요금은 없습니다.\n",
            "\"category : SageMaker, question : Amazon SageMaker Inference Recommender란 무엇인가요?, answer : SageMaker Inference Recommender는 성능 벤치마킹을 자동화하고 SageMaker 기계 학습 인스턴스에서 모델 성능을 조정하여 프로덕션에서 ML 모델을 가져오는 데 필요한 시간을 줄여줍니다. 이제 SageMaker Inference Recommender를 사용하여 엔드포인트에 최상의 성능을 제공하고 비용을 최소화하는 모델을 배포할 수 있습니다. 몇 분 만에 SageMaker Inference Recommender를 시작하면서 인스턴스 유형을 선택하고 몇 시간 안에 최적의 엔드포인트 구성에 대한 권장 사항을 얻을 수 있으므로 몇 주에 걸친 수동 테스트 및 조정 시간이 필요 없습니다. SageMaker Inference Recommender를 사용하면 로드 테스트 중에 사용된 SageMaker 기계 학습 인스턴스에 대해서만 비용을 지불하고 추가 요금은 없습니다.\"\n",
            "SageMaker Inference Recommender를 사용해야 하는 이유는 무엇인가요?\n",
            "성능을 개선하고 비용을 줄이기 위해 올바른 엔드포인트 구성에 대한 권장 사항이 필요한 경우 SageMaker Inference Recommender를 사용해야 합니다. 이전에는 모델을 배포하려는 데이터 사이언티스트가 올바른 엔드포인트 구성을 선택하기 위해 수동 벤치마크를 실행해야 했습니다. 또한 먼저 모델 및 샘플 페이로드의 리소스 요구 사항을 기반으로 사용 가능한 70개 이상의 인스턴스 유형 중에서 올바른 기계 학습 인스턴스 유형을 선택한 다음 다양한 하드웨어를 설명하도록 모델을 최적화해야 했습니다. 그런 다음 대기 시간 및 처리량 요구 사항이 충족되고 비용이 저렴한지 검증하기 위해 광범위한 로드 테스트를 수행해야 했습니다. SageMaker Inference Recommender는 다음을 용이하게 함으로써 이러한 복잡성을 제거합니다. 1) 인스턴스 권장 사항으로 몇 분 안에 시작합니다. 2) 인스턴스 유형 전반에 걸쳐 로드 테스트를 수행하여 몇 시간 내에 엔드포인트 구성에 대한 권장 사항을 얻습니다. 3) 컨테이너 및 모델 서버 파라미터를 자동으로 조정하고 주어진 인스턴스 유형에 대한 모델 최적화를 수행합니다.\n",
            "\"category : SageMaker, question : SageMaker Inference Recommender를 사용해야 하는 이유는 무엇인가요?, answer : 성능을 개선하고 비용을 줄이기 위해 올바른 엔드포인트 구성에 대한 권장 사항이 필요한 경우 SageMaker Inference Recommender를 사용해야 합니다. 이전에는 모델을 배포하려는 데이터 사이언티스트가 올바른 엔드포인트 구성을 선택하기 위해 수동 벤치마크를 실행해야 했습니다. 또한 먼저 모델 및 샘플 페이로드의 리소스 요구 사항을 기반으로 사용 가능한 70개 이상의 인스턴스 유형 중에서 올바른 기계 학습 인스턴스 유형을 선택한 다음 다양한 하드웨어를 설명하도록 모델을 최적화해야 했습니다. 그런 다음 대기 시간 및 처리량 요구 사항이 충족되고 비용이 저렴한지 검증하기 위해 광범위한 로드 테스트를 수행해야 했습니다. SageMaker Inference Recommender는 다음을 용이하게 함으로써 이러한 복잡성을 제거합니다. 1) 인스턴스 권장 사항으로 몇 분 안에 시작합니다. 2) 인스턴스 유형 전반에 걸쳐 로드 테스트를 수행하여 몇 시간 내에 엔드포인트 구성에 대한 권장 사항을 얻습니다. 3) 컨테이너 및 모델 서버 파라미터를 자동으로 조정하고 주어진 인스턴스 유형에 대한 모델 최적화를 수행합니다.\"\n",
            "SageMaker Inference Recommender는 다른 AWS 서비스와 어떻게 작동하나요?\n",
            "데이터 사이언티스트는 SageMaker Studio, Python용 AWS SDK(Boto3) 또는 AWS CLI에서 SageMaker Inference Recommender에 액세스할 수 있습니다. 등록된 모델 버전에 대한 SageMaker 모델 레지스트리의 SageMaker Studio 내에서 배포 권장 사항을 얻을 수 있습니다. 데이터 사이언티스트는 SageMaker Studio, AWS SDK 또는 AWS CLI를 통해 권장 사항을 검색하고 필터링할 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker Inference Recommender는 다른 AWS 서비스와 어떻게 작동하나요?, answer : 데이터 사이언티스트는 SageMaker Studio, Python용 AWS SDK(Boto3) 또는 AWS CLI에서 SageMaker Inference Recommender에 액세스할 수 있습니다. 등록된 모델 버전에 대한 SageMaker 모델 레지스트리의 SageMaker Studio 내에서 배포 권장 사항을 얻을 수 있습니다. 데이터 사이언티스트는 SageMaker Studio, AWS SDK 또는 AWS CLI를 통해 권장 사항을 검색하고 필터링할 수 있습니다.\"\n",
            "SageMaker Inference Recommender는 다중 모델 엔드포인트 또는 다중 컨테이너 엔드포인트를 지원할 수 있나요?\n",
            "아니요. 현재 엔드포인트당 단일 모델만 지원합니다.\n",
            "\"category : SageMaker, question : SageMaker Inference Recommender는 다중 모델 엔드포인트 또는 다중 컨테이너 엔드포인트를 지원할 수 있나요?, answer : 아니요. 현재 엔드포인트당 단일 모델만 지원합니다.\"\n",
            "SageMaker Inference Recommender는 어떤 유형의 엔드포인트를 지원하나요?\n",
            "현재는 실시간 엔드포인트만 지원합니다.\n",
            "\"category : SageMaker, question : SageMaker Inference Recommender는 어떤 유형의 엔드포인트를 지원하나요?, answer : 현재는 실시간 엔드포인트만 지원합니다.\"\n",
            "한 리전에서 SageMaker Inference Recommender를 사용하고 다른 리전에서 벤치마킹할 수 있나요?\n",
            "AWS 중국 리전을 제외하고 Amazon SageMaker에서 지원하는 모든 리전이 지원됩니다.\n",
            "\"category : SageMaker, question : 한 리전에서 SageMaker Inference Recommender를 사용하고 다른 리전에서 벤치마킹할 수 있나요?, answer : AWS 중국 리전을 제외하고 Amazon SageMaker에서 지원하는 모든 리전이 지원됩니다.\"\n",
            "SageMaker Inference Recommender는 Amazon EC2 Inf1 인스턴스를 지원하나요?\n",
            "예. 모든 유형의 컨테이너를 지원합니다. AWS Inferentia 칩을 기반으로 하는 Amazon EC2 Inf1에는 Neuron 컴파일러 또는 Amazon SageMaker Neo를 사용하는 컴파일된 모델 아티팩트가 필요합니다. Inferentia 대상 및 연결된 컨테이너 이미지 URI에 대한 컴파일된 모델이 있으면 SageMaker Inference Recommender를 사용하여 다양한 Inferentia 인스턴스 유형을 벤치마킹할 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker Inference Recommender는 Amazon EC2 Inf1 인스턴스를 지원하나요?, answer : 예. 모든 유형의 컨테이너를 지원합니다. AWS Inferentia 칩을 기반으로 하는 Amazon EC2 Inf1에는 Neuron 컴파일러 또는 Amazon SageMaker Neo를 사용하는 컴파일된 모델 아티팩트가 필요합니다. Inferentia 대상 및 연결된 컨테이너 이미지 URI에 대한 컴파일된 모델이 있으면 SageMaker Inference Recommender를 사용하여 다양한 Inferentia 인스턴스 유형을 벤치마킹할 수 있습니다.\"\n",
            "Amazon SageMaker Model Monitor란 무엇인가요?\n",
            "SageMaker Model Monitor를 사용하면 개념 드리프트를 탐지하고 해결할 수 있습니다. SageMaker Model Monitor는 배포된 모델에서 개념 드리프트를 자동으로 감지하고 문제 원인을 파악하는 데 도움이 되는 자세한 알림을 제공합니다. SageMaker에서 훈련된 모든 모델은 SageMaker Studio에서 수집하고 볼 수 있는 주요 지표를 자동으로 생성합니다. SageMaker Studio 내부에서 수집할 데이터, 확인하는 방법, 알림 수신 시기를 구성할 수 있습니다.\n",
            "\"category : SageMaker, question : Amazon SageMaker Model Monitor란 무엇인가요?, answer : SageMaker Model Monitor를 사용하면 개념 드리프트를 탐지하고 해결할 수 있습니다. SageMaker Model Monitor는 배포된 모델에서 개념 드리프트를 자동으로 감지하고 문제 원인을 파악하는 데 도움이 되는 자세한 알림을 제공합니다. SageMaker에서 훈련된 모든 모델은 SageMaker Studio에서 수집하고 볼 수 있는 주요 지표를 자동으로 생성합니다. SageMaker Studio 내부에서 수집할 데이터, 확인하는 방법, 알림 수신 시기를 구성할 수 있습니다.\"\n",
            "SageMaker를 실행하는 인프라에 액세스할 수 있나요?\n",
            "아니요. SageMaker가 사용자 대신 컴퓨팅 인프라를 운영하므로, 상태 확인을 수행하고 보안 패치를 적용하며 그 외 주기적인 유지 관리를 수행합니다. 사용자는 자체 호스팅 환경에 사용자 지정 추론 코드로 교육한 모델 아티팩트를 배포할 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker를 실행하는 인프라에 액세스할 수 있나요?, answer : 아니요. SageMaker가 사용자 대신 컴퓨팅 인프라를 운영하므로, 상태 확인을 수행하고 보안 패치를 적용하며 그 외 주기적인 유지 관리를 수행합니다. 사용자는 자체 호스팅 환경에 사용자 지정 추론 코드로 교육한 모델 아티팩트를 배포할 수 있습니다.\"\n",
            "SageMaker 모델이 프로덕션에 배포된 후에는 크기와 성능을 어떻게 조정하나요?\n",
            "SageMaker 호스팅은 애플리케이션 Auto Scaling을 사용해 애플리케이션에 필요한 성능에 맞춰 자동으로 조정됩니다. 또한 엔드포인트 구성을 수정하여 가동 중단 없이 인스턴스 수와 유형을 수동으로 변경할 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker 모델이 프로덕션에 배포된 후에는 크기와 성능을 어떻게 조정하나요?, answer : SageMaker 호스팅은 애플리케이션 Auto Scaling을 사용해 애플리케이션에 필요한 성능에 맞춰 자동으로 조정됩니다. 또한 엔드포인트 구성을 수정하여 가동 중단 없이 인스턴스 수와 유형을 수동으로 변경할 수 있습니다.\"\n",
            "SageMaker 프로덕션 환경을 모니터링하려면 어떻게 해야 하나요?\n",
            "SageMaker는 성능 지표를 Amazon CloudWatch 지표로 내보내므로, 지표를 추적하고, 경보를 설정하고, 프로덕션 트래픽의 변경 사항에 자동으로 대응할 수 있습니다. 또한 SageMaker는 사용자가 프로덕션 환경을 모니터링하고 문제를 해결할 수 있도록 Amazon CloudWatch Logs에 로그를 기록합니다.\n",
            "\"category : SageMaker, question : SageMaker 프로덕션 환경을 모니터링하려면 어떻게 해야 하나요?, answer : SageMaker는 성능 지표를 Amazon CloudWatch 지표로 내보내므로, 지표를 추적하고, 경보를 설정하고, 프로덕션 트래픽의 변경 사항에 자동으로 대응할 수 있습니다. 또한 SageMaker는 사용자가 프로덕션 환경을 모니터링하고 문제를 해결할 수 있도록 Amazon CloudWatch Logs에 로그를 기록합니다.\"\n",
            "SageMaker에서 호스팅할 수 있는 모델 유형은 무엇인가요?\n",
            "SageMaker는 추론 도커 이미지의 문서화된 사양을 준수하는 모든 모델을 호스팅할 수 있습니다. SageMaker 모델 아티팩트 및 추론 코드에서 생성된 모델이 이에 포함됩니다.\n",
            "\"category : SageMaker, question : SageMaker에서 호스팅할 수 있는 모델 유형은 무엇인가요?, answer : SageMaker는 추론 도커 이미지의 문서화된 사양을 준수하는 모든 모델을 호스팅할 수 있습니다. SageMaker 모델 아티팩트 및 추론 코드에서 생성된 모델이 이에 포함됩니다.\"\n",
            "SageMaker에서는 동시에 몇 개의 실시간 API 요청을 처리할 수 있나요?\n",
            "SageMaker는 많은 수의 초당 트랜잭션으로 확장할 수 있도록 설계되었습니다. 정확한 수는 배포된 모델과 모델이 배포된 인스턴스의 수 및 유형에 따라 달라집니다.\n",
            "\"category : SageMaker, question : SageMaker에서는 동시에 몇 개의 실시간 API 요청을 처리할 수 있나요?, answer : SageMaker는 많은 수의 초당 트랜잭션으로 확장할 수 있도록 설계되었습니다. 정확한 수는 배포된 모델과 모델이 배포된 인스턴스의 수 및 유형에 따라 달라집니다.\"\n",
            "SageMaker는 완전관리형 모델 호스팅 및 관리를 어떻게 지원하나요?\n",
            "완전관리형 서비스인 Amazon SageMaker는 인스턴스, 소프트웨어 버전 호환성, 패치 버전 설정 및 관리를 모두 처리합니다. 또한 알림을 모니터링하고 수신하는 데 사용할 수 있는 엔드포인트에 대한 기본 제공 지표 및 로그도 제공합니다. SageMaker 도구 및 안내형 워크플로를 사용하여 전체 ML 모델 패키징 및 배포 프로세스가 간소화되므로 엔드포인트를 손쉽게 최적화하여 원하는 성능을 달성하고 비용을 절감할 수 있습니다. SageMaker Studio에서 또는 새로운 PySDK를 사용하여 클릭 몇 번으로 파운데이션 모델을 비롯한 ML 모델을 쉽게 배포할 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker는 완전관리형 모델 호스팅 및 관리를 어떻게 지원하나요?, answer : 완전관리형 서비스인 Amazon SageMaker는 인스턴스, 소프트웨어 버전 호환성, 패치 버전 설정 및 관리를 모두 처리합니다. 또한 알림을 모니터링하고 수신하는 데 사용할 수 있는 엔드포인트에 대한 기본 제공 지표 및 로그도 제공합니다. SageMaker 도구 및 안내형 워크플로를 사용하여 전체 ML 모델 패키징 및 배포 프로세스가 간소화되므로 엔드포인트를 손쉽게 최적화하여 원하는 성능을 달성하고 비용을 절감할 수 있습니다. SageMaker Studio에서 또는 새로운 PySDK를 사용하여 클릭 몇 번으로 파운데이션 모델을 비롯한 ML 모델을 쉽게 배포할 수 있습니다.\"\n",
            "배치 변환이란 무엇인가요?\n",
            "배치 변환은 대규모 또는 소규모 배치 데이터에 대한 예측을 실행할 수 있게 해 줍니다. 이제 데이터 집합을 여러 데이터 청크로 분할하고 실시간 엔드포인트를 관리할 필요가 없습니다. 간단한 API를 통해 대량 데이터 레코드에 대한 예측을 요청하고 데이터를 쉽고 빠르게 변환할 수 있습니다.\n",
            "\"category : SageMaker, question : 배치 변환이란 무엇인가요?, answer : 배치 변환은 대규모 또는 소규모 배치 데이터에 대한 예측을 실행할 수 있게 해 줍니다. 이제 데이터 집합을 여러 데이터 청크로 분할하고 실시간 엔드포인트를 관리할 필요가 없습니다. 간단한 API를 통해 대량 데이터 레코드에 대한 예측을 요청하고 데이터를 쉽고 빠르게 변환할 수 있습니다.\"\n",
            "Amazon SageMaker Edge Manager란?\n",
            "SageMaker Edge Manager를 사용하면 스마트 카메라, 로봇, 개인용 컴퓨터 및 모바일 디바이스 같은 엣지 디바이스 플릿에서 기계 학습 모델을 손쉽게 최적화, 보안, 모니터링 및 유지 관리할 수 있습니다. SageMaker Edge Manager는 기계 학습 개발 시 다양한 엣지 디바이스에서 대규모로 ML 모델을 가동하는 데 도움이 됩니다.\n",
            "\"category : SageMaker, question : Amazon SageMaker Edge Manager란?, answer : SageMaker Edge Manager를 사용하면 스마트 카메라, 로봇, 개인용 컴퓨터 및 모바일 디바이스 같은 엣지 디바이스 플릿에서 기계 학습 모델을 손쉽게 최적화, 보안, 모니터링 및 유지 관리할 수 있습니다. SageMaker Edge Manager는 기계 학습 개발 시 다양한 엣지 디바이스에서 대규모로 ML 모델을 가동하는 데 도움이 됩니다.\"\n",
            "SageMaker가 지원하는 배포 엔드포인트 옵션으로는 어떤 것이 있나요?\n",
            "SageMaker는 다음과 같은 엔드포인트 옵션을 지원합니다. 단일 모델 엔드포인트 - 짧은 지연 시간과 높은 처리량을 위해 전용 인스턴스 또는 서버리스에서 호스팅되는 컨테이너의 단일 모델. 다중 모델 엔드포인트 - 공유 인프라를 사용하여 여러 모델을 호스팅함으로써 비용 효율성을 높이고 활용도를 극대화합니다. 각 모델에서 사용할 수 있는 컴퓨팅 및 메모리 양을 제어하여 각 모델이 효율적으로 실행하는 데 필요한 만큼의 리소스에만 액세스하도록 할 수 있습니다. 직렬 추론 파이프라인 - 여러 컨테이너가 전용 인스턴스를 공유하면서 순차적으로 실행됩니다. 추론 파이프라인을 사용하여 전처리, 예측 및 후처리 데이터 과학 작업을 결합할 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker가 지원하는 배포 엔드포인트 옵션으로는 어떤 것이 있나요?, answer : SageMaker는 다음과 같은 엔드포인트 옵션을 지원합니다. 단일 모델 엔드포인트 - 짧은 지연 시간과 높은 처리량을 위해 전용 인스턴스 또는 서버리스에서 호스팅되는 컨테이너의 단일 모델. 다중 모델 엔드포인트 - 공유 인프라를 사용하여 여러 모델을 호스팅함으로써 비용 효율성을 높이고 활용도를 극대화합니다. 각 모델에서 사용할 수 있는 컴퓨팅 및 메모리 양을 제어하여 각 모델이 효율적으로 실행하는 데 필요한 만큼의 리소스에만 액세스하도록 할 수 있습니다. 직렬 추론 파이프라인 - 여러 컨테이너가 전용 인스턴스를 공유하면서 순차적으로 실행됩니다. 추론 파이프라인을 사용하여 전처리, 예측 및 후처리 데이터 과학 작업을 결합할 수 있습니다.\"\n",
            "SageMaker Edge Manager를 시작하려면 어떻게 해야 하나요?\n",
            "SageMaker Edge Manager를 시작하려면 클라우드에서 훈련된 ML 모델을 컴파일하고 패키지로 작성한 후 디바이스를 등록하고 SageMaker Edge Manager SDK에서 디바이스를 준비해야 합니다. 배포를 위해 모델을 준비하도록 SageMaker Edge Manager는 SageMaker Neo를 사용하여 대상 엣지 하드웨어에 맞게 모델을 컴파일합니다. 모델이 컴파일되면 SageMaker Edge Manager는 빠르게 배포할 수 있도록 AWS 생성 키로 모델에 서명한 후 런타임 및 필수 자격 증명을 통해 모델을 패키지로 작성합니다. 디바이스 측에서는, SageMaker Edge Manager에 디바이스를 등록하고, SageMaker Edge Manager SDK를 다운로드한 후 지침에 따라 SageMaker Edge Manager 에이전트를 디바이스에 설치합니다. 자습서 노트북에서는 SageMaker Edge Manager에서 모델을 준비하고 엣지 디바이스에 모델을 연결하는 방법에 대한 단계별 예제를 제공합니다.\n",
            "\"category : SageMaker, question : SageMaker Edge Manager를 시작하려면 어떻게 해야 하나요?, answer : SageMaker Edge Manager를 시작하려면 클라우드에서 훈련된 ML 모델을 컴파일하고 패키지로 작성한 후 디바이스를 등록하고 SageMaker Edge Manager SDK에서 디바이스를 준비해야 합니다. 배포를 위해 모델을 준비하도록 SageMaker Edge Manager는 SageMaker Neo를 사용하여 대상 엣지 하드웨어에 맞게 모델을 컴파일합니다. 모델이 컴파일되면 SageMaker Edge Manager는 빠르게 배포할 수 있도록 AWS 생성 키로 모델에 서명한 후 런타임 및 필수 자격 증명을 통해 모델을 패키지로 작성합니다. 디바이스 측에서는, SageMaker Edge Manager에 디바이스를 등록하고, SageMaker Edge Manager SDK를 다운로드한 후 지침에 따라 SageMaker Edge Manager 에이전트를 디바이스에 설치합니다. 자습서 노트북에서는 SageMaker Edge Manager에서 모델을 준비하고 엣지 디바이스에 모델을 연결하는 방법에 대한 단계별 예제를 제공합니다.\"\n",
            "탄력성을 위한 오토 스케일링이란 무엇인가요?\n",
            "규모 조정 정책을 사용하여 추론 요청량의 변동에 따라 기반 컴퓨팅 리소스의 규모를 자동으로 조정할 수 있습니다. 각 ML 모델의 규모 조정 정책을 개별적으로 제어하여 모델 사용량의 변화에 쉽게 대응하는 동시에, 인프라 비용을 최적화할 수 있습니다.\n",
            "\"category : SageMaker, question : 탄력성을 위한 오토 스케일링이란 무엇인가요?, answer : 규모 조정 정책을 사용하여 추론 요청량의 변동에 따라 기반 컴퓨팅 리소스의 규모를 자동으로 조정할 수 있습니다. 각 ML 모델의 규모 조정 정책을 개별적으로 제어하여 모델 사용량의 변화에 쉽게 대응하는 동시에, 인프라 비용을 최적화할 수 있습니다.\"\n",
            "SageMaker Edge Manager는 어떤 디바이스를 지원하나요?\n",
            "SageMaker Edge Manager는 Linux 및 Windows 운영 체제에서 일반적인 CPU(ARM, x86) 및 GPU(ARM, Nvidia) 기반 디바이스를 지원합니다. 앞으로 SageMaker Edge Manager는 SageMaker Neo에서도 지원하는 더 많은 임베디드 프로세서와 모바일 플랫폼을 지원하도록 확장할 계획입니다.\n",
            "\"category : SageMaker, question : SageMaker Edge Manager는 어떤 디바이스를 지원하나요?, answer : SageMaker Edge Manager는 Linux 및 Windows 운영 체제에서 일반적인 CPU(ARM, x86) 및 GPU(ARM, Nvidia) 기반 디바이스를 지원합니다. 앞으로 SageMaker Edge Manager는 SageMaker Neo에서도 지원하는 더 많은 임베디드 프로세서와 모바일 플랫폼을 지원하도록 확장할 계획입니다.\"\n",
            "SageMaker Edge Manager를 사용하려면 SageMaker를 사용하여 모델을 훈련해야 하나요?\n",
            "아니요. 오픈 소스 또는 모델 공급 업체의 사전 훈련된 모델을 사용하거나 그 외 위치에서 모델을 훈련할 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker Edge Manager를 사용하려면 SageMaker를 사용하여 모델을 훈련해야 하나요?, answer : 아니요. 오픈 소스 또는 모델 공급 업체의 사전 훈련된 모델을 사용하거나 그 외 위치에서 모델을 훈련할 수 있습니다.\"\n",
            "SageMaker Edge Manager를 사용하려면 SageMaker Neo를 사용하여 모델을 컴파일해야 하나요?\n",
            "예. SageMaker Neo는 엣지 디바이스에서 패키지로 작성하고 배포할 수 있도록 모델을 실행 가능 항목으로 변환하고 컴파일합니다. 모델 패키지를 배포하면 SageMaker Edge Manager 에이전트가 모델 패키지를 압축 해제하고 디바이스에서 모델을 실행합니다.\n",
            "\"category : SageMaker, question : SageMaker Edge Manager를 사용하려면 SageMaker Neo를 사용하여 모델을 컴파일해야 하나요?, answer : 예. SageMaker Neo는 엣지 디바이스에서 패키지로 작성하고 배포할 수 있도록 모델을 실행 가능 항목으로 변환하고 컴파일합니다. 모델 패키지를 배포하면 SageMaker Edge Manager 에이전트가 모델 패키지를 압축 해제하고 디바이스에서 모델을 실행합니다.\"\n",
            "엣지 디바이스에 모델을 배포하려면 어떻게 해야 하나요?\n",
            "SageMaker Edge Manager는 지정된 Amazon S3 버킷에 모델 패키지를 저장합니다. AWS IoT Greengrass에서 제공하는 무선 업데이트(OTA) 배포 기능을 사용하거나 원하는 다른 배포 메커니즘을 통해 S3 버킷에서 디바이스로 모델 패키지를 배포할 수 있습니다.\n",
            "\"category : SageMaker, question : 엣지 디바이스에 모델을 배포하려면 어떻게 해야 하나요?, answer : SageMaker Edge Manager는 지정된 Amazon S3 버킷에 모델 패키지를 저장합니다. AWS IoT Greengrass에서 제공하는 무선 업데이트(OTA) 배포 기능을 사용하거나 원하는 다른 배포 메커니즘을 통해 S3 버킷에서 디바이스로 모델 패키지를 배포할 수 있습니다.\"\n",
            "SageMaker Edge Manager SDK는 SageMaker Neo 런타임(dlr)과 어떻게 다른가요?\n",
            "Neo dlr은 SageMaker Neo 서비스에서 컴파일된 모델만 실행하는 오픈 소스 런타임입니다. 오픈 소스 dlr과 달리, SageMaker Edge Manager SDK는 추가 보안, 모델 관리 및 모델 지원 특성을 포함하는 엔터프라이즈급 디바이스 기반 에이전트를 포함합니다. SageMaker Edge Manager SDK는 대규모 프로덕션 배포에 적합합니다.\n",
            "\"category : SageMaker, question : SageMaker Edge Manager SDK는 SageMaker Neo 런타임(dlr)과 어떻게 다른가요?, answer : Neo dlr은 SageMaker Neo 서비스에서 컴파일된 모델만 실행하는 오픈 소스 런타임입니다. 오픈 소스 dlr과 달리, SageMaker Edge Manager SDK는 추가 보안, 모델 관리 및 모델 지원 특성을 포함하는 엔터프라이즈급 디바이스 기반 에이전트를 포함합니다. SageMaker Edge Manager SDK는 대규모 프로덕션 배포에 적합합니다.\"\n",
            "SageMaker Edge Manager는 AWS IoT Greengrass와 어떤 관계인가요?\n",
            "SageMaker Edge Manager와 AWS IoT Greengrass는 IoT 솔루션에서 함께 사용할 수 있습니다. 기계 학습 모델을 SageMaker Edge Manager에서 패키지로 작성한 후에 AWS IoT Greengrass의 OTA 업데이트 기능을 사용하여 디바이스에 모델 패키지를 배포할 수 있습니다. AWS IoT Greengrass를 사용하면 원격으로 IoT 디바이스를 모니터링하는 동시에, SageMaker Edge Manager가 디바이스에서 기계 학습 모델을 모니터링하고 유지 관리할 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker Edge Manager는 AWS IoT Greengrass와 어떤 관계인가요?, answer : SageMaker Edge Manager와 AWS IoT Greengrass는 IoT 솔루션에서 함께 사용할 수 있습니다. 기계 학습 모델을 SageMaker Edge Manager에서 패키지로 작성한 후에 AWS IoT Greengrass의 OTA 업데이트 기능을 사용하여 디바이스에 모델 패키지를 배포할 수 있습니다. AWS IoT Greengrass를 사용하면 원격으로 IoT 디바이스를 모니터링하는 동시에, SageMaker Edge Manager가 디바이스에서 기계 학습 모델을 모니터링하고 유지 관리할 수 있습니다.\"\n",
            "SageMaker Edge Manager는 AWS Panorama와 어떤 관계인가요? 언제 SageMaker Edge Manager를 사용하고, 언제 AWS Panorama를 사용해야 하나요?\n",
            "AWS는 엣지 디바이스에서 모델을 실행하기 위한 가장 포괄적이고 심층적인 기능을 제공합니다. 그리고 컴퓨터 비전, 음성 인식 및 예측 유지 관리를 포함하여 다양한 사용 사례를 지원하는 서비스도 갖추었습니다.\n",
            "카메라 및 어플라이언스와 같은 엣지 디바이스에서 컴퓨터 비전을 실행하려는 회사는 AWS Panorama를 사용할 수 있습니다. AWS Panorama는 엣지 디바이스에서 컴퓨터 비전 애플리케이션을 쉽게 배포할 수 있도록 지원합니다. 클라우드 콘솔에 로그인하고 Amazon S3 또는 SageMaker에서 사용하려는 모델을 지정한 후 Python 스크립트로 비즈니스 로직을 작성하는 방법으로 간편하게 AWS Panorama를 시작할 수 있습니다. AWS Panorama는 대상 디바이스에 대해 모델을 컴파일하고 애플리케이션 패키지를 생성하므로, 몇 번의 클릭으로 디바이스에 해당 패키지를 배포할 수 있습니다. 사용자 지정 애플리케이션을 구축하려는 독립 소프트웨어 개발 판매 회사(ISV)도 AWS Panorama SDK를 사용할 수 있습니다. 디바이스 제조업체는 Device SDK를 사용하여 AWS Panorama에 대해 디바이스를 인증할 수 있습니다.\n",
            "고유한 모델을 구축하고 모델 특성에 대한 세분화된 제어 기능을 원하는 고객이라면 SageMaker Edge Manager를 사용할 수 있습니다. SageMaker Edge Manager는 자연어 처리, 사기 탐지 및 예측 유지 관리와 같은 모든 유형의 사용 사례에 사용하는 스마트 카메라, 스마트 스피커, 로봇과 같은 여러 엣지 디바이스 플릿에서 기계 학습 모델을 준비, 실행, 모니터링 및 업데이트하는 관리형 서비스입니다. SageMaker Edge Manager는 서로 다른 모델 특성 및 드리프트에 대한 모니터 모델 엔지니어링을 포함하여 모델에 대한 제어를 원하는 기계 학습 엣지 개발자에게 적합합니다. 모든 ML 엣지 개발자는 SageMaker 콘솔 및 SageMaker API를 통해 SageMaker Edge Manager를 사용할 수 있습니다. SageMaker Edge Manager는 클라우드의 모델을 엣지 디바이스로 구축, 훈련 및 배포하는 SageMaker의 기능을 활용합니다.\n",
            "\"category : SageMaker, question : SageMaker Edge Manager는 AWS Panorama와 어떤 관계인가요? 언제 SageMaker Edge Manager를 사용하고, 언제 AWS Panorama를 사용해야 하나요?, answer : AWS는 엣지 디바이스에서 모델을 실행하기 위한 가장 포괄적이고 심층적인 기능을 제공합니다. 그리고 컴퓨터 비전, 음성 인식 및 예측 유지 관리를 포함하여 다양한 사용 사례를 지원하는 서비스도 갖추었습니다.\n",
            "카메라 및 어플라이언스와 같은 엣지 디바이스에서 컴퓨터 비전을 실행하려는 회사는 AWS Panorama를 사용할 수 있습니다. AWS Panorama는 엣지 디바이스에서 컴퓨터 비전 애플리케이션을 쉽게 배포할 수 있도록 지원합니다. 클라우드 콘솔에 로그인하고 Amazon S3 또는 SageMaker에서 사용하려는 모델을 지정한 후 Python 스크립트로 비즈니스 로직을 작성하는 방법으로 간편하게 AWS Panorama를 시작할 수 있습니다. AWS Panorama는 대상 디바이스에 대해 모델을 컴파일하고 애플리케이션 패키지를 생성하므로, 몇 번의 클릭으로 디바이스에 해당 패키지를 배포할 수 있습니다. 사용자 지정 애플리케이션을 구축하려는 독립 소프트웨어 개발 판매 회사(ISV)도 AWS Panorama SDK를 사용할 수 있습니다. 디바이스 제조업체는 Device SDK를 사용하여 AWS Panorama에 대해 디바이스를 인증할 수 있습니다.\n",
            "고유한 모델을 구축하고 모델 특성에 대한 세분화된 제어 기능을 원하는 고객이라면 SageMaker Edge Manager를 사용할 수 있습니다. SageMaker Edge Manager는 자연어 처리, 사기 탐지 및 예측 유지 관리와 같은 모든 유형의 사용 사례에 사용하는 스마트 카메라, 스마트 스피커, 로봇과 같은 여러 엣지 디바이스 플릿에서 기계 학습 모델을 준비, 실행, 모니터링 및 업데이트하는 관리형 서비스입니다. SageMaker Edge Manager는 서로 다른 모델 특성 및 드리프트에 대한 모니터 모델 엔지니어링을 포함하여 모델에 대한 제어를 원하는 기계 학습 엣지 개발자에게 적합합니다. 모든 ML 엣지 개발자는 SageMaker 콘솔 및 SageMaker API를 통해 SageMaker Edge Manager를 사용할 수 있습니다. SageMaker Edge Manager는 클라우드의 모델을 엣지 디바이스로 구축, 훈련 및 배포하는 SageMaker의 기능을 활용합니다.\"\n",
            "SageMaker Edge Manager는 어느 리전에서 사용할 수 있나요?\n",
            "SageMaker Edge Manager는 미국 동부(버지니아 북부), 미국 동부(오하이오), 미국 서부(오레곤), 유럽(아일랜드), 유럽(프랑크푸르트), 아시아 태평양(도쿄)의 6개 리전에서 사용할 수 있습니다. 자세한 내용은 AWS 리전 서비스 목록을 참조하세요.\n",
            "\"category : SageMaker, question : SageMaker Edge Manager는 어느 리전에서 사용할 수 있나요?, answer : SageMaker Edge Manager는 미국 동부(버지니아 북부), 미국 동부(오하이오), 미국 서부(오레곤), 유럽(아일랜드), 유럽(프랑크푸르트), 아시아 태평양(도쿄)의 6개 리전에서 사용할 수 있습니다. 자세한 내용은 AWS 리전 서비스 목록을 참조하세요.\"\n",
            "Q: Amazon SageMaker Neo란 무엇입니까?\n",
            "SageMaker Neo를 사용하면 기계 학습 모델을 한 번 훈련하여 클라우드와 엣지의 모든 위치에서 실행할 수 있습니다. SageMaker Neo는 여러 하드웨어 플랫폼에 배포하는 데 사용할 수 있는 인기 DL 프레임워크로 구축된 모델을 자동으로 최적화합니다. 최적화된 모델은 최대 25배 더 빨리 실행되며 일반적인 기계 학습 모델과 비교해 10분의 1 미만의 리소스를 사용합니다.\n",
            "\"category : SageMaker, question : Q: Amazon SageMaker Neo란 무엇입니까?, answer : SageMaker Neo를 사용하면 기계 학습 모델을 한 번 훈련하여 클라우드와 엣지의 모든 위치에서 실행할 수 있습니다. SageMaker Neo는 여러 하드웨어 플랫폼에 배포하는 데 사용할 수 있는 인기 DL 프레임워크로 구축된 모델을 자동으로 최적화합니다. 최적화된 모델은 최대 25배 더 빨리 실행되며 일반적인 기계 학습 모델과 비교해 10분의 1 미만의 리소스를 사용합니다.\"\n",
            "SageMaker Neo를 시작하려면 어떻게 해야 하나요?\n",
            "SageMaker Neo를 시작하려면 SageMaker 콘솔에 로그인하고, 훈련된 모델을 선택한 다음, 예제를 따라 모델을 컴파일하고, 결과 모델을 대상 하드웨어 플랫폼에 배포합니다.\n",
            "\"category : SageMaker, question : SageMaker Neo를 시작하려면 어떻게 해야 하나요?, answer : SageMaker Neo를 시작하려면 SageMaker 콘솔에 로그인하고, 훈련된 모델을 선택한 다음, 예제를 따라 모델을 컴파일하고, 결과 모델을 대상 하드웨어 플랫폼에 배포합니다.\"\n",
            "SageMaker Neo의 주요 구성 요소는 무엇인가요?\n",
            "SageMaker Neo에는 컴파일러와 런타임이라는 2가지 주요 구성 요소가 포함됩니다. 먼저, SageMaker Neo 컴파일러는 다른 프레임워크에서 내보낸 모델을 읽습니다. 그런 다음 프레임워크별 함수와 작업을 프레임워크와 무관한 중간 표시로 변환합니다. 다음에는 일련의 최적화를 수행합니다. 그런 다음, 컴파일러는 최적화된 작업에 대한 바이너리 코드를 생성하고 이 코드를 공유 객체 라이브러리에 기록합니다. 또한 컴파일러는 모델 정의와 파라미터를 개별 파일로 저장합니다. 실행 중에 SageMaker Neo 런타임은 컴파일러에서 생성된 아티팩트(모델 정의, 파라미터, 모델을 실행하기 위한 공유 객체 라이브러리)를 로드합니다.\n",
            "\"category : SageMaker, question : SageMaker Neo의 주요 구성 요소는 무엇인가요?, answer : SageMaker Neo에는 컴파일러와 런타임이라는 2가지 주요 구성 요소가 포함됩니다. 먼저, SageMaker Neo 컴파일러는 다른 프레임워크에서 내보낸 모델을 읽습니다. 그런 다음 프레임워크별 함수와 작업을 프레임워크와 무관한 중간 표시로 변환합니다. 다음에는 일련의 최적화를 수행합니다. 그런 다음, 컴파일러는 최적화된 작업에 대한 바이너리 코드를 생성하고 이 코드를 공유 객체 라이브러리에 기록합니다. 또한 컴파일러는 모델 정의와 파라미터를 개별 파일로 저장합니다. 실행 중에 SageMaker Neo 런타임은 컴파일러에서 생성된 아티팩트(모델 정의, 파라미터, 모델을 실행하기 위한 공유 객체 라이브러리)를 로드합니다.\"\n",
            "SageMaker Neo를 사용하여 모델을 변환하려면 SageMaker를 사용하여 모델을 훈련해야 하나요?\n",
            "아니요. 다른 곳에서 모델을 훈련하고 SageMaker Neo를 사용하여 SageMaker ML 인스턴스 또는 AWS IoT Greengrass 지원 디바이스에 맞게 모델을 최적화할 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker Neo를 사용하여 모델을 변환하려면 SageMaker를 사용하여 모델을 훈련해야 하나요?, answer : 아니요. 다른 곳에서 모델을 훈련하고 SageMaker Neo를 사용하여 SageMaker ML 인스턴스 또는 AWS IoT Greengrass 지원 디바이스에 맞게 모델을 최적화할 수 있습니다.\"\n",
            "Q: SageMaker Neo는 어떤 모델을 지원합니까?\n",
            "현재, SageMaker Neo는 컴퓨터 비전 애플리케이션을 구동하는 가장 인기 있는 DL 모델과 오늘날 SageMaker에서 사용되는 가장 인기 있는 결정 트리 모델을 지원합니다. SageMaker Neo는 MXNet 및 TensorFlow에서 훈련된 AlexNet, ResNet, VGG, Inception, MobileNet, SqueezeNet, DenseNet 모델과 XGBoost에서 훈련된 분류 및 Random Cut Forest 모델을 최적화합니다.\n",
            "\"category : SageMaker, question : Q: SageMaker Neo는 어떤 모델을 지원합니까?, answer : 현재, SageMaker Neo는 컴퓨터 비전 애플리케이션을 구동하는 가장 인기 있는 DL 모델과 오늘날 SageMaker에서 사용되는 가장 인기 있는 결정 트리 모델을 지원합니다. SageMaker Neo는 MXNet 및 TensorFlow에서 훈련된 AlexNet, ResNet, VGG, Inception, MobileNet, SqueezeNet, DenseNet 모델과 XGBoost에서 훈련된 분류 및 Random Cut Forest 모델을 최적화합니다.\"\n",
            "SageMaker Neo는 어떤 하드웨어 플랫폼을 지원하나요?\n",
            "SageMaker Neo 설명서에서 지원되는 클라우드 인스턴스, 엣지 디바이스 및 프레임워크 버전의 목록을 확인할 수 있습니다.\n",
            "\"category : SageMaker, question : SageMaker Neo는 어떤 하드웨어 플랫폼을 지원하나요?, answer : SageMaker Neo 설명서에서 지원되는 클라우드 인스턴스, 엣지 디바이스 및 프레임워크 버전의 목록을 확인할 수 있습니다.\"\n",
            "SageMaker Neo는 어느 리전에서 사용할 수 있나요?\n",
            "지원되는 리전 목록은 AWS 리전 서비스 목록을 참조하세요.\n",
            "\"category : SageMaker, question : SageMaker Neo는 어느 리전에서 사용할 수 있나요?, answer : 지원되는 리전 목록은 AWS 리전 서비스 목록을 참조하세요.\"\n",
            "Amazon SageMaker 절감형 플랜이란 무엇인가요?\n",
            "SageMaker 절감형 플랜은 1년 또는 3년의 일정 사용량 약정(시간당 USD 요금으로 측정)을 조건으로 SageMaker에서 유연한 사용량 기반 요금 모델을 제공합니다. SageMaker 절감형 플랜은 최대 64%까지 비용을 절감할 수 있는 가장 유연한 요금 모델입니다. 이 요금은 인스턴스 패밀리, 크기 또는 리전과 관계없이 SageMaker Studio 노트북, SageMaker 온디맨드 노트북, SageMaker 처리, SageMaker Data Wrangler, SageMaker 훈련, SageMaker 실시간 추론 및 SageMaker 배치 변환을 포함하여 적격 SageMaker ML 인스턴스 사용량에 자동으로 적용됩니다. 예를 들어 추론 워크로드를 위해 미국 동부(오하이오)에서 실행되는 CPU 인스턴스 ml.c5.xlarge에서 미국 서부(오레곤)의 ml.Inf1 인스턴스로 언제든지 변경할 수 있으며, 절감형 플랜 요금이 자동으로 계속 적용됩니다.\n",
            "\"category : SageMaker, question : Amazon SageMaker 절감형 플랜이란 무엇인가요?, answer : SageMaker 절감형 플랜은 1년 또는 3년의 일정 사용량 약정(시간당 USD 요금으로 측정)을 조건으로 SageMaker에서 유연한 사용량 기반 요금 모델을 제공합니다. SageMaker 절감형 플랜은 최대 64%까지 비용을 절감할 수 있는 가장 유연한 요금 모델입니다. 이 요금은 인스턴스 패밀리, 크기 또는 리전과 관계없이 SageMaker Studio 노트북, SageMaker 온디맨드 노트북, SageMaker 처리, SageMaker Data Wrangler, SageMaker 훈련, SageMaker 실시간 추론 및 SageMaker 배치 변환을 포함하여 적격 SageMaker ML 인스턴스 사용량에 자동으로 적용됩니다. 예를 들어 추론 워크로드를 위해 미국 동부(오하이오)에서 실행되는 CPU 인스턴스 ml.c5.xlarge에서 미국 서부(오레곤)의 ml.Inf1 인스턴스로 언제든지 변경할 수 있으며, 절감형 플랜 요금이 자동으로 계속 적용됩니다.\"\n",
            "SageMaker 절감형 플랜을 사용해야 하는 이유는 무엇인가요?\n",
            "SageMaker 인스턴스 사용량이 일정한 크기이고(시간당 USD 요금으로 측정됨), 여러 SageMaker 구성 요소를 사용하거나 시간이 지남에 따라 기술 구성(예: 인스턴스 패밀리, 리전)이 변경될 것으로 예상되는 경우 SageMaker 절감형 플랜을 사용하면 절감 효과를 극대화하면서 애플리케이션 요구 사항 또는 새로운 혁신에 따라 기본적인 기술 구성을 유연하게 변경할 수 있습니다. 절감형 플랜 요금은 수동으로 수정하지 않고도 모든 적격 기계 학습 인스턴스 사용량에 자동으로 적용됩니다.\n",
            "\"category : SageMaker, question : SageMaker 절감형 플랜을 사용해야 하는 이유는 무엇인가요?, answer : SageMaker 인스턴스 사용량이 일정한 크기이고(시간당 USD 요금으로 측정됨), 여러 SageMaker 구성 요소를 사용하거나 시간이 지남에 따라 기술 구성(예: 인스턴스 패밀리, 리전)이 변경될 것으로 예상되는 경우 SageMaker 절감형 플랜을 사용하면 절감 효과를 극대화하면서 애플리케이션 요구 사항 또는 새로운 혁신에 따라 기본적인 기술 구성을 유연하게 변경할 수 있습니다. 절감형 플랜 요금은 수동으로 수정하지 않고도 모든 적격 기계 학습 인스턴스 사용량에 자동으로 적용됩니다.\"\n",
            "SageMaker 절감형 플랜을 시작하려면 어떻게 해야 하나요?\n",
            "AWS Management Console의 AWS Cost Explorer에서 또는 API/CLI를 사용하여 절감형 플랜 이용을 시작할 수 있습니다. AWS Cost Explorer에서 제공되는 권장 사항에 따라 손쉽게 Savings Plans 약정을 체결하여 가장 큰 절감 효과를 얻을 수 있습니다. 권장되는 시간 약정은 이전 온디맨드 사용량 기록과 고객이 선택한 요금제 유형, 기간, 결제 옵션을 기준으로 합니다. Savings Plans에 가입하면 컴퓨팅 사용량에 자동으로 할인된 Savings Plans 요금이 청구되고 약정 사용량을 초과하는 사용량에 대해서는 일반적인 온디맨드 요금이 청구됩니다.\n",
            "\"category : SageMaker, question : SageMaker 절감형 플랜을 시작하려면 어떻게 해야 하나요?, answer : AWS Management Console의 AWS Cost Explorer에서 또는 API/CLI를 사용하여 절감형 플랜 이용을 시작할 수 있습니다. AWS Cost Explorer에서 제공되는 권장 사항에 따라 손쉽게 Savings Plans 약정을 체결하여 가장 큰 절감 효과를 얻을 수 있습니다. 권장되는 시간 약정은 이전 온디맨드 사용량 기록과 고객이 선택한 요금제 유형, 기간, 결제 옵션을 기준으로 합니다. Savings Plans에 가입하면 컴퓨팅 사용량에 자동으로 할인된 Savings Plans 요금이 청구되고 약정 사용량을 초과하는 사용량에 대해서는 일반적인 온디맨드 요금이 청구됩니다.\"\n",
            "SageMaker용 절감형 플랜은 Amazon EC2용 컴퓨팅 절감형 플랜과 어떻게 다른가요?\n",
            "SageMaker용 절감형 플랜과 Amazon EC2용 절감형 플랜의 차이는 포함되는 서비스에 있습니다. SageMaker 절감형 플랜은 SageMaker 기계 학습 인스턴스 사용량에만 적용됩니다.\n",
            "\"category : SageMaker, question : SageMaker용 절감형 플랜은 Amazon EC2용 컴퓨팅 절감형 플랜과 어떻게 다른가요?, answer : SageMaker용 절감형 플랜과 Amazon EC2용 절감형 플랜의 차이는 포함되는 서비스에 있습니다. SageMaker 절감형 플랜은 SageMaker 기계 학습 인스턴스 사용량에만 적용됩니다.\"\n",
            "AWS Organizations/통합 결제에서 Savings Plans는 어떻게 작동합니까?\n",
            "Savings Plans는 AWS Organization/통합 결제 패밀리 내의 모든 계정에서 구매할 수 있습니다. 기본적으로 Savings Plans에서 제공하는 혜택은 AWS Organization/통합 결제 패밀리 내 모든 계정의 사용량에 적용 가능합니다. 그러나 Savings Plans 혜택을 해당 용량을 구매한 계정으로만 제한할 수도 있습니다.\n",
            "\"category : SageMaker, question : AWS Organizations/통합 결제에서 Savings Plans는 어떻게 작동합니까?, answer : Savings Plans는 AWS Organization/통합 결제 패밀리 내의 모든 계정에서 구매할 수 있습니다. 기본적으로 Savings Plans에서 제공하는 혜택은 AWS Organization/통합 결제 패밀리 내 모든 계정의 사용량에 적용 가능합니다. 그러나 Savings Plans 혜택을 해당 용량을 구매한 계정으로만 제한할 수도 있습니다.\"\n",
            "========== CloudFormation  :  https://aws.amazon.com/ko/cloudformation/faqs/ 사이트 크롤링 진행중 ==========\n",
            "46\n",
            "AWS CloudFormation이란 무엇입니까?\n",
            "AWS CloudFormation은 개발자와 기업이 손쉽게 관련 AWS 및 서드 파티 리소스의 모음을 쉽게 생성하고 순서에 따라 예측 가능한 방식으로 프로비저닝 및 관리할 수 있는 방법을 제공하는 서비스입니다.\n",
            "\"category : CloudFormation, question : AWS CloudFormation이란 무엇입니까?, answer : AWS CloudFormation은 개발자와 기업이 손쉽게 관련 AWS 및 서드 파티 리소스의 모음을 쉽게 생성하고 순서에 따라 예측 가능한 방식으로 프로비저닝 및 관리할 수 있는 방법을 제공하는 서비스입니다.\"\n",
            "개발자는 AWS CloudFormation을 통해 어떤 일을 할 수 있나요?\n",
            "개발자는 특정 리소스 API의 복잡성을 해소하는 단순한 선언적인 스타일로 컴퓨팅, 데이터베이스를 비롯해 그 밖의 많은 리소스를 배포하고 업데이트할 수 있습니다. AWS CloudFormation은 리소스 수명 주기를 예측 가능한 방식으로 반복해서 안전하게 관리할 수 있게 해 줄 뿐 아니라, 자동 롤백, 자동 상태 관리 그리고 계정 및 리전 전반에 걸친 리소스 관리를 지원합니다. 최근에 향상된 기능 및 추가된 옵션을 통해 다양한 방법으로 리소스를 생성할 수 있게 되었는데, 여기에는 상위 수준 언어 코딩을 위한 AWS CDK 개선, 기존 리소스 가져오기, 구성 편차 감지 그리고 CloudFormation의 많은 핵심적인 이점을 그대로 상속받는 사용자 지정 유형을 더 쉽게 생성할 수 있게 해 주는 새로운 Registry가 포함됩니다.\n",
            "\"category : CloudFormation, question : 개발자는 AWS CloudFormation을 통해 어떤 일을 할 수 있나요?, answer : 개발자는 특정 리소스 API의 복잡성을 해소하는 단순한 선언적인 스타일로 컴퓨팅, 데이터베이스를 비롯해 그 밖의 많은 리소스를 배포하고 업데이트할 수 있습니다. AWS CloudFormation은 리소스 수명 주기를 예측 가능한 방식으로 반복해서 안전하게 관리할 수 있게 해 줄 뿐 아니라, 자동 롤백, 자동 상태 관리 그리고 계정 및 리전 전반에 걸친 리소스 관리를 지원합니다. 최근에 향상된 기능 및 추가된 옵션을 통해 다양한 방법으로 리소스를 생성할 수 있게 되었는데, 여기에는 상위 수준 언어 코딩을 위한 AWS CDK 개선, 기존 리소스 가져오기, 구성 편차 감지 그리고 CloudFormation의 많은 핵심적인 이점을 그대로 상속받는 사용자 지정 유형을 더 쉽게 생성할 수 있게 해 주는 새로운 Registry가 포함됩니다.\"\n",
            "CloudFormation은 AWS Elastic Beanstalk와 어떻게 다른가요?\n",
            "이 두 서비스는 상호 보완할 수 있도록 설계되었습니다. AWS Elastic Beanstalk는 클라우드에서 손쉽게 애플리케이션을 배포하고 실행할 수 있는 환경을 제공합니다. 개발자 도구와 통합되며 애플리케이션의 수명 주기를 한곳에서 관리할 수 있는 환경을 제공합니다. 애플리케이션 워크로드를 Elastic Beanstalk 워크로드로 관리할 수 있는 경우 애플리케이션을 생성하고 업데이트할 때 좀 더 턴키 스타일에 가까운 환경을 경험할 수 있습니다. Elastic Beanstalk는 내부적으로 CloudFormation을 사용하여 리소스를 생성 및 유지 관리합니다. 애플리케이션에 사용자 지정 수준이 좀 더 높은 제어가 요구되는 경우 CloudFormation의 추가 기능을 통해 더 다양한 옵션으로 워크로드를 제어할 수 있습니다.\n",
            "AWS CloudFormation은 광범위한 AWS 및 서드 파티 리소스를 편리하게 배포할 수 있는 메커니즘입니다. 기존의 엔터프라이즈 애플리케이션, 레거시 애플리케이션, 다양한 AWS 리소스 및 컨테이너 기반 솔루션을 사용해 구축된 애플리케이션(AWS Elastic Beanstalk를 사용해 구축된 애플리케이션 포함)과 같은 각종 애플리케이션의 인프라 요구를 지원합니다.\n",
            "AWS CloudFormation은 AWS 리소스 유형 중 하나로 Elastic Beanstalk 애플리케이션 환경을 지원합니다. 따라서 애플리케이션 데이터를 저장하는 RDS 데이터베이스와 함께 AWS Elastic Beanstalk에 호스팅되는 애플리케이션을 생성하고 관리할 수 있습니다. 지원되는 모든 AWS 리소스를 그룹에 추가할 수 있습니다.\n",
            "\"category : CloudFormation, question : CloudFormation은 AWS Elastic Beanstalk와 어떻게 다른가요?, answer : 이 두 서비스는 상호 보완할 수 있도록 설계되었습니다. AWS Elastic Beanstalk는 클라우드에서 손쉽게 애플리케이션을 배포하고 실행할 수 있는 환경을 제공합니다. 개발자 도구와 통합되며 애플리케이션의 수명 주기를 한곳에서 관리할 수 있는 환경을 제공합니다. 애플리케이션 워크로드를 Elastic Beanstalk 워크로드로 관리할 수 있는 경우 애플리케이션을 생성하고 업데이트할 때 좀 더 턴키 스타일에 가까운 환경을 경험할 수 있습니다. Elastic Beanstalk는 내부적으로 CloudFormation을 사용하여 리소스를 생성 및 유지 관리합니다. 애플리케이션에 사용자 지정 수준이 좀 더 높은 제어가 요구되는 경우 CloudFormation의 추가 기능을 통해 더 다양한 옵션으로 워크로드를 제어할 수 있습니다.\n",
            "AWS CloudFormation은 광범위한 AWS 및 서드 파티 리소스를 편리하게 배포할 수 있는 메커니즘입니다. 기존의 엔터프라이즈 애플리케이션, 레거시 애플리케이션, 다양한 AWS 리소스 및 컨테이너 기반 솔루션을 사용해 구축된 애플리케이션(AWS Elastic Beanstalk를 사용해 구축된 애플리케이션 포함)과 같은 각종 애플리케이션의 인프라 요구를 지원합니다.\n",
            "AWS CloudFormation은 AWS 리소스 유형 중 하나로 Elastic Beanstalk 애플리케이션 환경을 지원합니다. 따라서 애플리케이션 데이터를 저장하는 RDS 데이터베이스와 함께 AWS Elastic Beanstalk에 호스팅되는 애플리케이션을 생성하고 관리할 수 있습니다. 지원되는 모든 AWS 리소스를 그룹에 추가할 수 있습니다.\"\n",
            "AWS CloudFormation에서 도입한 새로운 개념은 무엇인가요?\n",
            "CloudFormation에는 4가지 개념이 새로 도입되었는데, 먼저 템플릿은 애플리케이션을 배포하는 데 필요한 모든 리소스의 의도된 상태를 설명하는 JSON 또는 YAML 선언형 코드 파일입니다. 스택은 템플릿에 명시된 리소스 그룹을 구현 및 관리하며, 이러한 리소스의 상태 및 종속성을 함께 관리할 수 있게 해 줍니다. 변경 세트는 리소스의 생성, 업데이트 또는 제거를 위해 스택 작업을 통해 실행할 변경 사항의 미리 보기입니다. 스택 세트는 그룹을 복제할 수 있으며 함께 관리하는 스택 그룹입니다.\n",
            "\"category : CloudFormation, question : AWS CloudFormation에서 도입한 새로운 개념은 무엇인가요?, answer : CloudFormation에는 4가지 개념이 새로 도입되었는데, 먼저 템플릿은 애플리케이션을 배포하는 데 필요한 모든 리소스의 의도된 상태를 설명하는 JSON 또는 YAML 선언형 코드 파일입니다. 스택은 템플릿에 명시된 리소스 그룹을 구현 및 관리하며, 이러한 리소스의 상태 및 종속성을 함께 관리할 수 있게 해 줍니다. 변경 세트는 리소스의 생성, 업데이트 또는 제거를 위해 스택 작업을 통해 실행할 변경 사항의 미리 보기입니다. 스택 세트는 그룹을 복제할 수 있으며 함께 관리하는 스택 그룹입니다.\"\n",
            "AWS CloudFormation에서는 어떤 리소스를 지원합니까?\n",
            "지원되는 AWS 리소스 및 해당 기능의 전체 목록을 보려면 설명서의 릴리스 기록에서 지원되는 AWS 서비스 페이지를 참조하십시오.\n",
            "AWS CloudFormation Registry 및 AWS CloudFormation 사용자 지정 리소스를 사용하면 추가 AWS 및 서드 파티 리소스를 관리할 수 있습니다.\n",
            "\"category : CloudFormation, question : AWS CloudFormation에서는 어떤 리소스를 지원합니까?, answer : 지원되는 AWS 리소스 및 해당 기능의 전체 목록을 보려면 설명서의 릴리스 기록에서 지원되는 AWS 서비스 페이지를 참조하십시오.\n",
            "AWS CloudFormation Registry 및 AWS CloudFormation 사용자 지정 리소스를 사용하면 추가 AWS 및 서드 파티 리소스를 관리할 수 있습니다.\"\n",
            "AWS CloudFormation 스택의 일부인 개별 AWS 리소스를 관리할 수 있나요?\n",
            "예. 가능합니다. CloudFormation은 전혀 방해가 되지 않으므로, 사용자는 인프라의 모든 요소에 대해 전체 권한을 그대로 유지하며, 모든 기존 AWS 및 서드 파티 도구를 계속 사용하여 AWS 리소스를 관리할 수 있습니다. 하지만 CloudFormation에서는 추가적인 규칙, 모범 사례 및 규정 준수 제어를 반영할 수 있기 때문에 CloudFormation에서 리소스의 변경 사항을 관리하도록 허용하는 것이 좋습니다. 이처럼 예측 가능하고 제어된 접근 방식을 활용하면 애플리케이션 포트폴리오에서 수백 또는 수천 개의 리소스를 관리하는 데 도움이 됩니다.\n",
            "\"category : CloudFormation, question : AWS CloudFormation 스택의 일부인 개별 AWS 리소스를 관리할 수 있나요?, answer : 예. 가능합니다. CloudFormation은 전혀 방해가 되지 않으므로, 사용자는 인프라의 모든 요소에 대해 전체 권한을 그대로 유지하며, 모든 기존 AWS 및 서드 파티 도구를 계속 사용하여 AWS 리소스를 관리할 수 있습니다. 하지만 CloudFormation에서는 추가적인 규칙, 모범 사례 및 규정 준수 제어를 반영할 수 있기 때문에 CloudFormation에서 리소스의 변경 사항을 관리하도록 허용하는 것이 좋습니다. 이처럼 예측 가능하고 제어된 접근 방식을 활용하면 애플리케이션 포트폴리오에서 수백 또는 수천 개의 리소스를 관리하는 데 도움이 됩니다.\"\n",
            "AWS CloudFormation 템플릿의 요소는 어떤 것들이 있나요?\n",
            "CloudFormation 템플릿은 다음과 같은 다섯 가지 유형의 요소로 구성된 JSON 또는 YAML 형식의 텍스트 파일입니다.\n",
            "1. 템플릿 매개 변수 목록(옵션이며 스택 생성 시 입력 값 제공) 2. 출력 값의 선택적 목록(예: 웹 애플리케이션에 대한 전체 URL) 3. 정적 구성 값을 조회하는 데 사용되는 데이터 테이블의 선택적 목록(예: AMI 이름) 4. AWS 리소스 및 해당 구성 값 목록 5. 템플릿 파일 형식 버전 번호\n",
            "템플릿 파라미터는 스택을 구축할 때 런타임 시 템플릿이 어떻게 작동할지를 사용자 정의하는 데 사용됩니다. 예를 들어, 스택이 생성되면 Amazon RDS 데이터베이스 크기, Amazon EC2 인스턴스 유형, 데이터베이스 및 웹 서버 포트 번호를 AWS CloudFormation으로 전달할 수 있습니다. 각 파라미터에는 기본값과 설명이 포함될 수 있으며, 화면에 입력한 실제 값을 숨기기 위해 AWS CloudFormation 이벤트 로그에 “NoEcho”로 표시될 수 있습니다. AWS CloudFormation 스택을 생성하면 AWS Management Console에서 자동으로 사용자가 매개 변수 값을 편집할 수 있는 팝업 대화 상자 양식을 구성하여 표시합니다.\n",
            "출력 값은 AWS Management Console 또는 명령줄 도구를 통해 스택의 주요 리소스(예: Elastic Load Balancing 로드 밸런서 또는 Amazon RDS 데이터베이스의 주소)를 사용자에게 제공할 수 있는 편리한 방법입니다. 단순한 함수를 사용해 실제 AWS 리소스와 연결된 문자열 리터럴 및 속성 값을 연결할 수 있습니다. 또한 템플릿은 Registry 리소스 유형, 자체 사용자 지정 프라이빗 유형, 자체 매크로를 활용할 수 있을 뿐 아니라, AWS Secrets Manager 및 AWS System Manager Parameter Store에서 구성 파라미터를 검색할 수도 있습니다.\n",
            "\"category : CloudFormation, question : AWS CloudFormation 템플릿의 요소는 어떤 것들이 있나요?, answer : CloudFormation 템플릿은 다음과 같은 다섯 가지 유형의 요소로 구성된 JSON 또는 YAML 형식의 텍스트 파일입니다.\n",
            "1. 템플릿 매개 변수 목록(옵션이며 스택 생성 시 입력 값 제공) 2. 출력 값의 선택적 목록(예: 웹 애플리케이션에 대한 전체 URL) 3. 정적 구성 값을 조회하는 데 사용되는 데이터 테이블의 선택적 목록(예: AMI 이름) 4. AWS 리소스 및 해당 구성 값 목록 5. 템플릿 파일 형식 버전 번호\n",
            "템플릿 파라미터는 스택을 구축할 때 런타임 시 템플릿이 어떻게 작동할지를 사용자 정의하는 데 사용됩니다. 예를 들어, 스택이 생성되면 Amazon RDS 데이터베이스 크기, Amazon EC2 인스턴스 유형, 데이터베이스 및 웹 서버 포트 번호를 AWS CloudFormation으로 전달할 수 있습니다. 각 파라미터에는 기본값과 설명이 포함될 수 있으며, 화면에 입력한 실제 값을 숨기기 위해 AWS CloudFormation 이벤트 로그에 “NoEcho”로 표시될 수 있습니다. AWS CloudFormation 스택을 생성하면 AWS Management Console에서 자동으로 사용자가 매개 변수 값을 편집할 수 있는 팝업 대화 상자 양식을 구성하여 표시합니다.\n",
            "출력 값은 AWS Management Console 또는 명령줄 도구를 통해 스택의 주요 리소스(예: Elastic Load Balancing 로드 밸런서 또는 Amazon RDS 데이터베이스의 주소)를 사용자에게 제공할 수 있는 편리한 방법입니다. 단순한 함수를 사용해 실제 AWS 리소스와 연결된 문자열 리터럴 및 속성 값을 연결할 수 있습니다. 또한 템플릿은 Registry 리소스 유형, 자체 사용자 지정 프라이빗 유형, 자체 매크로를 활용할 수 있을 뿐 아니라, AWS Secrets Manager 및 AWS System Manager Parameter Store에서 구성 파라미터를 검색할 수도 있습니다.\"\n",
            "AWS CloudFormation에서 실제 리소스 이름을 선택하려면 어떻게 해야 하나요?\n",
            "템플릿에서 AWS 리소스에 논리적 이름을 할당할 수 있습니다. 스택이 생성되면 AWS CloudFormation은 논리적 이름을 이에 해당하는 실제 AWS 리소스의 이름에 바인딩합니다. 실제 리소스 이름은 스택 및 논리적 리소스 이름의 조합입니다. 따라서 AWS 리소스 간의 이름 충돌을 염려할 필요 없이 하나의 템플릿에서 여러 스택을 생성할 수 있습니다.\n",
            "\"category : CloudFormation, question : AWS CloudFormation에서 실제 리소스 이름을 선택하려면 어떻게 해야 하나요?, answer : 템플릿에서 AWS 리소스에 논리적 이름을 할당할 수 있습니다. 스택이 생성되면 AWS CloudFormation은 논리적 이름을 이에 해당하는 실제 AWS 리소스의 이름에 바인딩합니다. 실제 리소스 이름은 스택 및 논리적 리소스 이름의 조합입니다. 따라서 AWS 리소스 간의 이름 충돌을 염려할 필요 없이 하나의 템플릿에서 여러 스택을 생성할 수 있습니다.\"\n",
            "일부 리소스에 이름을 지정할 수 없는 이유는 무엇입니까?\n",
            "AWS CloudFormation에서는 Amazon S3 버킷 등 일부 리소스에 이름을 지정할 수 있지만, 모든 리소스에 이름을 지정할 수는 없습니다. 리소스에 이름을 지정하면 템플릿을 재사용하기 힘들고 업데이트하기 위해 리소스를 교체할 때 이름에 충돌이 생깁니다. CloudFormation에서는 이러한 문제를 최소화하기 위해 상황에 따라 리소스 이름 지정을 지원합니다.\n",
            "\"category : CloudFormation, question : 일부 리소스에 이름을 지정할 수 없는 이유는 무엇입니까?, answer : AWS CloudFormation에서는 Amazon S3 버킷 등 일부 리소스에 이름을 지정할 수 있지만, 모든 리소스에 이름을 지정할 수는 없습니다. 리소스에 이름을 지정하면 템플릿을 재사용하기 힘들고 업데이트하기 위해 리소스를 교체할 때 이름에 충돌이 생깁니다. CloudFormation에서는 이러한 문제를 최소화하기 위해 상황에 따라 리소스 이름 지정을 지원합니다.\"\n",
            "AWS CloudFormation을 사용해 스택을 생성할 때 소프트웨어를 설치할 수 있나요?\n",
            "예. AWS CloudFormation은 애플리케이션 부트스트랩 스크립트 세트를 제공하므로, CloudFormation 템플릿에 이들에 대한 설명을 제공하는 간단한 방식으로 EC2 인스턴스에 패키지, 파일 및 서비스를 설치할 수 있습니다. 자세한 내용과 방법은 AWS CloudFormation을 통한 애플리케이션 부트스트랩을 참조하세요.\n",
            "CloudFormation을 Systems Manager와 통합하면 Systems Manager Automation 문서를 통해 소프트웨어 설치를 진행하고 유지 관리할 수 있습니다.\n",
            "\"category : CloudFormation, question : AWS CloudFormation을 사용해 스택을 생성할 때 소프트웨어를 설치할 수 있나요?, answer : 예. AWS CloudFormation은 애플리케이션 부트스트랩 스크립트 세트를 제공하므로, CloudFormation 템플릿에 이들에 대한 설명을 제공하는 간단한 방식으로 EC2 인스턴스에 패키지, 파일 및 서비스를 설치할 수 있습니다. 자세한 내용과 방법은 AWS CloudFormation을 통한 애플리케이션 부트스트랩을 참조하세요.\n",
            "CloudFormation을 Systems Manager와 통합하면 Systems Manager Automation 문서를 통해 소프트웨어 설치를 진행하고 유지 관리할 수 있습니다.\"\n",
            "AWS CloudFormation을 Chef와 함께 사용할 수 있나요?\n",
            "예. AWS CloudFormation은 EC2 인스턴스에서 Chef Server 및 Chef Client 소프트웨어를 부트스트랩하는 데 사용할 수 있습니다. 자세한 내용과 방법은 AWS CloudFormation과 Chef의 통합을 참조하세요.\n",
            "\"category : CloudFormation, question : AWS CloudFormation을 Chef와 함께 사용할 수 있나요?, answer : 예. AWS CloudFormation은 EC2 인스턴스에서 Chef Server 및 Chef Client 소프트웨어를 부트스트랩하는 데 사용할 수 있습니다. 자세한 내용과 방법은 AWS CloudFormation과 Chef의 통합을 참조하세요.\"\n",
            "AWS CloudFormation을 Puppet과 함께 사용할 수 있나요?\n",
            "예. AWS CloudFormation은 EC2 인스턴스에서 Puppet Master 및 Puppet Client 소프트웨어를 부트스트랩하는 데 사용할 수 있습니다. 자세한 내용과 방법은 AWS CloudFormation과 Puppet의 통합을 참조하세요.\n",
            "\"category : CloudFormation, question : AWS CloudFormation을 Puppet과 함께 사용할 수 있나요?, answer : 예. AWS CloudFormation은 EC2 인스턴스에서 Puppet Master 및 Puppet Client 소프트웨어를 부트스트랩하는 데 사용할 수 있습니다. 자세한 내용과 방법은 AWS CloudFormation과 Puppet의 통합을 참조하세요.\"\n",
            "AWS CloudFormation을 Terraform과 함께 사용할 수 있나요?\n",
            "예. CloudFormation은 EC2 인스턴스에서 Terraform 엔진을 부트스트랩하고, Terraform 리소스 공급자를 통해 스택 리소스 관리, 종속성, 안정화 및 롤백을 활용하여 스택에 리소스를 생성할 수 있습니다.\n",
            "\"category : CloudFormation, question : AWS CloudFormation을 Terraform과 함께 사용할 수 있나요?, answer : 예. CloudFormation은 EC2 인스턴스에서 Terraform 엔진을 부트스트랩하고, Terraform 리소스 공급자를 통해 스택 리소스 관리, 종속성, 안정화 및 롤백을 활용하여 스택에 리소스를 생성할 수 있습니다.\"\n",
            "AWS CloudFormation에서는 Amazon EC2 태깅을 지원하나요?\n",
            "예. 태그 지정 기능을 지원하는 Amazon EC2 리소스에도 AWS 템플릿에서 태그를 지정할 수 있습니다. 템플릿 파라미터, 기타 리소스 이름, 리소스 속성 값(예: 주소), 단순 함수로 계산한 값(예: 연결된 문자열 목록) 등을 태그 값으로 나타낼 수 있습니다. CloudFormation은 Amazon EBS 볼륨 및 Amazon EC2 인스턴스가 속한 CloudFormation 스택의 이름을 사용하여 해당 볼륨 및 인스턴스에 자동으로 태그를 지정합니다.\n",
            "\"category : CloudFormation, question : AWS CloudFormation에서는 Amazon EC2 태깅을 지원하나요?, answer : 예. 태그 지정 기능을 지원하는 Amazon EC2 리소스에도 AWS 템플릿에서 태그를 지정할 수 있습니다. 템플릿 파라미터, 기타 리소스 이름, 리소스 속성 값(예: 주소), 단순 함수로 계산한 값(예: 연결된 문자열 목록) 등을 태그 값으로 나타낼 수 있습니다. CloudFormation은 Amazon EBS 볼륨 및 Amazon EC2 인스턴스가 속한 CloudFormation 스택의 이름을 사용하여 해당 볼륨 및 인스턴스에 자동으로 태그를 지정합니다.\"\n",
            "Amazon EC2 인스턴스 또는 Auto Scaling 시작 구성 사용자 데이터 필드에 액세스할 수 있나요?\n",
            "예. 단순한 함수를 사용해 AWS 리소스의 문자열 리터럴과 속성 값을 연결하고, 이를 템플릿의 사용자 데이터 필드에 전달할 수 있습니다. 이 사용하기 쉬운 기능에 대한 자세한 내용은 Amazon의 샘플 템플릿을 참조하십시오.\n",
            "\"category : CloudFormation, question : Amazon EC2 인스턴스 또는 Auto Scaling 시작 구성 사용자 데이터 필드에 액세스할 수 있나요?, answer : 예. 단순한 함수를 사용해 AWS 리소스의 문자열 리터럴과 속성 값을 연결하고, 이를 템플릿의 사용자 데이터 필드에 전달할 수 있습니다. 이 사용하기 쉬운 기능에 대한 자세한 내용은 Amazon의 샘플 템플릿을 참조하십시오.\"\n",
            "스택의 리소스 중 하나가 생성되지 않을 경우 어떻게 되나요?\n",
            "기본적으로 “Automatic rollback on error” 기능이 활성화되어 있습니다. 이 경우 CloudFormation은 모든 개별 작업에 성공할 경우 스택의 모든 리소스를 생성 또는 업데이트만 하게 됩니다. 그렇지 않을 경우 CloudFormation은 마지막으로 알려진 안정된 구성으로 스택을 되돌립니다. 이 기능은 예를 들어, 실수로 탄력적 IP 주소 제한을 초과했거나 실행하려는 EC2 AMI에 액세스할 수 있는 권한이 없는 경우에 유용합니다. 이 기능을 사용하면 스택이 완전히 생성되거나 전혀 생성되지 않는다는 점을 이용해, 시스템 관리와 CloudFormation에 구축된 계층형 솔루션을 간소화할 수 있습니다.\n",
            "\"category : CloudFormation, question : 스택의 리소스 중 하나가 생성되지 않을 경우 어떻게 되나요?, answer : 기본적으로 “Automatic rollback on error” 기능이 활성화되어 있습니다. 이 경우 CloudFormation은 모든 개별 작업에 성공할 경우 스택의 모든 리소스를 생성 또는 업데이트만 하게 됩니다. 그렇지 않을 경우 CloudFormation은 마지막으로 알려진 안정된 구성으로 스택을 되돌립니다. 이 기능은 예를 들어, 실수로 탄력적 IP 주소 제한을 초과했거나 실행하려는 EC2 AMI에 액세스할 수 있는 권한이 없는 경우에 유용합니다. 이 기능을 사용하면 스택이 완전히 생성되거나 전혀 생성되지 않는다는 점을 이용해, 시스템 관리와 CloudFormation에 구축된 계층형 솔루션을 간소화할 수 있습니다.\"\n",
            "애플리케이션이 가동될 때까지 기다린 후에 스택을 생성할 수 있나요?\n",
            "예. CloudFormation의 옵션 중 하나는 장벽처럼 작용해 애플리케이션 또는 관리 시스템과 같은 외부 소스에서 완료 신호가 수신될 때까지 다른 리소스의 생성을 차단하는 WaitCondition 리소스입니다. 그 밖의 옵션에는 AWS Lambda 함수를 통한 사용자 지정 로직 생성 등이 있습니다.\n",
            "\"category : CloudFormation, question : 애플리케이션이 가동될 때까지 기다린 후에 스택을 생성할 수 있나요?, answer : 예. CloudFormation의 옵션 중 하나는 장벽처럼 작용해 애플리케이션 또는 관리 시스템과 같은 외부 소스에서 완료 신호가 수신될 때까지 다른 리소스의 생성을 차단하는 WaitCondition 리소스입니다. 그 밖의 옵션에는 AWS Lambda 함수를 통한 사용자 지정 로직 생성 등이 있습니다.\"\n",
            "스택이 삭제된 경우 데이터를 저장할 수 있나요?\n",
            "예. CloudFormation을 사용해 템플릿에서 리소스에 대한 삭제 정책을 정의할 수 있습니다. Amazon EBS 볼륨 또는 Amazon RDS 데이터베이스 인스턴스가 삭제되기 전에 이에 대해 생성할 스냅샷을 지정할 수 있습니다. 스택이 삭제될 때 리소스를 삭제하지 않고 보존하도록 지정할 수도 있습니다. 이 기능은 스택이 삭제될 때 Amazon S3 버킷을 보존하려는 경우에 유용합니다.\n",
            "\"category : CloudFormation, question : 스택이 삭제된 경우 데이터를 저장할 수 있나요?, answer : 예. CloudFormation을 사용해 템플릿에서 리소스에 대한 삭제 정책을 정의할 수 있습니다. Amazon EBS 볼륨 또는 Amazon RDS 데이터베이스 인스턴스가 삭제되기 전에 이에 대해 생성할 스냅샷을 지정할 수 있습니다. 스택이 삭제될 때 리소스를 삭제하지 않고 보존하도록 지정할 수도 있습니다. 이 기능은 스택이 삭제될 때 Amazon S3 버킷을 보존하려는 경우에 유용합니다.\"\n",
            "Virtual Private Cloud(VPC)에 스택을 생성할 수 있나요?\n",
            "예. CloudFormation은 VPC에서 탄력적 IP, Amazon EC2 인스턴스, EC2 보안 그룹, Auto Scaling 그룹, Elastic Load Balancer, Amazon RDS 데이터베이스 인스턴스 및 Amazon RDS 보안 그룹과 같은 리소스를 생성할 뿐 아니라, VPC, 서브넷, 게이트웨이, 라우팅 테이블 및 네트워크 ACL을 생성하는 기능도 지원합니다.\n",
            "\"category : CloudFormation, question : Virtual Private Cloud(VPC)에 스택을 생성할 수 있나요?, answer : 예. CloudFormation은 VPC에서 탄력적 IP, Amazon EC2 인스턴스, EC2 보안 그룹, Auto Scaling 그룹, Elastic Load Balancer, Amazon RDS 데이터베이스 인스턴스 및 Amazon RDS 보안 그룹과 같은 리소스를 생성할 뿐 아니라, VPC, 서브넷, 게이트웨이, 라우팅 테이블 및 네트워크 ACL을 생성하는 기능도 지원합니다.\"\n",
            "스택을 생성한 후 업데이트할 수 있나요?\n",
            "예. CloudFormation을 사용해 제어되고 있는 기존 스택의 리소스를 예측 가능한 방식으로 수정 및 업데이트할 수 있습니다. 템플릿을 사용해 스택 변경 사항을 관리하여, 버전 관리를 AWS 인프라에서 실행되는 소프트웨어에 적용하는 것과 같은 방법으로 AWS 인프라에 적용할 수 있습니다.\n",
            "\"category : CloudFormation, question : 스택을 생성한 후 업데이트할 수 있나요?, answer : 예. CloudFormation을 사용해 제어되고 있는 기존 스택의 리소스를 예측 가능한 방식으로 수정 및 업데이트할 수 있습니다. 템플릿을 사용해 스택 변경 사항을 관리하여, 버전 관리를 AWS 인프라에서 실행되는 소프트웨어에 적용하는 것과 같은 방법으로 AWS 인프라에 적용할 수 있습니다.\"\n",
            "CloudFormation 커뮤니티에 참여하려면 어떻게 해야 하나요?\n",
            "AWS CloudFormation GitHub 커뮤니티에 가입하세요.\n",
            "\"category : CloudFormation, question : CloudFormation 커뮤니티에 참여하려면 어떻게 해야 하나요?, answer : AWS CloudFormation GitHub 커뮤니티에 가입하세요.\"\n",
            "CloudFormation 외부에서 생성된 리소스를 관리할 수 있나요?\n",
            "예! 리소스 가져오기를 활용하면 리소스 가져오기를 통해 기존 리소스를 AWS CloudFormation 관리로 가져올 수 있습니다.\n",
            "\"category : CloudFormation, question : CloudFormation 외부에서 생성된 리소스를 관리할 수 있나요?, answer : 예! 리소스 가져오기를 활용하면 리소스 가져오기를 통해 기존 리소스를 AWS CloudFormation 관리로 가져올 수 있습니다.\"\n",
            "AWS CloudFormation에 가입하려면 어떻게 해야 하나요?\n",
            "CloudFormation에 가입하려면 CloudFormation 제품 페이지에서 무료 계정 생성을 클릭하세요. 가입한 후 CloudFormation 설명서를 참조하세요. 여기에는 시작하기 안내서도 포함되어 있습니다.\n",
            "\"category : CloudFormation, question : AWS CloudFormation에 가입하려면 어떻게 해야 하나요?, answer : CloudFormation에 가입하려면 CloudFormation 제품 페이지에서 무료 계정 생성을 클릭하세요. 가입한 후 CloudFormation 설명서를 참조하세요. 여기에는 시작하기 안내서도 포함되어 있습니다.\"\n",
            "AWS CloudFormation에 가입할 때 전화번호를 묻는 메시지가 표시되는 이유는 무엇입니까?\n",
            "CloudFormation에 가입하려면 연락이 필요한 경우에 대비하여 유효한 전화번호와 이메일 주소를 AWS에 제출해야 합니다. 전화번호를 확인하는 데는 몇 분밖에 걸리지 않습니다. 가입 프로세스 중에 자동 전화를 받고 전화기의 키패드를 사용해 PIN 번호를 입력합니다.\n",
            "\"category : CloudFormation, question : AWS CloudFormation에 가입할 때 전화번호를 묻는 메시지가 표시되는 이유는 무엇입니까?, answer : CloudFormation에 가입하려면 연락이 필요한 경우에 대비하여 유효한 전화번호와 이메일 주소를 AWS에 제출해야 합니다. 전화번호를 확인하는 데는 몇 분밖에 걸리지 않습니다. 가입 프로세스 중에 자동 전화를 받고 전화기의 키패드를 사용해 PIN 번호를 입력합니다.\"\n",
            "가입 후 어떻게 시작하나요?\n",
            "CloudFormation을 시작하는 가장 좋은 방법은 AWS 기술 설명서에 포함된 시작하기 안내서를 참조하는 것입니다. 몇 분 내에 WordPress 등과 같은 애플리케이션을 실행하는 데 필요한 인프라 생성 방법을 설명하는 샘플 템플릿 중 하나를 배포하여 사용할 수 있습니다. 서드 파티 커리큘럼 공급자부터 웹상의 자습서 및 문서에 이르기까지 그 밖의 다양한 CloudFormation 교육 소스도 마련되어 있습니다. 자세한 내용은 CloudFormation 리소스를 참조하세요.\n",
            "\"category : CloudFormation, question : 가입 후 어떻게 시작하나요?, answer : CloudFormation을 시작하는 가장 좋은 방법은 AWS 기술 설명서에 포함된 시작하기 안내서를 참조하는 것입니다. 몇 분 내에 WordPress 등과 같은 애플리케이션을 실행하는 데 필요한 인프라 생성 방법을 설명하는 샘플 템플릿 중 하나를 배포하여 사용할 수 있습니다. 서드 파티 커리큘럼 공급자부터 웹상의 자습서 및 문서에 이르기까지 그 밖의 다양한 CloudFormation 교육 소스도 마련되어 있습니다. 자세한 내용은 CloudFormation 리소스를 참조하세요.\"\n",
            "AWS CloudFormation을 확인하는 데 사용할 수 있는 샘플 템플릿이 있나요?\n",
            "예, CloudFormation에는 테스트 드라이브를 통해 해당 기능을 탐색하는 데 사용할 수 있는 샘플 템플릿이 포함되어 있습니다. AWS의 샘플 템플릿에서는 여러 AWS 리소스를 한 번에 상호 연결해서 사용하는 방법과 여러 가용 영역 중복성에 대한 모범 사례 준수, 확장 및 경보에 대해 설명합니다. AWS Management Console로 이동하여 스택 생성을 클릭하고 단계를 따라 샘플 중 하나를 선택하여 시작하면 됩니다. 스택이 생성되고 나면 콘솔에서 해당 스택을 선택한 다음, 각각의 스택을 생성하는 데 사용된 템플릿 파일의 세부 정보를 템플릿과 파라미터 탭에서 확인합니다. GitHub에서도 샘플 템플릿을 사용할 수 있습니다.\n",
            "\"category : CloudFormation, question : AWS CloudFormation을 확인하는 데 사용할 수 있는 샘플 템플릿이 있나요?, answer : 예, CloudFormation에는 테스트 드라이브를 통해 해당 기능을 탐색하는 데 사용할 수 있는 샘플 템플릿이 포함되어 있습니다. AWS의 샘플 템플릿에서는 여러 AWS 리소스를 한 번에 상호 연결해서 사용하는 방법과 여러 가용 영역 중복성에 대한 모범 사례 준수, 확장 및 경보에 대해 설명합니다. AWS Management Console로 이동하여 스택 생성을 클릭하고 단계를 따라 샘플 중 하나를 선택하여 시작하면 됩니다. 스택이 생성되고 나면 콘솔에서 해당 스택을 선택한 다음, 각각의 스택을 생성하는 데 사용된 템플릿 파일의 세부 정보를 템플릿과 파라미터 탭에서 확인합니다. GitHub에서도 샘플 템플릿을 사용할 수 있습니다.\"\n",
            "AWS CloudFormation Registry란 무엇인가요?\n",
            "AWS CloudFormation Registry는 AWS 및 서드 파티 리소스 유형을 등록, 사용 및 검색할 수 있는 관리형 서비스입니다. 서드 파티 리소스 유형을 등록해야 AWS CloudFormation 템플릿으로 리소스를 프로비저닝하는 데 사용할 수 있습니다. 자세한 내용은 AWS 설명서에서 AWS CloudFormation Registry 사용을 참조하세요.\n",
            "\"category : CloudFormation, question : AWS CloudFormation Registry란 무엇인가요?, answer : AWS CloudFormation Registry는 AWS 및 서드 파티 리소스 유형을 등록, 사용 및 검색할 수 있는 관리형 서비스입니다. 서드 파티 리소스 유형을 등록해야 AWS CloudFormation 템플릿으로 리소스를 프로비저닝하는 데 사용할 수 있습니다. 자세한 내용은 AWS 설명서에서 AWS CloudFormation Registry 사용을 참조하세요.\"\n",
            "AWS CloudFormation의 리소스 유형은 무엇입니까?\n",
            "리소스 공급자는 생성, 읽기, 업데이트, 삭제 및 나열 작업을 통해 기본 리소스의 수명주기를 제어하는 사양과 핸들러가 포함된 리소스 유형 세트입니다. CloudFormation으로 리소스 공급자를 사용하고 리소스를 프로비저닝할 수 있습니다. 예를 들어, AWS::EC2::Instance는 Amazon EC2 공급자의 리소스 유형입니다. 이 유형을 사용하여 CloudFormation으로 Amazon EC2 인스턴스를 모델링하고 프로비저닝할 수 있습니다. CloudFormation Registry를 사용하면 리소스 공급자를 만들고 사용하여 SaaS 모니터링, 팀 생산성 또는 소스 코드 관리 리소스 등의 타사 리소스를 모델링하고 프로비저닝할 수 있습니다.\n",
            "\"category : CloudFormation, question : AWS CloudFormation의 리소스 유형은 무엇입니까?, answer : 리소스 공급자는 생성, 읽기, 업데이트, 삭제 및 나열 작업을 통해 기본 리소스의 수명주기를 제어하는 사양과 핸들러가 포함된 리소스 유형 세트입니다. CloudFormation으로 리소스 공급자를 사용하고 리소스를 프로비저닝할 수 있습니다. 예를 들어, AWS::EC2::Instance는 Amazon EC2 공급자의 리소스 유형입니다. 이 유형을 사용하여 CloudFormation으로 Amazon EC2 인스턴스를 모델링하고 프로비저닝할 수 있습니다. CloudFormation Registry를 사용하면 리소스 공급자를 만들고 사용하여 SaaS 모니터링, 팀 생산성 또는 소스 코드 관리 리소스 등의 타사 리소스를 모델링하고 프로비저닝할 수 있습니다.\"\n",
            "AWS 리소스 공급자와 타사 리소스 공급자의 차이점은 무엇인가요?\n",
            "AWS 리소스 공급자와 타사 리소스 공급자의 차이점은 출처입니다. AWS 리소스 공급자는 Amazon과 AWS에서 AWS 리소스 및 서비스를 관리하기 위해 만들고 유지 관리합니다. 예를 들어, 3개의 AWS 리소스 공급자가 Amazon DynamoDB, AWS Lambda 및 Amazon EC2 리소스를 관리하는 데 도움이 됩니다. 이러한 공급자에는 AWS::DynamoDB::Table, AWS::Lambda::Function 및 AWS::EC2::Instance 등의 리소스 유형이 들어 있습니다. 자세한 내용은 당사 설명서를 참조하십시오.\n",
            "타사 리소스 공급자는 다른 회사, 조직 또는 개발자 커뮤니티에서 만듭니다. 타사 리소스 공급자를 통해 모니터링, 팀 생산성, 인시던트 관리 또는 버전 제어 관리 도구 등의 비 AWS SaaS소스트웨어 서비스와 AWS 애플리케이션 리소스를 비롯한 비 AWS 리소스와 AWS 리소스를 모두 관리할 수 있습니다.\n",
            "\"category : CloudFormation, question : AWS 리소스 공급자와 타사 리소스 공급자의 차이점은 무엇인가요?, answer : AWS 리소스 공급자와 타사 리소스 공급자의 차이점은 출처입니다. AWS 리소스 공급자는 Amazon과 AWS에서 AWS 리소스 및 서비스를 관리하기 위해 만들고 유지 관리합니다. 예를 들어, 3개의 AWS 리소스 공급자가 Amazon DynamoDB, AWS Lambda 및 Amazon EC2 리소스를 관리하는 데 도움이 됩니다. 이러한 공급자에는 AWS::DynamoDB::Table, AWS::Lambda::Function 및 AWS::EC2::Instance 등의 리소스 유형이 들어 있습니다. 자세한 내용은 당사 설명서를 참조하십시오.\n",
            "타사 리소스 공급자는 다른 회사, 조직 또는 개발자 커뮤니티에서 만듭니다. 타사 리소스 공급자를 통해 모니터링, 팀 생산성, 인시던트 관리 또는 버전 제어 관리 도구 등의 비 AWS SaaS소스트웨어 서비스와 AWS 애플리케이션 리소스를 비롯한 비 AWS 리소스와 AWS 리소스를 모두 관리할 수 있습니다.\"\n",
            "리소스 스키마란 무엇입니까?\n",
            "리소스 스키마는 정형화되고 일관된 형식으로 리소스 유형을 정의합니다. 이 스키마는 리소스 유형의 정의를 확인하는 데도 사용됩니다. 스키마에는 특정 리소스 유형에 대해 지원되는 모드 파라미터 및 속성은 물론, 최소한의 권한으로 리소스를 생성하는 데 필요한 권한이 포함됩니다.\n",
            "\"category : CloudFormation, question : 리소스 스키마란 무엇입니까?, answer : 리소스 스키마는 정형화되고 일관된 형식으로 리소스 유형을 정의합니다. 이 스키마는 리소스 유형의 정의를 확인하는 데도 사용됩니다. 스키마에는 특정 리소스 유형에 대해 지원되는 모드 파라미터 및 속성은 물론, 최소한의 권한으로 리소스를 생성하는 데 필요한 권한이 포함됩니다.\"\n",
            "리소스 유형을 개발하려면 어떻게 해야 하나요?\n",
            "AWS CloudFormation CLI를 사용하여 리소스 공급자를 만듭니다. 먼저 필요한 권한과 다른 리소스와의 관계를 포함하는 리소스에 대한 단순한 선언적 스키마를 정의합니다. 그런 다음 CloudFormation CLI를 사용하여 유닛 및 통합 테스트를 위한 테스트 스텁과 함께 리소스 수명 주기 핸들러(만들기, 읽기, 업데이트, 삭제 및 나열)에 대한 스캐폴딩을 생성합니다.\n",
            "\"category : CloudFormation, question : 리소스 유형을 개발하려면 어떻게 해야 하나요?, answer : AWS CloudFormation CLI를 사용하여 리소스 공급자를 만듭니다. 먼저 필요한 권한과 다른 리소스와의 관계를 포함하는 리소스에 대한 단순한 선언적 스키마를 정의합니다. 그런 다음 CloudFormation CLI를 사용하여 유닛 및 통합 테스트를 위한 테스트 스텁과 함께 리소스 수명 주기 핸들러(만들기, 읽기, 업데이트, 삭제 및 나열)에 대한 스캐폴딩을 생성합니다.\"\n",
            "리소스 공급자를 등록하려면 어떻게 해야 하나요?\n",
            "오픈 소스 AWS CloudFormation CLI를 사용할 수도 있고, AWS SDK와 AWS CLI를 통해 사용할 수 있는 RegisterType과 관련 Registry API를 직접 호출할 수도 있습니다. 자세한 내용은 AWS 설명서에서 AWS CloudFormation Registry 사용을 참조하세요. AWS 리소스 공급자는 바로 사용 가능하며 사용 전에 어떠한 추가 등록 단계도 필요 없습니다.\n",
            "\"category : CloudFormation, question : 리소스 공급자를 등록하려면 어떻게 해야 하나요?, answer : 오픈 소스 AWS CloudFormation CLI를 사용할 수도 있고, AWS SDK와 AWS CLI를 통해 사용할 수 있는 RegisterType과 관련 Registry API를 직접 호출할 수도 있습니다. 자세한 내용은 AWS 설명서에서 AWS CloudFormation Registry 사용을 참조하세요. AWS 리소스 공급자는 바로 사용 가능하며 사용 전에 어떠한 추가 등록 단계도 필요 없습니다.\"\n",
            "CloudFormation 퍼블릭 레지스트리는 CloudFormation 레지스트리와 어떤 관계가 있습니까?\n",
            "2019년 출시된 CloudFormation 레지스트리는 프라이빗 리스팅으로 구성되었으며, 이를 통해 고객은 전용 사용 목적으로 CloudFormation을 확장할 수 있습니다. 퍼블릭 레지스트리는 CloudFormation 레지스트리를 확장하며, 리소스 유형 및 모듈을 공유, 검색, 이용 및 관리하기 위한 검색 가능한 퍼블릭 중앙 위치를 추가합니다.<> 이를 통해 AWS 및 서드 파티 제품 모두에 대해 일관된 방식으로 인프라와 애플리케이션을 보다 쉽게 구성하고 관리할 수 있습니다.\n",
            "\"category : CloudFormation, question : CloudFormation 퍼블릭 레지스트리는 CloudFormation 레지스트리와 어떤 관계가 있습니까?, answer : 2019년 출시된 CloudFormation 레지스트리는 프라이빗 리스팅으로 구성되었으며, 이를 통해 고객은 전용 사용 목적으로 CloudFormation을 확장할 수 있습니다. 퍼블릭 레지스트리는 CloudFormation 레지스트리를 확장하며, 리소스 유형 및 모듈을 공유, 검색, 이용 및 관리하기 위한 검색 가능한 퍼블릭 중앙 위치를 추가합니다.<> 이를 통해 AWS 및 서드 파티 제품 모두에 대해 일관된 방식으로 인프라와 애플리케이션을 보다 쉽게 구성하고 관리할 수 있습니다.\"\n",
            "CloudFormation 퍼블릭 레지스트리에서 사용 가능한 서드 파티 리소스 유형을 사용할 때 비용이 발생하나요?\n",
            "예. CloudFormation 요금 페이지를 참조하세요.\n",
            "\"category : CloudFormation, question : CloudFormation 퍼블릭 레지스트리에서 사용 가능한 서드 파티 리소스 유형을 사용할 때 비용이 발생하나요?, answer : 예. CloudFormation 요금 페이지를 참조하세요.\"\n",
            "AWS는 CloudFormation 퍼블릭 레지스트리에서 서드 파티 확장 기능의 게시자를 확인하나요?\n",
            "예. CloudFormation 퍼블릭 레지스트리에서 확인된 게시자의 큐레이팅된 콘텐츠에 액세스할 수 있습니다. 먼저, AWS Marketplace 또는 서드 파티(예: GitHub 및 Bitbucket)를 통해 각 게시자의 자격 증명을 확인합니다.\n",
            "\"category : CloudFormation, question : AWS는 CloudFormation 퍼블릭 레지스트리에서 서드 파티 확장 기능의 게시자를 확인하나요?, answer : 예. CloudFormation 퍼블릭 레지스트리에서 확인된 게시자의 큐레이팅된 콘텐츠에 액세스할 수 있습니다. 먼저, AWS Marketplace 또는 서드 파티(예: GitHub 및 Bitbucket)를 통해 각 게시자의 자격 증명을 확인합니다.\"\n",
            "AWS CloudFormation 퍼블릭 레지스트리란 무엇인가요?\n",
            "CloudFormation 퍼블릭 레지스트리는 AWS 파트너 네트워크(APN) 파트너 및 개발자 커뮤니티에서 게시한 리소스 유형(프로비저닝 로직) 및 모듈을 포함하는, 검색 가능한 새로운 관리형 확장 기능 카탈로그입니다. 이제 누구나 CloudFormation 퍼블릭 레지스트리를 통해 레지스트리에서 리소스 유형 및 모듈을 게시할 수 있습니다. 고객은 이러한 게시된 리소스 유형 및 모듈을 쉽게 검색하고 사용할 수 있으므로, 직접 구축하고 관리하지 않아도 됩니다.\n",
            "\"category : CloudFormation, question : AWS CloudFormation 퍼블릭 레지스트리란 무엇인가요?, answer : CloudFormation 퍼블릭 레지스트리는 AWS 파트너 네트워크(APN) 파트너 및 개발자 커뮤니티에서 게시한 리소스 유형(프로비저닝 로직) 및 모듈을 포함하는, 검색 가능한 새로운 관리형 확장 기능 카탈로그입니다. 이제 누구나 CloudFormation 퍼블릭 레지스트리를 통해 레지스트리에서 리소스 유형 및 모듈을 게시할 수 있습니다. 고객은 이러한 게시된 리소스 유형 및 모듈을 쉽게 검색하고 사용할 수 있으므로, 직접 구축하고 관리하지 않아도 됩니다.\"\n",
            "리소스와 모듈의 차이는 무엇인가요?\n",
            "리소스 유형은 프로비저닝 로직을 포함하는 코드 패키지로, 이를 통해 Amazon EC2 인스턴스 또는 Amazon DynamoDB 테이블과 같은 리소스의 수명 주기(생성부터 삭제)를 관리하여 복잡한 API 상호 작용을 추상화할 수 있습니다. 리소스 유형은 리소스의 형태 및 속성을 정의하는 스키마와 리소스를 프로비저닝, 업데이트, 삭제 및 기술하는 데 필요한 로직을 포함합니다. CloudFormation 퍼블릭 레지스트리에서 서드 파티 리소스 유형의 예로는, Datadog 모니터, MongoDB Atlas Project 또는 Atlassian Opsgenie User가 있습니다.  모듈은 여러 CloudFormation 템플릿에서 재사용할 수 있는 구성 요소로, 네이티브 CloudFormation 리소스와 같이 사용됩니다. 이러한 구성 요소는 Amazon Elastic Compute Cloud(Amazon EC2) 인스턴스 정의 모범 사례와 같은 단일 리소스에 대한 항목이거나 애플리케이션 아키텍처의 공통 패턴을 정의하기 위한 다중 리소스에 대한 항목일 수 있습니다.\n",
            "\"category : CloudFormation, question : 리소스와 모듈의 차이는 무엇인가요?, answer : 리소스 유형은 프로비저닝 로직을 포함하는 코드 패키지로, 이를 통해 Amazon EC2 인스턴스 또는 Amazon DynamoDB 테이블과 같은 리소스의 수명 주기(생성부터 삭제)를 관리하여 복잡한 API 상호 작용을 추상화할 수 있습니다. 리소스 유형은 리소스의 형태 및 속성을 정의하는 스키마와 리소스를 프로비저닝, 업데이트, 삭제 및 기술하는 데 필요한 로직을 포함합니다. CloudFormation 퍼블릭 레지스트리에서 서드 파티 리소스 유형의 예로는, Datadog 모니터, MongoDB Atlas Project 또는 Atlassian Opsgenie User가 있습니다.  모듈은 여러 CloudFormation 템플릿에서 재사용할 수 있는 구성 요소로, 네이티브 CloudFormation 리소스와 같이 사용됩니다. 이러한 구성 요소는 Amazon Elastic Compute Cloud(Amazon EC2) 인스턴스 정의 모범 사례와 같은 단일 리소스에 대한 항목이거나 애플리케이션 아키텍처의 공통 패턴을 정의하기 위한 다중 리소스에 대한 항목일 수 있습니다.\"\n",
            "AWS CloudFormation 레지스트리에 리소스 또는 모듈을 개발하여 추가하려면 어떻게 해야 하나요?\n",
            "이 링크를 참조하여 AWS CloudFormation 레지스트리에 리소스 또는 모듈을 개발하여 추가할 수 있습니다. 그리고 비공개로 게시하거나 퍼블릭 레지스트리에 게시할 수 있습니다.\n",
            "\"category : CloudFormation, question : AWS CloudFormation 레지스트리에 리소스 또는 모듈을 개발하여 추가하려면 어떻게 해야 하나요?, answer : 이 링크를 참조하여 AWS CloudFormation 레지스트리에 리소스 또는 모듈을 개발하여 추가할 수 있습니다. 그리고 비공개로 게시하거나 퍼블릭 레지스트리에 게시할 수 있습니다.\"\n",
            "AWS CloudFormation을 사용하려면 비용이 얼마나 드나요?\n",
            "AWS::*, Alexa::* 및 Custom::*과 같은 네임스페이스의 리소스 공급자에서 AWS CloudFormation을 사용하는 경우 추가 요금은 부과되지 않습니다. 이 경우 수동으로 생성했을 때와 같이 AWS CloudFormation을 사용해 생성한 AWS 리소스(예: Amazon EC2 인스턴스, Elastic Load Balancing 로드 밸런서 등)에 대해서만 요금을 지불하면 됩니다. 종량제로 요금이 청구되며 최소 요금 및 선수금은 없습니다.\n",
            "위에서 언급한 네임스페이스 외부의 AWS CloudFormation에서 리소스 공급자를 사용하는 경우 핸들러 작업당 요금이 부과됩니다. 핸들러 작업은 리소스에서 생성, 업데이트, 삭제, 읽기 또는 나열 작업을 말합니다. 자세한 내용은 요금 페이지를 참조하세요.\n",
            "\"category : CloudFormation, question : AWS CloudFormation을 사용하려면 비용이 얼마나 드나요?, answer : AWS::*, Alexa::* 및 Custom::*과 같은 네임스페이스의 리소스 공급자에서 AWS CloudFormation을 사용하는 경우 추가 요금은 부과되지 않습니다. 이 경우 수동으로 생성했을 때와 같이 AWS CloudFormation을 사용해 생성한 AWS 리소스(예: Amazon EC2 인스턴스, Elastic Load Balancing 로드 밸런서 등)에 대해서만 요금을 지불하면 됩니다. 종량제로 요금이 청구되며 최소 요금 및 선수금은 없습니다.\n",
            "위에서 언급한 네임스페이스 외부의 AWS CloudFormation에서 리소스 공급자를 사용하는 경우 핸들러 작업당 요금이 부과됩니다. 핸들러 작업은 리소스에서 생성, 업데이트, 삭제, 읽기 또는 나열 작업을 말합니다. 자세한 내용은 요금 페이지를 참조하세요.\"\n",
            "실패한 스택 생성을 시도하는 도중 롤백된 리소스에 대한 비용을 지불해야 합니까?\n",
            "예. 템플릿 인스턴스화 도중 생성된 AWS 리소스에 대한 비용은 전체 스택이 성공적으로 생성되었는지 여부에 관계없이 부과됩니다.\n",
            "\"category : CloudFormation, question : 실패한 스택 생성을 시도하는 도중 롤백된 리소스에 대한 비용을 지불해야 합니까?, answer : 예. 템플릿 인스턴스화 도중 생성된 AWS 리소스에 대한 비용은 전체 스택이 성공적으로 생성되었는지 여부에 관계없이 부과됩니다.\"\n",
            "템플릿 또는 스택 수에 제한이 있나요?\n",
            "생성할 수 있는 AWS CloudFormation 스택의 최대 수에 대한 자세한 내용은 AWS CloudFormation 할당량에서 스택을 참조하세요. 여기에서 한도 증가 요청을 작성해 주세요. 업무일 기준 2일 이내에 답변해 드리겠습니다.\n",
            "\"category : CloudFormation, question : 템플릿 또는 스택 수에 제한이 있나요?, answer : 생성할 수 있는 AWS CloudFormation 스택의 최대 수에 대한 자세한 내용은 AWS CloudFormation 할당량에서 스택을 참조하세요. 여기에서 한도 증가 요청을 작성해 주세요. 업무일 기준 2일 이내에 답변해 드리겠습니다.\"\n",
            "설명 필드의 문자 수에 제한이 있나요?\n",
            "자세한 내용은 AWS CloudFormation 할당량에서 템플릿 설명과, AWS 설명서에서 파라미터, 리소스 및 출력을 참조하세요.\n",
            "\"category : CloudFormation, question : 설명 필드의 문자 수에 제한이 있나요?, answer : 자세한 내용은 AWS CloudFormation 할당량에서 템플릿 설명과, AWS 설명서에서 파라미터, 리소스 및 출력을 참조하세요.\"\n",
            "템플릿의 파라미터 또는 출력 수에 대한 제한이 있나요?\n",
            "템플릿에 지정할 수 있는 파라미터 및 출력 수에 대한 자세한 내용은 AWS CloudFormation 할당량에서 파라미터와 출력 섹션을 참조하세요.\n",
            "\"category : CloudFormation, question : 템플릿의 파라미터 또는 출력 수에 대한 제한이 있나요?, answer : 템플릿에 지정할 수 있는 파라미터 및 출력 수에 대한 자세한 내용은 AWS CloudFormation 할당량에서 파라미터와 출력 섹션을 참조하세요.\"\n",
            "스택에 생성할 수 있는 리소스의 수에 제한이 있나요?\n",
            "템플릿에 선언할 수 있는 리소스의 수에 대한 자세한 내용은 AWS CloudFormation 할당량에서 리소스를 참조하세요. 템플릿과 스택을 이보다 적게 생성하고 여러 스택에 걸쳐 애플리케이션을 모듈화하는 것이 모범 사례입니다. 이 경우 리소스 변경 사항에 대한 영향 범위가 최소화될 뿐 아니라, 소규모 리소스 그룹은 대규모 그룹보다 종속성이 덜 복잡하기 때문에 여러 리소스 종속성과 관련된 문제를 더 빨리 해결할 수 있게 됩니다.\n",
            "\"category : CloudFormation, question : 스택에 생성할 수 있는 리소스의 수에 제한이 있나요?, answer : 템플릿에 선언할 수 있는 리소스의 수에 대한 자세한 내용은 AWS CloudFormation 할당량에서 리소스를 참조하세요. 템플릿과 스택을 이보다 적게 생성하고 여러 스택에 걸쳐 애플리케이션을 모듈화하는 것이 모범 사례입니다. 이 경우 리소스 변경 사항에 대한 영향 범위가 최소화될 뿐 아니라, 소규모 리소스 그룹은 대규모 그룹보다 종속성이 덜 복잡하기 때문에 여러 리소스 종속성과 관련된 문제를 더 빨리 해결할 수 있게 됩니다.\"\n",
            "각 리전의 AWS CloudFormation 서비스 액세스 지점은 어디인가요?\n",
            "각 리전에 대한 엔드포인트는 기술 설명서의 AWS CloudFormation 엔드포인트에서 확인할 수 있습니다.\n",
            "\"category : CloudFormation, question : 각 리전의 AWS CloudFormation 서비스 액세스 지점은 어디인가요?, answer : 각 리전에 대한 엔드포인트는 기술 설명서의 AWS CloudFormation 엔드포인트에서 확인할 수 있습니다.\"\n",
            "현재 AWS CloudFormation을 사용할 수 있는 AWS 리전은 어디인가요?\n",
            "CloudFormation의 리전별 가용성에 대한 자세한 정보는 리전별 제품 및 서비스를 참조하세요.\n",
            "\"category : CloudFormation, question : 현재 AWS CloudFormation을 사용할 수 있는 AWS 리전은 어디인가요?, answer : CloudFormation의 리전별 가용성에 대한 자세한 정보는 리전별 제품 및 서비스를 참조하세요.\"\n",
            "========== CloudWatch  :  https://aws.amazon.com/ko/cloudwatch/faqs/ 사이트 크롤링 진행중 ==========\n",
            "147\n",
            "Amazon CloudWatch란 무엇인가요?\n",
            "Amazon CloudWatch는 클라우드 리소스와 AWS에서 실행되는 애플리케이션을 위한 AWS 모니터링 서비스입니다. Amazon CloudWatch를 사용하여 지표를 수집 및 추적하고 로그 파일을 수집 및 모니터링하고 경보를 설정할 수 있습니다. Amazon CloudWatch는 Amazon EC2 인스턴스, Amazon DynamoDB 테이블, Amazon RDS DB 인스턴스 같은 AWS 리소스뿐만 아니라 애플리케이션과 서비스에서 생성된 사용자 정의 지표 및 애플리케이션에서 생성되고 온프레미스, 하이브리드 또는 기타 클라우드에서 호스팅된 모든 로그 파일을 모니터링할 수 있습니다. Amazon CloudWatch를 사용하여 시스템 전반의 리소스 사용률, 애플리케이션 성능, 운영 상태를 파악할 수 있습니다. 이러한 통찰력을 사용하여 문제에 적절히 대응하고 애플리케이션 실행을 원활하게 유지할 수 있습니다.\n",
            "모니터링을 시작하려면, 내장된 AWS 모범 사례와 함께 자동 대시보드를 사용하여 지표 및 경보에 대한 계정 및 리소스 기반의 보기를 탐색하고, 쉽게 세부 정보를 드릴다운하여 성능 문제의 원인을 파악할 수 있습니다.\n",
            "\"category : CloudWatch, question : Amazon CloudWatch란 무엇인가요?, answer : Amazon CloudWatch는 클라우드 리소스와 AWS에서 실행되는 애플리케이션을 위한 AWS 모니터링 서비스입니다. Amazon CloudWatch를 사용하여 지표를 수집 및 추적하고 로그 파일을 수집 및 모니터링하고 경보를 설정할 수 있습니다. Amazon CloudWatch는 Amazon EC2 인스턴스, Amazon DynamoDB 테이블, Amazon RDS DB 인스턴스 같은 AWS 리소스뿐만 아니라 애플리케이션과 서비스에서 생성된 사용자 정의 지표 및 애플리케이션에서 생성되고 온프레미스, 하이브리드 또는 기타 클라우드에서 호스팅된 모든 로그 파일을 모니터링할 수 있습니다. Amazon CloudWatch를 사용하여 시스템 전반의 리소스 사용률, 애플리케이션 성능, 운영 상태를 파악할 수 있습니다. 이러한 통찰력을 사용하여 문제에 적절히 대응하고 애플리케이션 실행을 원활하게 유지할 수 있습니다.\n",
            "모니터링을 시작하려면, 내장된 AWS 모범 사례와 함께 자동 대시보드를 사용하여 지표 및 경보에 대한 계정 및 리소스 기반의 보기를 탐색하고, 쉽게 세부 정보를 드릴다운하여 성능 문제의 원인을 파악할 수 있습니다.\"\n",
            "CloudWatch에 어떻게 액세스할 수 있나요?\n",
            "Amazon CloudWatch에는 API, 명령줄 인터페이스, AWS SDK 및 AWS Management Console을 통해 액세스할 수 있습니다.\n",
            "\"category : CloudWatch, question : CloudWatch에 어떻게 액세스할 수 있나요?, answer : Amazon CloudWatch에는 API, 명령줄 인터페이스, AWS SDK 및 AWS Management Console을 통해 액세스할 수 있습니다.\"\n",
            "Amazon CloudWatch가 지원하는 운영 체제는 무엇인가요?\n",
            "Amazon CloudWatch는 모든 Amazon EC2 인스턴스의 메트릭을 수신하고 제공하며 현재 Amazon EC2 서비스에서 지원하는 운영 체제를 사용해야 합니다.\n",
            "\"category : CloudWatch, question : Amazon CloudWatch가 지원하는 운영 체제는 무엇인가요?, answer : Amazon CloudWatch는 모든 Amazon EC2 인스턴스의 메트릭을 수신하고 제공하며 현재 Amazon EC2 서비스에서 지원하는 운영 체제를 사용해야 합니다.\"\n",
            "CloudWatch에 구현할 수 있는 액세스 관리 정책은 무엇인가요?\n",
            "Amazon CloudWatch는 AWS 계정을 가진 사용자가 수행할 수 있는 CloudWatch 작업을 지정할 수 있도록 AWS Identity and Access Management(IAM)와 통합되어 있습니다. 예를 들어 조직의 특정 사용자에게만 GetMetricStatistics를 사용할 수 있는 권한을 부여하는 IAM 정책을 만들 수 있습니다. 그런 다음 이 작업을 사용하여 클라우드 리소스에 대한 데이터를 검색할 수 있습니다.\n",
            "특정 리소스의 CloudWatch 데이터에 대한 액세스를 제어할 때는 IAM을 사용할 수 없습니다. 예를 들어, 특정 인스턴스 세트 또는 특정 LoadBalancer에 대한 CloudWatch 데이터에 대해서만 사용자 액세스 권한을 부여할 수는 없습니다. IAM을 통해 부여된 권한은 CloudWatch에서 사용하는 클라우드 리소스 전체에 적용됩니다. 또한, IAM 역할은 Amazon CloudWatch 명령줄 도구와 함께 사용할 수 없습니다.\n",
            "\"category : CloudWatch, question : CloudWatch에 구현할 수 있는 액세스 관리 정책은 무엇인가요?, answer : Amazon CloudWatch는 AWS 계정을 가진 사용자가 수행할 수 있는 CloudWatch 작업을 지정할 수 있도록 AWS Identity and Access Management(IAM)와 통합되어 있습니다. 예를 들어 조직의 특정 사용자에게만 GetMetricStatistics를 사용할 수 있는 권한을 부여하는 IAM 정책을 만들 수 있습니다. 그런 다음 이 작업을 사용하여 클라우드 리소스에 대한 데이터를 검색할 수 있습니다.\n",
            "특정 리소스의 CloudWatch 데이터에 대한 액세스를 제어할 때는 IAM을 사용할 수 없습니다. 예를 들어, 특정 인스턴스 세트 또는 특정 LoadBalancer에 대한 CloudWatch 데이터에 대해서만 사용자 액세스 권한을 부여할 수는 없습니다. IAM을 통해 부여된 권한은 CloudWatch에서 사용하는 클라우드 리소스 전체에 적용됩니다. 또한, IAM 역할은 Amazon CloudWatch 명령줄 도구와 함께 사용할 수 없습니다.\"\n",
            "Amazon CloudWatch Logs란 무엇입니까?\n",
            "Amazon CloudWatch 로그를 사용하면 기존 시스템, 애플리케이션 및 사용자 정의 로그 파일을 이용하여 시스템 및 애플리케이션을 모니터링하고 문제를 해결할 수 있습니다.\n",
            "CloudWatch 로그를 통해 특정 구문, 값 또는 패턴에 대한 로그를 거의 실시간으로 모니터링할 수 있습니다. 예를 들어 시스템 로그에 발생하는 오류 수에 대한 경보를 설정하거나 애플리케이션 로그에 기록된 웹 요청 지연 시간에 대한 그래프를 볼 수 있습니다. 그런 다음 원본 로그 데이터를 확인하여 문제의 원인을 파악할 수 있습니다. 로그 데이터는 안정성이 높고 저렴한 스토리지에 무기한으로 저장 및 액세스할 수 있으므로 하드 드라이브의 용량을 걱정하지 않아도 됩니다.\n",
            "\"category : CloudWatch, question : Amazon CloudWatch Logs란 무엇입니까?, answer : Amazon CloudWatch 로그를 사용하면 기존 시스템, 애플리케이션 및 사용자 정의 로그 파일을 이용하여 시스템 및 애플리케이션을 모니터링하고 문제를 해결할 수 있습니다.\n",
            "CloudWatch 로그를 통해 특정 구문, 값 또는 패턴에 대한 로그를 거의 실시간으로 모니터링할 수 있습니다. 예를 들어 시스템 로그에 발생하는 오류 수에 대한 경보를 설정하거나 애플리케이션 로그에 기록된 웹 요청 지연 시간에 대한 그래프를 볼 수 있습니다. 그런 다음 원본 로그 데이터를 확인하여 문제의 원인을 파악할 수 있습니다. 로그 데이터는 안정성이 높고 저렴한 스토리지에 무기한으로 저장 및 액세스할 수 있으므로 하드 드라이브의 용량을 걱정하지 않아도 됩니다.\"\n",
            "CloudWatch Logs로 어떤 작업을 수행할 수 있나요?\n",
            "CloudWatch 로그를 사용하면 로그를 모니터링하고 저장할 수 있어 시스템 및 애플리케이션을 이해하고 운영하는 데 도움이 됩니다. CloudWatch Logs는 다양한 방법으로 사용할 수 있습니다.\n",
            "실시간 애플리케이션 및 시스템 모니터링: CloudWatch Logs를 사용하면 로그 데이터를 이용해 애플리케이션과 시스템을 모니터링할 수 있습니다. 예를 들어 CloudWatch Logs에서는 애플리케이션 로그에서 발생하는 오류의 수를 추적하고 오류 비율이 지정한 임계값을 초과할 때마다 알림을 전송할 수 있습니다. CloudWatch Logs는 모니터링하는 데 로그 데이터를 사용하므로 코드를 변경할 필요가 없습니다.\n",
            "로그 장기 보존: CloudWatch Logs를 사용하면 하드 드라이브 용량을 걱정할 필요 없이 내구성이 우수하고 비용 효율적인 스토리지에 무기한으로 로그 데이터를 저장할 수 있습니다. CloudWatch Logs 에이전트를 사용하면 순환 로그 파일과 비순환 로그 파일을 모두 호스트에서 로그 서비스로 쉽고 빠르게 이동할 수 있습니다. 그런 다음 필요한 경우 원시 로그 이벤트 데이터에 액세스할 수 있습니다.\n",
            "\"category : CloudWatch, question : CloudWatch Logs로 어떤 작업을 수행할 수 있나요?, answer : CloudWatch 로그를 사용하면 로그를 모니터링하고 저장할 수 있어 시스템 및 애플리케이션을 이해하고 운영하는 데 도움이 됩니다. CloudWatch Logs는 다양한 방법으로 사용할 수 있습니다.\n",
            "실시간 애플리케이션 및 시스템 모니터링: CloudWatch Logs를 사용하면 로그 데이터를 이용해 애플리케이션과 시스템을 모니터링할 수 있습니다. 예를 들어 CloudWatch Logs에서는 애플리케이션 로그에서 발생하는 오류의 수를 추적하고 오류 비율이 지정한 임계값을 초과할 때마다 알림을 전송할 수 있습니다. CloudWatch Logs는 모니터링하는 데 로그 데이터를 사용하므로 코드를 변경할 필요가 없습니다.\n",
            "로그 장기 보존: CloudWatch Logs를 사용하면 하드 드라이브 용량을 걱정할 필요 없이 내구성이 우수하고 비용 효율적인 스토리지에 무기한으로 로그 데이터를 저장할 수 있습니다. CloudWatch Logs 에이전트를 사용하면 순환 로그 파일과 비순환 로그 파일을 모두 호스트에서 로그 서비스로 쉽고 빠르게 이동할 수 있습니다. 그런 다음 필요한 경우 원시 로그 이벤트 데이터에 액세스할 수 있습니다.\"\n",
            "CloudWatch Logs 에이전트는 어떤 플랫폼을 지원하나요?\n",
            "CloudWatch 로그 에이전트는 Amazon Linux, Ubuntu, CentOS, Red Hat Enterprise Linux 및 Windows를 지원합니다. 이 에이전트는 호스트의 개별 로그 파일을 모니터링하는 기능을 지원합니다.\n",
            "\"category : CloudWatch, question : CloudWatch Logs 에이전트는 어떤 플랫폼을 지원하나요?, answer : CloudWatch 로그 에이전트는 Amazon Linux, Ubuntu, CentOS, Red Hat Enterprise Linux 및 Windows를 지원합니다. 이 에이전트는 호스트의 개별 로그 파일을 모니터링하는 기능을 지원합니다.\"\n",
            "CloudWatch Logs 에이전트는 IAM 역할을 지원하나요?\n",
            "예. CloudWatch 로그 에이전트는 Identity and Access Management(IAM)와 통합되어 있으며 액세스 키와 IAM 역할을 모두 지원합니다.\n",
            "\"category : CloudWatch, question : CloudWatch Logs 에이전트는 IAM 역할을 지원하나요?, answer : 예. CloudWatch 로그 에이전트는 Identity and Access Management(IAM)와 통합되어 있으며 액세스 키와 IAM 역할을 모두 지원합니다.\"\n",
            "Amazon CloudWatch Logs 인사이트란 무엇인가요?\n",
            "Amazon CloudWatch Logs Insights는 CloudWatch Logs를 위한 통합된 인터랙티브 로그 분석 기능으로, 사용량에 따라 비용을 지불하는 서비스입니다. 이를 통해 개발자, 운영자, 시스템 엔지니어는 로그를 검색하고 시각화하여 애플리케이션을 이해하고 개선하며 디버그할 수 있습니다. Logs Insights는 CloudWatch에 완벽하게 통합되어 로그를 관리하고 탐색하고 분석하도록 지원합니다. 또한 Logs와 함께 CloudWatch Metrics, Alarms 및 대시보드를 활용하여 애플리케이션에 대한 완벽한 시각적 운영 정보를 확인할 수 있습니다. 그러면 애플리케이션을 이해하고 개선하고 문제를 신속하게 찾아 해결할 수 있으므로 지속적으로 빠르게 혁신해나갈 수 있습니다. 집계, 필터 및 정규 표현식으로 쿼리를 작성하여 로그로부터 조치 가능한 통찰을 도출할 수 있습니다. 또한, 시계열 데이터를 시각화하고, 개별 로그 이벤트로 드릴 다운하고, 쿼리 결과를 CloudWatch 대시보드로 내보낼 수 있습니다.\n",
            "\"category : CloudWatch, question : Amazon CloudWatch Logs 인사이트란 무엇인가요?, answer : Amazon CloudWatch Logs Insights는 CloudWatch Logs를 위한 통합된 인터랙티브 로그 분석 기능으로, 사용량에 따라 비용을 지불하는 서비스입니다. 이를 통해 개발자, 운영자, 시스템 엔지니어는 로그를 검색하고 시각화하여 애플리케이션을 이해하고 개선하며 디버그할 수 있습니다. Logs Insights는 CloudWatch에 완벽하게 통합되어 로그를 관리하고 탐색하고 분석하도록 지원합니다. 또한 Logs와 함께 CloudWatch Metrics, Alarms 및 대시보드를 활용하여 애플리케이션에 대한 완벽한 시각적 운영 정보를 확인할 수 있습니다. 그러면 애플리케이션을 이해하고 개선하고 문제를 신속하게 찾아 해결할 수 있으므로 지속적으로 빠르게 혁신해나갈 수 있습니다. 집계, 필터 및 정규 표현식으로 쿼리를 작성하여 로그로부터 조치 가능한 통찰을 도출할 수 있습니다. 또한, 시계열 데이터를 시각화하고, 개별 로그 이벤트로 드릴 다운하고, 쿼리 결과를 CloudWatch 대시보드로 내보낼 수 있습니다.\"\n",
            "CloudWatch Logs Insights를 시작하려면 어떻게 해야 하나요?\n",
            "Logs Insights 사용을 바로 시작하여 CloudWatch Logs로 전송되는 모든 로그에 대한 쿼리를 실행할 수 있습니다. 설정하거나 인프라를 관리할 필요가 없습니다. AWS Management Console에서 Logs Insights에 액세스하거나 AWS SDK를 사용하여 애플리케이션을 통해 프로그래밍 방식으로 액세스할 수 있습니다.\n",
            "\"category : CloudWatch, question : CloudWatch Logs Insights를 시작하려면 어떻게 해야 하나요?, answer : Logs Insights 사용을 바로 시작하여 CloudWatch Logs로 전송되는 모든 로그에 대한 쿼리를 실행할 수 있습니다. 설정하거나 인프라를 관리할 필요가 없습니다. AWS Management Console에서 Logs Insights에 액세스하거나 AWS SDK를 사용하여 애플리케이션을 통해 프로그래밍 방식으로 액세스할 수 있습니다.\"\n",
            "Amazon CloudWatch 이상 탐지란 무엇인가요?\n",
            "Amazon CloudWatch 이상 탐지는 기계 학습 알고리즘을 적용하여 시스템 및 애플리케이션의 단일 시계열을 지속적으로 분석하고, 정상적인 기준을 파악하고, 최소한의 사용자 개입으로 이상 항목을 드러냅니다. 이 기능을 사용하면 하루 중 시간, 요일 기반의 계절성 또는 변화하는 추세와 같은 자연적인 지표 패턴을 기반으로 임계값을 자동 조정하는 경보를 생성할 수 있습니다. 또한 대시보드에서 이상 탐지 반대를 사용하여 지표를 시각화함으로써, 예기치 않은 지표 변화를 모니터링, 격리 및 문제 해결할 수 있습니다.\n",
            "\"category : CloudWatch, question : Amazon CloudWatch 이상 탐지란 무엇인가요?, answer : Amazon CloudWatch 이상 탐지는 기계 학습 알고리즘을 적용하여 시스템 및 애플리케이션의 단일 시계열을 지속적으로 분석하고, 정상적인 기준을 파악하고, 최소한의 사용자 개입으로 이상 항목을 드러냅니다. 이 기능을 사용하면 하루 중 시간, 요일 기반의 계절성 또는 변화하는 추세와 같은 자연적인 지표 패턴을 기반으로 임계값을 자동 조정하는 경보를 생성할 수 있습니다. 또한 대시보드에서 이상 탐지 반대를 사용하여 지표를 시각화함으로써, 예기치 않은 지표 변화를 모니터링, 격리 및 문제 해결할 수 있습니다.\"\n",
            "Amazon CloudWatch 이상 탐지를 시작하려면 어떻게 해야 하나요?\n",
            "이상 탐지를 시작하는 방법은 쉽습니다. CloudWatch 콘솔의 탐색 창에서 경보로 이동하여 경보를 생성하거나 지표에서 시작하여 지표의 예상 값을 구간으로 그래프에 오버레이할 수 있습니다. AWS CLI, AWS SDK 또는 AWS CloudFormation 템플릿을 사용하여 이상 탐지를 활성화할 수도 있습니다. 자세히 알아보려면 CloudWatch 이상 탐지 설명서 및 요금 페이지를 참조하세요.\n",
            "\"category : CloudWatch, question : Amazon CloudWatch 이상 탐지를 시작하려면 어떻게 해야 하나요?, answer : 이상 탐지를 시작하는 방법은 쉽습니다. CloudWatch 콘솔의 탐색 창에서 경보로 이동하여 경보를 생성하거나 지표에서 시작하여 지표의 예상 값을 구간으로 그래프에 오버레이할 수 있습니다. AWS CLI, AWS SDK 또는 AWS CloudFormation 템플릿을 사용하여 이상 탐지를 활성화할 수도 있습니다. 자세히 알아보려면 CloudWatch 이상 탐지 설명서 및 요금 페이지를 참조하세요.\"\n",
            "Amazon CloudWatch Contributor Insights란 무엇인가요?\n",
            "시계열 데이터를 분석하여 시스템 성능에 가장 큰 영향을 미치는 요소를 보여주는 Contributor Insights가 Amazon CloudWatch에 포함되어 있습니다. Contributor Insights는 설정하고 나면 추가적인 사용자 개입 없이 연속으로 실행됩니다. 이렇게 하면 운영 이벤트 발생 시, 개발자와 운영자가 신속하게 문제를 분리하여 진단하고 복구하는 데 도움이 됩니다.\n",
            "\"category : CloudWatch, question : Amazon CloudWatch Contributor Insights란 무엇인가요?, answer : 시계열 데이터를 분석하여 시스템 성능에 가장 큰 영향을 미치는 요소를 보여주는 Contributor Insights가 Amazon CloudWatch에 포함되어 있습니다. Contributor Insights는 설정하고 나면 추가적인 사용자 개입 없이 연속으로 실행됩니다. 이렇게 하면 운영 이벤트 발생 시, 개발자와 운영자가 신속하게 문제를 분리하여 진단하고 복구하는 데 도움이 됩니다.\"\n",
            "CloudWatch Contributor Insights를 시작하려면 어떻게 해야 하나요?\n",
            "CloudWatch 콘솔의 탐색 창에서 Contributor Insights로 이동하여 Contributor Insights 규칙을 생성하면 됩니다. AWS CLI, AWS SDK 또는 AWS CloudFormation 템플릿을 사용하여 Contributor Insights를 활성화할 수도 있습니다. Contributor Insights는 모든 상업적인 AWS 리전에 제공됩니다. 자세한 정보는 CloudWatch Contributor Insights 설명서를 참조하십시오.\n",
            "\"category : CloudWatch, question : CloudWatch Contributor Insights를 시작하려면 어떻게 해야 하나요?, answer : CloudWatch 콘솔의 탐색 창에서 Contributor Insights로 이동하여 Contributor Insights 규칙을 생성하면 됩니다. AWS CLI, AWS SDK 또는 AWS CloudFormation 템플릿을 사용하여 Contributor Insights를 활성화할 수도 있습니다. Contributor Insights는 모든 상업적인 AWS 리전에 제공됩니다. 자세한 정보는 CloudWatch Contributor Insights 설명서를 참조하십시오.\"\n",
            "Amazon CloudWatch ServiceLens란 무엇인가요?\n",
            "Amazon CloudWatch ServiceLens는 애플리케이션의 상태, 성능 및 가용성을 한 곳에서 시각화하고 분석할 수 있는 기능입니다. CloudWatch ServiceLens는 CloudWatch 지표 및 로그는 물론이고 AWS X-Ray의 트레이스와도 결합되어 포괄적인 애플리케이션 및 애플리케이션 종속성 정보를 제공합니다. 이를 통해 성능 병목 현상을 신속하게 포착하고, 애플리케이션 문제의 근본 원인을 확인하고, 영향을 받는 사용자를 파악할 수 있습니다. CloudWatch ServiceLens를 사용하면 인프라 모니터링(지표 및 로그를 사용하여 애플리케이션을 지원하는 리소스 파악)과 트랜잭션 모니터링(트레이스를 사용하여 리소스 간 종속성 파악), 최종 사용자 모니터링(카나리아를 사용하여 엔드포인트를 모니터링하고 최종 사용자 환경이 저하되었을 때 알림)이라는 세 가지 주요 영역에서 애플리케이션을 심층 분석할 수 있습니다.\n",
            "\"category : CloudWatch, question : Amazon CloudWatch ServiceLens란 무엇인가요?, answer : Amazon CloudWatch ServiceLens는 애플리케이션의 상태, 성능 및 가용성을 한 곳에서 시각화하고 분석할 수 있는 기능입니다. CloudWatch ServiceLens는 CloudWatch 지표 및 로그는 물론이고 AWS X-Ray의 트레이스와도 결합되어 포괄적인 애플리케이션 및 애플리케이션 종속성 정보를 제공합니다. 이를 통해 성능 병목 현상을 신속하게 포착하고, 애플리케이션 문제의 근본 원인을 확인하고, 영향을 받는 사용자를 파악할 수 있습니다. CloudWatch ServiceLens를 사용하면 인프라 모니터링(지표 및 로그를 사용하여 애플리케이션을 지원하는 리소스 파악)과 트랜잭션 모니터링(트레이스를 사용하여 리소스 간 종속성 파악), 최종 사용자 모니터링(카나리아를 사용하여 엔드포인트를 모니터링하고 최종 사용자 환경이 저하되었을 때 알림)이라는 세 가지 주요 영역에서 애플리케이션을 심층 분석할 수 있습니다.\"\n",
            "CloudWatch ServiceLens를 시작하려면 어떻게 해야 하나요?\n",
            "이미 AWS X-Ray를 사용 중이라면 기본적으로 CloudWatch 콘솔에서 CloudWatch ServiceLens에 액세스할 수 있습니다. 아직 AWS X-Ray를 사용하지 않고 있다면 X-Ray SDK를 사용하여 애플리케이션에 AWS X-Ray를 활성화한 후 시작할 수 있습니다. Amazon CloudWatch ServiceLens는 AWS-X-Ray를 사용할 수 있는 모든 퍼블릭 AWS 리전에서 사용 가능합니다. 자세한 내용은 Amazon CloudWatch ServiceLens 설명서를 참조하십시오.\n",
            "\"category : CloudWatch, question : CloudWatch ServiceLens를 시작하려면 어떻게 해야 하나요?, answer : 이미 AWS X-Ray를 사용 중이라면 기본적으로 CloudWatch 콘솔에서 CloudWatch ServiceLens에 액세스할 수 있습니다. 아직 AWS X-Ray를 사용하지 않고 있다면 X-Ray SDK를 사용하여 애플리케이션에 AWS X-Ray를 활성화한 후 시작할 수 있습니다. Amazon CloudWatch ServiceLens는 AWS-X-Ray를 사용할 수 있는 모든 퍼블릭 AWS 리전에서 사용 가능합니다. 자세한 내용은 Amazon CloudWatch ServiceLens 설명서를 참조하십시오.\"\n",
            "Amazon CloudWatch Synthetics란 무엇인가요?\n",
            "Amazon CloudWatch Synthetics를 사용하면 애플리케이션 엔드포인트를 보다 쉽게 모니터링할 수 있습니다. 쉬지 않고 1분마다 엔드포인트에서 검사를 실행해 애플리케이션 엔드포인트가 예상과 다른 동작을 하는 즉시 알림을 보냅니다. 이 테스트는 맞춤 설정을 통해 가용성, 지연, 트랜잭션, 손상되거나 연결이 끊어진 링크, 단계별 작업 완료 상태, 페이지 부하 오류, UI 자신의 부하 지연, 복잡한 마법사 플로 또는 애플리케이션의 체크아웃 플로를 검사하도록 설정할 수 있습니다. 또한, CloudWatch Synthetics를 사용하여 문제가 있는 애플리케이션 엔드포인트를 분리하고 기본 인프라 문제로 다시 매핑해 문제 해결에 걸리는 시간을 단축할 수 있습니다.\n",
            "\"category : CloudWatch, question : Amazon CloudWatch Synthetics란 무엇인가요?, answer : Amazon CloudWatch Synthetics를 사용하면 애플리케이션 엔드포인트를 보다 쉽게 모니터링할 수 있습니다. 쉬지 않고 1분마다 엔드포인트에서 검사를 실행해 애플리케이션 엔드포인트가 예상과 다른 동작을 하는 즉시 알림을 보냅니다. 이 테스트는 맞춤 설정을 통해 가용성, 지연, 트랜잭션, 손상되거나 연결이 끊어진 링크, 단계별 작업 완료 상태, 페이지 부하 오류, UI 자신의 부하 지연, 복잡한 마법사 플로 또는 애플리케이션의 체크아웃 플로를 검사하도록 설정할 수 있습니다. 또한, CloudWatch Synthetics를 사용하여 문제가 있는 애플리케이션 엔드포인트를 분리하고 기본 인프라 문제로 다시 매핑해 문제 해결에 걸리는 시간을 단축할 수 있습니다.\"\n",
            "CloudWatch Synthetics를 시작하려면 어떻게 해야 하나요?\n",
            "CloudWatch Synthetics는 쉽게 시작할 수 있습니다. 첫 번째 통과 Canary를 몇 분 안에 작성할 수 있습니다. 자세한 알아보려면 Amazon CloudWatch Synthetics에 대한 설명서를 참조하세요.\n",
            "\"category : CloudWatch, question : CloudWatch Synthetics를 시작하려면 어떻게 해야 하나요?, answer : CloudWatch Synthetics는 쉽게 시작할 수 있습니다. 첫 번째 통과 Canary를 몇 분 안에 작성할 수 있습니다. 자세한 알아보려면 Amazon CloudWatch Synthetics에 대한 설명서를 참조하세요.\"\n",
            "Amazon CloudWatch의 사용 요금은 얼마인가요?\n",
            "최신 정보는 요금 페이지를 참조하세요.\n",
            "\"category : CloudWatch, question : Amazon CloudWatch의 사용 요금은 얼마인가요?, answer : 최신 정보는 요금 페이지를 참조하세요.\"\n",
            "모니터링하는 Amazon EC2 인스턴스의 유형에 따라 Amazon CloudWatch 모니터링 비용이 다른가요?\n",
            "모든 Amazon EC2 인스턴스 유형은 별도의 비용 없이 키 상태 및 성능 지표를 CloudWatch에 자동으로 전송합니다. EC2 세부 모니터링을 활성화한 경우, 해당 인스턴스에 대해 CloudWatch로 전송된 지표의 수를 기준으로 사용자 지정 지표에 대한 요금이 청구됩니다. 인스턴스에 대해 전송된 지표의 수는 인스턴스 유형에 따라 다릅니다. 자세한 내용은 인스턴스에 대해 사용 가능한 CloudWatch 지표를 참조하세요.\n",
            "\"category : CloudWatch, question : 모니터링하는 Amazon EC2 인스턴스의 유형에 따라 Amazon CloudWatch 모니터링 비용이 다른가요?, answer : 모든 Amazon EC2 인스턴스 유형은 별도의 비용 없이 키 상태 및 성능 지표를 CloudWatch에 자동으로 전송합니다. EC2 세부 모니터링을 활성화한 경우, 해당 인스턴스에 대해 CloudWatch로 전송된 지표의 수를 기준으로 사용자 지정 지표에 대한 요금이 청구됩니다. 인스턴스에 대해 전송된 지표의 수는 인스턴스 유형에 따라 다릅니다. 자세한 내용은 인스턴스에 대해 사용 가능한 CloudWatch 지표를 참조하세요.\"\n",
            "요금에 세금이 포함되어 있습니까?\n",
            "달리 명시하지 않는 한 가격에는 VAT 및 해당 판매세를 포함한 관련 조세 공과가 포함되지 않습니다. 자세히 알아보십시오.\n",
            "\"category : CloudWatch, question : 요금에 세금이 포함되어 있습니까?, answer : 달리 명시하지 않는 한 가격에는 VAT 및 해당 판매세를 포함한 관련 조세 공과가 포함되지 않습니다. 자세히 알아보십시오.\"\n",
            "2017년 7월과 그 이전 달의 CloudWatch용 AWS 월별 청구서가 다르게 표시되는 이유는 무엇인가요?\n",
            "2017년 7월 이전에는 AWS 청구서와 비용 및 사용 보고서에서 CloudWatch 요금이 2개의 서로 다른 섹션으로 나뉘어 표시되었습니다. 기록상의 이유로 CloudWatch 경보, CloudWatch 지표 및 CloudWatch API 사용 요금은 청구서의 'Elastic Compute Cloud'(EC2) 세부 정보 섹션 아래에 제시되어 있으며 CloudWatch Logs 및 CloudWatch 대시보드 요금은 'CloudWatch' 세부 정보 섹션에 표시되었습니다. 월별 AWS CloudWatch 사용 및 결제를 통합하고 간소화하기 위해 CloudWatch 지표, 경보 및 API 사용 요금을 청구서의 'EC2' 섹션에서 'CloudWatch' 섹션으로 옮겨 모든 CloudWatch 모니터링 요금을 'CloudWatch' 섹션에서 볼 수 있도록 했습니다. 총 AWS 결제 금액에는 변동이 없습니다. 이제 청구서와 비용 및 사용 보고서에서는 CloudWatch 요금이 단일 섹션에 표시됩니다.\n",
            "또한, CloudWatch에는 총 예상 요금을 보거나 서비스별 예상 요금을 볼 수 있는 'Estimated Charges'라는 결제 지표가 있습니다. '총 예상 요금' 지표에는 변경 사항이 없습니다. 하지만 서비스별로 나뉜 'EstimatedCharges' 지표는 'AmazonEC2'와 같은 차원 서비스 이름 및 'AmazonCloudWatch'와 같은 차원 서비스 이름으로 변경됩니다. 결제 통합으로 인해 EC2에서 사용 및 결제 요금이 CloudWatch로 이동하면서 AmazonEC2 결제 지표가 감소하고 AmazonCloudWatch 결제 지표는 증가하는 것을 볼 수도 있습니다.\n",
            "\"category : CloudWatch, question : 2017년 7월과 그 이전 달의 CloudWatch용 AWS 월별 청구서가 다르게 표시되는 이유는 무엇인가요?, answer : 2017년 7월 이전에는 AWS 청구서와 비용 및 사용 보고서에서 CloudWatch 요금이 2개의 서로 다른 섹션으로 나뉘어 표시되었습니다. 기록상의 이유로 CloudWatch 경보, CloudWatch 지표 및 CloudWatch API 사용 요금은 청구서의 'Elastic Compute Cloud'(EC2) 세부 정보 섹션 아래에 제시되어 있으며 CloudWatch Logs 및 CloudWatch 대시보드 요금은 'CloudWatch' 세부 정보 섹션에 표시되었습니다. 월별 AWS CloudWatch 사용 및 결제를 통합하고 간소화하기 위해 CloudWatch 지표, 경보 및 API 사용 요금을 청구서의 'EC2' 섹션에서 'CloudWatch' 섹션으로 옮겨 모든 CloudWatch 모니터링 요금을 'CloudWatch' 섹션에서 볼 수 있도록 했습니다. 총 AWS 결제 금액에는 변동이 없습니다. 이제 청구서와 비용 및 사용 보고서에서는 CloudWatch 요금이 단일 섹션에 표시됩니다.\n",
            "또한, CloudWatch에는 총 예상 요금을 보거나 서비스별 예상 요금을 볼 수 있는 'Estimated Charges'라는 결제 지표가 있습니다. '총 예상 요금' 지표에는 변경 사항이 없습니다. 하지만 서비스별로 나뉜 'EstimatedCharges' 지표는 'AmazonEC2'와 같은 차원 서비스 이름 및 'AmazonCloudWatch'와 같은 차원 서비스 이름으로 변경됩니다. 결제 통합으로 인해 EC2에서 사용 및 결제 요금이 CloudWatch로 이동하면서 AmazonEC2 결제 지표가 감소하고 AmazonCloudWatch 결제 지표는 증가하는 것을 볼 수도 있습니다.\"\n",
            "CloudWatch Logs Insights 요금은 어떻게 부과되나요?\n",
            "Logs Insights는 쿼리당 요금이 부과되며 쿼리로 스캔한 수집된 로그 데이터의 양을 기준으로 요금이 청구됩니다. 요금에 대한 자세한 내용은 CloudWatch 요금을 참조하세요.\n",
            "\"category : CloudWatch, question : CloudWatch Logs Insights 요금은 어떻게 부과되나요?, answer : Logs Insights는 쿼리당 요금이 부과되며 쿼리로 스캔한 수집된 로그 데이터의 양을 기준으로 요금이 청구됩니다. 요금에 대한 자세한 내용은 CloudWatch 요금을 참조하세요.\"\n",
            "CloudWatch Logs Insights에서 취소된 쿼리에 대해 요금을 부과하나요?\n",
            "예. 쿼리를 수동으로 취소하면 이 쿼리를 취소한 시점까지 스캔한 수집된 로그 데이터의 양에 대한 요금이 부과됩니다.\n",
            "\"category : CloudWatch, question : CloudWatch Logs Insights에서 취소된 쿼리에 대해 요금을 부과하나요?, answer : 예. 쿼리를 수동으로 취소하면 이 쿼리를 취소한 시점까지 스캔한 수집된 로그 데이터의 양에 대한 요금이 부과됩니다.\"\n",
            "CloudWatch Logs Insights에서 실패한 쿼리에 대해 요금을 부과하나요?\n",
            "아니요. 실패한 쿼리에 대해서는 요금이 청구되지 않습니다.\n",
            "\"category : CloudWatch, question : CloudWatch Logs Insights에서 실패한 쿼리에 대해 요금을 부과하나요?, answer : 아니요. 실패한 쿼리에 대해서는 요금이 청구되지 않습니다.\"\n",
            "CloudWatch의 크로스 계정 관찰성이란 무엇인가요?\n",
            "CloudWatch의 크로스 계정 관측성을 사용하면 단일 리전 내 여러 계정에 걸쳐 있는 애플리케이션을 모니터링하고 문제를 해결할 수 있습니다. 크로스 계정 관측성을 사용하면 계정 경계에 대해 걱정할 필요 없이 지표, 로그 및 트레이스를 원활하게 검색하고 시각화하고 분석할 수 있습니다. 애플리케이션의 집계된 크로스 계정 보기에서 오류가 있는 리소스를 시각적으로 식별하고 관련된 트레이스, 지표 및 로그를 확인하여 문제의 근본 원인을 찾을 수 있습니다. 크로스 계정 모니터링은 원활한 크로스 계정 데이터 액세스 및 탐색을 지원하므로 문제 해결에 필요한 수동 작업과 해결 시간을 줄이는 데 도움이 됩니다. 크로스 계정 관측성은 CloudWatch의 통합 관측성 기능과 함께 추가로 제공되는 기능입니다.\n",
            "\"category : CloudWatch, question : CloudWatch의 크로스 계정 관찰성이란 무엇인가요?, answer : CloudWatch의 크로스 계정 관측성을 사용하면 단일 리전 내 여러 계정에 걸쳐 있는 애플리케이션을 모니터링하고 문제를 해결할 수 있습니다. 크로스 계정 관측성을 사용하면 계정 경계에 대해 걱정할 필요 없이 지표, 로그 및 트레이스를 원활하게 검색하고 시각화하고 분석할 수 있습니다. 애플리케이션의 집계된 크로스 계정 보기에서 오류가 있는 리소스를 시각적으로 식별하고 관련된 트레이스, 지표 및 로그를 확인하여 문제의 근본 원인을 찾을 수 있습니다. 크로스 계정 모니터링은 원활한 크로스 계정 데이터 액세스 및 탐색을 지원하므로 문제 해결에 필요한 수동 작업과 해결 시간을 줄이는 데 도움이 됩니다. 크로스 계정 관측성은 CloudWatch의 통합 관측성 기능과 함께 추가로 제공되는 기능입니다.\"\n",
            "크로스 계정 관찰성을 시작하려면 어떻게 해야 하나요?\n",
            "크로스 계정 관측성은 2가지 새로운 계정 개념을 도입합니다. ‘모니터링 계정’은 중앙의 AWS 계정으로, 다른 계정에서 생성된 관측성 데이터를 보고 이 데이터와 상호 작용할 수 있습니다. ‘소스 계정’은 계정에 있는 리소스에 대한 관측성 데이터를 생성하는 개별 AWS 계정입니다. 모니터링 계정과 소스 계정을 식별한 후에는 모니터링 계정과 공유할 텔레메트리 데이터를 선택하여 크로스 계정 모니터링 구성을 완료합니다. 몇 분 안에 중앙 모니터링 계정을 손쉽게 설정하고 이 계정으로 다수의 관련 계정 또는 전체 AWS 조직에 배포된 애플리케이션의 상태 및 성능을 완벽하게 파악할 수 있습니다. CloudWatch의 크로스 계정 관측성을 사용하면 서비스 가용성에 영향을 미칠 수 있는 크로스 애플리케이션 종속성을 한 눈에 보고 문제를 사전에 포착하며 문제 해결에 들어가는 평균 해결 시간을 줄일 수 있습니다.\n",
            "\"category : CloudWatch, question : 크로스 계정 관찰성을 시작하려면 어떻게 해야 하나요?, answer : 크로스 계정 관측성은 2가지 새로운 계정 개념을 도입합니다. ‘모니터링 계정’은 중앙의 AWS 계정으로, 다른 계정에서 생성된 관측성 데이터를 보고 이 데이터와 상호 작용할 수 있습니다. ‘소스 계정’은 계정에 있는 리소스에 대한 관측성 데이터를 생성하는 개별 AWS 계정입니다. 모니터링 계정과 소스 계정을 식별한 후에는 모니터링 계정과 공유할 텔레메트리 데이터를 선택하여 크로스 계정 모니터링 구성을 완료합니다. 몇 분 안에 중앙 모니터링 계정을 손쉽게 설정하고 이 계정으로 다수의 관련 계정 또는 전체 AWS 조직에 배포된 애플리케이션의 상태 및 성능을 완벽하게 파악할 수 있습니다. CloudWatch의 크로스 계정 관측성을 사용하면 서비스 가용성에 영향을 미칠 수 있는 크로스 애플리케이션 종속성을 한 눈에 보고 문제를 사전에 포착하며 문제 해결에 들어가는 평균 해결 시간을 줄일 수 있습니다.\"\n",
            "여러 AWS 계정에 걸쳐 사용할 수 있는 CloudWatch 모니터링 기능에는 어떤 것이 있나요?\n",
            "크로스 계정 관측성을 사용하면 중앙 보기에서 여러 계정에 저장된 로그 그룹을 검색하고 크로스 계정 Logs Insights 쿼리, Live Tail을 실행할 수 있으며 여러 계정에서 Contributor Insights 규칙을 생성하여 로그 항목을 생성하는 상위 N명의 기여자를 파악할 수 있습니다. 지표 검색을 사용하여 여러 계정의 지표를 통합 보기에서 시각화하고, 다른 계정의 지표를 평가하는 경보를 생성하여 이상 항목과 최신 문제 관련 알림을 받고, 중앙 집중식 대시보드에서 이를 시각화할 수 있습니다. 이 기능을 사용하면 단일의 교차 계정 지표 스트림을 설정하고 AWS 리전의 여러 계정에 걸친 지표를 이 스트림에 포함할 수도 있습니다. 교차 계정 관측성을 사용하면 ServiceLens를 사용하여 교차 계정 애플리케이션의 대화형 맵을 확인할 수 있으며, 한 단계로 간편하게 관련 지표, 로그, 트레이스로 드릴다운할 수도 있습니다.\n",
            "\"category : CloudWatch, question : 여러 AWS 계정에 걸쳐 사용할 수 있는 CloudWatch 모니터링 기능에는 어떤 것이 있나요?, answer : 크로스 계정 관측성을 사용하면 중앙 보기에서 여러 계정에 저장된 로그 그룹을 검색하고 크로스 계정 Logs Insights 쿼리, Live Tail을 실행할 수 있으며 여러 계정에서 Contributor Insights 규칙을 생성하여 로그 항목을 생성하는 상위 N명의 기여자를 파악할 수 있습니다. 지표 검색을 사용하여 여러 계정의 지표를 통합 보기에서 시각화하고, 다른 계정의 지표를 평가하는 경보를 생성하여 이상 항목과 최신 문제 관련 알림을 받고, 중앙 집중식 대시보드에서 이를 시각화할 수 있습니다. 이 기능을 사용하면 단일의 교차 계정 지표 스트림을 설정하고 AWS 리전의 여러 계정에 걸친 지표를 이 스트림에 포함할 수도 있습니다. 교차 계정 관측성을 사용하면 ServiceLens를 사용하여 교차 계정 애플리케이션의 대화형 맵을 확인할 수 있으며, 한 단계로 간편하게 관련 지표, 로그, 트레이스로 드릴다운할 수도 있습니다.\"\n",
            "CloudWatch 크로스 계정, 크로스 리전 기능을 내 콘솔에서 사용할 수 있나요?\n",
            "CloudWatch의 크로스 계정 모니터링과 크로스 계정 크로스 리전 기능 모두 CloudWatch 콘솔에서 사용할 수 있습니다. CloudWatch에서 크로스 계정 관측성을 설정하면 크로스 계정 크로스 리전 드롭다운 메뉴가 콘솔에서 제거됩니다. 참고로 CloudWatch의 크로스 계정 관측성 경험은 한 번에 한 리전에서 사용할 수 있습니다. 크로스 계정 크로스 리전 기능을 사용하면 IAM 역할을 통해 조직 전체 텔레메트리에 액세스할 수 있습니다. CloudWatch의 크로스 계정 관측성은 Observability Access Manager API를 사용하여 액세스 정책을 정의합니다. 설명서에서 자세히 알아보세요.\n",
            "\"category : CloudWatch, question : CloudWatch 크로스 계정, 크로스 리전 기능을 내 콘솔에서 사용할 수 있나요?, answer : CloudWatch의 크로스 계정 모니터링과 크로스 계정 크로스 리전 기능 모두 CloudWatch 콘솔에서 사용할 수 있습니다. CloudWatch에서 크로스 계정 관측성을 설정하면 크로스 계정 크로스 리전 드롭다운 메뉴가 콘솔에서 제거됩니다. 참고로 CloudWatch의 크로스 계정 관측성 경험은 한 번에 한 리전에서 사용할 수 있습니다. 크로스 계정 크로스 리전 기능을 사용하면 IAM 역할을 통해 조직 전체 텔레메트리에 액세스할 수 있습니다. CloudWatch의 크로스 계정 관측성은 Observability Access Manager API를 사용하여 액세스 정책을 정의합니다. 설명서에서 자세히 알아보세요.\"\n",
            "Amazon CloudWatch 지표로 무엇을 측정할 수 있나요?\n",
            "Amazon CloudWatch를 사용하면 AWS 클라우드 리소스 및 AWS에서 실행하는 애플리케이션을 모니터링할 수 있습니다. Amazon EC2 인스턴스, EBS 볼륨, Elastic Load Balancer, Auto Scaling 그룹, EMR 작업 흐름, RDS DB 인스턴스, DynamoDB 테이블, ElastiCache 클러스터, RedShift 클러스터, OpsWorks 스택, Route 53 상태 확인, SNS 주제, SQS 대기열, SWF 워크플로, Storage Gateway를 비롯한 다양한 AWS 제품 및 서비스에 대한 지표가 자동으로 제공됩니다. 또한, 자체 애플리케이션 및 서비스에서 생성된 사용자 정의 지표도 모니터링할 수 있습니다.\n",
            "\"category : CloudWatch, question : Amazon CloudWatch 지표로 무엇을 측정할 수 있나요?, answer : Amazon CloudWatch를 사용하면 AWS 클라우드 리소스 및 AWS에서 실행하는 애플리케이션을 모니터링할 수 있습니다. Amazon EC2 인스턴스, EBS 볼륨, Elastic Load Balancer, Auto Scaling 그룹, EMR 작업 흐름, RDS DB 인스턴스, DynamoDB 테이블, ElastiCache 클러스터, RedShift 클러스터, OpsWorks 스택, Route 53 상태 확인, SNS 주제, SQS 대기열, SWF 워크플로, Storage Gateway를 비롯한 다양한 AWS 제품 및 서비스에 대한 지표가 자동으로 제공됩니다. 또한, 자체 애플리케이션 및 서비스에서 생성된 사용자 정의 지표도 모니터링할 수 있습니다.\"\n",
            "모든 지표의 보존 기간은 어떻게 되나요?\n",
            "최하 1초 단위로 사용자 정의 지표를 게시하고 저장할 수 있습니다. 지표 보존 기간 연장 기능은 2016년 11월 1일에 출시되었으며 이전에 14일이었던 고객의 모든 지표 저장 기간을 15개월까지 연장할 수 있게 되었습니다. CloudWatch는 다음과 같이 지표 데이터를 보존합니다.\n",
            "간격이 60초 미만인 데이터 포인트는 3시간 동안 유지됩니다. 이러한 데이터 포인트는 고분석 사용자 지정 지표입니다.\n",
            "간격이 60초(1분)인 데이터 포인트는 15일 동안 유지됩니다.\n",
            "간격이 300초(5분)인 데이터 포인트는 63일 동안 유지됩니다. \n",
            "간격이 3,600초(1시간)인 데이터 포인트는 455일(15개월) 동안 유지됩니다.\n",
            "처음에 더 짧은 기간으로 게시되었던 데이터 포인트는 장기 보관을 위해 함께 집계됩니다. 예를 들어 1분 간격으로 데이터를 수집한 경우, 1분 단위로 15일 동안 유지됩니다. 15일이 지난 후에도 이 데이터는 계속 유지되지만, 5분 단위로만 검색할 수 있습니다. 63일이 지난 후에는 이 데이터가 다시 집계되어 1시간 단위로 제공됩니다. 이 기간보다 더 오랫동안 지표를 유지해야 하는 경우 GetMetricStatistics API를 사용하여 데이터 포인트를 오프라인 또는 다른 스토리지에서 검색할 수 있습니다.\n",
            "이 기능은 현재 미국 동부(버지니아 북부), 미국 서부(오레곤), 미국 서부(캘리포니아 북부), EU(아일랜드), EU(프랑크푸르트), 남아메리카(상파울루), 아시아 태평양(싱가포르), 아시아 태평양(도쿄), 아시아 태평양(서울), 아시아 태평양(뭄바이), 아시아 태평양(시드니), EU(런던), 캐나다(중부), 미국 동부(오하이오) 및 중국(베이징)에서 사용할 수 있습니다.\n",
            "\"category : CloudWatch, question : 모든 지표의 보존 기간은 어떻게 되나요?, answer : 최하 1초 단위로 사용자 정의 지표를 게시하고 저장할 수 있습니다. 지표 보존 기간 연장 기능은 2016년 11월 1일에 출시되었으며 이전에 14일이었던 고객의 모든 지표 저장 기간을 15개월까지 연장할 수 있게 되었습니다. CloudWatch는 다음과 같이 지표 데이터를 보존합니다.\n",
            "간격이 60초 미만인 데이터 포인트는 3시간 동안 유지됩니다. 이러한 데이터 포인트는 고분석 사용자 지정 지표입니다.\n",
            "간격이 60초(1분)인 데이터 포인트는 15일 동안 유지됩니다.\n",
            "간격이 300초(5분)인 데이터 포인트는 63일 동안 유지됩니다. \n",
            "간격이 3,600초(1시간)인 데이터 포인트는 455일(15개월) 동안 유지됩니다.\n",
            "처음에 더 짧은 기간으로 게시되었던 데이터 포인트는 장기 보관을 위해 함께 집계됩니다. 예를 들어 1분 간격으로 데이터를 수집한 경우, 1분 단위로 15일 동안 유지됩니다. 15일이 지난 후에도 이 데이터는 계속 유지되지만, 5분 단위로만 검색할 수 있습니다. 63일이 지난 후에는 이 데이터가 다시 집계되어 1시간 단위로 제공됩니다. 이 기간보다 더 오랫동안 지표를 유지해야 하는 경우 GetMetricStatistics API를 사용하여 데이터 포인트를 오프라인 또는 다른 스토리지에서 검색할 수 있습니다.\n",
            "이 기능은 현재 미국 동부(버지니아 북부), 미국 서부(오레곤), 미국 서부(캘리포니아 북부), EU(아일랜드), EU(프랑크푸르트), 남아메리카(상파울루), 아시아 태평양(싱가포르), 아시아 태평양(도쿄), 아시아 태평양(서울), 아시아 태평양(뭄바이), 아시아 태평양(시드니), EU(런던), 캐나다(중부), 미국 동부(오하이오) 및 중국(베이징)에서 사용할 수 있습니다.\"\n",
            "Amazon CloudWatch가 수신하고 집계하는 데이터의 최소 단위는 어떻게 되나요?\n",
            "CloudWatch가 지원하는 최소 단위는 고단위 지표인 1초 데이터 포인트이며, 1분 간격으로 지표를 저장할 수도 있습니다. Cloudwatch는 3분 또는 5분 간격과 같이 서로 다른 간격으로 지표를 수신할 때도 있습니다. PutMetricData API 요청에서 StorageResolution 필드를 설정하여 지표를 고단위로 지정하지 않으며, 기본적으로 CloudWatch는 1분 단위로 지표를 집계하고 저장합니다.\n",
            "요청한 데이터가 생성된 지 얼마나 되었는지에 따라 위의 보존 일정에 정의된 단위로 지표가 제공됩니다. 예를 들어 10일 전 하루에 대한 1분 단위 데이터를 요청한 경우 1,440개의 데이터 포인트를 받게 됩니다. 하지만 5개월 전의 1분 단위 데이터를 요청하는 경우 UI는 해당 단위를 자동으로 1시간으로 변경하고 GetMetricStatistics API는 어떤 출력도 반환하지 않습니다.\n",
            "\"category : CloudWatch, question : Amazon CloudWatch가 수신하고 집계하는 데이터의 최소 단위는 어떻게 되나요?, answer : CloudWatch가 지원하는 최소 단위는 고단위 지표인 1초 데이터 포인트이며, 1분 간격으로 지표를 저장할 수도 있습니다. Cloudwatch는 3분 또는 5분 간격과 같이 서로 다른 간격으로 지표를 수신할 때도 있습니다. PutMetricData API 요청에서 StorageResolution 필드를 설정하여 지표를 고단위로 지정하지 않으며, 기본적으로 CloudWatch는 1분 단위로 지표를 집계하고 저장합니다.\n",
            "요청한 데이터가 생성된 지 얼마나 되었는지에 따라 위의 보존 일정에 정의된 단위로 지표가 제공됩니다. 예를 들어 10일 전 하루에 대한 1분 단위 데이터를 요청한 경우 1,440개의 데이터 포인트를 받게 됩니다. 하지만 5개월 전의 1분 단위 데이터를 요청하는 경우 UI는 해당 단위를 자동으로 1시간으로 변경하고 GetMetricStatistics API는 어떤 출력도 반환하지 않습니다.\"\n",
            "지표를 삭제할 수 있나요?\n",
            "CloudWatch에서는 지표 삭제를 지원하지 않습니다. 지표는 위에 명시된 보존 일정에 따라 삭제됩니다.\n",
            "\"category : CloudWatch, question : 지표를 삭제할 수 있나요?, answer : CloudWatch에서는 지표 삭제를 지원하지 않습니다. 지표는 위에 명시된 보존 일정에 따라 삭제됩니다.\"\n",
            "하이브리드 및 멀티클라우드 워크로드, Amazon RDS, S3와 같은 다른 데이터 소스나 자체 사용자 지정 데이터에서 지표를 쿼리할 수 있나요?\n",
            "예. Amazon CloudWatch는 여러 소스에서 데이터를 쿼리하는 것을 지원하므로 AWS, 온프레미스, 다른 클라우드에서 지표를 모니터링할 수 있습니다. 이제 몇 시간이 아닌 몇 분 안에 중요한 이벤트를 해결하고 애플리케이션 상태에 대한 가시성을 확보하고 원활한 운영을 위한 인사이트를 더 빠르게 얻을 수 있습니다. 모든 모니터링 도구의 쿼리, 시각화 및 경보를 한 곳에서 중앙 집중화할 수 있습니다.\n",
            "\"category : CloudWatch, question : 하이브리드 및 멀티클라우드 워크로드, Amazon RDS, S3와 같은 다른 데이터 소스나 자체 사용자 지정 데이터에서 지표를 쿼리할 수 있나요?, answer : 예. Amazon CloudWatch는 여러 소스에서 데이터를 쿼리하는 것을 지원하므로 AWS, 온프레미스, 다른 클라우드에서 지표를 모니터링할 수 있습니다. 이제 몇 시간이 아닌 몇 분 안에 중요한 이벤트를 해결하고 애플리케이션 상태에 대한 가시성을 확보하고 원활한 운영을 위한 인사이트를 더 빠르게 얻을 수 있습니다. 모든 모니터링 도구의 쿼리, 시각화 및 경보를 한 곳에서 중앙 집중화할 수 있습니다.\"\n",
            "Amazon EC2 인스턴스에 대한 모니터링을 비활성화하면 지표 데이터가 손실되나요?\n",
            "아니요. 위에 명시된 보존 일정에 따라 언제든 Amazon EC2 인스턴스에 대한 지표를 검색할 수 있습니다. 하지만 CloudWatch 콘솔은 네임스페이스에 가장 최신 인스턴스가 표시되도록 하기 위해 지표가 마지막으로 수집된 후 2주까지로 지표 검색을 제한하고 있습니다.\n",
            "\"category : CloudWatch, question : Amazon EC2 인스턴스에 대한 모니터링을 비활성화하면 지표 데이터가 손실되나요?, answer : 아니요. 위에 명시된 보존 일정에 따라 언제든 Amazon EC2 인스턴스에 대한 지표를 검색할 수 있습니다. 하지만 CloudWatch 콘솔은 네임스페이스에 가장 최신 인스턴스가 표시되도록 하기 위해 지표가 마지막으로 수집된 후 2주까지로 지표 검색을 제한하고 있습니다.\"\n",
            "여러 데이터 소스에서 지표를 쿼리하려면 어떻게 해야 하나요?\n",
            "시작하려면 Amazon CloudWatch 콘솔에서 지표 쿼리 작성기로 이동하여 데이터 소스 선택기를 엽니다. 선택기를 사용하여 마법사를 시작하고 쿼리 및 경보에 사용할 새 데이터 소스를 추가할 수 있습니다. 쿼리하려는 데이터 소스를 선택하고 URL, 경로, 보안 인증 정보와 같은 액세스 세부 정보를 지정합니다. 자세한 내용은 설명서를 참조하세요.\n",
            "\"category : CloudWatch, question : 여러 데이터 소스에서 지표를 쿼리하려면 어떻게 해야 하나요?, answer : 시작하려면 Amazon CloudWatch 콘솔에서 지표 쿼리 작성기로 이동하여 데이터 소스 선택기를 엽니다. 선택기를 사용하여 마법사를 시작하고 쿼리 및 경보에 사용할 새 데이터 소스를 추가할 수 있습니다. 쿼리하려는 데이터 소스를 선택하고 URL, 경로, 보안 인증 정보와 같은 액세스 세부 정보를 지정합니다. 자세한 내용은 설명서를 참조하세요.\"\n",
            "종료된 Amazon EC2 인스턴스 또는 삭제된 Elastic Load Balancer의 지표 데이터에 액세스할 수 있나요?\n",
            "예. Amazon CloudWatch는 종료된 Amazon EC2 인스턴스 또는 삭제된 Elastic Load Balancer의 지표를 15개월 동안 저장합니다.\n",
            "\"category : CloudWatch, question : 종료된 Amazon EC2 인스턴스 또는 삭제된 Elastic Load Balancer의 지표 데이터에 액세스할 수 있나요?, answer : 예. Amazon CloudWatch는 종료된 Amazon EC2 인스턴스 또는 삭제된 Elastic Load Balancer의 지표를 15개월 동안 저장합니다.\"\n",
            "지표를 5분 단위로 볼 때와 1분 단위로 볼 때 동일한 기간의 그래프가 다른 이유는 무엇인가요?\n",
            "동일한 시간 창을 5분 단위로 볼 때와 1분 단위로 볼 때를 비교할 경우 해당 데이터 포인트가 그래프의 각기 다른 위치에 표시될 수 있습니다. 그래프에 지정한 기간 동안 Amazon CloudWatch가 모든 가용 데이터 요소를 찾은 다음 단일 집계 요소를 계산해 전체 기간을 표시합니다. 5분 단위의 경우 단일 데이터 요소가 5분 시간 창의 맨 앞에 배치됩니다. 1분 단위의 경우 단일 데이터 요소가 1분 표시에 배치됩니다. 문제 해결 및 더 정밀한 기간 그래프가 필요한 기타 작업의 경우 1분 단위를 사용하는 것이 좋습니다.\n",
            "\"category : CloudWatch, question : 지표를 5분 단위로 볼 때와 1분 단위로 볼 때 동일한 기간의 그래프가 다른 이유는 무엇인가요?, answer : 동일한 시간 창을 5분 단위로 볼 때와 1분 단위로 볼 때를 비교할 경우 해당 데이터 포인트가 그래프의 각기 다른 위치에 표시될 수 있습니다. 그래프에 지정한 기간 동안 Amazon CloudWatch가 모든 가용 데이터 요소를 찾은 다음 단일 집계 요소를 계산해 전체 기간을 표시합니다. 5분 단위의 경우 단일 데이터 요소가 5분 시간 창의 맨 앞에 배치됩니다. 1분 단위의 경우 단일 데이터 요소가 1분 표시에 배치됩니다. 문제 해결 및 더 정밀한 기간 그래프가 필요한 기타 작업의 경우 1분 단위를 사용하는 것이 좋습니다.\"\n",
            "사용자 정의 지표란 무엇인가요?\n",
            "Amazon CloudWatch를 사용하면 자체 애플리케이션, 스크립트 및 서비스에서 생성된 데이터를 모니터링할 수 있습니다. 사용자 정의 지표란 사용자가 Amazon CloudWatch에 제공하는 모든 지표를 말합니다. 예를 들어 웹 페이지 로드 시간, 요청 오류 비율, 인스턴스의 프로세스나 스레드 수 또는 애플리케이션에서 수행한 작업량을 모니터링하는 방법으로 사용자 정의 지표를 사용할 수 있습니다. PutMetricData API, Windows 및 Linux용 샘플 모니터링 스크립트, CloudWatch 수집 플러그인뿐만 아니라 AWS 파트너가 제공하는 여러 애플리케이션 및 도구를 사용하여 사용자 정의 지표를 시작할 수 있습니다.\n",
            "\"category : CloudWatch, question : 사용자 정의 지표란 무엇인가요?, answer : Amazon CloudWatch를 사용하면 자체 애플리케이션, 스크립트 및 서비스에서 생성된 데이터를 모니터링할 수 있습니다. 사용자 정의 지표란 사용자가 Amazon CloudWatch에 제공하는 모든 지표를 말합니다. 예를 들어 웹 페이지 로드 시간, 요청 오류 비율, 인스턴스의 프로세스나 스레드 수 또는 애플리케이션에서 수행한 작업량을 모니터링하는 방법으로 사용자 정의 지표를 사용할 수 있습니다. PutMetricData API, Windows 및 Linux용 샘플 모니터링 스크립트, CloudWatch 수집 플러그인뿐만 아니라 AWS 파트너가 제공하는 여러 애플리케이션 및 도구를 사용하여 사용자 정의 지표를 시작할 수 있습니다.\"\n",
            "사용자 지정 지표의 최소 시간 단위는 몇인가요?\n",
            "사용자 지정 지표는 다음 중 하나가 될 수 있습니다.\n",
            "1분 간격으로 데이터를 저장하는 표준 단위\n",
            "1초 간격으로 데이터를 저장하는 고단위입니다.\n",
            "CloudWatch에서는 기본적으로 1분 단위로 지표를 저장합니다. 지표를 고단위로 정의하려면 PutMetricData API 요청에서 StorageResolution 파라미터를 1로 설정하면 됩니다. 선택 사항인 StorageResolution 파라미터를 설정하지 않는 경우에는 CloudWatch가 지표를 1분 단위로 저장하게 됩니다.\n",
            "고단위 지표를 게시하는 경우 CloudWatch가 이를 1초 단위로 저장하며, 사용자는 1초, 5초, 10초, 30초 또는 60초의 배수로 읽고 검색할 수 있습니다.\n",
            "사용자 정의 지표는 위에 명시된 것과 같은 보존 일정을 따릅니다.\n",
            "\"category : CloudWatch, question : 사용자 지정 지표의 최소 시간 단위는 몇인가요?, answer : 사용자 지정 지표는 다음 중 하나가 될 수 있습니다.\n",
            "1분 간격으로 데이터를 저장하는 표준 단위\n",
            "1초 간격으로 데이터를 저장하는 고단위입니다.\n",
            "CloudWatch에서는 기본적으로 1분 단위로 지표를 저장합니다. 지표를 고단위로 정의하려면 PutMetricData API 요청에서 StorageResolution 파라미터를 1로 설정하면 됩니다. 선택 사항인 StorageResolution 파라미터를 설정하지 않는 경우에는 CloudWatch가 지표를 1분 단위로 저장하게 됩니다.\n",
            "고단위 지표를 게시하는 경우 CloudWatch가 이를 1초 단위로 저장하며, 사용자는 1초, 5초, 10초, 30초 또는 60초의 배수로 읽고 검색할 수 있습니다.\n",
            "사용자 정의 지표는 위에 명시된 것과 같은 보존 일정을 따릅니다.\"\n",
            "고단위로 사용할 수 있는 지표는 무엇인가요?\n",
            "현재 CloudWatch에 게시하는 사용자 지정 지표만 고단위로 제공됩니다. 고단위 사용자 정의 지표는 1초 단위로 CloudWatch에 저장됩니다. 고단위는 PutMetricData API에 있는 StorageResolution 파라미터 값을 1로 설정하여 정의하며 이 파라미터는 필수 필드가 아닙니다. 선택 사항인 StorageResolution 필드 값을 지정하지 않는 경우 CloudWatch는 기본적으로 1분 단위로 사용자 정의 지표를 저장합니다.\n",
            "\"category : CloudWatch, question : 고단위로 사용할 수 있는 지표는 무엇인가요?, answer : 현재 CloudWatch에 게시하는 사용자 지정 지표만 고단위로 제공됩니다. 고단위 사용자 정의 지표는 1초 단위로 CloudWatch에 저장됩니다. 고단위는 PutMetricData API에 있는 StorageResolution 파라미터 값을 1로 설정하여 정의하며 이 파라미터는 필수 필드가 아닙니다. 선택 사항인 StorageResolution 필드 값을 지정하지 않는 경우 CloudWatch는 기본적으로 1분 단위로 사용자 정의 지표를 저장합니다.\"\n",
            "고단위 사용자 정의 지표의 요금은 일반 사용자 지표의 요금과 다르게 책정되나요?\n",
            "아니요. 고단위 사용자 정의 지표는 표준 1분 단위 사용자 정의 지표와 동일한 방식으로 요금이 책정됩니다.\n",
            "\"category : CloudWatch, question : 고단위 사용자 정의 지표의 요금은 일반 사용자 지표의 요금과 다르게 책정되나요?, answer : 아니요. 고단위 사용자 정의 지표는 표준 1분 단위 사용자 정의 지표와 동일한 방식으로 요금이 책정됩니다.\"\n",
            "프로그램에서 CloudWatch Logs로 로그를 내보내는 대신 사용자 정의 지표를 사용해야 하는 경우는 어떤 경우인가요?\n",
            "사용자 정의 지표, CloudWatch 로그 또는 두 가지 모두를 사용하여 데이터를 모니터링할 수 있습니다. 운영 체제 프로세스나 성능 측정값과 같이 아직 로그 형식으로 생성되지 않은 데이터의 경우 사용자 지표를 사용하는 것이 좋습니다. 또는 애플리케이션이나 스크립트를 직접 작성하거나 AWS 파트너가 제공하는 애플리케이션이나 스크립트를 사용할 수 있습니다. 추가 세부정보가 포함된 개별 측정값을 저장하려면 CloudWatch 로그를 사용하는 것이 좋습니다.\n",
            "\"category : CloudWatch, question : 프로그램에서 CloudWatch Logs로 로그를 내보내는 대신 사용자 정의 지표를 사용해야 하는 경우는 어떤 경우인가요?, answer : 사용자 정의 지표, CloudWatch 로그 또는 두 가지 모두를 사용하여 데이터를 모니터링할 수 있습니다. 운영 체제 프로세스나 성능 측정값과 같이 아직 로그 형식으로 생성되지 않은 데이터의 경우 사용자 지표를 사용하는 것이 좋습니다. 또는 애플리케이션이나 스크립트를 직접 작성하거나 AWS 파트너가 제공하는 애플리케이션이나 스크립트를 사용할 수 있습니다. 추가 세부정보가 포함된 개별 측정값을 저장하려면 CloudWatch 로그를 사용하는 것이 좋습니다.\"\n",
            "CloudWatch에서 확인 및 그래프 작성이 가능한 통계는 무엇인가요?\n",
            "Amazon CloudWatch 지표의 통계치인 Average, Sum, Minimum, Maximum 및 Sample Count를 가져오고, 그래프로 작성하고, 경보를 설정할 수 있습니다. 1분 또는 60초의 배수인 시간 간격에 대한 통계를 계산할 수 있습니다. 고단위 사용자 정의 지표의 경우 1초에서 3시간 사이의 모든 기간에 대해 통계를 계산할 수 있습니다.\n",
            "\"category : CloudWatch, question : CloudWatch에서 확인 및 그래프 작성이 가능한 통계는 무엇인가요?, answer : Amazon CloudWatch 지표의 통계치인 Average, Sum, Minimum, Maximum 및 Sample Count를 가져오고, 그래프로 작성하고, 경보를 설정할 수 있습니다. 1분 또는 60초의 배수인 시간 간격에 대한 통계를 계산할 수 있습니다. 고단위 사용자 정의 지표의 경우 1초에서 3시간 사이의 모든 기간에 대해 통계를 계산할 수 있습니다.\"\n",
            ".NET 및 SQL Server용 CloudWatch Application Insights란 무엇인가요?\n",
            ".NET 및 SQL Server용 Amazon CloudWatch Application Insights는 .NET 및 SQL Server 애플리케이션을 쉽게 모니터링하는 데 사용할 수 있는 기능입니다. 이 기능은 애플리케이션 리소스 및 기술 스택(예: 데이터베이스, 웹(IIS) 및 애플리케이션 서버, OS, 로드 밸런서, 대기열 등) 전반에서 주요 지표 및 로그를 파악하고 설정할 수 있도록 지원합니다. 또한 이러한 텔레메트리 데이터를 지속적으로 모니터링함으로써 이상 항목 및 오류를 탐지하고 그 상관관계를 분석하여 애플리케이션의 문제를 사용자에게 알려줍니다. 이 기능은 문제 해결을 돕기 위해 자동 대시보드를 생성함으로써 탐지한 문제를 시각화합니다. 여기에는 상관관계가 있는 지표 이상 항목 및 로그 오류와 함께 잠재적인 근본 원인을 알려주는 추가 인사이트가 포함됩니다.\n",
            "\"category : CloudWatch, question : .NET 및 SQL Server용 CloudWatch Application Insights란 무엇인가요?, answer : .NET 및 SQL Server용 Amazon CloudWatch Application Insights는 .NET 및 SQL Server 애플리케이션을 쉽게 모니터링하는 데 사용할 수 있는 기능입니다. 이 기능은 애플리케이션 리소스 및 기술 스택(예: 데이터베이스, 웹(IIS) 및 애플리케이션 서버, OS, 로드 밸런서, 대기열 등) 전반에서 주요 지표 및 로그를 파악하고 설정할 수 있도록 지원합니다. 또한 이러한 텔레메트리 데이터를 지속적으로 모니터링함으로써 이상 항목 및 오류를 탐지하고 그 상관관계를 분석하여 애플리케이션의 문제를 사용자에게 알려줍니다. 이 기능은 문제 해결을 돕기 위해 자동 대시보드를 생성함으로써 탐지한 문제를 시각화합니다. 여기에는 상관관계가 있는 지표 이상 항목 및 로그 오류와 함께 잠재적인 근본 원인을 알려주는 추가 인사이트가 포함됩니다.\"\n",
            ".NET 및 SQL Server용 CloudWatch Application Insights를 사용할 때의 이점은 무엇인가요?\n",
            "애플리케이션 지표 및 로그 자동 인식: 애플리케이션 리소스를 검색하여 모니터링할 권장 지표 및 로그 목록을 제공하고 자동으로 설정합니다. 따라서 애플리케이션에 대한 모니터링을 더욱 쉽게 설정할 수 있습니다. \n",
            "지능형 문제 탐지: 내장된 규칙 및 기계 학습 알고리즘을 사용하여 애플리케이션 스택 전반에 걸쳐 문제의 증상을 동적으로 모니터링 및 분석하고 애플리케이션 문제를 탐지합니다. 이에 따라 개별 지표 급증이나 이벤트 또는 로그 예외를 처리하는 데 드는 오버헤드를 줄이는 대신, 실제 문제에 대한 알림을 받고 이러한 문제에 대한 컨텍스트 기반 정보를 얻을 수 있습니다.\n",
            "신속한 문제 해결: 탐지한 문제를 평가하여 그 문제에 대한 인사이트를 제공합니다. 예를 들어 탐지한 문제의 가능한 근본 원인과 함께 문제로 인해 영향을 받은 지표 및 로그 목록을 제공합니다. 사용자는 생성된 인사이트에 대한 피드백을 제공하여 사용 사례에 고유한 문제 탐지 엔진을 만들 수 있습니다.\n",
            "\"category : CloudWatch, question : .NET 및 SQL Server용 CloudWatch Application Insights를 사용할 때의 이점은 무엇인가요?, answer : 애플리케이션 지표 및 로그 자동 인식: 애플리케이션 리소스를 검색하여 모니터링할 권장 지표 및 로그 목록을 제공하고 자동으로 설정합니다. 따라서 애플리케이션에 대한 모니터링을 더욱 쉽게 설정할 수 있습니다. \n",
            "지능형 문제 탐지: 내장된 규칙 및 기계 학습 알고리즘을 사용하여 애플리케이션 스택 전반에 걸쳐 문제의 증상을 동적으로 모니터링 및 분석하고 애플리케이션 문제를 탐지합니다. 이에 따라 개별 지표 급증이나 이벤트 또는 로그 예외를 처리하는 데 드는 오버헤드를 줄이는 대신, 실제 문제에 대한 알림을 받고 이러한 문제에 대한 컨텍스트 기반 정보를 얻을 수 있습니다.\n",
            "신속한 문제 해결: 탐지한 문제를 평가하여 그 문제에 대한 인사이트를 제공합니다. 예를 들어 탐지한 문제의 가능한 근본 원인과 함께 문제로 인해 영향을 받은 지표 및 로그 목록을 제공합니다. 사용자는 생성된 인사이트에 대한 피드백을 제공하여 사용 사례에 고유한 문제 탐지 엔진을 만들 수 있습니다.\"\n",
            ".NET 및 SQL Server용 CloudWatch Application Insights를 사용하여 모니터링을 시작하려면 어떻게 해야 하나요?\n",
            "애플리케이션 온보딩: 연결된 AWS Resource Group을 선택하여 모니터링하려는 애플리케이션을 지정합니다.\n",
            "애플리케이션 구성 요소 파악: 애플리케이션 리소스를 분석하여 애플리케이션 구성 요소(독립 실행형 리소스 또는 오토 스케일링 및 로드 밸런서 그룹과 같은 관련 리소스 그룹)를 파악합니다. 또한 더 나은 통찰력 및 쉬운 온보딩을 위해 리소스를 그룹화하여 구성 요소를 사용자 지정할 수도 있습니다.\n",
            "모니터링 활성화: 애플리케이션 구성 요소에 대해 기술 티어(예: IIS 프런트 엔드, .NET 작업자 티어 등)를 지정할 수 있습니다. 사용자의 선택을 기반으로, 필요에 따라 사용자 지정할 수 있는 권장 지표 및 로그 세트를 제공합니다. 이러한 “모니터”를 저장하면 .NET 및 SQL Server용 Application Insights는 CloudWatch를 설정하여 사용자를 대신해 이를 수집합니다.\n",
            "온보딩 이후 .NET 및 SQL Server용 Application Insights는 사전 구축된 규칙과 기계 학습 모델 조합을 사용하여 애플리케이션 문제를 파악하기 시작합니다. 그리고 탐지한 문제 목록을 비롯하여 관련 이상 항목 및 오류와 함께 이러한 문제에 대한 세부 정보 보기를 제공하는 CloudWatch 자동 대시보드를 만듭니다.\n",
            "\"category : CloudWatch, question : .NET 및 SQL Server용 CloudWatch Application Insights를 사용하여 모니터링을 시작하려면 어떻게 해야 하나요?, answer : 애플리케이션 온보딩: 연결된 AWS Resource Group을 선택하여 모니터링하려는 애플리케이션을 지정합니다.\n",
            "애플리케이션 구성 요소 파악: 애플리케이션 리소스를 분석하여 애플리케이션 구성 요소(독립 실행형 리소스 또는 오토 스케일링 및 로드 밸런서 그룹과 같은 관련 리소스 그룹)를 파악합니다. 또한 더 나은 통찰력 및 쉬운 온보딩을 위해 리소스를 그룹화하여 구성 요소를 사용자 지정할 수도 있습니다.\n",
            "모니터링 활성화: 애플리케이션 구성 요소에 대해 기술 티어(예: IIS 프런트 엔드, .NET 작업자 티어 등)를 지정할 수 있습니다. 사용자의 선택을 기반으로, 필요에 따라 사용자 지정할 수 있는 권장 지표 및 로그 세트를 제공합니다. 이러한 “모니터”를 저장하면 .NET 및 SQL Server용 Application Insights는 CloudWatch를 설정하여 사용자를 대신해 이를 수집합니다.\n",
            "온보딩 이후 .NET 및 SQL Server용 Application Insights는 사전 구축된 규칙과 기계 학습 모델 조합을 사용하여 애플리케이션 문제를 파악하기 시작합니다. 그리고 탐지한 문제 목록을 비롯하여 관련 이상 항목 및 오류와 함께 이러한 문제에 대한 세부 정보 보기를 제공하는 CloudWatch 자동 대시보드를 만듭니다.\"\n",
            "CloudWatch Metric Streams란 무엇인가요?\n",
            "CloudWatch Metric Streams는 최소한의 설정과 구성으로 선택한 대상으로 CloudWatch 지표를 지속적으로 스트리밍할 수 있는 기능입니다. 완전 관리형 솔루션이며 코드를 작성하거나 인프라를 유지관리할 필요가 없습니다. 몇 번의 클릭만으로 Amazon Simple Storage Service(S3)와 같은 대상에 대한 지표 스트림을 구성할 수 있습니다. 또한 운영 대시 보드를 최신 상태로 유지하기 위해 선택한 타사 서비스 공급자에게 지표를 보낼 수도 있습니다.\n",
            "\"category : CloudWatch, question : CloudWatch Metric Streams란 무엇인가요?, answer : CloudWatch Metric Streams는 최소한의 설정과 구성으로 선택한 대상으로 CloudWatch 지표를 지속적으로 스트리밍할 수 있는 기능입니다. 완전 관리형 솔루션이며 코드를 작성하거나 인프라를 유지관리할 필요가 없습니다. 몇 번의 클릭만으로 Amazon Simple Storage Service(S3)와 같은 대상에 대한 지표 스트림을 구성할 수 있습니다. 또한 운영 대시 보드를 최신 상태로 유지하기 위해 선택한 타사 서비스 공급자에게 지표를 보낼 수도 있습니다.\"\n",
            "CloudWatch Metric Streams를 사용해야 하는 이유는 무엇인가요?\n",
            "Metric Streams는 API를 폴링할 필요없이 CloudWatch에서 지표 데이터를 얻는 대체 방법을 제공합니다. 몇 번의 클릭만으로 지표 스트림을 만들 수 있으며 지표 데이터가 대상으로 흐르기 시작합니다. Amazon S3와 같은 Data Lake on AWS로 지표를 쉽게 전달하고 Amazon Athena와 같은 도구를 사용하여 사용량 또는 성능 분석을 시작할 수 있습니다. 또한 Metrics Streams를 사용하면 Amazon Kinesis Data Firehose HTTP 엔드포인트를 사용하여 CloudWatch 지표를 인기있는 타사 서비스 제공 업체에 쉽게 보낼 수 있습니다. 최신 CloudWatch 메트릭 데이터를 포함하여 확장 가능한 연속 스트림을 생성하여 정확하고 시기 적절한 메트릭 데이터를 사용하는 대시보드, 경보 및 기타 도구에 전원을 공급할 수 있습니다.\n",
            "\"category : CloudWatch, question : CloudWatch Metric Streams를 사용해야 하는 이유는 무엇인가요?, answer : Metric Streams는 API를 폴링할 필요없이 CloudWatch에서 지표 데이터를 얻는 대체 방법을 제공합니다. 몇 번의 클릭만으로 지표 스트림을 만들 수 있으며 지표 데이터가 대상으로 흐르기 시작합니다. Amazon S3와 같은 Data Lake on AWS로 지표를 쉽게 전달하고 Amazon Athena와 같은 도구를 사용하여 사용량 또는 성능 분석을 시작할 수 있습니다. 또한 Metrics Streams를 사용하면 Amazon Kinesis Data Firehose HTTP 엔드포인트를 사용하여 CloudWatch 지표를 인기있는 타사 서비스 제공 업체에 쉽게 보낼 수 있습니다. 최신 CloudWatch 메트릭 데이터를 포함하여 확장 가능한 연속 스트림을 생성하여 정확하고 시기 적절한 메트릭 데이터를 사용하는 대시보드, 경보 및 기타 도구에 전원을 공급할 수 있습니다.\"\n",
            "CloudWatch 지표 스트림을 생성하고 관리하려면 어떻게 해야 하나요?\n",
            "CloudWatch 콘솔을 통해 또는 CloudWatch API, AWS SDK, AWS CLI 또는 AWS CloudFormation을 통해 프로그래밍 방식으로 Metric Streams를 생성 및 관리하여 Metric Streams를 프로비저닝하고 구성 할 수 있습니다. 타사 서비스 공급자가 제공하는 AWS CloudFormation 템플릿을 사용하여 AWS 이외 대상으로 지표 스트림 전달을 설정할 수도 있습니다. 자세한 내용은 CloudWatch 지표 스트림 설명서를 참조하세요.\n",
            "\"category : CloudWatch, question : CloudWatch 지표 스트림을 생성하고 관리하려면 어떻게 해야 하나요?, answer : CloudWatch 콘솔을 통해 또는 CloudWatch API, AWS SDK, AWS CLI 또는 AWS CloudFormation을 통해 프로그래밍 방식으로 Metric Streams를 생성 및 관리하여 Metric Streams를 프로비저닝하고 구성 할 수 있습니다. 타사 서비스 공급자가 제공하는 AWS CloudFormation 템플릿을 사용하여 AWS 이외 대상으로 지표 스트림 전달을 설정할 수도 있습니다. 자세한 내용은 CloudWatch 지표 스트림 설명서를 참조하세요.\"\n",
            "내 CloudWatch 지표 스트림에 포함할 지표를 관리할 수 있나요?\n",
            "예. 기본적으로 모든 메트릭을 보내거나 네임스페이스로 정의된 지표 그룹을 포함 및 제외하는 필터 규칙을 만들 수 있습니다(예: AWS/EC2). Metric Streams는 필터 규칙과 일치하는 새 지표를 자동으로 감지하고 지표 업데이트를 스트림에 포함합니다. 리소스가 종료되면 Metric Streams는 비활성 지표에 대한 업데이트 전송을 자동으로 중지합니다.\n",
            "\"category : CloudWatch, question : 내 CloudWatch 지표 스트림에 포함할 지표를 관리할 수 있나요?, answer : 예. 기본적으로 모든 메트릭을 보내거나 네임스페이스로 정의된 지표 그룹을 포함 및 제외하는 필터 규칙을 만들 수 있습니다(예: AWS/EC2). Metric Streams는 필터 규칙과 일치하는 새 지표를 자동으로 감지하고 지표 업데이트를 스트림에 포함합니다. 리소스가 종료되면 Metric Streams는 비활성 지표에 대한 업데이트 전송을 자동으로 중지합니다.\"\n",
            "CloudWatch 지표 스트림은 어떤 형식을 지원하나요?\n",
            "Metric Streams는 OpenTelemetry 또는 JSON 형식으로 출력할 수 있습니다. 지표 스트림을 생성하거나 관리할 때 출력 형식을 선택할 수 있습니다.\n",
            "\"category : CloudWatch, question : CloudWatch 지표 스트림은 어떤 형식을 지원하나요?, answer : Metric Streams는 OpenTelemetry 또는 JSON 형식으로 출력할 수 있습니다. 지표 스트림을 생성하거나 관리할 때 출력 형식을 선택할 수 있습니다.\"\n",
            "CloudWatch Metric Streams에서 제공하는 데이터의 비용과 볼륨을 모니터링할 수 있나요?\n",
            "예. Metric Streams 콘솔 페이지의 모니터링 섹션을 방문할 수 있습니다. 시간에 따른 지표 업데이트 볼륨에 대한 자동 대시 보드가 표시됩니다. 이러한 지표는 AWS/CloudWatch 네임스페이스에서도 이용할 수 있으며 볼륨이 비정상적으로 급증하는 경우 알림을 위한 경보를 생성하는 데 사용할 수 있습니다.\n",
            "\"category : CloudWatch, question : CloudWatch Metric Streams에서 제공하는 데이터의 비용과 볼륨을 모니터링할 수 있나요?, answer : 예. Metric Streams 콘솔 페이지의 모니터링 섹션을 방문할 수 있습니다. 시간에 따른 지표 업데이트 볼륨에 대한 자동 대시 보드가 표시됩니다. 이러한 지표는 AWS/CloudWatch 네임스페이스에서도 이용할 수 있으며 볼륨이 비정상적으로 급증하는 경우 알림을 위한 경보를 생성하는 데 사용할 수 있습니다.\"\n",
            "Amazon CloudWatch에서는 어떤 로그 모니터링을 제공하나요?\n",
            "CloudWatch 로그를 사용하면 기존 시스템, 애플리케이션 및 사용자 정의 로그 파일을 이용하여 시스템 및 애플리케이션을 모니터링하고 문제를 해결할 수 있습니다.\n",
            "CloudWatch 로그를 통해 특정 구문, 값 또는 패턴에 대한 로그를 거의 실시간으로 모니터링할 수 있습니다. 예를 들어 시스템 로그에 발생하는 오류 수에 대한 경보를 설정하거나 애플리케이션 로그에 기록된 웹 요청 지연 시간에 대한 그래프를 볼 수 있습니다. 그런 다음 원본 로그 데이터를 확인하여 문제의 원인을 파악할 수 있습니다. 로그 데이터는 안정성이 높고 저렴한 스토리지에 필요한 기간만큼 저장 및 액세스할 수 있으므로 하드 드라이브의 용량을 걱정하지 않아도 됩니다.\n",
            "\"category : CloudWatch, question : Amazon CloudWatch에서는 어떤 로그 모니터링을 제공하나요?, answer : CloudWatch 로그를 사용하면 기존 시스템, 애플리케이션 및 사용자 정의 로그 파일을 이용하여 시스템 및 애플리케이션을 모니터링하고 문제를 해결할 수 있습니다.\n",
            "CloudWatch 로그를 통해 특정 구문, 값 또는 패턴에 대한 로그를 거의 실시간으로 모니터링할 수 있습니다. 예를 들어 시스템 로그에 발생하는 오류 수에 대한 경보를 설정하거나 애플리케이션 로그에 기록된 웹 요청 지연 시간에 대한 그래프를 볼 수 있습니다. 그런 다음 원본 로그 데이터를 확인하여 문제의 원인을 파악할 수 있습니다. 로그 데이터는 안정성이 높고 저렴한 스토리지에 필요한 기간만큼 저장 및 액세스할 수 있으므로 하드 드라이브의 용량을 걱정하지 않아도 됩니다.\"\n",
            "Amazon CloudWatch Vended 로그란 무엇인가요?\n",
            "Amazon CloudWatch Vended 로그는 고객을 대신해 AWS 서비스에서 기본적으로 게시하는 로그입니다. VPC 흐름 로그는 이 단계별 모델이 최초로 적용되는 Vended 로그 유형입니다. 하지만 앞으로 더 많은 AWS 서비스 로그 유형이 Vended 로그 유형에 추가될 예정입니다.\n",
            "\"category : CloudWatch, question : Amazon CloudWatch Vended 로그란 무엇인가요?, answer : Amazon CloudWatch Vended 로그는 고객을 대신해 AWS 서비스에서 기본적으로 게시하는 로그입니다. VPC 흐름 로그는 이 단계별 모델이 최초로 적용되는 Vended 로그 유형입니다. 하지만 앞으로 더 많은 AWS 서비스 로그 유형이 Vended 로그 유형에 추가될 예정입니다.\"\n",
            "CloudWatch Logs는 모든 리전에서 사용할 수 있나요?\n",
            "CloudWatch Logs 서비스의 리전별 가용성에 대한 자세한 정보는 리전별 제품 및 서비스를 참조하세요.\n",
            "\"category : CloudWatch, question : CloudWatch Logs는 모든 리전에서 사용할 수 있나요?, answer : CloudWatch Logs 서비스의 리전별 가용성에 대한 자세한 정보는 리전별 제품 및 서비스를 참조하세요.\"\n",
            "CloudWatch Logs의 사용 요금은 얼마인가요?\n",
            "최신 정보는 요금 페이지를 참조하세요.\n",
            "\"category : CloudWatch, question : CloudWatch Logs의 사용 요금은 얼마인가요?, answer : 최신 정보는 요금 페이지를 참조하세요.\"\n",
            "내 로그와 Amazon CloudWatch로 어떤 작업을 수행할 수 있나요?\n",
            "CloudWatch 로그를 사용하면 로그를 모니터링하고 저장할 수 있어 시스템 및 애플리케이션을 이해하고 운영하는 데 도움이 됩니다. 로그와 CloudWatch Logs를 함께 사용하면, 모니터링에 기존 로그 데이터를 사용하므로 코드를 변경할 필요가 없습니다. Amazon CloudWatch와 로그를 사용하여 수행할 수 있는 작업의 두 가지 예는 다음과 같습니다.\n",
            "실시간 애플리케이션 및 시스템 모니터링: CloudWatch Logs를 사용하면 로그 데이터를 이용해 애플리케이션 및 시스템을 거의 실시간으로 모니터링할 수 있습니다. 예를 들어 CloudWatch Logs에서는 애플리케이션 로그에서 발생하는 오류의 수를 추적하고 오류 비율이 지정한 임계값을 초과할 때마다 알림을 전송할 수 있습니다. Amazon CloudWatch는 로그 데이터를 사용해 모니터링하므로 사용자가 코드를 변경할 필요가 없습니다.\n",
            "로그 장기 보존: CloudWatch Logs를 사용하면 하드 드라이브의 용량을 걱정할 필요 없이 안정성이 높고 비용 효율적인 스토리지에 필요한 기간만큼 로그 데이터를 저장할 수 있습니다. CloudWatch Logs 에이전트를 사용하면 순환 로그 파일과 비순환 로그 파일을 모두 호스트에서 로그 서비스로 쉽고 빠르게 이동할 수 있습니다. 그런 다음 필요한 경우 원시 로그 이벤트 데이터에 액세스할 수 있습니다.\n",
            "\"category : CloudWatch, question : 내 로그와 Amazon CloudWatch로 어떤 작업을 수행할 수 있나요?, answer : CloudWatch 로그를 사용하면 로그를 모니터링하고 저장할 수 있어 시스템 및 애플리케이션을 이해하고 운영하는 데 도움이 됩니다. 로그와 CloudWatch Logs를 함께 사용하면, 모니터링에 기존 로그 데이터를 사용하므로 코드를 변경할 필요가 없습니다. Amazon CloudWatch와 로그를 사용하여 수행할 수 있는 작업의 두 가지 예는 다음과 같습니다.\n",
            "실시간 애플리케이션 및 시스템 모니터링: CloudWatch Logs를 사용하면 로그 데이터를 이용해 애플리케이션 및 시스템을 거의 실시간으로 모니터링할 수 있습니다. 예를 들어 CloudWatch Logs에서는 애플리케이션 로그에서 발생하는 오류의 수를 추적하고 오류 비율이 지정한 임계값을 초과할 때마다 알림을 전송할 수 있습니다. Amazon CloudWatch는 로그 데이터를 사용해 모니터링하므로 사용자가 코드를 변경할 필요가 없습니다.\n",
            "로그 장기 보존: CloudWatch Logs를 사용하면 하드 드라이브의 용량을 걱정할 필요 없이 안정성이 높고 비용 효율적인 스토리지에 필요한 기간만큼 로그 데이터를 저장할 수 있습니다. CloudWatch Logs 에이전트를 사용하면 순환 로그 파일과 비순환 로그 파일을 모두 호스트에서 로그 서비스로 쉽고 빠르게 이동할 수 있습니다. 그런 다음 필요한 경우 원시 로그 이벤트 데이터에 액세스할 수 있습니다.\"\n",
            "Microsoft SQL Server 및 Microsoft Windows Server를 실행하는 내 EC2 인스턴스에서 Amazon CloudWatch Logs로 어떤 유형의 데이터를 전송할 수 있나요?\n",
            "EC2Config 서비스에서 CloudWatch로 사용자 정의 텍스트 로그, 이벤트(애플리케이션, 사용자 정의, 보안, 시스템) 로그, 이벤트 추적(ETW) 로그, 성능 카운터(PCW) 데이터 등의 다양한 데이터 및 로그 파일을 전송하도록 구성할 수 있습니다. 여기에서 EC2Config 서비스에 대해 자세히 알아보세요.\n",
            "\"category : CloudWatch, question : Microsoft SQL Server 및 Microsoft Windows Server를 실행하는 내 EC2 인스턴스에서 Amazon CloudWatch Logs로 어떤 유형의 데이터를 전송할 수 있나요?, answer : EC2Config 서비스에서 CloudWatch로 사용자 정의 텍스트 로그, 이벤트(애플리케이션, 사용자 정의, 보안, 시스템) 로그, 이벤트 추적(ETW) 로그, 성능 카운터(PCW) 데이터 등의 다양한 데이터 및 로그 파일을 전송하도록 구성할 수 있습니다. 여기에서 EC2Config 서비스에 대해 자세히 알아보세요.\"\n",
            "CloudWatch Logs 에이전트는 데이터를 얼마나 자주 전송하나요?\n",
            "CloudWatch 로그 에이전트는 기본적으로 5초마다 로그 데이터를 전송하며 전송 빈도는 사용자가 구성할 수 있습니다.\n",
            "\"category : CloudWatch, question : CloudWatch Logs 에이전트는 데이터를 얼마나 자주 전송하나요?, answer : CloudWatch 로그 에이전트는 기본적으로 5초마다 로그 데이터를 전송하며 전송 빈도는 사용자가 구성할 수 있습니다.\"\n",
            "CloudWatch Logs는 어떤 로그 형식을 지원하나요?\n",
            "CloudWatch 로그는 텍스트 기반의 모든 일반적인 로그 데이터 또는 JSON 형식 로그를 수집, 집계 및 모니터링할 수 있습니다.\n",
            "\"category : CloudWatch, question : CloudWatch Logs는 어떤 로그 형식을 지원하나요?, answer : CloudWatch 로그는 텍스트 기반의 모든 일반적인 로그 데이터 또는 JSON 형식 로그를 수집, 집계 및 모니터링할 수 있습니다.\"\n",
            "비 텍스트 로그 데이터를 전송하도록 CloudWatch Logs 에이전트를 구성하면 어떻게 되나요?\n",
            "비텍스트 로그 데이터를 보고하도록 구성된 경우 CloudWatch 로그 에이전트에서는 이를 오류로 기록합니다. 이 오류는 /var/logs/awslogs.log에 기록됩니다.\n",
            "\"category : CloudWatch, question : 비 텍스트 로그 데이터를 전송하도록 CloudWatch Logs 에이전트를 구성하면 어떻게 되나요?, answer : 비텍스트 로그 데이터를 보고하도록 구성된 경우 CloudWatch 로그 에이전트에서는 이를 오류로 기록합니다. 이 오류는 /var/logs/awslogs.log에 기록됩니다.\"\n",
            "CloudWatch Logs를 사용해서 로그를 모니터링하기 시작하려면 어떻게 해야 하나요?\n",
            "지표 필터를 생성하여 CloudWatch 로그로 전송된 로그 이벤트를 모니터링할 수 있습니다. 지표 필터는 그래프를 작성하거나 경보를 생성할 수 있도록 로그 데이터를 Amazon CloudWatch 지표로 변경합니다. 지표 필터는 콘솔 또는 CLI를 사용해 생성할 수 있습니다. 지표 필터는 로그 이벤트에서 일치하는 단어, 구문 또는 값을 검색합니다. 지표 필터가 로그 이벤트에서 단어, 구문 또는 값을 발견하면, 이를 사용자가 선택한 Amazon CloudWatch 지표로 계산합니다. 예를 들어 지표 필터를 생성하여 로그 이벤트에 '오류'라는 단어가 표시되는 경우를 검색하고 수를 파악할 수 있습니다. 지표 필터는 웹 요청에 대한 지연 시간과 같이 공백으로 구분된 로그 이벤트에서도 값을 추출할 수 있습니다. 또한 조건 연산자 및 와일드카드를 사용하여 정확히 일치되는 값을 찾을 수도 있습니다. Amazon CloudWatch 콘솔을 사용하면 지표 필터를 생성하기 전에 패턴을 테스트할 수 있습니다.\n",
            "\"category : CloudWatch, question : CloudWatch Logs를 사용해서 로그를 모니터링하기 시작하려면 어떻게 해야 하나요?, answer : 지표 필터를 생성하여 CloudWatch 로그로 전송된 로그 이벤트를 모니터링할 수 있습니다. 지표 필터는 그래프를 작성하거나 경보를 생성할 수 있도록 로그 데이터를 Amazon CloudWatch 지표로 변경합니다. 지표 필터는 콘솔 또는 CLI를 사용해 생성할 수 있습니다. 지표 필터는 로그 이벤트에서 일치하는 단어, 구문 또는 값을 검색합니다. 지표 필터가 로그 이벤트에서 단어, 구문 또는 값을 발견하면, 이를 사용자가 선택한 Amazon CloudWatch 지표로 계산합니다. 예를 들어 지표 필터를 생성하여 로그 이벤트에 '오류'라는 단어가 표시되는 경우를 검색하고 수를 파악할 수 있습니다. 지표 필터는 웹 요청에 대한 지연 시간과 같이 공백으로 구분된 로그 이벤트에서도 값을 추출할 수 있습니다. 또한 조건 연산자 및 와일드카드를 사용하여 정확히 일치되는 값을 찾을 수도 있습니다. Amazon CloudWatch 콘솔을 사용하면 지표 필터를 생성하기 전에 패턴을 테스트할 수 있습니다.\"\n",
            "지표 필터 패턴의 구문은 무엇인가요?\n",
            "측정치 필터 패턴에는 검색어를 포함하거나 일반적인 로그 또는 JSON 이벤트 형식을 지정할 수 있습니다.\n",
            "예를 들어 Error라는 단어를 검색하려면 지표 필터의 패턴은 Error라는 단어가 됩니다. 여러 검색어를 포함하면 여러 단어를 검색할 수 있습니다. 예를 들어 Error와 Exception이라는 단어가 포함된 이벤트의 수를 파악하려면 Error Exception이라는 패턴을 사용하면 됩니다. 오류 예외라는 단어와 정확하게 일치하는 단어를 검색하려면 \"오류 예외\"라는 검색어를 큰따옴표로 묶으면 됩니다. 검색어는 원하는 만큼 지정할 수 있습니다.\n",
            "CloudWatch 로그는 일반적인 로그 또는 JSON 형식의 로그 이벤트에서 값을 추출하는 데 사용할 수도 있습니다. 예를 들어 Apache 액세스 로그에서 전송된 바이트를 추적할 수 있습니다. 또한 조건 연산자 및 와일드카드를 사용해 관심 있는 데이터를 검색하고 추출할 수 있습니다. 측정치 필터의 추출 기능을 사용하려면 로그 이벤트가 공백으로 구분되어야 하며, 시작 및 종료 큰따옴표 '\"' 또는 시작 대괄호 '[' 및 종료 대괄호 ']'를 사용하여 필드를 둘러싸야 합니다. 아니면 JSON 형식의 로그 이벤트여도 됩니다. 구문에 대한 세부 정보 및 예는 지표 필터에 대한 개발자 안내서를 참조하세요.\n",
            "\"category : CloudWatch, question : 지표 필터 패턴의 구문은 무엇인가요?, answer : 측정치 필터 패턴에는 검색어를 포함하거나 일반적인 로그 또는 JSON 이벤트 형식을 지정할 수 있습니다.\n",
            "예를 들어 Error라는 단어를 검색하려면 지표 필터의 패턴은 Error라는 단어가 됩니다. 여러 검색어를 포함하면 여러 단어를 검색할 수 있습니다. 예를 들어 Error와 Exception이라는 단어가 포함된 이벤트의 수를 파악하려면 Error Exception이라는 패턴을 사용하면 됩니다. 오류 예외라는 단어와 정확하게 일치하는 단어를 검색하려면 \"오류 예외\"라는 검색어를 큰따옴표로 묶으면 됩니다. 검색어는 원하는 만큼 지정할 수 있습니다.\n",
            "CloudWatch 로그는 일반적인 로그 또는 JSON 형식의 로그 이벤트에서 값을 추출하는 데 사용할 수도 있습니다. 예를 들어 Apache 액세스 로그에서 전송된 바이트를 추적할 수 있습니다. 또한 조건 연산자 및 와일드카드를 사용해 관심 있는 데이터를 검색하고 추출할 수 있습니다. 측정치 필터의 추출 기능을 사용하려면 로그 이벤트가 공백으로 구분되어야 하며, 시작 및 종료 큰따옴표 '\"' 또는 시작 대괄호 '[' 및 종료 대괄호 ']'를 사용하여 필드를 둘러싸야 합니다. 아니면 JSON 형식의 로그 이벤트여도 됩니다. 구문에 대한 세부 정보 및 예는 지표 필터에 대한 개발자 안내서를 참조하세요.\"\n",
            "지정한 지표 필터 패턴이 로그 이벤트와 일치하는지 어떻게 알 수 있나요?\n",
            "CloudWatch 로그를 사용하면 지표 필터를 생성하기 전에 지표 필터 패턴을 테스트할 수 있습니다. 이미 CloudWatch 로그에 있는 로그 데이터에 대해 패턴을 테스트하거나 테스트할 로그 이벤트를 제공할 수도 있습니다. 패턴을 테스트하면 지표 필터 패턴과 일치하는 로그 이벤트를 파악할 수 있고 값을 추출하는 경우 테스트 데이터에서 추출되는 값을 확인할 수 있습니다. 지표 필터 테스트 기능은 콘솔 및 CLI에서 사용할 수 있습니다.\n",
            "\"category : CloudWatch, question : 지정한 지표 필터 패턴이 로그 이벤트와 일치하는지 어떻게 알 수 있나요?, answer : CloudWatch 로그를 사용하면 지표 필터를 생성하기 전에 지표 필터 패턴을 테스트할 수 있습니다. 이미 CloudWatch 로그에 있는 로그 데이터에 대해 패턴을 테스트하거나 테스트할 로그 이벤트를 제공할 수도 있습니다. 패턴을 테스트하면 지표 필터 패턴과 일치하는 로그 이벤트를 파악할 수 있고 값을 추출하는 경우 테스트 데이터에서 추출되는 값을 확인할 수 있습니다. 지표 필터 테스트 기능은 콘솔 및 CLI에서 사용할 수 있습니다.\"\n",
            "로그 데이터에 정규 표현식을 사용할 수 있나요?\n",
            "Amazon CloudWatch 지표 필터는 정규 표현식을 지원하지 않습니다. 로그 데이터를 정규 표현식으로 처리하려면, Amazon Kinesis 사용을 검토하여 스트림을 정규 표현식 처리 엔진과 연결하시기 바랍니다.\n",
            "\"category : CloudWatch, question : 로그 데이터에 정규 표현식을 사용할 수 있나요?, answer : Amazon CloudWatch 지표 필터는 정규 표현식을 지원하지 않습니다. 로그 데이터를 정규 표현식으로 처리하려면, Amazon Kinesis 사용을 검토하여 스트림을 정규 표현식 처리 엔진과 연결하시기 바랍니다.\"\n",
            "내 로그 데이터를 가져오려면 어떻게 해야 하나요?\n",
            "CloudWatch 로그 콘솔 또는 CloudWatch 로그 CLI를 통해 모든 로그 데이터를 가져올 수 있습니다. 로그 이벤트는 로그 그룹, 로그 스트림 및 관련 시간에 따라 가져올 수 있습니다. 로그 이벤트를 가져오는 CloudWatch 로그 API는 GetLogEvents입니다.\n",
            "\"category : CloudWatch, question : 내 로그 데이터를 가져오려면 어떻게 해야 하나요?, answer : CloudWatch 로그 콘솔 또는 CloudWatch 로그 CLI를 통해 모든 로그 데이터를 가져올 수 있습니다. 로그 이벤트는 로그 그룹, 로그 스트림 및 관련 시간에 따라 가져올 수 있습니다. 로그 이벤트를 가져오는 CloudWatch 로그 API는 GetLogEvents입니다.\"\n",
            "내 로그를 검색하려면 어떻게 해야 하나요?\n",
            "CLI를 사용해 로그 이벤트를 가져오고 명령줄 grep 또는 이와 유사한 검색 기능을 사용해 검색할 수 있습니다.\n",
            "\"category : CloudWatch, question : 내 로그를 검색하려면 어떻게 해야 하나요?, answer : CLI를 사용해 로그 이벤트를 가져오고 명령줄 grep 또는 이와 유사한 검색 기능을 사용해 검색할 수 있습니다.\"\n",
            "로그 데이터는 CloudWatch Logs에 얼마 동안 저장되나요?\n",
            "원하는 기간만큼 CloudWatch 로그에 로그 데이터를 저장할 수 있습니다. 기본적으로 CloudWatch 로그는 로그 데이터를 무기한으로 저장합니다. 언제든지 로그 그룹별로 보존 기간을 변경할 수 있습니다.\n",
            "\"category : CloudWatch, question : 로그 데이터는 CloudWatch Logs에 얼마 동안 저장되나요?, answer : 원하는 기간만큼 CloudWatch 로그에 로그 데이터를 저장할 수 있습니다. 기본적으로 CloudWatch 로그는 로그 데이터를 무기한으로 저장합니다. 언제든지 로그 그룹별로 보존 기간을 변경할 수 있습니다.\"\n",
            "Amazon CloudWatch Logs Standard란 무엇인가요?\n",
            "Amazon CloudWatch Logs Standard는 CloudWatch에서 제공하는 두 가지 로그 클래스 중 하나입니다. Logs Standard는 Live Tail, 지표 추출, 경보 발령, 데이터 보호와 같은 고급 분석 기능과 실시간 모니터링을 지원하기 위한 포괄적인 로그 관리를 제공합니다. 이 클래스를 사용하면 특정 구문, 값 또는 패턴에 대한 로그를 거의 실시간으로 모니터링할 수 있습니다. 예를 들어 시스템 로그에 발생하는 오류 수에 대한 경보를 설정하거나 애플리케이션 로그에 기록된 웹 요청 지연 시간에 대한 그래프를 볼 수 있습니다. 그런 다음 원본 로그 데이터를 확인하여 문제의 원인을 파악할 수 있습니다.\n",
            "\"category : CloudWatch, question : Amazon CloudWatch Logs Standard란 무엇인가요?, answer : Amazon CloudWatch Logs Standard는 CloudWatch에서 제공하는 두 가지 로그 클래스 중 하나입니다. Logs Standard는 Live Tail, 지표 추출, 경보 발령, 데이터 보호와 같은 고급 분석 기능과 실시간 모니터링을 지원하기 위한 포괄적인 로그 관리를 제공합니다. 이 클래스를 사용하면 특정 구문, 값 또는 패턴에 대한 로그를 거의 실시간으로 모니터링할 수 있습니다. 예를 들어 시스템 로그에 발생하는 오류 수에 대한 경보를 설정하거나 애플리케이션 로그에 기록된 웹 요청 지연 시간에 대한 그래프를 볼 수 있습니다. 그런 다음 원본 로그 데이터를 확인하여 문제의 원인을 파악할 수 있습니다.\"\n",
            "Amazon CloudWatch Logs Infrequent Access(Logs-IA)란 무엇인가요?\n",
            "Amazon CloudWatch Logs Infrequent Access(Logs-IA)는 CloudWatch에서 제공하는 두 가지 로그 클래스 중 하나입니다. Logs-IA는 AWS의 모든 로그를 기본적으로 통합하기 위해 특별히 구축되었습니다. 저렴한 GB당 수집 요금으로 관리형 수집, 교차 계정 로그 분석, CloudWatch Logs Standard의 암호화를 사용할 수 있습니다. CloudWatch Logs-IA는 맞춤화된 기능을 저렴한 비용으로 사용할 수 있다는 점에서 임시 쿼리와 사후 포렌식 분석에 적합합니다. 로그 데이터는 안정성이 높고 저렴한 스토리지에 무기한으로 저장 및 액세스할 수 있으므로 하드 드라이브의 용량을 걱정하지 않아도 됩니다.\n",
            "\"category : CloudWatch, question : Amazon CloudWatch Logs Infrequent Access(Logs-IA)란 무엇인가요?, answer : Amazon CloudWatch Logs Infrequent Access(Logs-IA)는 CloudWatch에서 제공하는 두 가지 로그 클래스 중 하나입니다. Logs-IA는 AWS의 모든 로그를 기본적으로 통합하기 위해 특별히 구축되었습니다. 저렴한 GB당 수집 요금으로 관리형 수집, 교차 계정 로그 분석, CloudWatch Logs Standard의 암호화를 사용할 수 있습니다. CloudWatch Logs-IA는 맞춤화된 기능을 저렴한 비용으로 사용할 수 있다는 점에서 임시 쿼리와 사후 포렌식 분석에 적합합니다. 로그 데이터는 안정성이 높고 저렴한 스토리지에 무기한으로 저장 및 액세스할 수 있으므로 하드 드라이브의 용량을 걱정하지 않아도 됩니다.\"\n",
            "Amazon CloudWatch Logs Infrequent Access(Logs-IA)를 활성화하려면 어떻게 해야 하나요?\n",
            "Amazon CloudWatch Logs Infrequent Access(Logs-IA)는 CloudWatch Logs가 제공되는 모든 AWS 리전에서 사용 가능합니다. 콘솔에서 시작하거나 AWS CLI 또는 API를 통해 프로그래밍 방식으로 시작할 수 있습니다.\n",
            "\"category : CloudWatch, question : Amazon CloudWatch Logs Infrequent Access(Logs-IA)를 활성화하려면 어떻게 해야 하나요?, answer : Amazon CloudWatch Logs Infrequent Access(Logs-IA)는 CloudWatch Logs가 제공되는 모든 AWS 리전에서 사용 가능합니다. 콘솔에서 시작하거나 AWS CLI 또는 API를 통해 프로그래밍 방식으로 시작할 수 있습니다.\"\n",
            "Logs Insights에 액세스하려면 어떤 권한이 필요한가요?\n",
            "Logs Insights에 액세스하려면 IAM 정책이 logs:DescribeLogGroups 및 logs:FilterLogEvents에 대한 권한을 포함해야 합니다.\n",
            "\"category : CloudWatch, question : Logs Insights에 액세스하려면 어떤 권한이 필요한가요?, answer : Logs Insights에 액세스하려면 IAM 정책이 logs:DescribeLogGroups 및 logs:FilterLogEvents에 대한 권한을 포함해야 합니다.\"\n",
            "CloudWatch Logs Insights에서 어떤 로그를 쿼리할 수 있나요?\n",
            "Logs Insights를 사용하여 CloudWatch로 보내는 모든 로그를 쿼리할 수 있습니다. Logs Insights는 AWS 서비스(예: Lambda, CloudTrail, Route53, VPC Flow Logs)의 로그와 JSON 형식에서 로그 이벤트를 생성하는 애플리케이션 로그에서 로그 필드를 자동으로 검색합니다. 또한 모든 로그 유형에 대해 CloudWatch로 보내는 모든 로그에 3개의 시스템 필드 @message, @logStream, @timestamp를 생성합니다. @message는 구문 분석되지 않은 원시 로그 이벤트를 포함하고, @logStream은 로그 이벤트를 생성한 원본의 이름을 포함하며, @timestamp는 로그 이벤트가 CloudWatch에 추가된 시간을 포함합니다.\n",
            "\"category : CloudWatch, question : CloudWatch Logs Insights에서 어떤 로그를 쿼리할 수 있나요?, answer : Logs Insights를 사용하여 CloudWatch로 보내는 모든 로그를 쿼리할 수 있습니다. Logs Insights는 AWS 서비스(예: Lambda, CloudTrail, Route53, VPC Flow Logs)의 로그와 JSON 형식에서 로그 이벤트를 생성하는 애플리케이션 로그에서 로그 필드를 자동으로 검색합니다. 또한 모든 로그 유형에 대해 CloudWatch로 보내는 모든 로그에 3개의 시스템 필드 @message, @logStream, @timestamp를 생성합니다. @message는 구문 분석되지 않은 원시 로그 이벤트를 포함하고, @logStream은 로그 이벤트를 생성한 원본의 이름을 포함하며, @timestamp는 로그 이벤트가 CloudWatch에 추가된 시간을 포함합니다.\"\n",
            "CloudWatch Logs Insights에서는 어떤 쿼리 언어를 지원하나요?\n",
            "Logs Insights는 로그 처리를 위해 특별히 구축된 새로운 쿼리 언어를 도입하였습니다. 쿼리 언어는 단순하지만 강력한 몇 가지 쿼리 명령을 지원합니다. 하나 이상의 로그 필드를 검색하고, 하나 이상의 검색 기준과 일치하는 로그 이벤트를 찾고, 로그 데이터를 집계하고, 텍스트 기반 로그에서 단기 필드를 추출하는 명령을 작성할 수 있습니다. 쿼리 언어는 배우기 쉬우며, Logs Insights는 시작하는 데 도움이 되는 샘플 쿼리, 명령 설명, 쿼리 자동 완성 기능의 양식으로 제품 내 도움말을 제공합니다. 여기에서 쿼리 언어에 대한 자세한 내용을 확인할 수 있습니다.\n",
            "\"category : CloudWatch, question : CloudWatch Logs Insights에서는 어떤 쿼리 언어를 지원하나요?, answer : Logs Insights는 로그 처리를 위해 특별히 구축된 새로운 쿼리 언어를 도입하였습니다. 쿼리 언어는 단순하지만 강력한 몇 가지 쿼리 명령을 지원합니다. 하나 이상의 로그 필드를 검색하고, 하나 이상의 검색 기준과 일치하는 로그 이벤트를 찾고, 로그 데이터를 집계하고, 텍스트 기반 로그에서 단기 필드를 추출하는 명령을 작성할 수 있습니다. 쿼리 언어는 배우기 쉬우며, Logs Insights는 시작하는 데 도움이 되는 샘플 쿼리, 명령 설명, 쿼리 자동 완성 기능의 양식으로 제품 내 도움말을 제공합니다. 여기에서 쿼리 언어에 대한 자세한 내용을 확인할 수 있습니다.\"\n",
            "CloudWatch Logs Insights에서 서비스 한도는 어떻게 되나요?\n",
            "서비스 한도는 여기에 나와 있습니다.\n",
            "\"category : CloudWatch, question : CloudWatch Logs Insights에서 서비스 한도는 어떻게 되나요?, answer : 서비스 한도는 여기에 나와 있습니다.\"\n",
            "CloudWatch Logs Insights를 사용할 수 있는 리전은 어디인가요?\n",
            "Log Insights는 미국 서부(오레곤), 미국 서부(캘리포니아 북부), 미국 동부(오하이오), 미국 동부(버지니아 북부), 아시아 태평양(뭄바이), 아시아 태평양(서울), 아시아 태평양(싱가포르), 아시아 태평양(시드니), 아시아 태평양(도쿄), 캐나다(중부) EU(프랑크푸르트), EU(아일랜드), EU(파리) 및 남아메리카(상파울루)에서 사용할 수 있습니다.\n",
            "\"category : CloudWatch, question : CloudWatch Logs Insights를 사용할 수 있는 리전은 어디인가요?, answer : Log Insights는 미국 서부(오레곤), 미국 서부(캘리포니아 북부), 미국 동부(오하이오), 미국 동부(버지니아 북부), 아시아 태평양(뭄바이), 아시아 태평양(서울), 아시아 태평양(싱가포르), 아시아 태평양(시드니), 아시아 태평양(도쿄), 캐나다(중부) EU(프랑크푸르트), EU(아일랜드), EU(파리) 및 남아메리카(상파울루)에서 사용할 수 있습니다.\"\n",
            "CloudWatch Logs Insights에서는 어떤 쿼리 유형을 지원하나요?\n",
            "집계, 필터, 정규 표현식, 텍스트 검색을 포함하는 쿼리를 작성할 수 있습니다. 또한 로그 이벤트에서 데이터를 추출하여 단기 필드를 생성할 수 있습니다. 그러면 찾으려는 정보에 액세스할 수 있도록 쿼리 언어에서 추가로 처리할 수 있습니다. 쿼리 언어는 문자열, 숫자, 수학 함수(예: concat, strlen, trim, log, sqrt 등)를 지원합니다. 또한 Boolean 및 논리 표현식과 집계 함수(예: min, max, sum, average, percentile 등)를 사용할 수도 있습니다. 여기에서 쿼리 언어 및 지원되는 함수에 대한 자세한 내용을 확인할 수 있습니다.\n",
            "\"category : CloudWatch, question : CloudWatch Logs Insights에서는 어떤 쿼리 유형을 지원하나요?, answer : 집계, 필터, 정규 표현식, 텍스트 검색을 포함하는 쿼리를 작성할 수 있습니다. 또한 로그 이벤트에서 데이터를 추출하여 단기 필드를 생성할 수 있습니다. 그러면 찾으려는 정보에 액세스할 수 있도록 쿼리 언어에서 추가로 처리할 수 있습니다. 쿼리 언어는 문자열, 숫자, 수학 함수(예: concat, strlen, trim, log, sqrt 등)를 지원합니다. 또한 Boolean 및 논리 표현식과 집계 함수(예: min, max, sum, average, percentile 등)를 사용할 수도 있습니다. 여기에서 쿼리 언어 및 지원되는 함수에 대한 자세한 내용을 확인할 수 있습니다.\"\n",
            "CloudWatch Logs Insights에서 어떤 쿼리 명령과 함수를 사용할 수 있나요?\n",
            "여기에서 쿼리 명령 목록을 찾을 수 있습니다. 여기에서 지원되는 함수 목록을 찾을 수 있습니다.\n",
            "\"category : CloudWatch, question : CloudWatch Logs Insights에서 어떤 쿼리 명령과 함수를 사용할 수 있나요?, answer : 여기에서 쿼리 명령 목록을 찾을 수 있습니다. 여기에서 지원되는 함수 목록을 찾을 수 있습니다.\"\n",
            "CloudWatch Logs Insights에서 어떤 데이터 시각화를 사용할 수 있나요?\n",
            "또한 시각화를 사용하여 로그에서 시간에 따라 나타나는 추세와 패턴을 식별할 수 있습니다. Logs Insights는 선형 차트와 누적 영역형 차트를 사용하여 데이터 시각화를 지원합니다. 그리고 하나 이상의 집계 함수를 포함하는 모든 쿼리에 대한 시각화를 생성합니다. 여기에서 데이터는 bin() 함수를 사용하여 지정된 기간에 따라 그룹화됩니다. 여기에서 시계열 데이터 시각화에 대한 자세한 내용을 확인할 수 있습니다.\n",
            "\"category : CloudWatch, question : CloudWatch Logs Insights에서 어떤 데이터 시각화를 사용할 수 있나요?, answer : 또한 시각화를 사용하여 로그에서 시간에 따라 나타나는 추세와 패턴을 식별할 수 있습니다. Logs Insights는 선형 차트와 누적 영역형 차트를 사용하여 데이터 시각화를 지원합니다. 그리고 하나 이상의 집계 함수를 포함하는 모든 쿼리에 대한 시각화를 생성합니다. 여기에서 데이터는 bin() 함수를 사용하여 지정된 기간에 따라 그룹화됩니다. 여기에서 시계열 데이터 시각화에 대한 자세한 내용을 확인할 수 있습니다.\"\n",
            "CloudWatch Logs Insights에서 정규 표현식을 사용할 수 있나요?\n",
            "Logs Insights에서는 Java 스타일의 정규 표현식을 사용할 수 있습니다. 정규 표현식은 필터 명령에서 사용할 수 있습니다. 제품 내 도움말이나 여기에서 정규 표현식을 사용하는 쿼리 예제를 찾을 수 있습니다.\n",
            "\"category : CloudWatch, question : CloudWatch Logs Insights에서 정규 표현식을 사용할 수 있나요?, answer : Logs Insights에서는 Java 스타일의 정규 표현식을 사용할 수 있습니다. 정규 표현식은 필터 명령에서 사용할 수 있습니다. 제품 내 도움말이나 여기에서 정규 표현식을 사용하는 쿼리 예제를 찾을 수 있습니다.\"\n",
            "CloudWatch Logs Insights 쿼리에서 특수 문자를 이스케이프 문자로 처리하려면 어떻게 해야 하나요?\n",
            "백틱을 사용하여 특수 문자를 이스케이프 문자로 처리할 수 있습니다. 영숫자, @, 마침표(.)를 제외한 문자를 포함하는 로그 필드 이름은 백틱을 사용하여 이스케이프 문자로 처리해야 합니다.\n",
            "\"category : CloudWatch, question : CloudWatch Logs Insights 쿼리에서 특수 문자를 이스케이프 문자로 처리하려면 어떻게 해야 하나요?, answer : 백틱을 사용하여 특수 문자를 이스케이프 문자로 처리할 수 있습니다. 영숫자, @, 마침표(.)를 제외한 문자를 포함하는 로그 필드 이름은 백틱을 사용하여 이스케이프 문자로 처리해야 합니다.\"\n",
            "특정 로그 필드에만 ‘@’ 기호가 있는 이유는 무엇인가요?\n",
            "Logs Insights에서 생성된 시스템 필드는 @ 기호로 시작합니다. Logs Insights는 현재 3개의 시스템 필드를 생성합니다. @message는 CloudWatch로 보내는 구문 분석되지 않은 원시 로그 이벤트를 포함하며, @logStream은 로그 이벤트를 생성한 원본의 이름을 포함하며, @timestamp는 로그 이벤트가 CloudWatch에 추가된 시간을 포함합니다.\n",
            "\"category : CloudWatch, question : 특정 로그 필드에만 ‘@’ 기호가 있는 이유는 무엇인가요?, answer : Logs Insights에서 생성된 시스템 필드는 @ 기호로 시작합니다. Logs Insights는 현재 3개의 시스템 필드를 생성합니다. @message는 CloudWatch로 보내는 구문 분석되지 않은 원시 로그 이벤트를 포함하며, @logStream은 로그 이벤트를 생성한 원본의 이름을 포함하며, @timestamp는 로그 이벤트가 CloudWatch에 추가된 시간을 포함합니다.\"\n",
            "CloudWatch Logs Insights에서 내역 로그를 쿼리할 수 있나요?\n",
            "Logs Insights에서는 2018년 11월 5일 이후 또는 CloudWatch Logs에 추가된 로그 데이터를 쿼리할 수 있습니다.\n",
            "\"category : CloudWatch, question : CloudWatch Logs Insights에서 내역 로그를 쿼리할 수 있나요?, answer : Logs Insights에서는 2018년 11월 5일 이후 또는 CloudWatch Logs에 추가된 로그 데이터를 쿼리할 수 있습니다.\"\n",
            "특정 로그 스트림에서 로그 이벤트를 검색할 수 있나요?\n",
            "쿼리 명령 필터 @logStream = \"log_stream_name\" 을 로그 쿼리에 추가하여 특정 로그 스트림에서 로그 이벤트를 검색할 수 있습니다.\n",
            "\"category : CloudWatch, question : 특정 로그 스트림에서 로그 이벤트를 검색할 수 있나요?, answer : 쿼리 명령 필터 @logStream = \"log_stream_name\" 을 로그 쿼리에 추가하여 특정 로그 스트림에서 로그 이벤트를 검색할 수 있습니다.\"\n",
            "오늘 AWS 파트너 ISV 솔루션을 사용하여 CloudWatch에서 내 로그를 분석하였습니다. CloudWatch Logs Insights는 내게 어떤 변화를 가져오나요?\n",
            "CloudWatch Logs는 이미 다른 AWS 서비스(예: Amazon Kinesis, Amazon Kinesis Data Firehose, Amazon Elasticsearch) 및 AWS Partner ISV 솔루션(예: Splunk, Sumo Logic, DataDog 등)과의 통합 옵션을 지원하여 사용자 정의 로그 처리, 보강, 분석, 시각화 요구를 위해 모든 환경에서 탄력성과 옵션을 제공합니다. 또한 CloudWatch Logs Insights의 쿼리 기능은 AWS SDK를 통한 프로그래밍 액세스에 사용할 수 있으며, 이를 통해 AWS ISV Partners는 보다 긴밀한 통합과 고급 분석을 구축하고 CloudWatch Logs Insights에서 추가 가치를 창출할 수 있습니다.\n",
            "\"category : CloudWatch, question : 오늘 AWS 파트너 ISV 솔루션을 사용하여 CloudWatch에서 내 로그를 분석하였습니다. CloudWatch Logs Insights는 내게 어떤 변화를 가져오나요?, answer : CloudWatch Logs는 이미 다른 AWS 서비스(예: Amazon Kinesis, Amazon Kinesis Data Firehose, Amazon Elasticsearch) 및 AWS Partner ISV 솔루션(예: Splunk, Sumo Logic, DataDog 등)과의 통합 옵션을 지원하여 사용자 정의 로그 처리, 보강, 분석, 시각화 요구를 위해 모든 환경에서 탄력성과 옵션을 제공합니다. 또한 CloudWatch Logs Insights의 쿼리 기능은 AWS SDK를 통한 프로그래밍 액세스에 사용할 수 있으며, 이를 통해 AWS ISV Partners는 보다 긴밀한 통합과 고급 분석을 구축하고 CloudWatch Logs Insights에서 추가 가치를 창출할 수 있습니다.\"\n",
            "AWS ISV Partner 솔루션을 통해 CloudWatch Logs Insights의 쿼리 기능에 액세스하면 어떤 장점이 있나요?\n",
            "CloudWatch Logs Insights와 ISV Partner의 통합을 통해 로그 데이터를 한 곳에 모을 수 있으며, 많은 데이터를 보유하지 않고도 경제적이고 성능이 뛰어난 방식으로 원하는 도구와 프레임워크를 사용하여 분석할 수 있습니다. 또한 연결된 데이터 전송 지연을 없애 로그에 더 빨리 액세스할 수 있으며, 특정 데이터 전송을 구성하고 관리하며 발생하는 운영 복잡성을 없앨 수 있습니다.\n",
            "\"category : CloudWatch, question : AWS ISV Partner 솔루션을 통해 CloudWatch Logs Insights의 쿼리 기능에 액세스하면 어떤 장점이 있나요?, answer : CloudWatch Logs Insights와 ISV Partner의 통합을 통해 로그 데이터를 한 곳에 모을 수 있으며, 많은 데이터를 보유하지 않고도 경제적이고 성능이 뛰어난 방식으로 원하는 도구와 프레임워크를 사용하여 분석할 수 있습니다. 또한 연결된 데이터 전송 지연을 없애 로그에 더 빨리 액세스할 수 있으며, 특정 데이터 전송을 구성하고 관리하며 발생하는 운영 복잡성을 없앨 수 있습니다.\"\n",
            "Amazon CloudWatch Logs Live Tail이란 무엇인가요?\n",
            "Amazon CloudWatch Logs Live Tail은 수신 로그를 실시간으로 볼 수 있는 새로운 대화형 분석 기능입니다. Live Tail을 사용하면 문제를 신속하게 해결할 수 있습니다. 개발자는 로그의 스트리밍 뷰를 활용하여 코드를 디버깅할 수 있고 IT 엔지니어는 배포 상태를 안정적으로 모니터링할 수 있습니다. Live Tail은 관련 이벤트의 컨텍스트에 대한 실시간 대화형 로그 보기를 제공하여 평균 탐지 시간을 줄이고 결과적으로 평균 해결 시간을 단축하는 데 도움이 됩니다.\n",
            "\"category : CloudWatch, question : Amazon CloudWatch Logs Live Tail이란 무엇인가요?, answer : Amazon CloudWatch Logs Live Tail은 수신 로그를 실시간으로 볼 수 있는 새로운 대화형 분석 기능입니다. Live Tail을 사용하면 문제를 신속하게 해결할 수 있습니다. 개발자는 로그의 스트리밍 뷰를 활용하여 코드를 디버깅할 수 있고 IT 엔지니어는 배포 상태를 안정적으로 모니터링할 수 있습니다. Live Tail은 관련 이벤트의 컨텍스트에 대한 실시간 대화형 로그 보기를 제공하여 평균 탐지 시간을 줄이고 결과적으로 평균 해결 시간을 단축하는 데 도움이 됩니다.\"\n",
            "Cloudwatch Logs Live Tail을 사용해야 하는 이유는 무엇인가요?\n",
            "네이티브 AWS Observability 도구 내에서 애플리케이션 또는 배포 문제를 즉시 감지하려면 대화형 CloudWatch Live Tail 기능을 사용해야 합니다. Live Tail을 사용하면 DevOps 팀이 여러 도구 간에 전환하지 않고도 개발 환경 내에서 중요한 애플리케이션 로그와 디버그 코드를 심층적으로 파악할 수 있습니다. Live Tail을 사용하여 배포 상태 및 상태를 모니터링하면 IT 엔지니어, 운영 지원 및 중앙 보안 팀이 서비스 및 애플리케이션을 효율적으로 모니터링하여 근본 원인 분석을 가속화하고 평균 해결 시간을 단축할 수 있습니다.\n",
            "\"category : CloudWatch, question : Cloudwatch Logs Live Tail을 사용해야 하는 이유는 무엇인가요?, answer : 네이티브 AWS Observability 도구 내에서 애플리케이션 또는 배포 문제를 즉시 감지하려면 대화형 CloudWatch Live Tail 기능을 사용해야 합니다. Live Tail을 사용하면 DevOps 팀이 여러 도구 간에 전환하지 않고도 개발 환경 내에서 중요한 애플리케이션 로그와 디버그 코드를 심층적으로 파악할 수 있습니다. Live Tail을 사용하여 배포 상태 및 상태를 모니터링하면 IT 엔지니어, 운영 지원 및 중앙 보안 팀이 서비스 및 애플리케이션을 효율적으로 모니터링하여 근본 원인 분석을 가속화하고 평균 해결 시간을 단축할 수 있습니다.\"\n",
            "Live Tail은 다른 AWS 또는 타사 서비스와 어떻게 통합되거나 상호 작용하나요?\n",
            "Live Tail은 사용자 지정 애플리케이션 로그에 Live Tail 기능을 제공하는 것 외에도 고객이 Amazon Virtual Private Cloud, Amazon Route53, AWS Lambda, Amazon Elastic Kubernetes 서비스, Amazon Elastic 컨테이너 서비스 등을 비롯한 AWS 서비스를 통해 로그에 대한 심층적인 통찰력을 얻을 수 있도록 지원합니다. AWS 서비스는 Live Tail 위젯을 사용하여 동일한 대화형 라이브 테일링 경험을 콘솔에 내장할 수 있습니다. 또한 다른 서비스(예: Amazon Managed Grafana, AWS Thinkbox)를 통해 직접 통합을 구현하여 자체 콘솔 및 로그 이벤트를 생성하는 모든 애플리케이션 로그 내에서 동일한 심층 분석 기능을 제공할 수도 있습니다.\n",
            "\"category : CloudWatch, question : Live Tail은 다른 AWS 또는 타사 서비스와 어떻게 통합되거나 상호 작용하나요?, answer : Live Tail은 사용자 지정 애플리케이션 로그에 Live Tail 기능을 제공하는 것 외에도 고객이 Amazon Virtual Private Cloud, Amazon Route53, AWS Lambda, Amazon Elastic Kubernetes 서비스, Amazon Elastic 컨테이너 서비스 등을 비롯한 AWS 서비스를 통해 로그에 대한 심층적인 통찰력을 얻을 수 있도록 지원합니다. AWS 서비스는 Live Tail 위젯을 사용하여 동일한 대화형 라이브 테일링 경험을 콘솔에 내장할 수 있습니다. 또한 다른 서비스(예: Amazon Managed Grafana, AWS Thinkbox)를 통해 직접 통합을 구현하여 자체 콘솔 및 로그 이벤트를 생성하는 모든 애플리케이션 로그 내에서 동일한 심층 분석 기능을 제공할 수도 있습니다.\"\n",
            "Live Tail에 액세스하려면 어떤 권한이 필요한가요?\n",
            "이 기능이 의도한 대로 작동하려면 사용자에게 다음 작업을 허용해야 합니다. Live Tail 세션을 시작할 때 관리자 역할의 일부가 아니거나 로그: *를 포함하는 정책이 있는 경우 정책 설명에 logs:StartLiveTail 및 logs:StopLiveTail과 같은 작업을 추가해야 합니다.\n",
            "\"category : CloudWatch, question : Live Tail에 액세스하려면 어떤 권한이 필요한가요?, answer : 이 기능이 의도한 대로 작동하려면 사용자에게 다음 작업을 허용해야 합니다. Live Tail 세션을 시작할 때 관리자 역할의 일부가 아니거나 로그: *를 포함하는 정책이 있는 경우 정책 설명에 logs:StartLiveTail 및 logs:StopLiveTail과 같은 작업을 추가해야 합니다.\"\n",
            "Live Tail의 서비스 한도는 어떻게 되나요?\n",
            "Live Tail 서비스 한도에 대해 자세히 알아보세요.\n",
            "\"category : CloudWatch, question : Live Tail의 서비스 한도는 어떻게 되나요?, answer : Live Tail 서비스 한도에 대해 자세히 알아보세요.\"\n",
            "Live Tail을 사용할 수 있는 AWS 리전은 어디인가요?\n",
            "Live Tail은 미국 동부(오하이오), 미국 동부(버지니아 북부), 미국 서부(캘리포니아 북부), 미국 서부(오레곤), 아시아 태평양(뭄바이), 아시아 태평양(서울), 아시아 태평양(싱가포르) 아시아 태평양(시드니), 아시아 태평양(도쿄), 캐나다(중부), EU(프랑크푸르트), EU(아일랜드), EU(런던), EU(파리) 및 남아메리카(상파울루) 리전에서 사용할 수 있습니다.\n",
            "\"category : CloudWatch, question : Live Tail을 사용할 수 있는 AWS 리전은 어디인가요?, answer : Live Tail은 미국 동부(오하이오), 미국 동부(버지니아 북부), 미국 서부(캘리포니아 북부), 미국 서부(오레곤), 아시아 태평양(뭄바이), 아시아 태평양(서울), 아시아 태평양(싱가포르) 아시아 태평양(시드니), 아시아 태평양(도쿄), 캐나다(중부), EU(프랑크푸르트), EU(아일랜드), EU(런던), EU(파리) 및 남아메리카(상파울루) 리전에서 사용할 수 있습니다.\"\n",
            "CloudWatch Logs Live Tail은 어떤 유형의 필터링을 지원하나요?\n",
            "로그 그룹, 로그 스트림을 기준으로 필터링하고 키워드로 필터링할 수 있습니다. 로그 그룹 선택은 모니터링 계정에 있을 때 여러 계정에서 여러 항목을 선택할 수 있도록 지원합니다(크로스 계정 관측성). 로그 스트림 선택은 이름 또는 접두사에 따른 다중 선택을 지원합니다. 키워드별 필터링은 대소문자를 구분합니다. 하나 이상의 키워드(예: 오류, 예외 또는 오류)를 입력하여 검색 범위를 더 좁힐 수 있습니다. 키워드를 입력하거나 정보 패널에 제공된 샘플을 복사하여 붙여넣을 수 있습니다. 필터 패턴에 대해 자세히 알아보세요.\n",
            "\"category : CloudWatch, question : CloudWatch Logs Live Tail은 어떤 유형의 필터링을 지원하나요?, answer : 로그 그룹, 로그 스트림을 기준으로 필터링하고 키워드로 필터링할 수 있습니다. 로그 그룹 선택은 모니터링 계정에 있을 때 여러 계정에서 여러 항목을 선택할 수 있도록 지원합니다(크로스 계정 관측성). 로그 스트림 선택은 이름 또는 접두사에 따른 다중 선택을 지원합니다. 키워드별 필터링은 대소문자를 구분합니다. 하나 이상의 키워드(예: 오류, 예외 또는 오류)를 입력하여 검색 범위를 더 좁힐 수 있습니다. 키워드를 입력하거나 정보 패널에 제공된 샘플을 복사하여 붙여넣을 수 있습니다. 필터 패턴에 대해 자세히 알아보세요.\"\n",
            "Live Tail로 과거 로그를 볼 수 있나요?\n",
            "아니요. Live Tail에서는 CloudWatch에서 수집한 로그 데이터를 실시간으로 볼 수 있습니다. 과거 로그는 로그 인사이트 및 로그 그룹 기능을 참조하세요.\n",
            "\"category : CloudWatch, question : Live Tail로 과거 로그를 볼 수 있나요?, answer : 아니요. Live Tail에서는 CloudWatch에서 수집한 로그 데이터를 실시간으로 볼 수 있습니다. 과거 로그는 로그 인사이트 및 로그 그룹 기능을 참조하세요.\"\n",
            "Amazon CloudWatch Logs의 데이터 보호란 무엇인가요?\n",
            "데이터 보호는 CloudWatch Logs의 기능 중 하나입니다. 데이터 보호를 사용하면 시스템 및 애플리케이션에서 수집되는 로그 안의 민감한 데이터를 자동으로 검색하고 마스킹하는 자체 규칙 및 정책을 정의할 수 있습니다. 이 작업은 기계 학습(ML) 및 패턴 일치를 사용하여 수행됩니다. 데이터를 봐야 하는 경우 Identity and Access Management(IAM) 권한을 승격하여 데이터의 마스킹을 해제할 수 있습니다.\n",
            "\"category : CloudWatch, question : Amazon CloudWatch Logs의 데이터 보호란 무엇인가요?, answer : 데이터 보호는 CloudWatch Logs의 기능 중 하나입니다. 데이터 보호를 사용하면 시스템 및 애플리케이션에서 수집되는 로그 안의 민감한 데이터를 자동으로 검색하고 마스킹하는 자체 규칙 및 정책을 정의할 수 있습니다. 이 작업은 기계 학습(ML) 및 패턴 일치를 사용하여 수행됩니다. 데이터를 봐야 하는 경우 Identity and Access Management(IAM) 권한을 승격하여 데이터의 마스킹을 해제할 수 있습니다.\"\n",
            "CloudWatch Logs의 데이터 보호를 사용해야 하는 이유는 무엇인가요?\n",
            "민감한 데이터가 로깅되는 것을 방지하기 위해 고객은 수동 조사에 의존하거나 로그를 삭제하는 간략한 로그 보존 정책을 구성하는데, 이렇게 하면 귀중한 운영 로그가 손실될 위험이 증가합니다. CloudWatch Logs 데이터 보호는 로그에 액세스할 필요 없이 패턴 일치 및 ML을 사용하여 로그의 민감한 정보를 자동으로 식별하고 마스킹합니다. 이 기능은 개인 정보를 저장하지 않아야 하는 엄격한 규제를 받는 산업에 유용합니다. 또한 다량의 개인 정보 및 민감한 정보가 필요한 결제 또는 인증 서비스를 구축하는 고객은 이 새로운 기능을 사용하여 정보가 불필요하게 로그에 저장될 가능성을 낮출 수 있습니다.\n",
            "\"category : CloudWatch, question : CloudWatch Logs의 데이터 보호를 사용해야 하는 이유는 무엇인가요?, answer : 민감한 데이터가 로깅되는 것을 방지하기 위해 고객은 수동 조사에 의존하거나 로그를 삭제하는 간략한 로그 보존 정책을 구성하는데, 이렇게 하면 귀중한 운영 로그가 손실될 위험이 증가합니다. CloudWatch Logs 데이터 보호는 로그에 액세스할 필요 없이 패턴 일치 및 ML을 사용하여 로그의 민감한 정보를 자동으로 식별하고 마스킹합니다. 이 기능은 개인 정보를 저장하지 않아야 하는 엄격한 규제를 받는 산업에 유용합니다. 또한 다량의 개인 정보 및 민감한 정보가 필요한 결제 또는 인증 서비스를 구축하는 고객은 이 새로운 기능을 사용하여 정보가 불필요하게 로그에 저장될 가능성을 낮출 수 있습니다.\"\n",
            "CloudWatch Logs에서 어떤 종류의 민감한 데이터를 보호할 수 있나요?\n",
            "CloudWatch Logs에서 데이터 보호 정책을 생성할 때 보호할 데이터를 지정할 수 있습니다. 이메일 주소, 여러 국가의 운전 면허증, 신용카드 번호, 주소와 같은 여러 데이터 식별자 중에서 선택할 수 있습니다. 다양한 대상 데이터 식별자가 제공되므로 애플리케이션에 사용되는 민감한 데이터를 유연하게 선택하고 쉽게 액세스할 수 없어야 하는 민감한 데이터를 마스킹할 수 있습니다. 애플리케이션에 민감한 정보가 어떤 것인지 결정하고 사용 사례와 관련된 식별자를 선택하는 것이 중요합니다.\n",
            "\"category : CloudWatch, question : CloudWatch Logs에서 어떤 종류의 민감한 데이터를 보호할 수 있나요?, answer : CloudWatch Logs에서 데이터 보호 정책을 생성할 때 보호할 데이터를 지정할 수 있습니다. 이메일 주소, 여러 국가의 운전 면허증, 신용카드 번호, 주소와 같은 여러 데이터 식별자 중에서 선택할 수 있습니다. 다양한 대상 데이터 식별자가 제공되므로 애플리케이션에 사용되는 민감한 데이터를 유연하게 선택하고 쉽게 액세스할 수 없어야 하는 민감한 데이터를 마스킹할 수 있습니다. 애플리케이션에 민감한 정보가 어떤 것인지 결정하고 사용 사례와 관련된 식별자를 선택하는 것이 중요합니다.\"\n",
            "어떤 유형의 CloudWatch 경보를 생성할 수 있나요?\n",
            "경보를 생성하여 계정에 있는 어떤 Amazon CloudWatch 지표든 모니터링할 수 있습니다. 예를 들어 Amazon EC2 인스턴스 CPU 사용률, Amazon ELB 요청 지연 시간, Amazon DynamoDB 테이블 처리량, Amazon SQS 대기열 길이에 대한 경보는 물론, AWS 청구서 요금에 대한 경보도 만들 수 있습니다.\n",
            "또한, 사용자 지정 애플리케이션 또는 인프라에 특정된 사용자 지정 지표에 대한 경보도 생성할 수 있습니다. 사용자 지정 지표가 고단위 지표인 경우, 10초 또는 30초 간격으로 알리도록 고단위 경보를 생성할 수 있습니다.\n",
            "복합 경보를 사용하면 여러 경보를 경보 계층 구조로 결합할 수 있습니다. 이렇게 하면 여러 경보가 동시에 울릴 때 한 번만 트리거되어 경보 소음이 줄어듭니다. 애플리케이션, AWS 리전 또는 가용 영역과 같은 리소스 그룹의 전반적인 상태를 확인할 수 있습니다.\n",
            "자세한 내용은 CloudWatch 요금 페이지를 참조하세요.\n",
            "\"category : CloudWatch, question : 어떤 유형의 CloudWatch 경보를 생성할 수 있나요?, answer : 경보를 생성하여 계정에 있는 어떤 Amazon CloudWatch 지표든 모니터링할 수 있습니다. 예를 들어 Amazon EC2 인스턴스 CPU 사용률, Amazon ELB 요청 지연 시간, Amazon DynamoDB 테이블 처리량, Amazon SQS 대기열 길이에 대한 경보는 물론, AWS 청구서 요금에 대한 경보도 만들 수 있습니다.\n",
            "또한, 사용자 지정 애플리케이션 또는 인프라에 특정된 사용자 지정 지표에 대한 경보도 생성할 수 있습니다. 사용자 지정 지표가 고단위 지표인 경우, 10초 또는 30초 간격으로 알리도록 고단위 경보를 생성할 수 있습니다.\n",
            "복합 경보를 사용하면 여러 경보를 경보 계층 구조로 결합할 수 있습니다. 이렇게 하면 여러 경보가 동시에 울릴 때 한 번만 트리거되어 경보 소음이 줄어듭니다. 애플리케이션, AWS 리전 또는 가용 영역과 같은 리소스 그룹의 전반적인 상태를 확인할 수 있습니다.\n",
            "자세한 내용은 CloudWatch 요금 페이지를 참조하세요.\"\n",
            "CloudWatch 경보에서 어떤 작업을 수행할 수 있나요?\n",
            "경보 생성 시, 모니터링하기로 선택한 측정치가 정의한 임계값을 초과할 때 경보에서 하나 이상의 자동화된 작업을 수행하도록 구성할 수 있습니다. 예를 들어 이메일을 전송하거나, SQS 대기열에 게시하거나, Amazon EC2 인스턴스를 중단 또는 종료하거나, Auto Scaling 정책을 실행하도록 경보를 설정할 수 있습니다. Amazon CloudWatch 경보는 Amazon Simple Notification Service와 통합되어 있으므로 SNS에서 지원되는 모든 알림 유형도 사용할 수 있습니다. AWS Systems Manager OpsCenter 작업을 사용하면 경보가 ALARM 상태로 들어갈 때 OpsItem을 자동으로 생성할 수 있습니다. 이렇게 하면 사용자가 단일 콘솔에서 AWS 리소스 관련 문제를 빠르게 진단하고 해결할 수 있습니다.\n",
            "\"category : CloudWatch, question : CloudWatch 경보에서 어떤 작업을 수행할 수 있나요?, answer : 경보 생성 시, 모니터링하기로 선택한 측정치가 정의한 임계값을 초과할 때 경보에서 하나 이상의 자동화된 작업을 수행하도록 구성할 수 있습니다. 예를 들어 이메일을 전송하거나, SQS 대기열에 게시하거나, Amazon EC2 인스턴스를 중단 또는 종료하거나, Auto Scaling 정책을 실행하도록 경보를 설정할 수 있습니다. Amazon CloudWatch 경보는 Amazon Simple Notification Service와 통합되어 있으므로 SNS에서 지원되는 모든 알림 유형도 사용할 수 있습니다. AWS Systems Manager OpsCenter 작업을 사용하면 경보가 ALARM 상태로 들어갈 때 OpsItem을 자동으로 생성할 수 있습니다. 이렇게 하면 사용자가 단일 콘솔에서 AWS 리소스 관련 문제를 빠르게 진단하고 해결할 수 있습니다.\"\n",
            "CloudWatch 경보를 트리거하려면 어떤 임계값을 설정할 수 있나요?\n",
            "경보를 만들 때 모니터링할 Amazon CloudWatch 지표를 먼저 선택합니다. 그런 다음 평가 기간(예: 5분 또는 1시간) 및 측정할 통계치(예: Average 또는 Maximum)를 선택합니다. 임계값을 설정하려면 목표 값을 설정하고 값이 해당 값보다 크거나(>), 크거나 같거나(>=), 작거나(<), 작거나 같을(<=) 때 경보를 트리거할지를 선택합니다.\n",
            "\"category : CloudWatch, question : CloudWatch 경보를 트리거하려면 어떤 임계값을 설정할 수 있나요?, answer : 경보를 만들 때 모니터링할 Amazon CloudWatch 지표를 먼저 선택합니다. 그런 다음 평가 기간(예: 5분 또는 1시간) 및 측정할 통계치(예: Average 또는 Maximum)를 선택합니다. 임계값을 설정하려면 목표 값을 설정하고 값이 해당 값보다 크거나(>), 크거나 같거나(>=), 작거나(<), 작거나 같을(<=) 때 경보를 트리거할지를 선택합니다.\"\n",
            "내 CloudWatch 경보가 지속적으로 경보 상태에 있습니다. 무엇이 잘못되었나요?\n",
            "경보는 이미 트리거된 후에도 선택한 임계값의 지표를 계속해서 평가합니다. 이를 통해 언제든지 경보의 최신 상태를 확인할 수 있습니다. 경보에 따라서는 한참 동안 경보 상태로 유지될 수도 있습니다. 지표 값이 임계값을 계속 초과하는 경우 임계값을 더는 초과하지 않을 때까지 경보가 경보 상태로 유지됩니다. 이 현상은 정상적인 동작입니다. 경보에서 이 새로운 수준이 정상으로 처리되도록 하려면 경보 임계값을 적절하게 조정해야 합니다.\n",
            "\"category : CloudWatch, question : 내 CloudWatch 경보가 지속적으로 경보 상태에 있습니다. 무엇이 잘못되었나요?, answer : 경보는 이미 트리거된 후에도 선택한 임계값의 지표를 계속해서 평가합니다. 이를 통해 언제든지 경보의 최신 상태를 확인할 수 있습니다. 경보에 따라서는 한참 동안 경보 상태로 유지될 수도 있습니다. 지표 값이 임계값을 계속 초과하는 경우 임계값을 더는 초과하지 않을 때까지 경보가 경보 상태로 유지됩니다. 이 현상은 정상적인 동작입니다. 경보에서 이 새로운 수준이 정상으로 처리되도록 하려면 경보 임계값을 적절하게 조정해야 합니다.\"\n",
            "경보 기록은 얼마 동안 조회할 수 있나요?\n",
            "경보 기록은 14일 동안 사용할 수 있습니다. 경보 기록을 보려면 AWS Management Console에서 CloudWatch에 로그인하고, 왼쪽 메뉴에서 Alarms를 선택하고, 원하는 경보를 선택한 다음, 아래쪽 창에서 History 탭을 클릭합니다. 여기에서 경보에 대한 모든 상태 변경 기록 및 경보 구성에 대한 수정 내역을 확인할 수 있습니다.\n",
            "\"category : CloudWatch, question : 경보 기록은 얼마 동안 조회할 수 있나요?, answer : 경보 기록은 14일 동안 사용할 수 있습니다. 경보 기록을 보려면 AWS Management Console에서 CloudWatch에 로그인하고, 왼쪽 메뉴에서 Alarms를 선택하고, 원하는 경보를 선택한 다음, 아래쪽 창에서 History 탭을 클릭합니다. 여기에서 경보에 대한 모든 상태 변경 기록 및 경보 구성에 대한 수정 내역을 확인할 수 있습니다.\"\n",
            "CloudWatch 대시보드란 무엇인가요?\n",
            "Amazon CloudWatch 대시보드를 사용하면 AWS 리소스 및 사용자 정의 지표의 그래프를 생성, 사용자 정의, 상호 작용 및 저장할 수 있습니다.\n",
            "\"category : CloudWatch, question : CloudWatch 대시보드란 무엇인가요?, answer : Amazon CloudWatch 대시보드를 사용하면 AWS 리소스 및 사용자 정의 지표의 그래프를 생성, 사용자 정의, 상호 작용 및 저장할 수 있습니다.\"\n",
            "CloudWatch 대시보드를 시작하려면 어떻게 해야 하나요?\n",
            "시작하려면 Amazon CloudWatch 콘솔에서 ‘대시보드’를 선택합니다. “Create Dashboard\" 버튼을 클릭합니다. Automatic Dashboards에서 [Options] -> [Add to Dashboard]를 클릭하여 원하는 보기를 복사할 수 있습니다.\n",
            "\"category : CloudWatch, question : CloudWatch 대시보드를 시작하려면 어떻게 해야 하나요?, answer : 시작하려면 Amazon CloudWatch 콘솔에서 ‘대시보드’를 선택합니다. “Create Dashboard\" 버튼을 클릭합니다. Automatic Dashboards에서 [Options] -> [Add to Dashboard]를 클릭하여 원하는 보기를 복사할 수 있습니다.\"\n",
            "Automatic Dashboards의 장점은 무엇인가요?\n",
            "Automatic Dashboards는 AWS 서비스에서 추천하는 모범 사례로 사전 구축되었으며, 리소스에 기반하여, 중요한 성능 지표의 최신 상태를 반영하도록 동적으로 업데이트됩니다. 이제 코드를 더 추가하지 않고도 AWS 리소스의 최신 상태를 반영하도록 특정 보기를 필터링하고 문제를 해결할 수 있습니다. 성능 문제의 근본 원인을 식별한 후에는 AWS 리소스로 직접 이동하여 빠르게 조치할 수 있습니다.\n",
            "\"category : CloudWatch, question : Automatic Dashboards의 장점은 무엇인가요?, answer : Automatic Dashboards는 AWS 서비스에서 추천하는 모범 사례로 사전 구축되었으며, 리소스에 기반하여, 중요한 성능 지표의 최신 상태를 반영하도록 동적으로 업데이트됩니다. 이제 코드를 더 추가하지 않고도 AWS 리소스의 최신 상태를 반영하도록 특정 보기를 필터링하고 문제를 해결할 수 있습니다. 성능 문제의 근본 원인을 식별한 후에는 AWS 리소스로 직접 이동하여 빠르게 조치할 수 있습니다.\"\n",
            "대시보드에서 자동 새로 고침을 지원하나요?\n",
            "예. 대시보드는 열려있는 동안 자동으로 새로 고침됩니다.\n",
            "\"category : CloudWatch, question : 대시보드에서 자동 새로 고침을 지원하나요?, answer : 예. 대시보드는 열려있는 동안 자동으로 새로 고침됩니다.\"\n",
            "대시보드를 공유할 수 있니요?\n",
            "예. 대시보드는 계정에서 대시보드에 대한 적절한 권한을 가진 누구나 사용할 수 있습니다.\n",
            "\"category : CloudWatch, question : 대시보드를 공유할 수 있니요?, answer : 예. 대시보드는 계정에서 대시보드에 대한 적절한 권한을 가진 누구나 사용할 수 있습니다.\"\n",
            "CloudWatch Events란 무엇인가요?\n",
            "Amazon CloudWatch Events(CWE)는 AWS 리소스의 변경을 설명하는 시스템 이벤트 스트림입니다. 이벤트 스트림은 기존 CloudWatch 지표 및 로그 스트림을 보강하여 애플리케이션 상태에 대한 좀 더 완벽한 설명을 제공합니다. 관심 있는 이벤트와 이에 따라 자동으로 수행할 작업을 연결하는 선언적 규칙을 작성합니다.\n",
            "\"category : CloudWatch, question : CloudWatch Events란 무엇인가요?, answer : Amazon CloudWatch Events(CWE)는 AWS 리소스의 변경을 설명하는 시스템 이벤트 스트림입니다. 이벤트 스트림은 기존 CloudWatch 지표 및 로그 스트림을 보강하여 애플리케이션 상태에 대한 좀 더 완벽한 설명을 제공합니다. 관심 있는 이벤트와 이에 따라 자동으로 수행할 작업을 연결하는 선언적 규칙을 작성합니다.\"\n",
            "CloudWatch Events 내보내기를 지원하는 서비스에는 어떤 것이 있나요?\n",
            "현재 Amazon EC2, Auto Scaling 및 AWS CloudTrail에서 지원합니다. 모든 서비스에서 API 호출(예: Describe*, List* 및 Get*를 제외한 모든 호출)을 변경하는 것을 AWS CloudTrail을 통해 CloudWatch Events에서 볼 수 있습니다.\n",
            "\"category : CloudWatch, question : CloudWatch Events 내보내기를 지원하는 서비스에는 어떤 것이 있나요?, answer : 현재 Amazon EC2, Auto Scaling 및 AWS CloudTrail에서 지원합니다. 모든 서비스에서 API 호출(예: Describe*, List* 및 Get*를 제외한 모든 호출)을 변경하는 것을 AWS CloudTrail을 통해 CloudWatch Events에서 볼 수 있습니다.\"\n",
            "이벤트를 수신하는 경우, 어떤 작업을 수행할 수 있나요?\n",
            "이벤트가 시스템에 생성해 둔 규칙과 일치하는 경우, AWS Lambda 함수를 자동으로 호출하고, 해당 이벤트를 Amazon Kinesis 스트림에 전달하고, Amazon SNS 주제를 알리거나, 내장 워크플로를 호출할 수 있습니다.\n",
            "\"category : CloudWatch, question : 이벤트를 수신하는 경우, 어떤 작업을 수행할 수 있나요?, answer : 이벤트가 시스템에 생성해 둔 규칙과 일치하는 경우, AWS Lambda 함수를 자동으로 호출하고, 해당 이벤트를 Amazon Kinesis 스트림에 전달하고, Amazon SNS 주제를 알리거나, 내장 워크플로를 호출할 수 있습니다.\"\n",
            "자체 이벤트를 생성할 수 있나요?\n",
            "예. PutEvents API를 사용하여 애플리케이션에서 사용자의 필요에 적합한 페이로드와 함께 사용자 정의 이벤트를 내보낼 수 있습니다.\n",
            "\"category : CloudWatch, question : 자체 이벤트를 생성할 수 있나요?, answer : 예. PutEvents API를 사용하여 애플리케이션에서 사용자의 필요에 적합한 페이로드와 함께 사용자 정의 이벤트를 내보낼 수 있습니다.\"\n",
            "고정된 일정에 따라 작업을 할 수 있나요?\n",
            "CloudWatch Events에서는 널리 사용되는 Unix Cron 구문을 사용하여 사용자가 설정한 일정에 따라 이벤트를 생성할 수 있습니다. 이러한 이벤트를 모니터링함으로써 예약된 애플리케이션을 구현할 수 있습니다.\n",
            "\"category : CloudWatch, question : 고정된 일정에 따라 작업을 할 수 있나요?, answer : CloudWatch Events에서는 널리 사용되는 Unix Cron 구문을 사용하여 사용자가 설정한 일정에 따라 이벤트를 생성할 수 있습니다. 이러한 이벤트를 모니터링함으로써 예약된 애플리케이션을 구현할 수 있습니다.\"\n",
            "CloudWatch Events와 AWS CloudTrail의 차이점은 무엇인가요?\n",
            "CloudWatch Events는 AWS 리소스에 대한 변경을 설명하는, 거의 실시간의 시스템 이벤트 스트림입니다. CloudWatch Events를 사용하면, 특정 이벤트를 모니터링하고 자동으로 작업을 수행하도록 규칙을 정의할 수 있습니다. AWS CloudTrail은 AWS 계정에 대한 API 호출을 기록하고 API 호출이 포함된 로그 파일을 Amazon S3 버킷이나 CloudWatch Logs 로그 그룹에 전달하는 서비스입니다. AWS CloudTrail을 사용하면, AWS 리소스의 생성, 삭제 및 변경과 관련된 API 활동 기록을 검색하고, 운영 문제나 보안 문제를 해결할 수 있습니다.\n",
            "\"category : CloudWatch, question : CloudWatch Events와 AWS CloudTrail의 차이점은 무엇인가요?, answer : CloudWatch Events는 AWS 리소스에 대한 변경을 설명하는, 거의 실시간의 시스템 이벤트 스트림입니다. CloudWatch Events를 사용하면, 특정 이벤트를 모니터링하고 자동으로 작업을 수행하도록 규칙을 정의할 수 있습니다. AWS CloudTrail은 AWS 계정에 대한 API 호출을 기록하고 API 호출이 포함된 로그 파일을 Amazon S3 버킷이나 CloudWatch Logs 로그 그룹에 전달하는 서비스입니다. AWS CloudTrail을 사용하면, AWS 리소스의 생성, 삭제 및 변경과 관련된 API 활동 기록을 검색하고, 운영 문제나 보안 문제를 해결할 수 있습니다.\"\n",
            "CloudWatch Events와 AWS Config의 차이점은 무엇인가요?\n",
            "AWS Config는 AWS 리소스 인벤토리, 구성 기록, 구성 변경 알림을 제공하여 보안 및 거버넌스를 실현하는 완벽한 관리형 서비스입니다. Config 규칙을 사용하면 구성 변경이 규정을 준수하는지 확인할 수 있습니다. CloudWatch Events는 리소스 상태 변경에 대해 거의 실시간으로 대응하기 위한 서비스입니다. 해당 변경이 정책을 준수하는지에 대한 의견을 주거나 Config/Config Rules에서 제공하는 것과 같은 상세한 기록을 제공하지 않습니다. CloudWatch Events는 범용 이벤트 스트림입니다.\n",
            "\"category : CloudWatch, question : CloudWatch Events와 AWS Config의 차이점은 무엇인가요?, answer : AWS Config는 AWS 리소스 인벤토리, 구성 기록, 구성 변경 알림을 제공하여 보안 및 거버넌스를 실현하는 완벽한 관리형 서비스입니다. Config 규칙을 사용하면 구성 변경이 규정을 준수하는지 확인할 수 있습니다. CloudWatch Events는 리소스 상태 변경에 대해 거의 실시간으로 대응하기 위한 서비스입니다. 해당 변경이 정책을 준수하는지에 대한 의견을 주거나 Config/Config Rules에서 제공하는 것과 같은 상세한 기록을 제공하지 않습니다. CloudWatch Events는 범용 이벤트 스트림입니다.\"\n",
            "CloudWatch Container Insights란 무엇인가요?\n",
            "CloudWatch Container Insights는 컨테이너식 애플리케이션 및 마이크로서비스에서 모니터링 및 문제를 해결하고 경보를 제공하는 기능입니다. Container Insights는 컨테이너 환경에 영향을 미치는 성능 문제를 간단히 분석할 수 있게 해 줍니다. DevOps 및 시스템 엔지니어는 CloudWatch 콘솔의 자동 대시보드에 액세스하여 Amazon Elastic Container Service for Kubernetes(EKS), Amazon Elastic Container Service(ECS), AWS Fargate 및 Kubernetes 클러스터의 성능 및 상태가 요약된 지표, 로그 및 분산 트레이스를 포드/작업, 컨테이너 및 서비스별로 운영 측면에서 포괄적으로 파악할 수 있습니다.\n",
            "\"category : CloudWatch, question : CloudWatch Container Insights란 무엇인가요?, answer : CloudWatch Container Insights는 컨테이너식 애플리케이션 및 마이크로서비스에서 모니터링 및 문제를 해결하고 경보를 제공하는 기능입니다. Container Insights는 컨테이너 환경에 영향을 미치는 성능 문제를 간단히 분석할 수 있게 해 줍니다. DevOps 및 시스템 엔지니어는 CloudWatch 콘솔의 자동 대시보드에 액세스하여 Amazon Elastic Container Service for Kubernetes(EKS), Amazon Elastic Container Service(ECS), AWS Fargate 및 Kubernetes 클러스터의 성능 및 상태가 요약된 지표, 로그 및 분산 트레이스를 포드/작업, 컨테이너 및 서비스별로 운영 측면에서 포괄적으로 파악할 수 있습니다.\"\n",
            "컨테이너 수준 상태 및 성능을 모니터링하려면 어떻게 해야 하나요?\n",
            "Amazon Elastic Kubernetes Service(EKS)에 Container Insights와 함께 향상된 관찰성을 사용하면 EKS 컨테이너 계층을 시각적으로 드릴업 및 드릴다운하여 개별 컨테이너에서 메모리 누수와 같은 문제를 손쉽게 발견하여 평균 해결 시간을 줄일 수 있습니다. 또한 이제 컨트롤 플레인 지표를 통해 Auto Scaling 상태를 모니터링하고 자동화된 테스트 기능에서 테스트 클러스터 수명 주기를 계획하여 운영 효율성을 개선할 수 있습니다. 이제 EKS에 향상된 관찰성을 사용하여 리소스 소비량을 기준으로 클러스터, 노드 또는 워크로드를 분류하고, 이상 현상을 신속하게 파악하고, 자체 경보를 설정하여 면밀히 모니터링하며, 최종 사용자 경험에 영향을 받기 전에 사전 예방적으로 위험을 완화할 수 있습니다.\n",
            "\"category : CloudWatch, question : 컨테이너 수준 상태 및 성능을 모니터링하려면 어떻게 해야 하나요?, answer : Amazon Elastic Kubernetes Service(EKS)에 Container Insights와 함께 향상된 관찰성을 사용하면 EKS 컨테이너 계층을 시각적으로 드릴업 및 드릴다운하여 개별 컨테이너에서 메모리 누수와 같은 문제를 손쉽게 발견하여 평균 해결 시간을 줄일 수 있습니다. 또한 이제 컨트롤 플레인 지표를 통해 Auto Scaling 상태를 모니터링하고 자동화된 테스트 기능에서 테스트 클러스터 수명 주기를 계획하여 운영 효율성을 개선할 수 있습니다. 이제 EKS에 향상된 관찰성을 사용하여 리소스 소비량을 기준으로 클러스터, 노드 또는 워크로드를 분류하고, 이상 현상을 신속하게 파악하고, 자체 경보를 설정하여 면밀히 모니터링하며, 최종 사용자 경험에 영향을 받기 전에 사전 예방적으로 위험을 완화할 수 있습니다.\"\n",
            "Container Insights는 컨트롤 플레인 수준에서 가시성을 제공하나요?\n",
            "예. Amazon Elastic Kubernetes Service(EKS)에 Container Insights와 함께 향상된 관찰성을 사용하여 컨트롤 플레인 상태를 모니터링할 수 있습니다. 이를 사용하여 Auto Scaling 상태를 파악하고 자동화된 테스트 기능 등에서 테스트 클러스터 수명 주기를 계획할 수 있습니다.\n",
            "\"category : CloudWatch, question : Container Insights는 컨트롤 플레인 수준에서 가시성을 제공하나요?, answer : 예. Amazon Elastic Kubernetes Service(EKS)에 Container Insights와 함께 향상된 관찰성을 사용하여 컨트롤 플레인 상태를 모니터링할 수 있습니다. 이를 사용하여 Auto Scaling 상태를 파악하고 자동화된 테스트 기능 등에서 테스트 클러스터 수명 주기를 계획할 수 있습니다.\"\n",
            "Container Insights에서 향상된 관찰성 사용 여부는 어떤 차이점이 있나요?\n",
            "Amazon Elastic Kubernetes Service(EKS)에 대한 향상된 관찰성을 사용하는 Container Insights는 빠른 문제 격리 및 해결을 위한 컨테이너 수준 EKS 성능 지표, Kube 상태 지표, EKS 컨트롤 플레인 지표 등 즉시 사용 가능한 상세한 상태 및 성능 지표를 제공합니다. 고객은 향상된 관찰성을 활용하여 다양한 컨테이너 계층 전체를 시각적으로 자세히 살펴보고 개별 컨테이너의 메모리 누수와 같은 문제를 손쉽게 발견하여 평균 해결 시간을 단축할 수 있습니다. 또한 이제 컨트롤 플레인 지표를 통해 Auto Scaling 상태를 모니터링하고 자동화된 테스트 기능에서 테스트 클러스터 수명 주기를 계획하여 운영 효율성을 개선할 수 있습니다. 이제 향상된 관찰성을 사용하여 리소스 소비량을 기준으로 클러스터, 노드 또는 워크로드를 분류하고, 이상 현상을 신속하게 파악하고, 자체 경보를 설정하여 면밀히 모니터링하며, 최종 사용자 경험에 영향을 받기 전에 사전 예방적으로 위험을 완화할 수 있습니다. 향상된 관찰성은 선택적 기능입니다. 향상된 관찰성을 사용하지 않는 Container Insights는 클러스터 수준 및 서비스 수준에서 지표를 집계합니다.\n",
            "\"category : CloudWatch, question : Container Insights에서 향상된 관찰성 사용 여부는 어떤 차이점이 있나요?, answer : Amazon Elastic Kubernetes Service(EKS)에 대한 향상된 관찰성을 사용하는 Container Insights는 빠른 문제 격리 및 해결을 위한 컨테이너 수준 EKS 성능 지표, Kube 상태 지표, EKS 컨트롤 플레인 지표 등 즉시 사용 가능한 상세한 상태 및 성능 지표를 제공합니다. 고객은 향상된 관찰성을 활용하여 다양한 컨테이너 계층 전체를 시각적으로 자세히 살펴보고 개별 컨테이너의 메모리 누수와 같은 문제를 손쉽게 발견하여 평균 해결 시간을 단축할 수 있습니다. 또한 이제 컨트롤 플레인 지표를 통해 Auto Scaling 상태를 모니터링하고 자동화된 테스트 기능에서 테스트 클러스터 수명 주기를 계획하여 운영 효율성을 개선할 수 있습니다. 이제 향상된 관찰성을 사용하여 리소스 소비량을 기준으로 클러스터, 노드 또는 워크로드를 분류하고, 이상 현상을 신속하게 파악하고, 자체 경보를 설정하여 면밀히 모니터링하며, 최종 사용자 경험에 영향을 받기 전에 사전 예방적으로 위험을 완화할 수 있습니다. 향상된 관찰성은 선택적 기능입니다. 향상된 관찰성을 사용하지 않는 Container Insights는 클러스터 수준 및 서비스 수준에서 지표를 집계합니다.\"\n",
            "Container Insights의 향상된 관찰성을 사용할 EKS 클러스터를 결정할 수 있나요?\n",
            "예. Container Insights를 사용할 때 클러스터별로 향상된 관찰성의 사용 여부를 결정할 수 있습니다. 클러스터 정보 보기의 애드온 탭을 사용하여 클러스터를 생성한 후 클러스터에 EKS용 CloudWatch Observability 애드온을 설치하여 클러스터에 향상된 관찰성을 사용할 수 있습니다. EKS에 향상된 관찰성을 사용하도록 CloudWatch 에이전트를 구성하는 방법에 대한 CloudWatch Container Insights 설명서를 참조하세요.\n",
            "\"category : CloudWatch, question : Container Insights의 향상된 관찰성을 사용할 EKS 클러스터를 결정할 수 있나요?, answer : 예. Container Insights를 사용할 때 클러스터별로 향상된 관찰성의 사용 여부를 결정할 수 있습니다. 클러스터 정보 보기의 애드온 탭을 사용하여 클러스터를 생성한 후 클러스터에 EKS용 CloudWatch Observability 애드온을 설치하여 클러스터에 향상된 관찰성을 사용할 수 있습니다. EKS에 향상된 관찰성을 사용하도록 CloudWatch 에이전트를 구성하는 방법에 대한 CloudWatch Container Insights 설명서를 참조하세요.\"\n",
            "향상된 관찰성은 어떤 컨테이너 서비스에서 지원되나요?\n",
            "Container Insights의 향상된 관찰성은 Amazon EKS를 지원합니다.\n",
            "\"category : CloudWatch, question : 향상된 관찰성은 어떤 컨테이너 서비스에서 지원되나요?, answer : Container Insights의 향상된 관찰성은 Amazon EKS를 지원합니다.\"\n",
            "CloudWatch Container Insights를 시작하려면 어떻게 해야 하나요?\n",
            "클릭 몇 번으로 컨테이너 및 클러스터에서 자세한 성능 지표, 로그 및 메타데이터를 수집하거나 CloudWatch Observability 애드온을 활성화하여 관찰성을 높일 수 있습니다. 컨테이너 인사이트 사용을 시작하려면 Amazon CloudWatch Container Insights 설명서에 제공된 단계를 따르세요.\n",
            "\"category : CloudWatch, question : CloudWatch Container Insights를 시작하려면 어떻게 해야 하나요?, answer : 클릭 몇 번으로 컨테이너 및 클러스터에서 자세한 성능 지표, 로그 및 메타데이터를 수집하거나 CloudWatch Observability 애드온을 활성화하여 관찰성을 높일 수 있습니다. 컨테이너 인사이트 사용을 시작하려면 Amazon CloudWatch Container Insights 설명서에 제공된 단계를 따르세요.\"\n",
            "CloudWatch Container Insights의 요금은 어떻게 부과되나요?\n",
            "Container Insights의 자세한 요금 정보는 CloudWatch 요금 페이지에서 확인할 수 있습니다.\n",
            "\"category : CloudWatch, question : CloudWatch Container Insights의 요금은 어떻게 부과되나요?, answer : Container Insights의 자세한 요금 정보는 CloudWatch 요금 페이지에서 확인할 수 있습니다.\"\n",
            "모든 Prometheus 지표 유형을 공개 베타에서 지원되나요?\n",
            "아니요, 현재 지원되는 지표 유형은 게이지와 카운터입니다. 히스토그램과 요약 지표는 이후의 릴리스에서 지원할 예정입니다.\n",
            "\"category : CloudWatch, question : 모든 Prometheus 지표 유형을 공개 베타에서 지원되나요?, answer : 아니요, 현재 지원되는 지표 유형은 게이지와 카운터입니다. 히스토그램과 요약 지표는 이후의 릴리스에서 지원할 예정입니다.\"\n",
            "Prometheus는 무엇이고 CloudWatch에서 Prometheus 지표를 수집해야 하는 이유는 무엇인가요?\n",
            "Prometheus는 CNCF(Cloud Native Compute Foundation)의 일부인 인기 오픈 소스 모니터링 프로젝트입니다. 이 오픈 소스 커뮤니티는 DevOps 팀에서 풀 기반 접근 방식을 사용하여 애플리케이션에서 수집할 사용자 지정 지표를 노출할 때 사용할 수 있는 프레임워크를 정의하고 150개 이상의 플러그인을 구축했습니다. DevOps 팀에서는 이 새로운 기능을 사용하여 AWS App Mesh, NGINX 및 Java/JMX와 같은 컨테이너식 워크로드를 위한 서비스를 자동으로 검색할 수 있습니다. 또한, 이러한 서비스에 대한 사용자 지정 지표를 노출하고 CloudWatch에서 해당 지표를 수집할 수 있습니다. CloudWatch 사용자는 Prometheus 지표의 수집 및 집계를 큐레이션하여 필요한 모니터링 도구의 수를 줄이면서 애플리케이션 성능 저하 및 장애에 대한 모니터링, 문제 해결 및 경보 작업을 더 빠르게 수행할 수 있습니다.\n",
            "\"category : CloudWatch, question : Prometheus는 무엇이고 CloudWatch에서 Prometheus 지표를 수집해야 하는 이유는 무엇인가요?, answer : Prometheus는 CNCF(Cloud Native Compute Foundation)의 일부인 인기 오픈 소스 모니터링 프로젝트입니다. 이 오픈 소스 커뮤니티는 DevOps 팀에서 풀 기반 접근 방식을 사용하여 애플리케이션에서 수집할 사용자 지정 지표를 노출할 때 사용할 수 있는 프레임워크를 정의하고 150개 이상의 플러그인을 구축했습니다. DevOps 팀에서는 이 새로운 기능을 사용하여 AWS App Mesh, NGINX 및 Java/JMX와 같은 컨테이너식 워크로드를 위한 서비스를 자동으로 검색할 수 있습니다. 또한, 이러한 서비스에 대한 사용자 지정 지표를 노출하고 CloudWatch에서 해당 지표를 수집할 수 있습니다. CloudWatch 사용자는 Prometheus 지표의 수집 및 집계를 큐레이션하여 필요한 모니터링 도구의 수를 줄이면서 애플리케이션 성능 저하 및 장애에 대한 모니터링, 문제 해결 및 경보 작업을 더 빠르게 수행할 수 있습니다.\"\n",
            "Prometheus 지표에 지표 스토리지 보존은 어떻게 적용되나요?\n",
            "Prometheus 지표는 CloudWatch 사용자 정의 지표로 자동 수집됩니다. 보존 기간은 지표 데이터 포인트당 15개월이며 자동 롤업이 적용됩니다(60초 미만은 3시간, 1분은 15일, 5분은 63일, 1시간은 15개월). 자세한 내용은 CloudWatch 지표 보존 설명서를 참조하세요.\n",
            "\"category : CloudWatch, question : Prometheus 지표에 지표 스토리지 보존은 어떻게 적용되나요?, answer : Prometheus 지표는 CloudWatch 사용자 정의 지표로 자동 수집됩니다. 보존 기간은 지표 데이터 포인트당 15개월이며 자동 롤업이 적용됩니다(60초 미만은 3시간, 1분은 15일, 5분은 63일, 1시간은 15개월). 자세한 내용은 CloudWatch 지표 보존 설명서를 참조하세요.\"\n",
            "PromQL을 쿼리 언어로 지원되나요?\n",
            "아니요. 모든 지표는 CloudWatch Logs 이벤트로 수집되고 CloudWatch Logs Insights 쿼리를 사용하여 쿼리할 수 있습니다. 자세한 내용은 CloudWatch Logs Insights 검색 언어 구문을 참조하세요.\n",
            "\"category : CloudWatch, question : PromQL을 쿼리 언어로 지원되나요?, answer : 아니요. 모든 지표는 CloudWatch Logs 이벤트로 수집되고 CloudWatch Logs Insights 쿼리를 사용하여 쿼리할 수 있습니다. 자세한 내용은 CloudWatch Logs Insights 검색 언어 구문을 참조하세요.\"\n",
            "Prometheus 지표에 구성 가능한 스토리지 보존은 CloudWatch Logs로 수집된 높은 카디널리티 이벤트인가요?\n",
            "예. 각 Kubernetes(k8s) 클러스터는 이벤트에 대한 자체적인 로그 그룹(예: /aws/containerinsights//prometheus)이 있고, 각각의 구성 보존 기간이 지정되어 있습니다. 자세한 내용은 로그 그룹 보존 설명서를 참조하세요.\n",
            "\"category : CloudWatch, question : Prometheus 지표에 구성 가능한 스토리지 보존은 CloudWatch Logs로 수집된 높은 카디널리티 이벤트인가요?, answer : 예. 각 Kubernetes(k8s) 클러스터는 이벤트에 대한 자체적인 로그 그룹(예: /aws/containerinsights//prometheus)이 있고, 각각의 구성 보존 기간이 지정되어 있습니다. 자세한 내용은 로그 그룹 보존 설명서를 참조하세요.\"\n",
            "컨테이너 환경에서 Prometheus 지표를 수집할 때 요금은 어떻게 부과되나요?\n",
            "다음과 같이 사용량에 따라 요금이 부과됩니다. (1) 수집된 CloudWatch Logs, 기가비트(GB) 단위, (2) 저장된 CloudWatch Logs 및 (3) CloudWatch 사용자 지정 지표. AWS 리전의 요금 세부 정보는 CloudWatch 요금 페이지를 참조하세요.\n",
            "\"category : CloudWatch, question : 컨테이너 환경에서 Prometheus 지표를 수집할 때 요금은 어떻게 부과되나요?, answer : 다음과 같이 사용량에 따라 요금이 부과됩니다. (1) 수집된 CloudWatch Logs, 기가비트(GB) 단위, (2) 저장된 CloudWatch Logs 및 (3) CloudWatch 사용자 지정 지표. AWS 리전의 요금 세부 정보는 CloudWatch 요금 페이지를 참조하세요.\"\n",
            "Amazon CloudWatch Internet Monitor란 무엇인가요?\n",
            "Amazon CloudWatch Internet Monitor는 AWS에서 호스팅되는 애플리케이션과 애플리케이션 최종 사용자 간의 인터넷 가용성 및 성능 지표를 지속적으로 모니터링하는 데 도움이 됩니다. Internet Monitor를 사용하면 문제의 영향을 빠르게 시각화하고, 영향을 받는 위치와 공급자를 정확하게 파악한 후, 최종 사용자의 네트워크 경험 개선을 위한 조치를 취할 수 있습니다. 트래픽 패턴과 상태 이벤트의 글로벌 뷰를 확인할 수 있으며, 다양한 지리적 세부 수준에서 이벤트 관련 정보로 드릴다운할 수 있습니다. AWS 네트워크로 인한 문제가 발생하면 AWS Health Dashboard 알림이 수신됩니다. 이 알림에서 문제 완화를 위해 AWS에서 진행 중인 단계를 확인할 수 있습니다. Internet Monitor는 또한 다른 AWS 서비스를 사용하여 사용자 경험을 개선하는 데 도움이 되는 관련 인사이트와 권장 사항도 제공합니다.\n",
            "\"category : CloudWatch, question : Amazon CloudWatch Internet Monitor란 무엇인가요?, answer : Amazon CloudWatch Internet Monitor는 AWS에서 호스팅되는 애플리케이션과 애플리케이션 최종 사용자 간의 인터넷 가용성 및 성능 지표를 지속적으로 모니터링하는 데 도움이 됩니다. Internet Monitor를 사용하면 문제의 영향을 빠르게 시각화하고, 영향을 받는 위치와 공급자를 정확하게 파악한 후, 최종 사용자의 네트워크 경험 개선을 위한 조치를 취할 수 있습니다. 트래픽 패턴과 상태 이벤트의 글로벌 뷰를 확인할 수 있으며, 다양한 지리적 세부 수준에서 이벤트 관련 정보로 드릴다운할 수 있습니다. AWS 네트워크로 인한 문제가 발생하면 AWS Health Dashboard 알림이 수신됩니다. 이 알림에서 문제 완화를 위해 AWS에서 진행 중인 단계를 확인할 수 있습니다. Internet Monitor는 또한 다른 AWS 서비스를 사용하여 사용자 경험을 개선하는 데 도움이 되는 관련 인사이트와 권장 사항도 제공합니다.\"\n",
            "Internet Monitor를 시작하려면 어떻게 해야 하나요?\n",
            "Internet Monitor를 사용하려면 모니터를 생성하고 애플리케이션 리소스를 모니터, Amazon Virtual Private Cloud(VPC), CloudFront 배포 또는 WorkSpaces 디렉터리에 연결합니다. 이렇게 하면 Internet Monitor에서 애플리케이션의 인터넷 트래픽 위치를 알 수 있습니다. Internet Monitor는 애플리케이션과 통신하는 위치 및 네트워크와 관련된 AWS의 인터넷 측정값을 제공합니다.\n",
            "고객은 CloudWatch 대시보드를 사용하여 상태 이벤트에 대해 알아보고, 성능 및 가용성 점수를 보고, 다양한 지리 단위에서 애플리케이션의 기록 데이터를 살펴보고, 최종 사용자에게 제공되는 성능을 개선하기 위해 애플리케이션을 어떻게 구성할지에 대한 인사이트를 얻을 수 있습니다.\n",
            "Internet Monitor는 인터넷 측정값을 CloudWatch Logs 및 CloudWatch Metrics에 게시하므로 CloudWatch 도구를 사용하여 애플리케이션과 관련된 지리 및 네트워크의 애플리케이션 상태를 손쉽게 파악할 수 있습니다. Internet Monitor는 Amazon EventBridge로도 상태 이벤트를 전송하므로 알림을 설정할 수 있습니다.\n",
            "\"category : CloudWatch, question : Internet Monitor를 시작하려면 어떻게 해야 하나요?, answer : Internet Monitor를 사용하려면 모니터를 생성하고 애플리케이션 리소스를 모니터, Amazon Virtual Private Cloud(VPC), CloudFront 배포 또는 WorkSpaces 디렉터리에 연결합니다. 이렇게 하면 Internet Monitor에서 애플리케이션의 인터넷 트래픽 위치를 알 수 있습니다. Internet Monitor는 애플리케이션과 통신하는 위치 및 네트워크와 관련된 AWS의 인터넷 측정값을 제공합니다.\n",
            "고객은 CloudWatch 대시보드를 사용하여 상태 이벤트에 대해 알아보고, 성능 및 가용성 점수를 보고, 다양한 지리 단위에서 애플리케이션의 기록 데이터를 살펴보고, 최종 사용자에게 제공되는 성능을 개선하기 위해 애플리케이션을 어떻게 구성할지에 대한 인사이트를 얻을 수 있습니다.\n",
            "Internet Monitor는 인터넷 측정값을 CloudWatch Logs 및 CloudWatch Metrics에 게시하므로 CloudWatch 도구를 사용하여 애플리케이션과 관련된 지리 및 네트워크의 애플리케이션 상태를 손쉽게 파악할 수 있습니다. Internet Monitor는 Amazon EventBridge로도 상태 이벤트를 전송하므로 알림을 설정할 수 있습니다.\"\n",
            "Internet Monitor의 구성 요소는 무엇인가요?\n",
            "Internet Monitor를 살펴보면 이 서비스에 참조된 구성 요소 및 개념을 익히는 데 도움이 됩니다. Internet Monitor는 Monitor, CloudWatch 로그, CloudWatch 지표, 도시 네트워크, 상태 이벤트, Autonomous System Number(ASN), 모니터링되는 리소스, 인터넷 측정값, 왕복 시간, 전송된 바이트, 성능 및 가용성 점수를 사용하거나 참조합니다.\n",
            "설명서에서 이러한 구성 요소에 대한 간략한 설명을 읽어보세요.\n",
            "\"category : CloudWatch, question : Internet Monitor의 구성 요소는 무엇인가요?, answer : Internet Monitor를 살펴보면 이 서비스에 참조된 구성 요소 및 개념을 익히는 데 도움이 됩니다. Internet Monitor는 Monitor, CloudWatch 로그, CloudWatch 지표, 도시 네트워크, 상태 이벤트, Autonomous System Number(ASN), 모니터링되는 리소스, 인터넷 측정값, 왕복 시간, 전송된 바이트, 성능 및 가용성 점수를 사용하거나 참조합니다.\n",
            "설명서에서 이러한 구성 요소에 대한 간략한 설명을 읽어보세요.\"\n",
            "Internet Monitor의 요금은 얼마인가요?\n",
            "Internet Monitor 요금에는 모니터링되는 리소스당 요금, 도시 네트워크당 요금, CloudWatch Logs에 게시된 진단 로그에 대한 요금과 같은 구성 요소가 있습니다. 자세한 내용은 Amazon CloudWatch Internet Monitor 요금 페이지를 참조하세요.\n",
            "\"category : CloudWatch, question : Internet Monitor의 요금은 얼마인가요?, answer : Internet Monitor 요금에는 모니터링되는 리소스당 요금, 도시 네트워크당 요금, CloudWatch Logs에 게시된 진단 로그에 대한 요금과 같은 구성 요소가 있습니다. 자세한 내용은 Amazon CloudWatch Internet Monitor 요금 페이지를 참조하세요.\"\n",
            "Internet Monitor를 사용할 수 있는 AWS 리전은 어디인가요?\n",
            "Internet Monitor의 리전별 지원은 모니터에 추가하는 리소스 유형에 따라 다릅니다. Amazon CloudFront 배포 및 Amazon WorkSpaces 디렉터리의 경우 지원되는 모든 리전에서 Internet Monitor를 사용할 수 있습니다. Amazon Virtual Private Cloud(VPC)의 경우 옵트인 리전의 VPC를 동일한 리전에 생성된 모니터에만 추가할 수 있습니다. 지원되는 AWS 리전의 전체 목록은 Amazon CloudWatch Internet Monitor 엔드포인트를 참조하세요.\n",
            "\"category : CloudWatch, question : Internet Monitor를 사용할 수 있는 AWS 리전은 어디인가요?, answer : Internet Monitor의 리전별 지원은 모니터에 추가하는 리소스 유형에 따라 다릅니다. Amazon CloudFront 배포 및 Amazon WorkSpaces 디렉터리의 경우 지원되는 모든 리전에서 Internet Monitor를 사용할 수 있습니다. Amazon Virtual Private Cloud(VPC)의 경우 옵트인 리전의 VPC를 동일한 리전에 생성된 모니터에만 추가할 수 있습니다. 지원되는 AWS 리전의 전체 목록은 Amazon CloudWatch Internet Monitor 엔드포인트를 참조하세요.\"\n",
            "CloudWatch Lambda Insights란 무엇인가요?\n",
            "CloudWatch Lambda Insights는 Lambda 함수의 성능 및 비용에 대한 모니터링, 문제 해결 및 최적화를 위한 기능입니다. Lambda Insights를 사용하면 Lambda 환경에 영향을 미치는 성능 문제를 간단히 분석할 수 있습니다. DevOps 및 시스템 엔지니어는 CloudWatch 콘솔에서 자동 대시보드에 액세스하여 운영 중인 AWS Lambda 함수의 성능 및 상태를 요약한 지표, 로그 및 트레이스를 완벽하게 파악할 수 있습니다.\n",
            "\"category : CloudWatch, question : CloudWatch Lambda Insights란 무엇인가요?, answer : CloudWatch Lambda Insights는 Lambda 함수의 성능 및 비용에 대한 모니터링, 문제 해결 및 최적화를 위한 기능입니다. Lambda Insights를 사용하면 Lambda 환경에 영향을 미치는 성능 문제를 간단히 분석할 수 있습니다. DevOps 및 시스템 엔지니어는 CloudWatch 콘솔에서 자동 대시보드에 액세스하여 운영 중인 AWS Lambda 함수의 성능 및 상태를 요약한 지표, 로그 및 트레이스를 완벽하게 파악할 수 있습니다.\"\n",
            "CloudWatch Lambda Insights를 시작하려면 어떻게 해야 되나요?\n",
            "CloudWatch Lambda Insights 설명서에 나와 있는 단계를 수행하면 Lambda 함수의 자세한 성능 지표, 로그 및 메타데이터를 수집할 수 있습니다.\n",
            "\"category : CloudWatch, question : CloudWatch Lambda Insights를 시작하려면 어떻게 해야 되나요?, answer : CloudWatch Lambda Insights 설명서에 나와 있는 단계를 수행하면 Lambda 함수의 자세한 성능 지표, 로그 및 메타데이터를 수집할 수 있습니다.\"\n",
            "CloudWatch Lambda Insights 요금은 어떻게 부과되나요?\n",
            "CloudWatch Lambda Insights는 Lambda 함수에서 CloudWatch Logs로 수집되는 성능 이벤트에서 사용자 지정 지표를 자동으로 수집합니다. 자세한 요금 정보는 CloudWatch 요금 페이지에서 확인할 수 있습니다.\n",
            "\"category : CloudWatch, question : CloudWatch Lambda Insights 요금은 어떻게 부과되나요?, answer : CloudWatch Lambda Insights는 Lambda 함수에서 CloudWatch Logs로 수집되는 성능 이벤트에서 사용자 지정 지표를 자동으로 수집합니다. 자세한 요금 정보는 CloudWatch 요금 페이지에서 확인할 수 있습니다.\"\n",
            "Amazon CloudWatch 디지털 경험 모니터링(DEM)이란 무엇인가요?\n",
            "Amazon CloudWatch DEM을 사용하면 최종 사용자가 애플리케이션을 경험하는 방식(성능, 가용성 및 사용 편의성 포함)을 모니터링할 수 있습니다. \n",
            "CloudWatch Synthetic 카나리아를 사용하여 간헐적인 문제를 발견하고, 사용자 트래픽이 없을 때도 알림을 받고, 엔드포인트와 UI를 모니터링합니다. CloudWatch RUM으로 종합 모니터링을 보완하여 최종 사용자에게 미치는 영향을 이해하고 디지털 경험을 더 잘 파악할 수 있습니다. CloudWatch Evidently를 사용하면 새로운 디자인과 기능을 실험하고 검증하여 최종 사용자의 디지털 경험을 개선할 수 있습니다.\n",
            "\"category : CloudWatch, question : Amazon CloudWatch 디지털 경험 모니터링(DEM)이란 무엇인가요?, answer : Amazon CloudWatch DEM을 사용하면 최종 사용자가 애플리케이션을 경험하는 방식(성능, 가용성 및 사용 편의성 포함)을 모니터링할 수 있습니다. \n",
            "CloudWatch Synthetic 카나리아를 사용하여 간헐적인 문제를 발견하고, 사용자 트래픽이 없을 때도 알림을 받고, 엔드포인트와 UI를 모니터링합니다. CloudWatch RUM으로 종합 모니터링을 보완하여 최종 사용자에게 미치는 영향을 이해하고 디지털 경험을 더 잘 파악할 수 있습니다. CloudWatch Evidently를 사용하면 새로운 디자인과 기능을 실험하고 검증하여 최종 사용자의 디지털 경험을 개선할 수 있습니다.\"\n",
            "Amazon CloudWatch RUM이란 무엇인가요?\n",
            "Amazon CloudWatch RUM은 애플리케이션의 클라이언트 측 성능에 대한 가시성을 제공하여 평균 해결 시간(MTTR)을 줄이는 데 도움이 되는 실제 사용자 모니터링 기능입니다. CloudWatch RUM을 사용하면 웹 애플리케이션 성능에 대한 클라이언트 측 데이터를 실시간으로 수집하여 문제를 식별하고 디버그할 수 있습니다. 또한 CloudWatch Synthetics 데이터를 보완하여 최종 사용자의 디지털 경험에 대한 가시성을 높입니다. 성능의 이상을 시각화하고 관련 디버깅 데이터(예: 오류 메시지, 스택 추적 정보 및 사용자 세션)를 사용하여 성능 문제(예: JavaScript 오류, 충돌 및 대기 시간)를 수정할 수 있습니다. 세션 수, 지리적 위치 또는 브라우저를 포함하여 최종 사용자에게 미치는 영향 범위를 파악할 수도 있습니다. CloudWatch RUM은 애플리케이션을 통한 사용자의 여정에 대한 데이터를 집계하여, 시작할 기능과 우선순위를 지정할 버그 수정을 결정하는 데 도움이 될 수 있습니다.\n",
            "\"category : CloudWatch, question : Amazon CloudWatch RUM이란 무엇인가요?, answer : Amazon CloudWatch RUM은 애플리케이션의 클라이언트 측 성능에 대한 가시성을 제공하여 평균 해결 시간(MTTR)을 줄이는 데 도움이 되는 실제 사용자 모니터링 기능입니다. CloudWatch RUM을 사용하면 웹 애플리케이션 성능에 대한 클라이언트 측 데이터를 실시간으로 수집하여 문제를 식별하고 디버그할 수 있습니다. 또한 CloudWatch Synthetics 데이터를 보완하여 최종 사용자의 디지털 경험에 대한 가시성을 높입니다. 성능의 이상을 시각화하고 관련 디버깅 데이터(예: 오류 메시지, 스택 추적 정보 및 사용자 세션)를 사용하여 성능 문제(예: JavaScript 오류, 충돌 및 대기 시간)를 수정할 수 있습니다. 세션 수, 지리적 위치 또는 브라우저를 포함하여 최종 사용자에게 미치는 영향 범위를 파악할 수도 있습니다. CloudWatch RUM은 애플리케이션을 통한 사용자의 여정에 대한 데이터를 집계하여, 시작할 기능과 우선순위를 지정할 버그 수정을 결정하는 데 도움이 될 수 있습니다.\"\n",
            "CloudWatch RUM을 시작하려면 어떻게 해야 하나요?\n",
            "CloudWatch RUM에서 앱 모니터를 생성하고 애플리케이션의 HTML 헤더에 경량 웹 클라이언트를 추가합니다. 그런 다음 CloudWatch RUM의 대시보드를 사용하여 다양한 지리적 위치, 장치, 플랫폼 및 브라우저에서 사용자 인사이트를 받습니다.\n",
            "\"category : CloudWatch, question : CloudWatch RUM을 시작하려면 어떻게 해야 하나요?, answer : CloudWatch RUM에서 앱 모니터를 생성하고 애플리케이션의 HTML 헤더에 경량 웹 클라이언트를 추가합니다. 그런 다음 CloudWatch RUM의 대시보드를 사용하여 다양한 지리적 위치, 장치, 플랫폼 및 브라우저에서 사용자 인사이트를 받습니다.\"\n",
            "Amazon CloudWatch Evidently란 무엇인가요?\n",
            "Amazon CloudWatch Evidently를 통해 새로운 기능을 일반 용도로 출시하기 전에 실험을 수행하고 의도하지 않은 결과를 식별할 수 있으므로 새로운 기능 출시와 관련된 위험을 줄일 수 있습니다. Evidently를 사용하면 릴리스 전에 전체 애플리케이션 스택에서 새로운 기능을 검증할 수 있으므로 릴리스가 더 안전해집니다. 새 기능을 출시할 때 해당 기능을 더 소규모의 사용자 기반에 노출하고 페이지 로드 시간 또는 전환과 같은 주요 지표를 모니터링한 다음 트래픽을 확장할 수 있습니다. 또한 개발자는 Evidently를 사용하여 다양한 디자인을 시도하고, 사용자 데이터를 수집하고, 프로덕션에서 가장 효과적인 디자인을 출시할 수 있습니다. 고급 통계 지식 없이도 실험 결과를 해석하고 이에 따라 조치를 취할 수 있습니다. Evidently의 통계 엔진에서 제공하는 인사이트(예: 상시 p-값 및 신뢰 구간)을 사용하여 실험이 진행되는 동안 결정을 내릴 수 있습니다.\n",
            "\"category : CloudWatch, question : Amazon CloudWatch Evidently란 무엇인가요?, answer : Amazon CloudWatch Evidently를 통해 새로운 기능을 일반 용도로 출시하기 전에 실험을 수행하고 의도하지 않은 결과를 식별할 수 있으므로 새로운 기능 출시와 관련된 위험을 줄일 수 있습니다. Evidently를 사용하면 릴리스 전에 전체 애플리케이션 스택에서 새로운 기능을 검증할 수 있으므로 릴리스가 더 안전해집니다. 새 기능을 출시할 때 해당 기능을 더 소규모의 사용자 기반에 노출하고 페이지 로드 시간 또는 전환과 같은 주요 지표를 모니터링한 다음 트래픽을 확장할 수 있습니다. 또한 개발자는 Evidently를 사용하여 다양한 디자인을 시도하고, 사용자 데이터를 수집하고, 프로덕션에서 가장 효과적인 디자인을 출시할 수 있습니다. 고급 통계 지식 없이도 실험 결과를 해석하고 이에 따라 조치를 취할 수 있습니다. Evidently의 통계 엔진에서 제공하는 인사이트(예: 상시 p-값 및 신뢰 구간)을 사용하여 실험이 진행되는 동안 결정을 내릴 수 있습니다.\"\n",
            "CloudWatch Evidently를 시작하려면 어떻게 해야 하나요?\n",
            "CloudWatch RUM JavaScript 코드 조각을 사용하여 클라이언트 측 사용자 여정과 성능 지표를 수집할 수 있습니다. 원하는 경우 Evidently API를 사용하여 전환과 같은 사용자 정의 지표를 추가할 수도 있습니다. 다음으로, 사용자가 새로운 기능에 노출되는 방식을 제어하는 기능을 제공하는 CloudWatch Evidently SDK로 테스트할 새로운 기능을 계측할 수 있습니다. 이제 AWS 콘솔 또는 CLI를 사용하여 시작 및 실험을 실행할 수 있습니다.\n",
            "\"category : CloudWatch, question : CloudWatch Evidently를 시작하려면 어떻게 해야 하나요?, answer : CloudWatch RUM JavaScript 코드 조각을 사용하여 클라이언트 측 사용자 여정과 성능 지표를 수집할 수 있습니다. 원하는 경우 Evidently API를 사용하여 전환과 같은 사용자 정의 지표를 추가할 수도 있습니다. 다음으로, 사용자가 새로운 기능에 노출되는 방식을 제어하는 기능을 제공하는 CloudWatch Evidently SDK로 테스트할 새로운 기능을 계측할 수 있습니다. 이제 AWS 콘솔 또는 CLI를 사용하여 시작 및 실험을 실행할 수 있습니다.\"\n",
            "Amazon CloudWatch Synthetics란 무엇인가요?\n",
            "Amazon CloudWatch Synthetics를 사용하면 애플리케이션 엔드포인트를 보다 쉽게 모니터링할 수 있습니다. 쉬지 않고 1분마다 엔드포인트에서 검사를 실행해 애플리케이션 엔드포인트가 예상과 다른 동작을 하는 즉시 알림을 보냅니다. 이 테스트는 맞춤 설정을 통해 가용성, 지연, 트랜잭션, 손상되거나 연결이 끊어진 링크, 단계별 작업 완료 상태, 페이지 부하 오류, UI 자신의 부하 지연, 복잡한 마법사 플로 또는 애플리케이션의 체크아웃 플로를 검사하도록 설정할 수 있습니다. 또한, CloudWatch Synthetics를 사용하여 문제가 있는 애플리케이션 엔드포인트를 분리하고 기본 인프라 문제로 다시 매핑해 문제 해결에 걸리는 시간을 단축할 수 있습니다.\n",
            "\"category : CloudWatch, question : Amazon CloudWatch Synthetics란 무엇인가요?, answer : Amazon CloudWatch Synthetics를 사용하면 애플리케이션 엔드포인트를 보다 쉽게 모니터링할 수 있습니다. 쉬지 않고 1분마다 엔드포인트에서 검사를 실행해 애플리케이션 엔드포인트가 예상과 다른 동작을 하는 즉시 알림을 보냅니다. 이 테스트는 맞춤 설정을 통해 가용성, 지연, 트랜잭션, 손상되거나 연결이 끊어진 링크, 단계별 작업 완료 상태, 페이지 부하 오류, UI 자신의 부하 지연, 복잡한 마법사 플로 또는 애플리케이션의 체크아웃 플로를 검사하도록 설정할 수 있습니다. 또한, CloudWatch Synthetics를 사용하여 문제가 있는 애플리케이션 엔드포인트를 분리하고 기본 인프라 문제로 다시 매핑해 문제 해결에 걸리는 시간을 단축할 수 있습니다.\"\n",
            "CloudWatch Synthetics를 시작하려면 어떻게 해야 하나요?\n",
            "CloudWatch Synthetics는 쉽게 시작할 수 있습니다. 첫 번째 통과 Canary를 몇 분 안에 작성할 수 있습니다. 자세한 알아보려면 Amazon CloudWatch Synthetics에 대한 설명서를 참조하세요.\n",
            "\"category : CloudWatch, question : CloudWatch Synthetics를 시작하려면 어떻게 해야 하나요?, answer : CloudWatch Synthetics는 쉽게 시작할 수 있습니다. 첫 번째 통과 Canary를 몇 분 안에 작성할 수 있습니다. 자세한 알아보려면 Amazon CloudWatch Synthetics에 대한 설명서를 참조하세요.\"\n",
            "언제 Amazon CloudWatch Evidently를 사용해야 하고 언제 AWS AppConfig를 사용해야 하나요?\n",
            "두 서비스를 별도로 사용할 수도 있지만 함께 사용하면 더욱 좋습니다.\n",
            "AppConfig는 기능 플래그 및 기타 애플리케이션 구성을 생성, 관리 및 배포하는 데 사용할 수 있는 AWS Systems Manager의 기능입니다. 새 기능을 개발할 때 AppConfig를 사용하여 새 기능을 프로덕션에 배포할 수 있지만 플래그 토글 뒤에 숨길 수 있습니다. 시작할 준비가 되면 구성을 업데이트하여 기능을 즉시 또는 점진적으로 릴리스하기만 하면 됩니다.\n",
            "고급 기능 관리 및 실험을 위해 Amazon CloudWatch의 새로운 기능인 Evidently를 사용할 수 있습니다. Evidently를 사용하면 다양한 기능 변형에 대한 실험을 실행하고 기준과 비교하거나 일정에 따라 기능 변형을 시작하면서 방문 기간 및 수익과 같은 비즈니스 지표를 모니터링할 수 있습니다. 또한 Evidently는 클라이언트 측 애플리케이션 성능 모니터링을 제공하는 CloudWatch RUM과도 통합되므로 RUM 지표를 Evidently에서 직접 사용할 수 있습니다.\n",
            "\"category : CloudWatch, question : 언제 Amazon CloudWatch Evidently를 사용해야 하고 언제 AWS AppConfig를 사용해야 하나요?, answer : 두 서비스를 별도로 사용할 수도 있지만 함께 사용하면 더욱 좋습니다.\n",
            "AppConfig는 기능 플래그 및 기타 애플리케이션 구성을 생성, 관리 및 배포하는 데 사용할 수 있는 AWS Systems Manager의 기능입니다. 새 기능을 개발할 때 AppConfig를 사용하여 새 기능을 프로덕션에 배포할 수 있지만 플래그 토글 뒤에 숨길 수 있습니다. 시작할 준비가 되면 구성을 업데이트하여 기능을 즉시 또는 점진적으로 릴리스하기만 하면 됩니다.\n",
            "고급 기능 관리 및 실험을 위해 Amazon CloudWatch의 새로운 기능인 Evidently를 사용할 수 있습니다. Evidently를 사용하면 다양한 기능 변형에 대한 실험을 실행하고 기준과 비교하거나 일정에 따라 기능 변형을 시작하면서 방문 기간 및 수익과 같은 비즈니스 지표를 모니터링할 수 있습니다. 또한 Evidently는 클라이언트 측 애플리케이션 성능 모니터링을 제공하는 CloudWatch RUM과도 통합되므로 RUM 지표를 Evidently에서 직접 사용할 수 있습니다.\"\n",
            "Amazon CloudWatch Metrics Insights란 무엇인가요?\n",
            "CloudWatch Metrics Insights는 표준 SQL 쿼리를 사용하여 실시간으로 운영 지표를 분할 및 분석하고 즉석에서 집계를 생성하는 데 도움이 되는 고성능 쿼리 엔진입니다. Metrics Insights는 규모에 맞게 지표를 분석할 수 있는 기능을 제공하여 애플리케이션 상태 및 성능 상태를 이해하는 데 도움이 됩니다. CloudWatch 대시보드와 통합되어 있으므로 쿼리를 상태 및 성능 대시보드에 저장하여 문제를 사전에 모니터링하고 신속하게 찾아낼 수 있습니다.\n",
            "\"category : CloudWatch, question : Amazon CloudWatch Metrics Insights란 무엇인가요?, answer : CloudWatch Metrics Insights는 표준 SQL 쿼리를 사용하여 실시간으로 운영 지표를 분할 및 분석하고 즉석에서 집계를 생성하는 데 도움이 되는 고성능 쿼리 엔진입니다. Metrics Insights는 규모에 맞게 지표를 분석할 수 있는 기능을 제공하여 애플리케이션 상태 및 성능 상태를 이해하는 데 도움이 됩니다. CloudWatch 대시보드와 통합되어 있으므로 쿼리를 상태 및 성능 대시보드에 저장하여 문제를 사전에 모니터링하고 신속하게 찾아낼 수 있습니다.\"\n",
            "CloudWatch Metrics Insights를 시작하려면 어떻게 해야 하나요?\n",
            "시작하려면 CloudWatch 콘솔에서 Metrics(지표) 탭을 클릭하면 됩니다. 그러면 추가 비용 없이 Query(쿼리) 탭에서 기본 제공 쿼리 엔진으로 Metrics Insights를 찾을 수 있습니다. Metrics Insights는 표준 SQL 언어와 함께 제공되지만 시각적 쿼리 빌더를 사용하여 Metrics Insights를 시작할 수도 있습니다. 쿼리 빌더를 사용하려면 관심 지표, 네임스페이스 및 차원을 시각적으로 선택하면 콘솔이 선택 사항에 따라 SQL 쿼리를 자동으로 생성합니다. 쿼리 편집기를 사용하여 언제든지 원시 SQL 쿼리를 입력하여 더 세분화된 세부 사항에 대해 깊이 분석하여 문제를 정확히 찾아낼 수 있습니다. Metrics Insights는 또한 즉시 애플리케이션 성능 모니터링 및 조사를 시작하는 데 도움이 되는 일련의 즉시 사용 가능한 샘플 쿼리와 함께 제공됩니다. Metrics Insights는 CloudFormation, AWS SDK 및 CLI를 통해 프로그래밍 방식으로도 사용할 수 있습니다.\n",
            "\"category : CloudWatch, question : CloudWatch Metrics Insights를 시작하려면 어떻게 해야 하나요?, answer : 시작하려면 CloudWatch 콘솔에서 Metrics(지표) 탭을 클릭하면 됩니다. 그러면 추가 비용 없이 Query(쿼리) 탭에서 기본 제공 쿼리 엔진으로 Metrics Insights를 찾을 수 있습니다. Metrics Insights는 표준 SQL 언어와 함께 제공되지만 시각적 쿼리 빌더를 사용하여 Metrics Insights를 시작할 수도 있습니다. 쿼리 빌더를 사용하려면 관심 지표, 네임스페이스 및 차원을 시각적으로 선택하면 콘솔이 선택 사항에 따라 SQL 쿼리를 자동으로 생성합니다. 쿼리 편집기를 사용하여 언제든지 원시 SQL 쿼리를 입력하여 더 세분화된 세부 사항에 대해 깊이 분석하여 문제를 정확히 찾아낼 수 있습니다. Metrics Insights는 또한 즉시 애플리케이션 성능 모니터링 및 조사를 시작하는 데 도움이 되는 일련의 즉시 사용 가능한 샘플 쿼리와 함께 제공됩니다. Metrics Insights는 CloudFormation, AWS SDK 및 CLI를 통해 프로그래밍 방식으로도 사용할 수 있습니다.\"\n"
          ]
        }
      ]
    }
  ]
}